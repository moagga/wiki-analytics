<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.9/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.9/ http://www.mediawiki.org/xml/export-0.9.xsd" version="0.9" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>http://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.24wmf21</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2600" case="first-letter">Topic</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Spaghetti stack</title>
    <ns>0</ns>
    <id>2066241</id>
    <revision>
      <id>624315965</id>
      <parentid>616678268</parentid>
      <timestamp>2014-09-05T18:01:13Z</timestamp>
      <contributor>
        <ip>70.210.229.41</ip>
      </contributor>
      <text xml:space="preserve" bytes="3631">[[File:spaghettistack.svg|thumb|120px|Spaghetti stack with an '&quot;active&quot; stack frame highlighted]]
A '''spaghetti stack''' (also called a '''cactus stack''', '''[[saguaro]] stack''' or '''in-tree''') in [[computer science]] is an N-ary [[tree data structure]] in which [[child node]]s have pointers to the [[parent node]]s (but not vice-versa).&lt;ref&gt;{{cite doi | 10.1145/62678.62692 }}&lt;/ref&gt; When a list of nodes is traversed from a leaf node to the [[root node]] by chasing these parent [[pointer (computer programming)|pointer]]s, the structure looks like a [[linked list]] [[stack (data structure)|stack]]. It can be compared to a linked list having one and only one parent pointer called &quot;next&quot; or &quot;link&quot;, and ignoring that each parent may have other children (which are not accessible anyway since there are no downward pointers).

Spaghetti stack structures arise in situations when records are dynamically pushed and popped onto a stack as execution progresses, but references to the popped records remain in use. 

For example, a [[compiler]] for a language such as [[C (programming language)|C]] creates a spaghetti stack as it opens and closes [[symbol table]]s representing block [[Scope (computer science)|scope]]s. When a new block scope is opened, a symbol table is pushed onto a stack. When the closing curly brace is encountered, the scope is closed and the symbol table is popped. But that symbol table is remembered, rather than destroyed. And of course it remembers its higher level &quot;parent&quot; symbol table and so on. Thus when the compiler is later performing translations over the [[abstract syntax tree]], for any given expression, it can fetch the symbol table representing that expression's environment and can resolve references to identifiers. If the expression refers to a variable X, it is first sought after in the leaf symbol table representing the inner-most lexical scope, then in the parent and so on.

A similar data structure appears in disjoint-set forests, a type of [[disjoint-set data structure]].

==Use in programming language runtimes==
The term ''spaghetti stack'' is closely associated with implementations of [[programming language]]s that support [[continuation]]s. Spaghetti stacks are used to implement the actual [[run-time stack]] containing variable bindings and other environmental features. When continuations must be supported, a function's local variables cannot be destroyed when that function returns: a saved continuation may later re-enter into that function, and will expect not only the variables there to be intact, but it will also expect the entire stack to be present so the function is able to return again. To resolve this problem, [[stack frame]]s can be [[dynamic memory allocation|dynamically allocated]] in a spaghetti stack structure, and simply left behind to be [[garbage collection (computer science)|garbage collected]] when no continuations refer to them any longer. This type of structure also solves both the upward and downward [[funarg problem]]s, so first-class lexical [[closure (computer science)|closure]]s are readily implemented in that substrate also.

Examples of languages that use spaghetti stacks are:
* Languages having first-class continuations such as [[Scheme (programming language)|Scheme]] and [[Standard ML of New Jersey]]
* Languages where the execution stack can be inspected and modified at runtime such as [[Smalltalk]]
* [[Felix (programming language)|Felix]]
* [[Cilk]]

==See also==
* [[Persistent data structure]]
* [[Burroughs large systems]]

==References==
{{reflist}}

[[Category:Data structures]]
[[Category:Continuations]]</text>
      <sha1>agqsfg0dpfdw6z5vcpsvu9i94t7bipf</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Dynamization</title>
    <ns>0</ns>
    <id>3641207</id>
    <revision>
      <id>623624310</id>
      <parentid>539362105</parentid>
      <timestamp>2014-08-31T21:11:21Z</timestamp>
      <contributor>
        <ip>77.56.53.183</ip>
      </contributor>
      <comment>/* Further reading */</comment>
      <text xml:space="preserve" bytes="3191">In [[computer science]], '''dynamization''' is the process of transforming a [[static data structure]] into a [[dynamic data structure|dynamic]] one. Although static data structures may provide very good functionality and fast queries, their utility is limited because of their inability to grow/shrink fast, thus making them inapplicable for the solution of [[dynamic problem (algorithms)|dynamic problem]]s, where the amount of the input data changes. Dynamization techniques provide uniform ways of creating dynamic data structures.

==Decomposable search problems==
We define problem &lt;math&gt;P&lt;/math&gt; of searching for the predicate &lt;math&gt;M&lt;/math&gt; match in the set &lt;math&gt;S&lt;/math&gt; as &lt;math&gt;P(M,S)&lt;/math&gt;. Problem &lt;math&gt;P&lt;/math&gt; is ''decomposable'' if the set &lt;math&gt;S&lt;/math&gt; can be decomposed into subsets &lt;math&gt;S_i&lt;/math&gt; and there exists an operation &lt;math&gt;+&lt;/math&gt; of result unification such that &lt;math&gt;P(M,S) = P(M,S_0) + P(M,S_1) + \dots + P(M,S_n)&lt;/math&gt;.

==Decomposition==
Decomposition is a term used in computer science to break static data structures into smaller units of unequal size. The basic principle is the idea that any decimal number can be translated into a representation in any other base. For more details about the topic see [[Decomposition (computer science)]]. For simplicity, binary system will be used in this article but any other base (as well as other possibilities such as [[Fibonacci number]]s) can also be utilized.

If using the binary system, a set of &lt;math&gt;n&lt;/math&gt; elements is broken down into subsets of sizes with 

:&lt;math&gt;2^{i}*n_{i}&lt;/math&gt;    

elements where &lt;math&gt;n_{i}&lt;/math&gt; is the  &lt;math&gt;i&lt;/math&gt;-th bit of &lt;math&gt;n&lt;/math&gt; in binary. This means that if &lt;math&gt;n&lt;/math&gt; has &lt;math&gt;i&lt;/math&gt;-th bit equal to 0, the corresponding set does not contain any elements. Each of the subset has the same property as the original static data structure. Operations performed on the new dynamic data structure may involve traversing &lt;math&gt;\log_{2}\left(n\right)&lt;/math&gt; sets formed by decomposition. As a result, this will add &lt;math&gt;O(\log\left(n\right))&lt;/math&gt;  factor as opposed to the static data structure operations but will allow insert/delete operation to be added. 

[[Kurt Mehlhorn]] proved several equations for time complexity of operations on the data structures dynamized according to this idea. Some of these equalities are listed. 

If 

 &lt;math&gt;P_S\left(n\right)\,\!&lt;/math&gt; = time to build the static data structure
 &lt;math&gt;Q_S\left(n\right)\,\!&lt;/math&gt; = time to query the static data structure
 &lt;math&gt;Q_D\left(n\right)\,\!&lt;/math&gt; = time to query the dynamic data structure formed by decomposition
 &lt;math&gt;\overline{I}&lt;/math&gt; = amortized insertion time

then

  &lt;math&gt;Q_D\left(n\right) = O(Q_S\left(n\right)\log\left(n\right))\,\!&lt;/math&gt;
  &lt;math&gt;\overline{I}=O(\left(P_S\left(n\right)/n\right)\log\left(n\right))&lt;/math&gt;

If &lt;math&gt;Q_S\left(n\right)&lt;/math&gt; is at least [[polynomial]], then &lt;math&gt;Q_D\left(n\right)=O\left(Q_S\left(n\right)\right)&lt;/math&gt;.

==Further reading==
*Kurt Mehlhorn, [http://www.mpi-sb.mpg.de/~mehlhorn/DatAlgbooks.html Data structures and algorithms] 3, . An EATCS Series, vol. 3, Springer, 1984.

[[Category:Data structures]]</text>
      <sha1>9iril64ivldms5duppte6qvhowmww6o</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Serialization</title>
    <ns>0</ns>
    <id>28555</id>
    <revision>
      <id>622007126</id>
      <parentid>619597874</parentid>
      <timestamp>2014-08-20T03:27:40Z</timestamp>
      <contributor>
        <username>Chmarkine</username>
        <id>15398482</id>
      </contributor>
      <minor/>
      <comment>/* Programming language support */change to https if the server sends [[HTTP Strict Transport Security|HSTS]] header, replaced: http://docs.python.org → https://docs.python.org (5) using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="28756">{{About|data structure encoding}}
{{debate|date=March 2012}}

In [[computer science]], in the context of data storage, '''serialization''' is the process of translating [[data structure]]s or [[object (computer science)|object]] state into a format that can be stored (for example, in a [[computer file|file]] or memory [[Data buffer|buffer]], or transmitted across a [[computer network|network]] connection link) and reconstructed later in the same or another computer environment.&lt;ref&gt;
Marshall Cline.
[http://www.parashift.com/c++-faq-lite/serialize-overview.html C++ FAQ: &quot;What's this &quot;serialization&quot; thing all about?&quot;]

It lets you take an object or group of objects, put them on a disk or send them through a wire or wireless transport mechanism, then later, perhaps on another computer, reverse the process, resurrecting the original object(s). The basic mechanisms are to flatten object(s) into a one-dimensional stream of bits, and to turn that stream of bits back into the original object(s).&lt;/ref&gt; When the resulting series of bits is reread according to the serialization format, it can be used to create a semantically identical clone of the original object. For many complex objects, such as those that make extensive use of [[reference (computer science)|references]], this process is not straightforward. Serialization of object-oriented [[object (computer science)|object]]s does not include any of their associated [[Method (computer science)|methods]] with which they were previously inextricably linked.

This process of serializing an object is also called [[Marshalling (computer science)|marshalling]] an object.&lt;ref&gt;[http://support.microsoft.com/kb/301116/en-us How to marshal an object to a remote server by value by using Visual Basic 2005 or Visual Basic .NET &amp;#91;…&amp;#93; Because the whole object is being serialized to the server (marshaling by value), the code will execute in the server's process.]&lt;/ref&gt; The opposite operation, extracting a data structure from a series of bytes, is '''deserialization''' (which is also called '''unmarshalling''').

== Uses ==
Serialization provides:
* a method of [[Persistence (computer science)|persisting]] objects, for example writing their properties to a file on disk, or saving them to a [[database]].
* a method of [[remote procedure call]]s, e.g., as in [[SOAP (protocol)|SOAP]].
* a method for distributing objects, especially in [[component-based software engineering]] such as [[Component Object Model|COM]], [[CORBA]], etc.
* a method for detecting changes in time-varying data.

For some of these features to be useful, architecture independence must be maintained. For example, for maximal use of distribution, a computer running on a different hardware architecture should be able to reliably reconstruct a serialized data stream, regardless of [[endianness]]. This means that the simpler and faster procedure of directly copying the memory layout of the data structure cannot work reliably for all architectures. Serializing the data structure in an architecture independent format means to prevent the problems of [[byte ordering]], memory layout, or simply different ways of representing data structures in different [[programming language]]s.

Inherent to any serialization scheme is that, because the encoding of the data is by definition serial, extracting one part of the serialized data structure requires that the entire object be read from start to end, and reconstructed. In many applications this linearity is an asset, because it enables simple, common I/O interfaces to be utilized to hold and pass on the state of an object. In applications where higher performance is an issue, it can make sense to expend more effort to deal with a more complex, non-linear storage organization.

Even on a single machine, primitive [[pointer (computer programming)|pointer]] objects are too fragile to save because the objects to which they point may be reloaded to a different location in memory. To deal with this, the serialization process includes a step called ''[[unswizzling]]'' or ''pointer unswizzling'', where direct pointer references are converted to references based on name or position .  The deserialization process includes an inverse step called ''[[pointer swizzling]]''.

Since both serializing and deserializing can be driven from common code (for example, the ''Serialize'' function in [[Microsoft Foundation Classes]]), it is possible for the common code to do both at the same time, and thus, 1) detect differences between the objects being serialized and their prior copies, and 2) provide the input for the next such detection.  It is not necessary to actually build the prior copy because differences can be detected on the fly.  The technique is called [[differential execution]]. It is useful in the programming of user interfaces whose contents are time-varying&amp;nbsp;— graphical objects can be created, removed, altered, or made to handle input events without necessarily having to write separate code to do those things.

== Consequences ==
Serialization, however, breaks the opacity of an [[abstract data type]] by potentially exposing private implementation details. Trivial implementations which serialize all data members may violate [[encapsulation (object-oriented programming)|encapsulation]].&lt;ref&gt;{{cite web|last=S. Miller|first=Mark|title=Safe Serialization Under Mutual Suspicion|url=http://erights.org/data/serial/jhu-paper/intro.html|work=ERights.org|quote=Serialization, explained below, is an example of a tool for use by objects within an object system for operating on the graph they are embedded in. This seems to require violating the encapsulation provided by the pure object model.}}&lt;/ref&gt;

To discourage competitors from making compatible products, publishers of [[proprietary software]] often keep the details of their programs' serialization formats a [[trade secret]]. Some deliberately [[obfuscated code|obfuscate]] or even [[encryption|encrypt]] the serialized data. Yet, interoperability requires that applications be able to understand each other's serialization formats. Therefore, [[RMI-IIOP|remote method call]] architectures such as [[CORBA]] define their serialization formats in detail.

Many institutions, such as archives and libraries, attempt to [[future proof]] their [[backup]] archives—in particular, [[database dump]]s—by storing them in some relatively [[human-readable]] serialized format.

== Serialization formats ==
{{Main|Comparison of data serialization formats}}
The [[Xerox Network Systems]] Courier technology in the early 1980s influenced the first widely adopted standard. [[Sun Microsystems]] published the [[External Data Representation]] (XDR) in 1987.&lt;ref&gt;{{cite web |title= XDR: External Data Representation Standard |author= Sun Microsystems |work= RFC 1014 |year= 1987 |publisher=Network Working Group |url= http://tools.ietf.org/html/rfc1014 |accessdate= July 11, 2011 }}&lt;/ref&gt;

In the late 1990s, a push to provide an alternative to the standard serialization protocols started: [[XML]] was used to produce a human readable [[binary-to-text encoding|text-based encoding]]. Such an encoding can be useful for persistent objects that may be read and understood by humans, or communicated to other systems regardless of programming language. It has the disadvantage of losing the more compact, byte-stream-based encoding, but by this point larger storage and transmission capacities made file size less of a concern than in the early days of computing.  [[Binary XML]] had been proposed as a compromise which was not readable by plain-text editors, but was more compact than regular XML.  In the 2000s, XML was often used for asynchronous transfer of structured data between client and server in [[Ajax (programming)|Ajax]] web applications.

[[JSON]] is a more lightweight plain-text alternative to XML which is also commonly used for client-server communication in web applications.  JSON is based on [[JavaScript syntax]], but is supported in other programming languages as well.

Another alternative, [[YAML]], is effectively a superset of JSON and includes features that make it more powerful for serialization, more &quot;human friendly,&quot; and potentially more compact.  These features include a notion of tagging data types, support for non-hierarchical data structures, the option to structure data with indentation, and multiple forms of scalar data quoting.

Another human-readable serialization format is the [[property list]] format used in [[NeXTSTEP]], [[GNUstep]], and [[Mac OS X]] [[Cocoa (API)|Cocoa]].

For large volume scientific datasets, such as satellite data and output of numerical climate, weather, or ocean models, specific binary serialization standards have been developed, e.g. [[Hierarchical Data Format|HDF]], [[netCDF]] and the older [[GRIB]].

== Programming language support ==
Several [[object-oriented programming]] languages directly support ''object serialization'' (or ''object archival''), either by [[syntactic sugar]] elements or providing a standard [[interface (computing)|interface]] for doing so. Some of these programming languages are [[Ruby programming language|Ruby]], [[Smalltalk]], [[Python (programming language)|Python]], [[PHP]], [[Objective-C]], [[Java (programming language)|Java]], and the [[.NET Framework|.NET]] family of languages. There are also libraries available that add serialization support to languages that lack native support for it.

;.NET Framework: In the [[.NET Framework|.NET]] languages, classes can be serialized and deserialized by adding the &lt;code&gt;Serializable&lt;/code&gt; attribute to the class. If new members are added to a serializable class, they can be tagged with the &lt;code&gt;OptionalField&lt;/code&gt; attribute to allow previous versions of the object to be deserialized without error. This attribute affects only deserialization, and prevents the runtime from throwing an exception if a member is missing from the serialized stream. A member can also be marked with the &lt;code&gt;NonSerialized&lt;/code&gt; attribute to indicate that it should not be serialized. This will allow the details of those members to be kept secret. Objects may be serialized in binary format for deserialization by other [[.NET Framework|.NET]] applications. There are also third party binary serializers that are documented, portable, use less memory footprint and CPU. The framework also provides the &lt;code&gt;SoapFormatter&lt;/code&gt; and &lt;code&gt;XmlSerializer &lt;/code&gt; objects to support serialization in human-readable, cross-platform XML.
;Objective-C: In the GNU runtime variant of the [[Objective-C]] programming language [[GNUstep]], serialization (more commonly known as ''archiving'') is achieved by overriding the &lt;code&gt;write:&lt;/code&gt; and &lt;code&gt;read:&lt;/code&gt; methods in the Object root class. In the NeXT-style runtime used by [[Cocoa (API)|Cocoa]] in [[Mac OS X]], the implementation is very similar. Newer versions of Cocoa and GNUstep also provide a more sophisticated serialization scheme which is referred to as ''keyed archiving''.
;Java: Java provides automatic serialization which requires that the object be [[Marker interface pattern|marked]] by implementing the {{Javadoc:SE|package=java.io|java/io|Serializable}} [[interface (Java)|interface]].  Implementing the interface marks the class as &quot;okay to serialize&quot;, and Java then handles serialization internally.  There are no serialization methods defined on the &lt;code&gt;Serializable&lt;/code&gt; interface, but a serializable class can optionally define methods with certain special names and signatures that if defined, will be called as part of the serialization/deserialization process.  The language also allows the developer to override the serialization process more thoroughly by implementing another interface, the {{Javadoc:SE|java/io|Externalizable}} interface, which includes two special methods that are used to save and restore the object's state. There are three primary reasons why objects are not serializable by default and must implement the &lt;code&gt;Serializable&lt;/code&gt; interface to access Java's serialization mechanism. Firstly, not all objects capture useful semantics in a serialized state.  For example, a {{Javadoc:SE|java/lang|Thread}} object is tied to the state of the current [[JVM]].  There is no context in which a deserialized &lt;code&gt;Thread&lt;/code&gt; object would maintain useful semantics. Secondly, the serialized state of an object forms part of its classes' compatibility contract.  Maintaining compatibility between versions of serializable classes requires additional effort and consideration.  Therefore, making a class serializable needs to be a deliberate design decision and not a default condition. Lastly, serialization allows access to non-[[Transient (computer programming)|transient]] private members of a class that are not otherwise accessible.  Classes containing sensitive information (for example, a password) should not be serializable nor externalizable. The standard encoding method uses a simple translation of the fields into a byte stream. Primitives as well as non-transient, non-static referenced objects are encoded into the stream. Each object that is referenced by the serialized object and not marked as &lt;code&gt;transient&lt;/code&gt; must also be serialized; and if any object in the complete graph of non-transient object references is not serializable, then serialization will fail.  The developer can influence this behavior by marking objects as transient, or by redefining the serialization for an object so that some portion of the reference graph is truncated and not serialized. It is possible to serialize Java objects through [[JDBC]] and store them into a database.{{citation needed|date=March 2014}} While [[Swing (Java)|Swing]] components do implement the Serializable interface, they are not portable between different versions of the Java Virtual Machine. As such, a Swing component, or any component which inherits it, may be serialized to an array of bytes, but it is not guaranteed that this storage will be readable on another machine.
;CFML: [[CFML]] allows data structures to be serialized to [[WDDX]] with the &lt;code&gt;[https://wikidocs.adobe.com/wiki/display/coldfusionen/cfwddx &lt;cfwddx&gt;]&lt;/code&gt; tag and to [[JSON]] with the [https://wikidocs.adobe.com/wiki/display/coldfusionen/serializejson SerializeJSON()] function.
;OCaml: [[OCaml]]'s standard library provides marshalling through the &lt;code&gt;Marshal&lt;/code&gt; module ([http://caml.inria.fr/pub/docs/manual-ocaml/libref/Marshal.html its documentation]) and the Pervasives functions &lt;code&gt;output_value&lt;/code&gt; and &lt;code&gt;input_value&lt;/code&gt;. While OCaml programming is statically type-checked, uses of the &lt;code&gt;Marshal&lt;/code&gt; module may break type guarantees, as there is no way to check whether an unmarshalled stream represents objects of the expected type. In OCaml it is difficult to marshal a function or a data structure which contains a function (e.g. an object which contains a method), because executable code in functions cannot be transmitted across different programs. (There is a flag to marshal the code position of a function but it can only be unmarshalled in exactly the same program).  The standard marshalling functions can preserve sharing and handle cyclic data, which can be configured by a flag.
;Perl: Several [[Perl]] modules available from [[CPAN]] provide serialization mechanisms, including &lt;code&gt;Storable&lt;/code&gt; and &lt;code&gt;FreezeThaw&lt;/code&gt;. Storable includes functions to serialize and deserialize Perl data structures to and from files or Perl scalars. In addition to serializing directly to files, &lt;code&gt;Storable&lt;/code&gt; includes the &lt;code&gt;freeze&lt;/code&gt; function to return a serialized copy of the data packed into a scalar, and &lt;code&gt;thaw&lt;/code&gt; to deserialize such a scalar. This is useful for sending a complex data structure over a network socket or storing it in a database. When serializing structures with &lt;code&gt;Storable&lt;/code&gt;, there are network safe functions that always store their data in a format that is readable on any computer at a small cost of speed. These functions are named &lt;code&gt;nstore&lt;/code&gt;, &lt;code&gt;nfreeze&lt;/code&gt;, etc. There are no &quot;n&quot; functions for deserializing these structures&amp;nbsp;— the regular &lt;code&gt;thaw&lt;/code&gt; and &lt;code&gt;retrieve&lt;/code&gt; deserialize structures serialized with the &quot;&lt;code&gt;n&lt;/code&gt;&quot; functions and their machine-specific equivalents.
;C and C++: [[C (programming language)|C]] and [[C++]] do not provide direct support for serialization. It is however possible to write your own serialization functions, since both languages support writing binary data. Besides, compiler-based solutions, such as the [[ODB (C++)|ODB]] [[object-relational mapping|ORM]] system for C++, are capable of automatically producing serialization code with few or no modifications to class declarations. Other popular serialization frameworks are Boost.Serialization&lt;ref&gt;[http://www.boost.org/doc/libs/1_46_1/libs/serialization/doc/index.html Documentation to Boost.Serialization]&lt;/ref&gt; from the [[Boost C++ Libraries|Boost Framework]] and S11n framework.&lt;ref&gt;[http://s11n.net/ s11n home page]&lt;/ref&gt; [[Microsoft Foundation Class Library|MFC framework]] (Microsoft) also provides serialization methodology as part of its Document-View architecture. The [http://webebenezer.net C++ Middleware Writer] automates the creation of serialization functions.
;Python: The core general serialization mechanism is the &lt;code&gt;[[pickle (Python)|pickle]]&lt;/code&gt; [[Python (programming language)#Libraries|standard library]] module. It is a cross-version [https://docs.python.org/library/pickle.html#pickle-protocol customisable] but unsafe (not secure against erroneous or malicious data) serialization format. The standard library also includes modules serializing to standard data formats: &lt;code&gt;[https://docs.python.org/library/json.html json]&lt;/code&gt; (with built-in support for basic scalar and collection types and able to support arbitrary types via [https://docs.python.org/library/json.html#encoders-and-decoders encoding and decoding hooks]) and XML-encoded [[property list]]s. (&lt;code&gt;[https://docs.python.org/library/plistlib.html plistlib]&lt;/code&gt;), limited to plist-supported types (numbers, strings, booleans, tuples, lists, dictionaries, datetime and binary blobs). Finally, it is recommended that an object's &lt;code&gt;[https://docs.python.org/reference/datamodel.html#object.__repr__ __repr__]&lt;/code&gt; be evaluable in the right environment, making it a rough match for Common Lisp's &lt;code&gt;[http://clhs.lisp.se/Body/f_pr_obj.htm print-object]&lt;/code&gt;.
;PHP: [[PHP]] originally implemented serialization through the built-in &lt;code&gt;serialize()&lt;/code&gt; and &lt;code&gt;unserialize()&lt;/code&gt; functions.&lt;ref&gt;http://ca.php.net/manual/en/language.oop5.serialization.php&lt;/ref&gt; PHP can serialize any of its data types except resources (file pointers, sockets, etc.). The built-in &lt;code&gt;unserialize()&lt;/code&gt; function is often dangerous when used on completely untrusted data.&lt;ref&gt;{{cite web|last=Esser|first=Stephen|title=Shocking News in PHP Exploitation|url=http://www.suspekt.org/2009/11/28/shocking-news-in-php-exploitation/|work=Suspekt...|date=2009-11-28}}&lt;/ref&gt; For objects, there are two &quot;[[Magic (programming)|magic]] methods&quot; that can be implemented within a class&amp;nbsp;— &lt;code&gt;__sleep()&lt;/code&gt; and &lt;code&gt;__wakeup()&lt;/code&gt;&amp;nbsp;— that are called from within &lt;code&gt;serialize()&lt;/code&gt; and &lt;code&gt;unserialize()&lt;/code&gt;, respectively, that can clean up and restore an object.  For example, it may be desirable to close a database connection on serialization and restore the connection on deserialization; this functionality would be handled in these two magic methods.  They also permit the object to pick which properties are serialized. Since PHP 5.1, there is an object-oriented serialization mechanism for objects, the &lt;code&gt;Serializable&lt;/code&gt; interface.&lt;ref name=&quot;Serializable&quot;&gt;[http://www.php.net/manual/en/class.serializable.php Serializable interface]&lt;/ref&gt;
;R: [[R (programming language)|R]] has the function &lt;code&gt;dput&lt;/code&gt; which writes an ASCII text representation of an R object to a file or connection.  A representation can be read from a file using &lt;code&gt;dget&lt;/code&gt;.&lt;ref&gt;[R manual http://stat.ethz.ch/R-manual/R-patched/library/base/html/dput.html]&lt;/ref&gt;
;REBOL: [[REBOL]] will serialize to file (&lt;code&gt;save/all&lt;/code&gt;) or to a &lt;code&gt;string!&lt;/code&gt; (&lt;code&gt;mold/all&lt;/code&gt;).  Strings and files can be deserialized using the [[Type polymorphism|polymorphic]] &lt;code&gt;load&lt;/code&gt; function.
;Ruby: [[Ruby programming language|Ruby]] includes the standard module &lt;code&gt;[http://www.ruby-doc.org/core/classes/Marshal.html Marshal]&lt;/code&gt; with 2 methods &lt;code&gt;dump&lt;/code&gt; and &lt;code&gt;load&lt;/code&gt;, akin to the standard Unix utilities &lt;code&gt;[[dump (program)|dump]]&lt;/code&gt; and &lt;code&gt;[[restore (program)|restore]]&lt;/code&gt;. These methods serialize to the standard class &lt;code&gt;String&lt;/code&gt;, that is, they effectively become a sequence of bytes. Some objects cannot be serialized (doing so would raise a &lt;code&gt;TypeError&lt;/code&gt; exception): bindings, procedure objects, instances of class IO, singleton objects and interfaces. If a class requires custom serialization (for example, it requires certain cleanup actions done on dumping / restoring), it can be done by implementing 2 methods: &lt;code&gt;_dump&lt;/code&gt; and &lt;code&gt;_load&lt;/code&gt;. The instance method &lt;code&gt;_dump&lt;/code&gt; should return a &lt;code&gt;String&lt;/code&gt; object containing all the information necessary to reconstitute objects of this class and all referenced objects up to a maximum depth given as an integer parameter (a value of -1 implies that depth checking should be disabled). The class method &lt;code&gt;_load&lt;/code&gt; should take a &lt;code&gt;String&lt;/code&gt; and return an object of this class.
;Smalltalk: In general, non-recursive and non-sharing objects can be stored and retrieved in a human readable form using the &lt;code&gt;storeOn:&lt;/code&gt;/&lt;code&gt;readFrom:&lt;/code&gt; protocol. The &lt;code&gt;storeOn:&lt;/code&gt; method generates the text of a Smalltalk expression which - when evaluated using &lt;code&gt;readFrom:&lt;/code&gt; - recreates the original object. This scheme is special, in that it uses a procedural description of the object, not the data itself. It is therefore very flexible, allowing for classes to define more compact representations. However, in its original form, it does not handle cyclic data structures or preserve the identity of shared references (i.e. two references a single object will be restored as references to two equal, but not identical copies). For this, various portable and non-portable alternatives exist. Some of them are specific to a particular Smalltalk implementation or class library. There are several ways in [[Squeak|Squeak Smalltalk]] to serialize and store objects.  The easiest and most used are &lt;code&gt;storeOn:/readFrom:&lt;/code&gt; and binary storage formats based on &lt;code&gt;SmartRefStream&lt;/code&gt; serializers. In addition, bundled objects can be stored and retrieved using &lt;code&gt;ImageSegments&lt;/code&gt;. Both provide a so-called &quot;binary-object storage framework&quot;, which support serialization into and retrieval from a compact binary form. Both handle cyclic, recursive and shared structures, storage/retrieval of class and metaclass info and include mechanisms for &quot;on the fly&quot; object migration (i.e. to convert instances which were written by an older version of a class with a different object layout). The APIs are similar (storeBinary/readBinary), but the encoding details are different, making these two formats incompatible. However, the Smalltalk/X code is open source and free and can be loaded into other Smalltalks to allow for cross-dialect object interchange. Object serialization is not part of the ANSI Smalltalk specification. As a result, the code to serialize an object varies by Smalltalk implementation. The resulting binary data also varies. For instance, a serialized object created in Squeak Smalltalk cannot be restored in [[Ambrai Smalltalk]]. Consequently, various applications that do work on multiple Smalltalk implementations that rely on object serialization cannot share data between these different implementations. These applications include the MinneStore object database [http://minnestore.sourceforge.net/] and some [[Remote procedure call|RPC]] packages.  A solution to this problem is SIXX [http://www.mars.dti.ne.jp/~umejava/smalltalk/sixx/index.html], which is a package for multiple Smalltalks that uses an [[XML]]-based format for serialization.
;Lisp: Generally a [[Lisp (programming language)|Lisp]]  data structure can be serialized with the functions &quot;&lt;code&gt;read&lt;/code&gt;&quot; and &quot;&lt;code&gt;print&lt;/code&gt;&quot;.  A variable foo containing, for example, a list of arrays would be printed by &lt;code&gt;(print foo)&lt;/code&gt;.  Similarly an object can be read from a stream named s by &lt;code&gt;(read s)&lt;/code&gt;.  These two parts of the Lisp implementation are called the Printer and the Reader.  The output of &quot;&lt;code&gt;print&lt;/code&gt;&quot; is human readable; it uses lists demarked by parentheses, for example: &lt;code&gt;(4 2.9 &quot;x&quot; y)&lt;/code&gt;. In many types of Lisp, including [[Common Lisp]], the printer cannot represent every type of data because it is not clear how to do so.  In Common Lisp for example the printer cannot print CLOS objects.  Instead the programmer may write a method on the generic function &lt;code&gt;print-object&lt;/code&gt;, this will be invoked when the object is printed.  This is somewhat similar to the method used in Ruby. Lisp code itself is written in the syntax of the reader, called read syntax.  Most languages use separate and different parsers to deal with code and data, Lisp only uses one.  A file containing lisp code may be read into memory as a data structure, transformed by another program, then possibly executed or written out, such as in a [[read–eval–print loop]]. Not all readers/writers support cyclic, recursive or shared structures.
;Haskell: In Haskell, serialization is supported for types that are members of the Read and Show [[type class]]es. Every type that is a member of the &lt;code&gt;Read&lt;/code&gt; type class defines a function that will extract the data from the string representation of the dumped data.  The &lt;code&gt;Show&lt;/code&gt; type class, in turn, contains the &lt;code&gt;show&lt;/code&gt; function from which a string representation of the object can be generated. The programmer need not define the functions explicitly—merely declaring a type to be deriving Read or deriving Show, or both, can make the compiler generate the appropriate functions for many cases (but not all: function types, for example, cannot automatically derive Show or Read). The auto-generated instance for Show also produces valid source code, so the same Haskell value can be generated by running the code produced by show in, for example, a Haskell interpreter.&lt;ref&gt;{{cite web|url=http://hackage.haskell.org/package/base-4.6.0.1/docs/Text-Show.html#t:Show|accessdate=15 January 2014 |title=Text.Show Documentation}}&lt;/ref&gt; For more efficient serialization, there are haskell libraries that allow high-speed serialization in binary format, e.g. [http://hackage.haskell.org/package/binary binary].
;Windows PowerShell: [[Windows PowerShell]] implements serialization through the [[Shell builtin|built-in]] cmdlet &lt;code&gt;Export-CliXML&lt;/code&gt;. &lt;code&gt;Export-CliXML&lt;/code&gt; serializes .NET objects and stores the resulting XML in a file. To reconstitute the objects, use the &lt;code&gt;Import-CliXML&lt;/code&gt; cmdlet, which generates a deserialized object from the XML in the exported file. Deserialized objects, often known as &quot;property bags&quot; are not live objects; they are snapshots that have properties, but no methods. Two dimensional data structures can also be (de)serialized in [[Comma-separated values|CSV]] format using the built-in cmdlets &lt;code&gt;Import-CSV&lt;/code&gt; and &lt;code&gt;Export-CSV&lt;/code&gt;.

== See also ==
* [[Hibernate (Java)]]
* [[Persistor.NET]]
* [[XML Schema (W3C)|XML Schema]]
* [[Basic Encoding Rules]]
* [[Google Protocol Buffers]]

== References ==

{{reflist|30em}}

== External links ==

* {{Javadoc:SE-guide|serialization|Java Object Serialization documentation}}
* [http://java.sun.com/j2se/1.4.2/docs/guide/serialization/index.html Java 1.4 Object Serialization documentation].
* [http://www.macchiato.com/columns/Durable4.html Durable Java: Serialization] {{Wayback|date=20051125013312|url=http://www.macchiato.com/columns/Durable4.html|df=yes}}
* [http://rpbourret.com/xml/XMLDataBinding.htm XML Data Binding Resources]
* [http://dev.simantics.org/index.php/Org.simantics.databoard Databoard] Binary serialization with partial and random access, type system, RPC, type adaption, and text format.

[[Category:Data structures]]
[[Category:Data serialization formats| ]]
[[Category:Persistence]]
[[Category:Articles with example code]]</text>
      <sha1>neinrp1wy0lpnhgujscinuzfxpbh1r1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Persistent data structure</title>
    <ns>0</ns>
    <id>662889</id>
    <revision>
      <id>618898607</id>
      <parentid>618898482</parentid>
      <timestamp>2014-07-29T00:57:06Z</timestamp>
      <contributor>
        <ip>2401:FA00:9:1:B044:1599:47BB:59FA</ip>
      </contributor>
      <comment>/* Examples of persistent data structures */</comment>
      <text xml:space="preserve" bytes="17024">In [[computing]], a '''persistent data structure''' is a [[data structure]] that always preserves the previous version of itself when it is modified. Such data structures are effectively [[Immutable object|immutable]], as their operations do not (visibly) update the structure in-place, but instead always yield a new updated structure. (A persistent data structure is ''not'' a data structure committed to [[persistent storage]], such as a disk; this is a different and unrelated sense of the word &quot;persistent.&quot;)

A data structure is partially persistent if all versions can be accessed but only the newest version can be modified.  The data structure is fully persistent if every version can be both accessed and modified. If there is also a meld or merge operation that can create a new version from two previous versions, the data structure is called confluently persistent.  Structures that are not persistent are called [[Ephemeral (disambiguation)|ephemeral]].&lt;ref name=&quot;kaplan&quot;&gt;{{Cite journal| author=Kaplan, Haim|title = Persistent data structures|journal=Handbook on Data Structures and Applications|year = 2001|url=http://www.math.tau.ac.il/~haimk/papers/persistent-survey.ps|publisher=CRC Press}}&lt;/ref&gt;

These types of data structures are particularly common in [[logic programming|logical]] and [[functional programming]], and in a [[purely functional]] program all data is immutable, so all data structures are automatically fully persistent.&lt;ref name=&quot;kaplan&quot;/&gt;  Persistent data structures can also be created using in-place updating of data and these may, in general, use less time or storage space than their purely functional counterparts.

While persistence can be achieved by simple copying, this is inefficient in CPU and RAM usage, because most operations make only small changes to a data structure. A better method is to exploit the similarity between the new and old versions to share structure between them, such as using the same subtree in a number of [[tree structure]]s. However, because it rapidly becomes infeasible to determine how many previous versions share which parts of the structure, and because it is often desirable to discard old versions, this necessitates an environment with [[garbage collection (computer science)|garbage collection]].

== Partially persistent ==
In the partial persistence model, we may query any previous version of the data structure, but we may only update the latest version. This implies a [[Total order|linear ordering]] among the versions.

''Three methods on [[balanced binary search tree]]:''

===Fat Node===

Fat node method is to record all changes made to node fields in the nodes themselves, without erasing old values of the fields. This requires that we allow nodes to become arbitrarily “fat”. In other words, each fat node contains the same information and [[Pointer (computer programming)|pointer]] fields as an ephemeral node, along with space for an arbitrary number of extra field values. Each extra field value has an associated field name and a version stamp which indicates the version in which the named field was changed to have the specified value. Besides, each fat node has its own version stamp, indicating the version in which the node was created. The only purpose of nodes having version stamps is to make sure that each node only contains one value per field name per version. In order to navigate through the structure, each original field value in a node has a version stamp of zero.

====Complexity of Fat Node====

With using fat node method, it requires O(1) space for every modification: just store the new data. Each modification takes O(1) additional time to store the modification at the end of the modification history. This is an [[Amortized analysis|amortized time]] bound, assuming we store the modification history in a growable [[Array data structure|array]]. For [[access time]], we must find the right version at each node as we traverse the structure. If we made m modifications, then each access operation has O(log m) slowdown resulting from the cost of finding the nearest modification in the array.

===Path Copying===

Path copy is to make a copy of all nodes on the path which contains the node we are about to insert or delete. Then we must [[Fractional cascading|cascade]] the change back through the data structure: all nodes that pointed to the old node must be modified to point to the new node instead. These modifications cause more cascading changes, and so on, until we reach to the root. We maintain an array of roots indexed by timestamp. The data structure pointed to by time t’s root is exactly time t’s date structure.

====Complexity of Path Copying====

With m modifications, this costs O(log m) additive [[lookup]] time. Modification time and space are bounded by the size of the structure, since a single modification may cause the entire structure to be copied. That is O(m) for one update, and thus O(n²) preprocessing time.

===A combination===

[[Daniel Sleator|Sleator]], [[Robert Tarjan|Tarjan]] et al. came up with a way to combine the advantages of fat nodes and path copying, getting O(1) access slowdown and O(1) modification space and time. 

In each node, we store one modification box. This box can hold one modification to the node—either a modification to one of the pointers, or to the node’s key, or to some other piece of node-specific data—and a timestamp for when that modification was applied. Initially, every node’s modification box is empty.

Whenever we access a node, we check the modification box, and compare its timestamp against the access time. (The access time specifies the version of the data structure that we care about.) If the modification box is empty, or the access time is before the modification time, then we ignore the modification box and just deal with the normal part of the node. On the other hand, if the access time is after the modification time, then we use the value in the modification box, overriding that value in the node. (Say the modification box has a new left pointer. Then we’ll use it instead of the normal left pointer, but we’ll still use the normal right pointer.)

Modifying a node works like this. (We assume that each modification touches one pointer or similar field.) If the node’s modification box is empty, then we fill it with the modification. Otherwise, the modification box is full. We make a copy of the node, but using only the latest values.(That is, we overwrite one of the node’s fields with the value that was stored in the modification box.) Then we perform the modification directly on the new node, without using the modification box. (We overwrite one of the new node’s fields, and its modification box stays empty.) Finally, we cascade this change to the node’s parent, just like path copying. (This may involve filling the parent’s modification box, or making a copy of the parent recursively. If the node has no parent—it’s the root—we add the new root to a [[sorted array]] of roots.)

With this [[algorithm]], given any time t, at most one modification box exists in the data structure with time t. Thus, a modification at time t splits the tree into three parts: one part contains the data from before time t, one part contains the data from after time t, and one part was unaffected by the modification.

====Complexity of the combination====

Time and space for modifications require amortized analysis. A modification takes O(1) amortized space, and O(1) amortized time. To see why, use a [[Potential method|potential function]] ϕ,where ϕ(T)is the number of full live nodes in T . The live nodes of T are just the nodes that are reachable from the current root at the current time (that is, after the last modification). The full live nodes are the live nodes whose modification boxes are full.

Each modification involves some number of copies, say k, followed by 1 change to a modification box. (Well, not quite—you could add a new root—but that doesn’t change the argument.) Consider each of the k copies. Each costs O(1) space and time, but decreases the potential function by one. (First, the node we copy must be full and live, so it contributes to the potential function. The potential function will only drop, however, if the old node isn’t reachable in the new tree. But we know it isn’t reachable in the new tree—the next step in the algorithm will be to modify the node’s parent to point at the copy. Finally, we know the copy’s modification box is empty. Thus, we’ve replaced a full live node with an empty live node, and ϕ goes down by one.) The final step fills a modification box, which costs O(1) time and increases ϕ by one.

Putting it all together, the change in ϕ is Δϕ =1− k.Thus, we’ve paid O(k +Δϕ)= O(1) space and O(k +Δϕ +1) = O(1) time.

== Fully persistent ==

In fully persistent model, both updates and queries are allowed on any version of the data structure.

== Confluently persistent ==

In confluently persistent model, we use combinators to combine input of more than one previous version to output a new single version. Rather than a branching tree, combinations of versions induce a [[Directed acyclic graph|DAG]] (directed acyclic graph) structure on the version graph.

== Examples of persistent data structures ==

Perhaps the simplest persistent data structure is the [[linked list|singly linked list]] or ''cons''-based list, a simple list of objects formed by each carrying a [[reference]] to the next in the list. This is persistent because we can take a ''tail'' of the list, meaning the last ''k'' items for some ''k'', and add new nodes on to the front of it. The tail will not be duplicated, instead becoming shared between both the old list and the new list.  So long as the contents of the tail are immutable, this sharing will be invisible to the program.

Many common reference-based data structures, such as [[red-black tree]]s,&lt;ref name=&quot;sarnak&quot;&gt;{{cite journal
  | author = Neil Sarnak, [[Robert Tarjan|Robert E. Tarjan]]
  | title = Planar Point Location Using Persistent Search Trees
  | journal = Communications of the ACM
  | volume = 29
  | issue = 7
  | pages = 669–679
  | year = 1986
  | url = http://www.link.cs.cmu.edu/15859-f07/papers/point-location.pdf
  | doi = 10.1145/6138.6151}}&lt;/ref&gt; [[Stack (data structure)|stack]]s,&lt;ref name=&quot;okasaki&quot;&gt;{{Cite journal | author=Chris Okasaki|url=http://www.cs.cmu.edu/~rwh/theses/okasaki.pdf|title=Purely Functional Data Structures (thesis)}}&lt;/ref&gt; and [[treap]]s,&lt;ref&gt;{{cite journal|last=Liljenzin|first=Olle|title=Confluently Persistent Sets and Maps|url=http://arxiv.org/abs/1301.3388}}&lt;/ref&gt;  can easily be adapted to create a persistent version. Some others need slightly more effort, for example: [[queue (data structure)|queues]], [[double-ended queue|deque]]s, and extensions including [[min-deque]]s (which have an additional ''O''(1) operation ''min'' returning the minimal element) and [[random access deque]]s (which have an additional operation of random access with sub-linear, most often logarithmic, complexity).

There also exist persistent data structures which use destructible{{huh|date=June 2013}} operations, making them impossible to implement efficiently in purely functional languages (like Haskell outside specialized monads like state or IO), but possible in languages like C or Java. These types of data structures can often be avoided with a different design. One primary advantage to using purely persistent data structures is that they often behave better in multi-threaded environments.

=== Linked lists ===

''This example is taken from Okasaki.  See the bibliography.''

Singly [[linked list]]s are the bread-and-butter data structure in functional languages. In [[ML programming language|ML]]-derived languages and [[Haskell (programming language)|Haskell]], they are purely functional because once a node in the list has been allocated, it cannot be modified, only copied or destroyed.  Note that ML itself is '''not''' purely functional.

Consider the two lists:
 xs = [0, 1, 2]
 ys = [3, 4, 5]

These would be represented in memory by:

[[Image:purely functional list before.svg]]

where a circle indicates a node in the list (the arrow out representing the second element of the node which is a pointer to another node).

Now concatenating the two lists:
 zs = xs ++ ys
results in the following memory structure:

[[Image:purely functional list after.svg]]

Notice that the nodes in list &lt;code&gt;xs&lt;/code&gt; have been copied, but the nodes in &lt;code&gt;ys&lt;/code&gt; are shared.  As a result, the original lists (&lt;code&gt;xs&lt;/code&gt; and &lt;code&gt;ys&lt;/code&gt;) persist and have not been modified.

The reason for the copy is that the last node in &lt;code&gt;xs&lt;/code&gt; (the node containing the original value &lt;code&gt;2&lt;/code&gt;) cannot
be modified to point to the start of &lt;code&gt;ys&lt;/code&gt;, because that would change the value of &lt;code&gt;xs&lt;/code&gt;.

=== Trees ===

''This example is taken from Okasaki.  See the bibliography.''

Consider a [[binary tree]] used for fast searching,
where every node has the [[recursion|recursive]]
[[invariant (computer science)|invariant]]
that subnodes on the
left are less than the node, and subnodes on the right
are greater than the node.

For instance, the set of data
 xs = [a, b, c, d, f, g, h]
might be represented by the following binary search tree:

[[Image:purely functional tree before.svg]]

A function which inserts data into the binary tree
and maintains the invariant is:

 fun insert (x, E) = T (E, x, E)
   | insert (x, s as T (a, y, b)) =
        if x &lt; y then T (insert (x, a), y, b)
        else if x &gt; y then T (a, y, insert (x, b))
        else s

After executing
 ys = insert (&quot;e&quot;, xs)
we end up with the following:

[[Image:purely functional tree after.svg]]

Notice two points: Firstly the original tree (&lt;code&gt;xs&lt;/code&gt;)
persists.  Secondly many common nodes are shared between
the old tree and the new tree.
Such persistence and sharing is difficult to
manage without some form of [[garbage collection (computer science)|garbage collection]] (GC) to
automatically free up nodes which have no live
references, and this is why GC is a feature commonly
found in [[Functional programming|functional programming languages]].

==Reference cycles==
Since every value in a purely functional computation is built up out of existing values, it would seem that it is impossible to create a cycle of references. In that case, the reference graph (the graph of the references from object to object) could only be a [[directed acyclic graph]]. However, in most functional languages, functions can be defined [[recursion|recursively]]; this capability allows recursive structures using functional [[suspension (computer science)|suspensions]]. In [[lazy evaluation|lazy]] languages, such as [[Haskell (programming language)|Haskell]], all data structures are represented as implicitly suspended [[thunk]]s; in these languages any data structure can be recursive because a value can be defined in terms of itself. Some other languages, such as [[OCaml]], allow the explicit definition of recursive values.

==See also==
* [[Persistent data]]
* [[Navigational database]]
* [[Copy-on-write]]
* [[Retroactive data structures]]

==References==
{{Reflist}}

==Further reading==
{{refbegin}}
* [http://www.infoq.com/presentations/Value-Identity-State-Rich-Hickey Persistent Data Structures and Managed References] - video presentation by Rich Hickey on Clojure's use of persistent data structures and how they support concurrency
* [http://www.cs.cmu.edu/~sleator/papers/Persistence.htm Making Data Structures Persistent] by James R. Driscoll, Neil Sarnak, Daniel D. Sleator, Robert E. Tarjan
* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.1317 Fully persistent arrays for efficient incremental updates and voluminous reads]
* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.2895 Real-Time Deques, Multihead Turing Machines, and Purely Functional Programming]
*''Purely functional data structures'' by Chris Okasaki, [[Cambridge University Press]], 1998, ISBN 0-521-66350-4.
*[http://www.cs.cmu.edu/~rwh/theses/okasaki.pdf Purely Functional Data Structures] thesis by Chris Okasaki (PDF format)
*[http://www.cs.cmu.edu/~sleator/papers/fully-persistent-lists.pdf Fully Persistent Lists with Catenation] by James R. Driscoll, Daniel D. Sleator, Robert E. Tarjan (PDF)
*[http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-854j-advanced-algorithms-fall-2005/lecture-notes/persistent.pdf Persistent Data Structures] from MIT open course [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-854j-advanced-algorithms-fall-2005 Advanced Algorithms]
{{refend}}

==External links==
* [http://wiki.edinburghhacklab.com/PersistentRedBlackTreeSet Lightweight Java implementation of Persistent Red-Black Trees]

[[Category:Data structures]]
[[Category:Functional data structures| ]]
[[Category:Persistence]]
**</text>
      <sha1>0qbcnr76fss2qmnhshn249y13808l00</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Recordset</title>
    <ns>0</ns>
    <id>2666135</id>
    <revision>
      <id>541704854</id>
      <parentid>538178241</parentid>
      <timestamp>2013-03-02T11:51:59Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 3 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q445390]]</comment>
      <text xml:space="preserve" bytes="846">A '''recordset''' is a [[data structure]] that consists of a group of [[database]] [[record (database)|records]], and can either come from a base [[table (database)|table]] or as the result of a [[Information retrieval|query]] to the table. 

The concept is common to a number of platforms, notably Microsoft's [[Data Access Objects]] (DAO) and [[ActiveX Data Objects]] (ADO). The Recordset object contains a Fields collection, and a Properties collection. At any time, the Recordset object refers to only a single record within the set as the current record. 
==See also==
*[[Bound control]]

==External links==
* [http://msdn.microsoft.com/en-us/library/ms681510.aspx Microsoft definition of a Recordset object in ADO]
* [http://www.w3schools.com/ado/ado_ref_recordset.asp]
{{software-stub}}

[[Category:Databases]]
[[Category:Data structures]]</text>
      <sha1>5lu5xy06ee2jpjznizxtlglm5ynz07k</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Zipper (data structure)</title>
    <ns>0</ns>
    <id>7872003</id>
    <revision>
      <id>624003551</id>
      <parentid>609929507</parentid>
      <timestamp>2014-09-03T12:45:01Z</timestamp>
      <contributor>
        <username>Dexbot</username>
        <id>16752040</id>
      </contributor>
      <minor/>
      <comment>Bot: Fixing broken section link</comment>
      <text xml:space="preserve" bytes="9369">{{Wikibooks|Haskell|Zippers}}

A '''zipper''' is a technique of representing an aggregate [[data structure]] so that it is convenient for writing programs that traverse the structure arbitrarily and update its contents, especially in [[purely functional|purely]] [[functional programming language]]s. The zipper was described by [[Gérard Huet]] in 1997.&lt;ref&gt;{{harvnb|Huet|1997}}&lt;/ref&gt; It includes and generalizes the [[gap buffer]] technique sometimes used with arrays.

The zipper technique is general in the sense that it can be adapted to [[List (computing)|lists]], [[Tree (data structure)|trees]], and other [[Recursive data type|recursively defined]] data structures.
Such modified data structures are usually referred to as &quot;a tree with zipper&quot; or &quot;a list with zipper&quot; to emphasize that the structure is conceptually a tree or list, while the zipper is a detail of the implementation.

A layman's explanation for a tree with zipper would be an ordinary computer filesystem with operations to go to parent (often &lt;code&gt;cd ..&lt;/code&gt;), and the possibility to go downwards (&lt;code&gt;cd subdirectory&lt;/code&gt;). The zipper is the pointer to the current path. Behind the scenes the zippers are efficient when making (functional) changes to a data structure, where a new, slightly changed, data structure is returned from an edit operation (instead of making a change in the current data structure).

==Example: Bidirectional list traversal==
Many common data structures in computer science can be expressed as the structure generated by a few primitive [[constructor operation]]s or [[observer operation]]s.
These include the structure of finite lists, which can be generated by two operations:

*Empty: Constructs an empty list
*Insert(x, L): Constructs a list by inserting value x in front of list L

The list [1, 2, 3] is then constructed as Insert(1, Insert(2, Insert(3, Empty))). It is possible to describe the location of a value in a list as the number of steps from the front of the list to that value.
More formally, a location is the number of additional Insert operations used to construct the whole list, after a particular value was inserted.

A context for a location in the list is constructed by recording not just the number of Insert operations, but all of the other information about them—namely, the values that were inserted.
These are represented in a separate list that is reversed from the order of the original data structure.
Specifically, the context of &quot;3&quot; in the list [1, 2, 3] is represented as [2, 1].
A list with a zipper represents the entire structure, and a location within the structure.
This is a pair consisting of the location's context, and the part of the structure that begins at the location. The list [1, 2, 3, 4] with a reference to the &quot;3&quot; is represented as ([2, 1], [3, 4]).

With the list represented this way, it is easy to define efficient operations that move the location forward or backward and manipulate the list at that location, for example by inserting or removing items.
Similarly, applying the zipper transformation to a tree makes it easy to insert or remove values at a particular location in the tree.

==Uses==
The zipper is often used where there is some concept of '[[Focus (computing)|focus']] or of moving around in some set of data, since its semantics reflect that of moving around but in a functional non-destructive manner.

The zipper has been used in
* [[Xmonad]], to manage focus and placement of [[Window (computing)|windows]] &lt;!-- and focus in general? --&gt;
* Huet's papers cover a [[structural editor]]&lt;ref&gt;[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.445 &quot;Functional Pearl: Weaving a web&quot;].&lt;/ref&gt; based on zippers and a [[Automated theorem prover|theorem prover]]
* A [[filesystem]] (ZipperFS) written in [[Haskell (programming language)|Haskell]] offering &quot;...transactional semantics; undo of any file and directory operation; snapshots; statically guaranteed the strongest, repeatable read, isolation mode for clients; pervasive copy-on-write for files and directories; built-in traversal facility; and just the right behavior for cyclic directory references.&quot;&lt;ref&gt;[http://okmij.org/ftp/continuations/zipper.html#zipper-fs Generic Zipper: the context of a traversal]&lt;/ref&gt;
* [[Clojure]] has extensive support for zippers. &lt;ref&gt;{{cite web|author=jafingerhut |url=http://clojuredocs.org/clojure_core/clojure.zip/zipper |title=clojure.zip/zipper |publisher=ClojureDocs |date=2010-10-22 |accessdate=2013-06-15}}&lt;/ref&gt;

==Zipper contexts and differentiation==
It has been shown that the type of the items in the context list produced by the zipper transformation is the &quot;[[derivative (generalizations)#Type theory|derivative]]&quot; of the original type in a sense that is related to [[derivative|differentiation]] in [[calculus]] by [[decategorification]].  
Most datatypes are constructed from products and sums of datatypes; any given datatype looks like a [[polynomial]] or a [[Taylor series]], and the representation of the type of context items looks like the derivative of that polynomial or series.&lt;ref&gt;Joyal, André  (1981), &quot;Une théorie combinatoire des séries formelles&quot;, Advances in Mathematics 42:1-82.&lt;/ref&gt;&lt;ref&gt;McBride, Conor  (2001), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.8611 &quot;The Derivative of a Regular Type is its Type of One-Hole Contexts&quot;]&lt;/ref&gt;  In a recursive datatype like a list or a tree, the derivative is taken with respect to the recursion variable.

Consider a recursive data structure like a binary tree labeled by data of type A.
&lt;div class=&quot;center&quot; style=&quot;width:auto; margin-left:auto; margin-right:auto;&quot;&gt;&lt;math&gt;T(A, R) = 1 + A\cdot R^2&lt;/math&gt;&lt;/div&gt;
That is, a tree is either empty, or a triple consisting of a value of type &lt;math&gt;A&lt;/math&gt; and two subtrees of type &lt;math&gt;R&lt;/math&gt;.  The datatype of the context is
&lt;div class=&quot;center&quot; style=&quot;width:auto; margin-left:auto; margin-right:auto;&quot;&gt;&lt;math&gt;\frac{d T(A, R)}{d R} = A\cdot 2\cdot R.&lt;/math&gt;&lt;/div&gt;
By taking the fixed point &lt;math&gt;R = T(A, R),&lt;/math&gt; we find that a zipper for a tree consists of a &quot;path&quot; and a downward subtree, where a path is a context list of triples consisting of

* a value for the root of the tree (type A)
* a choice of left or right subtree in which to find the hole (type 2), and
* the value of the other subtree (type R).

In general, then, a zipper for a datatype &lt;math&gt;T&lt;/math&gt; parameterized by some other type &lt;math&gt;A&lt;/math&gt; and a recursion variable &lt;math&gt;R&lt;/math&gt; consists of two parts: a context list with items of type &lt;math&gt;\frac{d T(A, R)}{dR}|_{R = T(A, R)}&lt;/math&gt; and a copy of the downward substructure &lt;math&gt;T(A,R)|_{R = T(A,R)}.&lt;/math&gt;

==Alternatives and extensions==
===Direct modification===
In a non-purely-functional programming language, it may be more convenient to simply traverse the original data structure and modify it directly (perhaps after [[object copy|deep cloning]] it, to avoid affecting other code that might hold a reference to it).

===Generic zipper===
The Generic Zipper&lt;ref&gt;{{cite web|url=http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WalkZip1/|title=From walking to zipping, part 1|author=Chung-chieh Shan, Oleg Kiselyov|date=17 August 2008|accessdate=29 August 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WalkZip2/|title=From walking to zipping, part 2|author=Chung-chieh Shan, Oleg Kiselyov|date=17 August 2008|accessdate=29 August 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WalkZip3/|title=From walking to zipping, part 3|author=Chung-chieh Shan, Oleg Kiselyov|date=17 August 2008|accessdate=29 August 2011}}&lt;/ref&gt; is a technique to achieve the same goal as the conventional zipper by capturing the state of the traversal in a continuation while visiting each node. (The Haskell code given in the reference uses [[Generic programming#Genericity in Haskell|generic programming]] to generate a traversal function for any data structure, but this is optional&amp;nbsp;– any suitable traversal function can be used.)

However, the Generic Zipper involves [[inversion of control]], so some uses of it require a [[state machine]] (or equivalent) to keep track of what to do next.

==References==
{{reflist|2}}

==Further reading==
* {{cite journal|last=Huet|first=Gerard|url=http://www.st.cs.uni-sb.de/edu/seminare/2005/advanced-fp/docs/huet-zipper.pdf|title=Functional Pearl: The Zipper|journal=Journal of Functional Programming|volume=7|issue=5|pages=549–554|date=September 1997|year=1997|ref=harv|doi=10.1017/s0956796897002864}}
* Hinze, Ralf, et al. [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.1.6342 &quot;Type-indexed data types&quot;]. 23 July 2003

==External links==
* [http://www.haskell.org/haskellwiki/Zipper Zipper]
* [http://en.wikibooks.org/wiki/Haskell/Zippers Theseus and the Zipper]
* [http://donsbot.wordpress.com/2007/05/17/roll-your-own-window-manager-tracking-focus-with-a-zipper/7 &quot;Roll Your Own Window Manager: Tracking Focus with a Zipper&quot;]
* [http://www.nist.gov/dads/HTML/zipper.html Definition]
* [http://www.eecs.harvard.edu/~nr/pubs/zipcfg-abstract.html &quot;An Applicative Control-Flow Graph Based on Huet's Zipper&quot;]
* [http://sigfpe.blogspot.com/2006/09/infinitesimal-types.html Infinitesimal Types]
{{Use dmy dates|date=February 2011}}

[[Category:Functional programming]]
[[Category:Data structures]]</text>
      <sha1>gaydlnt2uj6e58b6ta85pv4azp2wl0f</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Sequential access</title>
    <ns>0</ns>
    <id>27162</id>
    <revision>
      <id>624647196</id>
      <parentid>624646879</parentid>
      <timestamp>2014-09-08T09:20:46Z</timestamp>
      <contributor>
        <username>Intgr</username>
        <id>246230</id>
      </contributor>
      <minor/>
      <comment>rv vandalism</comment>
      <text xml:space="preserve" bytes="3629">{{Refimprove|date=July 2011}}
[[File:Random vs sequential access.svg|thumb|right|Sequential access compared to [[random access]].]]
In [[computer science]], '''sequential access''' means that a group of elements (such as data in a memory array or a disk file or on [[magnetic tape data storage]]) is accessed in a predetermined, ordered [[sequence]]. Sequential access is sometimes the only way of accessing the data, for example if it is on a tape. It may also be the access method of choice, for example if all that is wanted is to process a sequence of data elements in order.&lt;ref&gt;&quot;[http://technet.microsoft.com/en-us/library/cc938619.aspx Microsoft TechNet]&quot;&lt;/ref&gt; 

However, there is no consistent definition of sequential access or sequentiality.&lt;ref&gt;''Irfan Ahmad'', [http://www.vmware.com/files/pdf/iiswc_2007_distribute.pdf Easy and Efficient Disk I/O Workload Characterization in VMware ESX Server], IISWC, 2007.&lt;/ref&gt;&lt;ref&gt;''Eric Anderson'', [https://www.usenix.org/legacy/event/fast09/tech/full_papers/anderson/anderson.pdf Capture, Conversion, and Analysis of an Intense NFS Workload], FAST, 2009.&lt;/ref&gt;&lt;ref&gt;''Yanpei Chen et al.'' [http://dl.acm.org/citation.cfm?id=2043562 Design Implications for Enterprise Storage Systems via Multi-dimensional Trace Analysis]. SOSP. 2011&lt;/ref&gt;&lt;ref&gt;''Andrew Leung et al.'' [http://www.ssrc.ucsc.edu/Papers/leung-usenix08.pdf Measurement and Analysis of Large-scale Network File System Workloads]. USENIX ATC. 2008&lt;/ref&gt;&lt;ref&gt;''Frank Schmuck and Roger Haskin'', [https://www.usenix.org/legacy/events/fast02/full_papers/schmuck/schmuck.pdf GPFS: A Shared-Disk File System for Large Computing Clusters], FAST. 2002&lt;/ref&gt;&lt;ref&gt;''Alan Smith''. [http://www-inst.eecs.berkeley.edu/~cs266/sp10/readings/smith78.pdf Sequentiality and Prefetching in Database Systems]. ACM TOS&lt;/ref&gt;&lt;ref&gt;''Hyong Shim et al.'' [http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/11747-atc13-shim.pdf Characterization of Incremental Data Changes for Efficient Data Protection]. USENIX ATC. 2013.&lt;/ref&gt;&lt;ref&gt;''Avishay Traeger et al.'' [http://www.fsl.cs.sunysb.edu/docs/fsbench/fsbench.pdf A Nine Year Study of File System and Storage Benchmarking]. ACM TOS. 2007.&lt;/ref&gt; In fact, different sequentiality definitions can lead to different sequentiality quantification results. In spatial dimension, request size, strided distance, backward accesses, re-accesses can affect sequentiality. For temporal sequentiality, characteristics such as multi-stream and inter-arrival time threshold has impact on the definition of sequentiality.&lt;ref&gt;''Cheng Li et al.'' [https://www.usenix.org/system/files/conference/hotstorage14/hotstorage14-paper-li_cheng.pdf Assert(!Defined(Sequential I/O))]. HotStorage. 2014&lt;/ref&gt;

In [[data structure]]s, a data structure is said to have sequential access if one can only visit the values it contains in one particular order. The canonical example is the [[linked list]]. Indexing into a list that has sequential access requires [[Big O notation|O]](''k'') time, where ''k'' is the index. As a result, many algorithms such as [[quicksort]] and [[binary search]] degenerate into bad algorithms that are even less efficient than their naïve alternatives; these algorithms are impractical without [[random access]]. On the other hand, some algorithms, typically those that do not have index, require only sequential access, such as [[mergesort]], and face no penalty.

==See also==
* [[Random access]]
* [[Direct access storage device]]
* [[Queued sequential access method]]

==References==
{{reflist}}

[[Category:Computer data]]
[[Category:Data structures]]</text>
      <sha1>gjqu9npu8jk2arpyp6lepyr0wqk5yy1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Linked data structure</title>
    <ns>0</ns>
    <id>22474945</id>
    <revision>
      <id>620745860</id>
      <parentid>620743337</parentid>
      <timestamp>2014-08-11T09:40:00Z</timestamp>
      <contributor>
        <username>Vanamonde93</username>
        <id>17956451</id>
      </contributor>
      <comment>Reverted 2 [[WP:AGF|good faith]] edits by [[Special:Contributions/115.117.11.140|115.117.11.140]] using [[WP:STiki|STiki]]</comment>
      <text xml:space="preserve" bytes="7483">In [[computer science]], a '''linked data structure''' is a [[data structure]] which consists of a set of [[record (computer science)|data records]] (''[[node (computer science)|nodes]]'') linked together and organized by [[reference (computer science)|references]] (''links'' or ''[[pointer (computer programming)|pointer]]s'').  The link between data can also be called a '''connector'''.

In linked data structures, the links are usually treated as special [[data type]]s that can only be [[reference (computer science)|dereferenced]] or compared for equality.  Linked data structures are thus contrasted with [[Array data structure|arrays]] and other data structures that require performing arithmetic operations on pointers.  This distinction holds even when the nodes are actually implemented as elements of a single array, and the references are actually array [[Array data structure|indices]]: as long as no arithmetic is done on those indices, the data structure is essentially a linked one.

Linking can be done in two ways - Using dynamic allocation and using array index linking.

Linked data structures include [[linked list]]s, [[search tree]]s, [[expression tree]]s, and many other widely used data structures.  They are also key building blocks for many efficient algorithms, such as [[topological sort]]&lt;ref name=&quot;knuth&quot;&gt;[[Donald Knuth]], [[The Art of Computer Programming]]&lt;/ref&gt; and [[disjoint-set data structure|set union-find]].&lt;ref name=&quot;galfis&quot;&gt;[[Bernard A. Galler]] and [[Michael J. Fischer]]. An improved equivalence algorithm. ''[[Communications of the ACM]],'' Volume 7, Issue 5 (May 1964), pages 301-303. The paper originating disjoint-set forests. [http://portal.acm.org/citation.cfm?doid=364099.364331 ACM Digital Library]&lt;/ref&gt;&lt;!--Needed: reference to its analysis by Tarjan--&gt;

==Common types of linked data structures==

===Linked lists===

A linked list is a collection of structures ordered not by their physical placement in memory but by logical links that are stored as part of the data in the structure itself. It is not necessary that it should be stored in  the adjacent memory locations. Every [[structure]] has a data field and an address field. The Address field contains the address of its [[successor (graph theory)|successor]].

Linked list can be singly, doubly or multiply linked and can either be linear or circular.

'''Basic Properties'''

* Objects, called '''nodes''', are linked in a linear sequence

* A reference to the first node of the list is always kept.  This is called the 'head' or 'front'.&lt;ref&gt;http://www.cs.toronto.edu/~hojjat/148s07/lectures/week5/07linked.pdf&lt;/ref&gt;

&lt;div class=&quot;center&quot;&gt;[[Image:Singly-linked-list.svg]]&lt;br /&gt;&lt;small&gt;''A linked list with three nodes contain two fields each: an integer value and a link to the next node''&lt;/small&gt;&lt;/div&gt;

[[File:single node1.jpg|thumb|center|A linked list with a single node.]]

====Example in Java====

This is an example of the node class used to store integers in a Java implementation of a linked list.
&lt;source lang=java&gt;
public class IntNode {
     public int value;
     public IntNode link;
     public IntNode(int v) { value = v; }
}
&lt;/source&gt;

====Example in C====

This is an example of the node structure used for implementation of linked list in C.
&lt;source lang=c&gt;
struct node
{
	int val;
	struct node *next;
};
&lt;/source&gt;

This is an example using [[typedef]]s.
&lt;source lang=c&gt;
typedef struct node_s node_t;

struct node_s
{
	int     val;
	node_t *next;
};
&lt;/source&gt;

'''Note:''' A structure like this which contains a member that points to the same structure is called a self-referential structure.

====Example in C++====

This is an example of the node class structure used for implementation of linked list in C++.
&lt;source lang=cpp&gt;
class Node
{
	int val;
	Node *next;
};
&lt;/source&gt;

===Search trees===

A search tree is a tree data structure in whose nodes data values can be stored from some [[ordered set]], which is such that in an in-order traversal of the tree the nodes are visited in ascending order of the stored values.

'''Basic Properties'''

* Objects, called nodes, are stored in an ordered set.

* [[In-order traversal]] provides an ascending readout of the data in the tree

* Sub trees of the tree are in themselves, trees.

==Advantages and disadvantages==

===Linked list versus arrays===
Compared to arrays, linked data structures allow more flexibility in organizing the data and in allocating space for it. In arrays, the size of the array must be specified precisely at the beginning, which can be a potential waste of memory.  A linked data structure is built dynamically and never needs to be bigger than the programmer requires.  It also requires no guessing in terms of how much space must be allocated when using a linked data structure.  This is a feature that is key in saving wasted memory.

In an array, the array elements have to be in a [[Contiguity#Computer_science|contiguous]] (connected and sequential) portion of memory. But in a linked data structure, the reference to each node gives users the information needed to find the next one. The nodes of a linked data structure can also be moved individually to different locations without affecting the logical connections between them, unlike arrays.  With due care, a [[process (computing)|process]] can add or delete nodes to one part of a data structure even while other processes are working on other parts. 

On the other hand, access to any particular node in a linked data structure requires following a chain of references that stored in it.  If the structure has ''n'' nodes, and each node contains at most ''b'' links, there will be some nodes that cannot be reached in less than log&lt;sub&gt;''b''&lt;/sub&gt; ''n'' steps.  For many structures, some nodes may require [[best, worst and average case|worst case]] up to ''n''&amp;minus;1 steps. In contrast, many array data structures allow access to any element with a constant number of operations, independent of the number of entries.

Broadly the implementation of these linked data structure is through [[dynamic data structures]]. It gives us the chance to use particular space again. Memory can be utilized more efficiently by using this data structures. Memory is allocated as per the need and when memory is not further needed, deallocation is done.

===General disadvantages===
Linked data structures may also incur in substantial [[dynamic memory allocation|memory allocation]] overhead (if nodes are allocated individually) and frustrate [[virtual memory|memory paging]] and [[cache (computing)|processor caching]] algorithms (since they generally have poor [[locality of reference]]). In some cases, linked data structures may also use more memory (for the link fields) than competing array structures.  This is because linked data structures are not contiguous.  Instances of data can be found all over in memory, unlike arrays.

In arrays, nth element can be accessed immediately, while in a linked data structure we have to follow multiple pointers so element access time varies according to where in the structure the element is.

In some [[theoretical computer science|theoretical models of computation]] that enforce the constraints of linked structures, such as the [[pointer machine]], many problems require more steps than in the unconstrained [[random access machine]] model.

==See also==
* [[List of data structures]]

==References==
{{Reflist}}

{{Data structures}}

[[Category:Data structures]]</text>
      <sha1>i1fs7cqcdgucbp34r1u7u8uouceiuzl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Array data structure</title>
    <ns>0</ns>
    <id>2052</id>
    <revision>
      <id>624541399</id>
      <parentid>621525662</parentid>
      <timestamp>2014-09-07T14:11:53Z</timestamp>
      <contributor>
        <username>Peter Flass</username>
        <id>7557079</id>
      </contributor>
      <comment>move sentence in lead</comment>
      <text xml:space="preserve" bytes="23431">{{Use dmy dates|date=June 2013}}
{{Distinguish|Array data type}}
{{Refimprove|date=September 2008}}

In [[computer science]], an '''array data structure''' or simply an '''array''' is a [[data structure]] consisting of a collection of ''elements'' ([[value (computer science)|values]] or [[variable (programming)|variables]]), each identified by at least one ''array index'' or ''key''. An array is stored so that the position of each element can be computed from its index [[tuple]] by a mathematical formula.&lt;ref&gt;{{cite web|url=http://www.nist.gov/dads/HTML/array.html|title=array|last=Black|first=Paul E.|date=13 November 2008|work=[[Dictionary of Algorithms and Data Structures]]|publisher=[[National Institute of Standards and Technology]]|accessdate=22 August 2010}}&lt;/ref&gt;&lt;ref name=&quot;andres&quot;&gt;{{cite arXiv |eprint=1008.2909 |author1=Bjoern Andres |author2=Ullrich Koethe |author3=Thorben Kroeger |author4=Hamprecht |title=Runtime-Flexible Multi-dimensional Arrays and Views for C++98 and C++0x |class=cs.DS |year=2010}}&lt;/ref&gt;&lt;ref name=&quot;garcia&quot;&gt;{{Cite journal|last1=Garcia|first1=Ronald |first2=Andrew |last2=Lumsdaine|year=2005|title=MultiArray: a C++ library for generic programming with arrays|journal=Software: Practice and Experience|volume=35|issue=2|pages=159–188|issn=0038-0644|doi=10.1002/spe.630}}&lt;/ref&gt; The simplest type of data structure is a linear array, also called one-dimensional array.  

For example, an array of 10 32-bit integer variables, with indices 0 through 9, may be stored as 10 [[Word (data type)|words]] at memory addresses 2000, 2004, 2008, … 2036, so that the element with index ''i'' has the address 2000 + 4 × ''i''.&lt;ref&gt;David R. Richardson (2002), The Book on Data Structures. iUniverse, 112 pages. ISBN 0-595-24039-9, ISBN 978-0-595-24039-5.&lt;/ref&gt;

Because the mathematical concept of a [[matrix (mathematics)|matrix]] can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called matrices. In some cases the term &quot;vector&quot; is used in computing to refer to an array, although [[tuple]]s rather than [[vector space|vectors]] are more correctly the mathematical equivalent. Arrays are often used to implement [[table (information)|table]]s, especially [[lookup table]]s; the word ''table'' is sometimes used as a synonym of ''array''.

Arrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as [[list (computing)|list]]s and [[string (computer science)|string]]s.  They effectively exploit the addressing logic of computers. In most modern computers and many [[external storage]] devices, the memory is a one-dimensional array of words, whose indices are their addresses.  [[Central processing unit|Processors]], especially [[vector processor]]s, are often optimized for array operations.

Arrays are useful mostly because the element indices can be computed at [[Run time (program lifecycle phase)|run time]].  Among other things, this feature allows a single iterative [[statement (programming)|statement]] to process arbitrarily many elements of an array.  For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually,&lt;ref name=&quot;garcia&quot; /&gt;&lt;ref name=&quot;veldhuizen&quot;&gt;T. Veldhuizen. Arrays in Blitz++. In Proc. of the 2nd Int. Conf. on Scientific Computing in Object-Oriented Parallel Environments (ISCOPE), LNCS 1505, pages 223-220. Springer, 1998.&lt;/ref&gt; but not always,&lt;ref name=&quot;andres&quot; /&gt; fixed while the array is in use.

The term ''array'' is often used to mean [[array data type]], a kind of [[data type]] provided by most [[high-level programming language]]s that consists of a collection of values or variables that can be selected by one or more indices computed at run-time.  Array types are often implemented by array structures; however, in some languages they may be implemented by [[hash table]]s, [[linked list]]s, [[search tree]]s, or other data structures.

The term is also used, especially in the description of [[algorithm]]s, to mean [[associative array]] or &quot;abstract array&quot;, a [[theoretical computer science]] model (an [[abstract data type]] or ADT) intended to capture the essential properties of arrays.

==History==
The first digital computers used machine-language programming to set up and access array structures for data tables, vector and matrix computations, and for many other purposes. [[John von Neumann|Von Neumann]] wrote the first array-sorting program ([[merge sort]]) in 1945, during the building of the [[EDVAC|first stored-program computer]].&lt;ref&gt;Donald Knuth, ''The Art of Computer Programming'', vol. 3. Addison-Wesley&lt;/ref&gt;&lt;sup&gt;p.&amp;nbsp;159&lt;/sup&gt;  Array indexing was originally done by [[self-modifying code]], and later using [[index register]]s and [[Addressing mode|indirect addressing]].  Some mainframes designed in the 1960s, such as the [[Burroughs large systems|Burroughs B5000]] and its successors, used [[memory segmentation]] to perform index-bounds checking in hardware.&lt;ref&gt;{{citation|title=Capability-based Computer Systems|first=Henry M.|last=Levy|publisher=Digital Press|year=1984|isbn=9780932376220|page=22}}.&lt;/ref&gt;

Assembly languages generally have no special support for arrays, other than what the machine itself provides. The earliest high-level programming languages, including [[Fortran|FORTRAN]] (1957), [[COBOL]] (1960), and [[ALGOL|ALGOL 60]] (1960), had support for multi-dimensional arrays, and so has [[C (programming language)|C]] (1972). In [[C++]] (1983), class templates exist for multi-dimensional arrays whose dimension is fixed at runtime&lt;ref name=&quot;garcia&quot; /&gt;&lt;ref name=&quot;veldhuizen&quot; /&gt; as well as for runtime-flexible arrays.&lt;ref name=&quot;andres&quot; /&gt;

{{Expand section|date=May 2009}}

==Applications==
Arrays are used to implement mathematical [[coordinate vector|vectors]] and [[matrix (mathematics)|matrices]], as well as other kinds of rectangular tables.  Many [[database]]s, small and large, consist of (or include) one-dimensional arrays whose elements are [[record (computer science)|record]]s.

Arrays are used to implement other data structures, such as [[heap (data structure)|heaps]], [[hash table]]s, [[double-ended queue|deque]]s, [[queue (data structure)|queue]]s, [[stack (data structure)|stacks]], [[String (computer science)|strings]], and [[VList]]s.

One or more large arrays are sometimes used to emulate in-program [[dynamic memory allocation]], particularly [[memory pool]] allocation. Historically, this has sometimes been the only way to allocate &quot;dynamic memory&quot; portably.

Arrays can be used to determine partial or complete [[control flow]] in programs, as a compact alternative to (otherwise repetitive) multiple &lt;code&gt;IF&lt;/code&gt; statements. They are known in this context as [[control table]]s and are used in conjunction with a purpose built interpreter whose [[control flow]] is altered according to values contained in the array. The array may contain [[subroutine]] [[Pointer (computer programming)|pointers]] (or relative subroutine numbers that can be acted upon by [[Switch statement|SWITCH]] statements) that direct the path of the execution.

==Element identifier and addressing formulas==

When data objects are stored in an array, individual objects are selected by an index that is usually a non-negative [[scalar (computing)|scalar]] [[integer]]. Indices are also called subscripts.  An index ''maps'' the array value to a stored object.

There are three ways in which the elements of an array can be indexed:

* '''0''' (''[[Zero-based numbering|zero-based indexing]]''): The first element of the array is indexed by subscript of 0.&lt;ref&gt;{{cite web
| accessdate = 8 April 2011
| location = http://www.configure-all.com/
| publisher = Computer Programming Web programming Tips
| title = Array Code Examples - PHP Array Functions - PHP code
| quote = In most computer languages array index (counting) starts from 0, not from 1. Index of the first element of the array is 0, index of the second element of the array is 1, and so on. In array of names below you can see indexes and values.
| url = http://www.configure-all.com/arrays.php}}&lt;/ref&gt;
* '''1''' (''one-based indexing''): The first element of the array is indexed by subscript of 1.&lt;ref&gt;{{cite web
| accessdate = 8 April 2011
| location = http://www.modula2.org/tutor/index.php
| work = Modula-2 Tutorial
| title = Chapter 6 - Arrays, Types, and Constants
| quote = The names of the twelve variables are given by Automobiles[1], Automobiles[2], ... Automobiles[12]. The variable name is &quot;Automobiles&quot; and the array subscripts are the numbers 1 through 12. [i.e. in Modula-2, the index starts by one!]
| url = http://www.modula2.org/tutor/chapter6.php}}&lt;/ref&gt;
* '''n''' (''n-based indexing''): The base index of an array can be freely chosen. Usually programming languages allowing ''n-based indexing'' also allow negative index values and other [[scalar (computing)|scalar]] data types like [[Enumerated type|enumerations]], or [[Character (computing)|characters]] may be used as an array index.

Arrays can have multiple dimensions, thus it is not uncommon to access an array using multiple indices. For example a two-dimensional array  &lt;code&gt;A&lt;/code&gt; with three rows and four columns might provide access to the element at the 2nd row and 4th column by the expression &lt;code&gt;A[1, 3]&lt;/code&gt; (in a [[row major]] language) or &lt;code&gt;A[3, 1]&lt;/code&gt; (in a [[column major]] language) in the case of a zero-based indexing system. Thus two indices are used for a two-dimensional array, three for a three-dimensional array, and ''n'' for an ''n''-dimensional array.

The number of indices needed to specify an element is called the dimension, dimensionality, or [[rank (computer programming)|rank]] of the array.

In standard arrays, each index is restricted to a certain range of consecutive integers (or consecutive values of some [[enumerated type]]), and the address of an element is computed by a &quot;linear&quot; formula on the indices.

===One-dimensional arrays===
A one-dimensional array (or single dimension array) is a type of linear array. Accessing its elements involves a single subscript which can either represent a row or column index.

As an example consider the C declaration &lt;code&gt;int anArrayName[10];&lt;/code&gt;

Syntax : datatype anArrayname[sizeofArray];

In the given example the array can contain 10 elements of any value available to the &lt;code&gt;int&lt;/code&gt; type.  In C, the array element indices are 0-9 inclusive in this case.  For example, the expressions &lt;code&gt;anArrayName[0]&lt;/code&gt; and &lt;code&gt;anArrayName[9]&lt;/code&gt; are the first and last elements respectively.

For a vector with linear addressing, the element with index ''i'' is located at the address ''B'' + ''c'' × ''i'', where ''B'' is a fixed ''base address'' and ''c'' a fixed constant, sometimes called the  ''address increment'' or ''stride''.

If the valid element indices begin at 0, the constant ''B'' is simply the address of the first element of the array. For this reason, the [[C (programming language)|C programming language]] specifies that array indices always begin at 0; and many programmers will call that element &quot;[[Zero-based numbering|zeroth]]&quot; rather than &quot;first&quot;.

However, one can choose the index of the first element by an appropriate choice of the base address ''B''.  For example, if the array has five elements, indexed 1 through 5, and the base address ''B'' is replaced by ''B'' + 30''c'', then the indices of those same elements will be 31 to 35.  If the numbering does not start at 0, the constant ''B'' may not be the address of any element.

===Multidimensional arrays===
For a two-dimensional array, the element with indices ''i'',''j'' would have address ''B'' + ''c'' · ''i'' + ''d'' · ''j'', where the coefficients ''c'' and ''d'' are the ''row'' and ''column address increments'', respectively.

More generally, in a ''k''-dimensional array, the address of an element with indices ''i''&lt;sub&gt;1&lt;/sub&gt;, ''i''&lt;sub&gt;2&lt;/sub&gt;, …, ''i''&lt;sub&gt;''k''&lt;/sub&gt; is
:''B'' + ''c''&lt;sub&gt;1&lt;/sub&gt; · ''i''&lt;sub&gt;1&lt;/sub&gt; + ''c''&lt;sub&gt;2&lt;/sub&gt; · ''i''&lt;sub&gt;2&lt;/sub&gt; + … + ''c''&lt;sub&gt;''k''&lt;/sub&gt; · ''i''&lt;sub&gt;''k''&lt;/sub&gt;.

For example: int a[3][2];

This means that array a has 3 rows and 2 columns, and the array is of integer type. Here we can store 6 elements they are stored linearly but starting from first row linear then continuing with second row. The above array will be stored as a&lt;sub&gt;11&lt;/sub&gt;, a&lt;sub&gt;12&lt;/sub&gt;, a&lt;sub&gt;13&lt;/sub&gt;, a&lt;sub&gt;21&lt;/sub&gt;, a&lt;sub&gt;22&lt;/sub&gt;, a&lt;sub&gt;23&lt;/sub&gt;.

This formula requires only ''k'' multiplications and ''k'' additions, for any array that can fit in memory. Moreover, if any coefficient is a fixed power of 2, the multiplication can be replaced by [[bitwise operation|bit shifting]].

The coefficients ''c''&lt;sub&gt;''k''&lt;/sub&gt; must be chosen so that every valid index tuple maps to the address of a distinct element.

If the minimum legal value for every index is 0, then ''B'' is the address of the element whose indices are all zero.  As in the one-dimensional case, the element indices may be changed by changing the base address ''B''.  Thus, if a two-dimensional array has rows and columns indexed from 1 to 10 and 1 to 20, respectively, then replacing  ''B'' by ''B'' + ''c''&lt;sub&gt;1&lt;/sub&gt; - &amp;minus; 3 ''c''&lt;sub&gt;1&lt;/sub&gt; will cause them to be renumbered from 0 through 9 and 4 through 23, respectively.  Taking advantage of this feature, some languages (like FORTRAN 77) specify that array indices begin at 1, as in mathematical tradition; while other languages (like Fortran 90, Pascal and Algol) let the user choose the minimum value for each index.

===Dope vectors===
The addressing formula is completely defined by the dimension ''d'', the base address ''B'', and the increments ''c''&lt;sub&gt;1&lt;/sub&gt;, ''c''&lt;sub&gt;2&lt;/sub&gt;, …, ''c''&lt;sub&gt;''k''&lt;/sub&gt;.  It is often  useful to pack these parameters into a record called the array's ''descriptor'' or ''stride vector'' or ''[[dope vector]]''.&lt;ref name=&quot;andres&quot; /&gt;&lt;ref name=&quot;garcia&quot; /&gt;  The size of each element, and the minimum and maximum values allowed for each index may also be included in the dope vector.  The dope vector is a complete [[handle (computing)|handle]] for the array, and is a convenient way to pass arrays as arguments to [[subroutine|procedures]].  Many useful [[array slicing]] operations (such as selecting a sub-array, swapping indices, or reversing the direction of the indices) can be performed very efficiently by manipulating the dope vector.&lt;ref name=&quot;andres&quot; /&gt;

===Compact layouts===
Often the coefficients are chosen so that the elements occupy a contiguous area of memory.  However, that is not necessary.  Even if arrays are always created with contiguous elements, some array slicing operations may create non-contiguous sub-arrays from them.

There are two systematic compact layouts for a two-dimensional array.  For example, consider the matrix
:&lt;math&gt;\mathbf{A} =
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6 \\
7 &amp; 8 &amp; 9
\end{bmatrix}.
&lt;/math&gt;
In the [[row-major order]] layout (adopted by C for statically declared arrays), the elements in each row are stored in consecutive positions and all of the elements of a row have a lower address than any of the elements of a consecutive row:
{| border=1
|-
| 1 || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9
|}
In [[Row-major order|column-major order]] (traditionally used by Fortran), the elements in each column are consecutive in memory and all of the elements of a column have a lower address than any of the elements of a consecutive column:
{| border=1
|-
| 1 || 4 || 7 || 2 || 5 || 8 || 3 || 6 || 9
|}
For arrays with three or more indices, &quot;row major order&quot; puts in consecutive positions any two elements whose index tuples differ only by one in the ''last'' index.  &quot;Column major order&quot; is analogous with respect to the ''first'' index.

In systems which use [[processor cache]] or [[virtual memory]], scanning an array is much faster if successive elements are stored in consecutive positions in memory, rather than sparsely scattered.  Many algorithms that use multidimensional arrays will scan them in a predictable order. A programmer (or a sophisticated compiler) may use this information to choose between row- or column-major layout for each array.  For example,  when computing the product ''A''·''B'' of two matrices, it would be best to have ''A'' stored in row-major order, and  ''B'' in column-major order.

===Resizing===
{{Main|Dynamic array}}

Static arrays have a size that is fixed when they are created and consequently do not allow elements to be inserted or removed. However, by allocating a new array and copying the contents of the old array to it, it is possible to effectively implement a ''dynamic'' version of an array; see [[dynamic array]]. If this operation is done infrequently, insertions at the end of the array require only amortized constant time.

Some array data structures do not reallocate storage, but do store a count of the number of elements of the array in use, called the count or size. This effectively makes the array a [[dynamic array]] with a fixed maximum size or capacity; [[Pascal string]]s are examples of this.

===Non-linear formulas===
More complicated (non-linear) formulas are occasionally used. For a compact two-dimensional [[triangular array]], for instance, the addressing formula is a polynomial of degree 2.

==Efficiency==
Both ''store'' and ''select'' take (deterministic worst case) [[constant time]]. Arrays take linear ([[Big-O notation|O]](''n'')) space in the number of elements ''n'' that they hold.

In an array with element size ''k'' and on a machine with a cache line size of B bytes, iterating through an array of ''n'' elements requires the minimum of ceiling(''nk''/B) cache misses, because its elements occupy contiguous memory locations. This is roughly a factor of B/''k'' better than the number of cache misses needed to access ''n'' elements at random memory locations. As a consequence, sequential iteration over an array is noticeably faster in practice than iteration over many other data structures, a property called [[locality of reference]] (this does ''not'' mean however, that using a [[Perfect hash function|perfect hash]] or [[hash function#Trivial hash function|trivial hash]] within the same (local) array, will not be even faster - and achievable in [[constant time]]). Libraries provide low-level optimized facilities for copying ranges of memory (such as [[String.h|memcpy]]) which can be used to move [[contiguous]] blocks of array elements significantly faster than can be achieved through individual element access. The speedup of such optimized routines varies by array element size, architecture, and implementation.

Memory-wise, arrays are compact data structures with no per-element [[Computational overhead|overhead]]. There may be a per-array overhead, e.g. to store index bounds, but this is language-dependent. It can also happen that elements stored in an array require ''less'' memory than the same elements stored in individual variables, because several array elements can be stored in a single [[Word (data type)|word]]; such arrays are often called ''packed'' arrays. An extreme (but commonly used) case is the [[bit array]], where every bit represents a single element. A single [[octet (computing)|octet]] can thus hold up to 256 different combinations of up to 8 different conditions, in the most compact form.

Array accesses with statically predictable access patterns are a major source of [[data parallelism]].

===Comparison with other data structures===
{{List data structure comparison}}
[[Dynamic array|Growable array]]s are similar to arrays but add the ability to insert and delete elements; adding and deleting at the end is particularly efficient. However, they reserve linear ([[Big-O notation#Family of Bachmann–Landau notations|Θ]](''n'')) additional storage, whereas arrays do not reserve additional storage.

[[Associative array]]s provide a mechanism for array-like functionality without huge storage overheads when the index values are sparse. For example, an array that contains values only at indexes 1 and 2 billion may benefit from using such a structure. Specialized associative arrays with integer keys include [[Radix tree|Patricia trie]]s, [[Judy array]]s, and [[van Emde Boas tree]]s.

[[Self-balancing binary search tree|Balanced trees]] require O(log ''n'') time for indexed access, but also permit inserting or deleting elements in O(log ''n'') time,&lt;ref&gt;[http://www.chiark.greenend.org.uk/~sgtatham/algorithms/cbtree.html Counted B-Tree]&lt;/ref&gt; whereas growable arrays require linear (Θ(''n'')) time to insert or delete elements at an arbitrary position.

[[Linked list]]s allow constant time removal and insertion in the middle but take linear time for indexed access. Their memory use is typically worse than arrays, but is still linear.

[[Image:Array of array storage.svg|120px|left|A two-dimensional array stored as a one-dimensional array of one-dimensional arrays (rows).]]
An [[Iliffe vector]] is an alternative to a multidimensional array structure. It uses a one-dimensional array of [[reference (computer science)|references]] to arrays of one dimension less. For two dimensions, in particular, this alternative structure would be a vector of pointers to vectors, one for each row.  Thus an element in row ''i'' and column ''j'' of an array ''A'' would be accessed by double indexing (''A''[''i''][''j''] in typical notation).  This alternative structure allows ''ragged'' or ''jagged'' arrays, where each row may have a different size — or, in general, where the valid range of each index depends on the values of all preceding indices.  It also saves one multiplication (by the column address increment) replacing it by a bit shift (to index the vector of row pointers) and one extra memory access (fetching the row address), which may be worthwhile in some architectures.&lt;br clear=left /&gt;

==Dimension==
The dimension of an array is the number of indices needed to select an element. Thus, if the array is seen as a function on a set of possible index combinations, it is the dimension of the space of which its domain is a discrete subset. Thus a one-dimensional array is a list of data, a two-dimensional array a rectangle of data, a three-dimensional array a block of data, etc.

This should not be confused with the dimension of the set of all matrices with a given domain, that is, the number of elements in the array. For example, an array with 5 rows and 4 columns is two-dimensional, but such matrices form a 20-dimensional space. Similarly, a three-dimensional vector can be represented by a one-dimensional array of size three.

==See also==
{{Portal|Computer programming}}

{{Div col||20em}}
* [[Dynamic array]]
* [[Parallel array]]
* [[Variable-length array]]
* [[Bit array]]
* [[Array slicing]]
* [[Offset (computer science)]]
* [[Row-major order]]
* [[Stride of an array]]
{{Div col end}}

==References==
{{Reflist|30em}}

==External links==
{{Wiktionary|array}}
{{Wikibooks|Data Structures/Arrays}}
{{Commons category|Array data structure}}

{{Data structures}}

{{DEFAULTSORT:Array Data Structure}}
[[Category:Arrays|*]]
[[Category:Data structures]]</text>
      <sha1>lmsaof4buthfflbt1c8haotl1j23i9i</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Compressed data structure</title>
    <ns>0</ns>
    <id>24757213</id>
    <revision>
      <id>570642049</id>
      <parentid>350086433</parentid>
      <timestamp>2013-08-29T08:05:43Z</timestamp>
      <contributor>
        <ip>50.157.202.44</ip>
      </contributor>
      <comment>you can't beat information theory</comment>
      <text xml:space="preserve" bytes="3176">The term '''compressed data structure''' arises in the [[computer science]] subfields of [[algorithms]], [[data structures]], and [[theoretical computer science]].  It refers to a data structure whose operations are roughly as fast as those of a conventional data structure for the problem, but whose size can be substantially smaller.  The size of the compressed data structure is typically highly dependent upon the entropy of the data being represented.  

Important examples of compressed data structures include the [[compressed suffix array]]&lt;ref&gt;R. Grossi and J. S. Vitter, Compressed Suffix Arrays and Suffix Trees with Applications to Text Indexing and String Matching], ''Proceedings of the 32nd ACM Symposium on Theory of Computing'', May 2000, 397-406.  Journal version in ''SIAM Journal on Computing'', 35(2), 2005, 378-407.&lt;/ref&gt;&lt;ref&gt;R. Grossi, A. Gupta, and J. S. Vitter, High-Order Entropy-Compressed Text Indexes, ''Proceedings of the 14th Annual SIAM/ACM Symposium on Discrete Algorithms'', January 2003, 841-850.&lt;/ref&gt; and the [[FM-index]],&lt;ref&gt;P. Ferragina and G. Manzini, Opportunistic Data Structures with Applications, ''Proceedings of the 41st IEEE Symposium on Foundations of Computer Science'', November 2000, 390-398.  Journal version in Indexing Compressed Text, ''Journal of the ACM'', 52(4), 2005, 552-581.&lt;/ref&gt; both of which can represent an arbitrary text of characters ''T'' for [[pattern matching]].  Given any input pattern ''P'', they support the operation of finding if and where ''P'' appears in ''T''.  The search time is proportional to the sum of the length of pattern ''P'', a very slow-growing function of the length of the text ''T'', and the number of reported matches.  The space they occupy is roughly equal to the size of the text ''T'' in entropy-compressed form, such as that obtained by [[Prediction by Partial Matching]] or [[gzip]].  Moreover, both data structures are self-indexing, in that they can reconstruct the text ''T'' in a random access manner, and thus the underlying text ''T'' can be discarded.  In other words, they simultaneously provide a compressed and quickly searchable representation of the text ''T''.  They represent a substantial space improvement over the conventional [[suffix tree]] and [[suffix array]], which occupy many times more space than the size of ''T''.  They also support searching for arbitrary patterns, as opposed to the [[inverted index]], which can support only word-based searches.  In addition, inverted indexes do not have the self-indexing feature. 

An important related notion is that of a [[succinct data structure]], which uses space roughly equal to the information-theoretic minimum, which is a worst-case notion of the space needed to represent the data.  In contrast, the size of a compressed data structure depends upon the particular data being represented.  When the data are compressible, as is often the case in practice for natural language text, the compressed data structure can occupy space very close to the information-theoretic minimum, and significantly less space than most compression schemes.

==References==
&lt;references/&gt;

[[Category:Data structures]]</text>
      <sha1>tlyqn31d8fyuknswdlp8gkx8lwtwutp</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Control table</title>
    <ns>0</ns>
    <id>21337396</id>
    <revision>
      <id>596282221</id>
      <parentid>593965563</parentid>
      <timestamp>2014-02-20T03:14:30Z</timestamp>
      <contributor>
        <username>Wavelength</username>
        <id>271168</id>
      </contributor>
      <comment>inserting 3 [[hyphen]]s: —&gt; &quot;One-dimensional&quot; and &quot;one-dimensional&quot; [2 instances]—[[wikt:one-dimensional]]</comment>
      <text xml:space="preserve" bytes="52749">[[File:Control table.png|thumb|220px|This simple control table directs program flow according to the value of the single input variable. Each table entry holds a possible input value to be tested for equality (implied) and a relevant subroutine to perform in the action column. The name of the subroutine could be replaced by a relative subroutine number if pointers are not supported]]
'''Control tables''' are [[array data structure|table]]s that control the [[control flow]] or play a major part in program control. There are no rigid rules about the structure or content of a control table&amp;mdash;its qualifying attribute is its ability to direct [[control flow]] in some way through &quot;execution&quot; by a [[Central processing unit|processor]] or [[Interpreter (computing)|interpreter]]. The design of such tables is sometimes referred to as '''table-driven design'''&lt;ref&gt;''Programs from decision tables'', Humby, E., 2007,Macdonald, 1973 ... Biggerstaff, Ted J. Englewood Cliffs, NJ : Prentice-Hall ISBN 0-444-19569-6&lt;/ref&gt;&lt;ref&gt;[http://www.dkl.com/html/CMFiles/53table_driven_design.pdf]&lt;/ref&gt; (although this typically refers to generating code automatically from external tables rather than direct run-time tables). In some cases, control tables can be specific implementations of [[Finite-state machine|finite-state-machine]]-based [[automata-based programming]]. If there are several hierarchical levels of control table they may behave in a manner equivalent to [[Hierarchical state machine|UML state machine]]s&lt;ref&gt;[[UML state machine#Hierarchically nested states]]&lt;/ref&gt;

Control tables often have the equivalent of [[Conditional (programming)|conditional expressions]] or [[subroutine|function]] [[Reference (computer science)|references]] embedded in them, usually implied by their relative column position in the [[association list]]. Control tables reduce the need for programming similar [[data structures|structures]] or program statements over and over again. The two-dimensional nature of most tables makes them easier to view and update than the one-dimensional nature of program code. In some cases, non-programmers can be assigned to maintain the control tables.

==Typical usage==
* Transformation of input values to:
** an [[Associative array|index]], for later branching or [[pointer (computer programming)|pointer]] [[lookup table|lookup]]
** a program name, relative [[subroutine]] number, [[Label (programming language)|program label]] or program [[offset (computer science)|offset]], to alter [[control flow]]
* Controlling a [[main loop]] in [[event-driven programming]] using a [[control variable]] for [[state transition]]s
* Controlling the program cycle for [[Online transaction processing]] applications

==More advanced usage==
* Acting as virtual instructions for a [[virtual machine]] processed by an [[Interpreter (computing)|interpreter]]
:similar to [[bytecode]] - but usually with operations implied by the table structure itself

==Table structure==
The tables can have multiple dimensions, of fixed or [[variable length code|variable length]]s and are usually [[Software portability|portable]] between [[computer platform]]s, requiring only a change to the interpreter, not the [[algorithm]] itself - the logic of which is essentially embodied within the table structure and content. The structure of the table may be similar to a [[Multimap (data structure)|multimap]] [[associative array]], where a data value (or combination of data values) may be mapped to one or more functions to be performed.

===One-dimensional tables===
In perhaps its simplest implementation, a control table may sometimes be a one-dimensional table for ''directly'' translating a [[raw data]] value to a corresponding subroutine [[Offset (computer science)|offset]], [[Array data structure|index]] or [[pointer (computer programming)|pointer]] using the raw data value either directly as the index to the array, or by performing some basic arithmetic on the data beforehand. This can be achieved in [[constant time]] (without a [[linear search]] or [[binary search]] using a typical [[lookup table]] on an [[associative array]]). In most [[Computer architecture|architecture]]s, this can be accomplished in two or three [[machine instruction]]s - without any comparisons or loops. The technique is known as a &quot;[[trivial hash function]]&quot; or, when used specifically for branch tables, &quot;[[double dispatch]]&quot;.
For this to be feasible, the range of all possible values of the data needs to be small (e.g. an [[ASCII]] or [[EBCDIC]] character value which have a range of [[hexadecimal]] '00' - 'FF'. If the actual range is ''guaranteed'' to be smaller than this, the array can be truncated to less than 256 bytes).

'''Table to translate raw ASCII values (A,D,M,S) to new subroutine index (1,4,3,2) in [[constant time]] using one-dimensional array'''

(gaps in the range are shown as '..' for this example, meaning 'all hex values up to next row'. The first two columns are not part of the array)
{| class=&quot;wikitable&quot; style=&quot;text-align:center; &quot;
|- style=&quot;vertical-align:bottom;&quot;
! [[ASCII]]!! [[Hexadecimal|Hex]] !! Array
|-
| [[Null character|null]] || 00 ||style=&quot;background:lightblue;&quot;| 00
|-
| .. || .. ||style=&quot;background:lightblue;&quot;| 00
|-
| [[@]] || 40 ||style=&quot;background:lightblue;&quot;| 00
|-
|A || 41 ||style=&quot;background:lightblue;&quot;| '''01'''
|-
| .. || .. ||style=&quot;background:lightblue;&quot;| 00
|-
|D || 44 ||style=&quot;background:lightblue;&quot;| '''04'''
|-
| .. || .. ||style=&quot;background:lightblue;&quot;| 00
|-
|M || 4D ||style=&quot;background:lightblue;&quot;| '''03'''
|-
| .. || .. ||style=&quot;background:lightblue;&quot;| 00
|-
|S || 53 ||style=&quot;background:lightblue;&quot;| '''02'''
|}
In [[automata-based programming]] and [[pseudoconversational transaction]] processing, if the number of distinct program states is small, a &quot;dense sequence&quot; control variable can be used to efficiently dictate the entire flow of the main program loop.

A two byte raw data value would require a ''minimum'' table size of 65,534 bytes - to handle all input possibilities - whilst allowing just 256 different output values. However, this direct translation technique provides an extremely fast [[data validation|validation]] &amp; conversion to a (relative) subroutine pointer if the [[heuristic]]s, together with sufficient fast access memory, permits its use.

===Branch tables===
{{Main|Branch table}}
A [[branch table]] is a one-dimensional 'array' of contiguous [[machine code]] [[Branch (computer science)|branch/jump]] instructions to effect a [[multiway branch]] to a program label when branched into by an immediately preceding, and indexed branch. It is sometimes generated by an [[optimizing compiler]] to execute a [[switch statement]] - provided that the input range is small and dense, with few gaps (as created by the previous array example)  [http://www.netrino.com/node/137].

Although quite compact - compared to the multiple equivalent &lt;code&gt;If&lt;/code&gt; statements - the branch instructions still carry some redundancy, since the branch [[opcode]] and condition code mask are repeated alongside the branch offsets. Control tables containing only the offsets to the program labels can be constructed to overcome this redundancy (at least in assembly languages) and yet requiring only minor execution time [[computational overhead|overhead]] compared to a conventional branch table.

===Multi-dimensional tables===
More usually, a control table can be thought of as a [[Truth table]] or as an executable (&quot;binary&quot;) implementation of a printed [[decision table]] (or a [[Tree (data structure)|tree]] of decision tables, at several levels). They contain (often implied) [[Propositional formula|propositions]], together with one or more associated 'actions'. These actions are usually performed by generic or custom-built [[subroutine]]s that are called by an &quot;[[Interpreter (computing)|interpreter]]&quot; program. The interpreter in this instance effectively functions as a [[virtual machine]], that 'executes' the control table entries and thus provides a higher level of [[Abstraction (computer science)|abstraction]] than the underlying code of the interpreter.

A control table can be constructed along similar lines to a language dependent [[switch statement]] but with the added possibility of testing for combinations of input values (using [[Boolean algebra (logic)|boolean]] style [[Logical conjunction|AND]]/[[Logical disjunction|OR]] conditions) and potentially calling multiple [[subroutine]]s (instead of just a single set of values and 'branch to' program labels). (The switch statement construct in any case may not be available, or has confusingly differing implementations in high level languages ([[High-level programming language|HLL]]). The control table concept, by comparison, has no intrinsic language dependencies, but might nevertheless be ''implemented'' differently according to the available data definition features of the chosen programming language.)

===Table content===
A control table essentially embodies the '[[essence]]' of a conventional program, stripped of its programming language syntax and platform dependent components (e.g. IF/THEN DO.., FOR.., DO WHILE.., SWITCH, GOTO, CALL) and 'condensed' to its variables (e.g. input1), values (e.g. 'A','S','M' and 'D'), and subroutine identities (e.g. 'Add','subtract,..'  or #1, #2,..). The structure of the table itself typically ''implies'' the (default) logical operations involved - such as 'testing for equality', performing a subroutine and 'next operation' or following the default sequence (rather than these being explicitly stated within program statements - as required in other [[programming paradigm]]s).

A multi-dimensional control table will normally, as a minimum, contain value/action pairs and may additionally contain operators and [[Type system|type]] information such as, the location, size and format of input or output data, whether [[data conversion]] (or other [[run time (program lifecycle phase)|run-time]] processing nuances) is required before or after processing (if not already implicit in the function itself). The table may or may not contain [[array index|indexes]] or relative or absolute [[pointer (computer programming)|pointer]]s to generic or customized [[Language primitive|primitives]] or [[subroutine]]s to be executed depending upon other values in the &quot;row&quot;.

The table illustrated below applies only to 'input1' since no specific input is specified in the table.

'''conditions and actions implied by structure'''
::{| class=&quot;wikitable&quot;
| '''(implied) IF ='''|| '''(implied) perform'''
|-
| value|| action
|-
| value|| action
|}
(This side-by-side pairing of value and action has similarities to constructs in [[Event-driven programming]], namely 'event-detection' and 'event-handling' but without (necessarily) the [[Asynchronous system|asynchronous]] nature of the event itself)

The variety of values that can be [[encoded]] within a control table is largely dependent upon the [[computer language]] used. [[Assembly language]] provides the widest scope for [[data types]] including (for the actions), the option of directly executable [[machine code]]. Typically a control table will contain values for each possible matching class of input together with a corresponding pointer to an action subroutine. Some languages claim not to support [[pointer (computer programming)|pointer]]s (directly) but nevertheless can instead support an [[Array data structure|index]] which can be used to represent a 'relative subroutine number' to perform conditional execution, controlled by the value in the table entry (e.g. for use in an optimized [[Switch statement|SWITCH]] statement - designed with zero gaps (i.e. a [[multiway branch]]) ).

Comments positioned above each column (or even embedded textual documentation) can render a decision table 'human readable' even after 'condensing down' (encoding) to its essentials (and still broadly in-line with the original program specification - especially if a printed decision table, [[Enumeration|enumerating]] each unique action, is created before coding begins).
The table entries can also optionally contain counters to collect run-time statistics for 'in-flight' or later optimization

==Table location==
Control tables can reside in [[Static variables|static]] storage, on [[auxiliary storage]], such as a [[flat file]] or on a [[database]] or may alternatively be partially or entirely built dynamically at program [[booting|initialization]] time from parameters (which themselves may reside in a table). For optimum efficiency, the table should be memory resident when the interpreter begins to use it.

==The interpreter and subroutines==
The interpreter can be written in any suitable programming language including a [[high level language]]. A suitably designed [[Generic programming|generic]] interpreter, together with a well chosen set of generic subroutines (able to process the most commonly occurring [[Language primitive|primitives]]), would require additional conventional coding only for new custom subroutines (in addition to specifying the control table itself). The interpreter, optionally, may only apply to some well-defined sections of a complete application program (such as the [[Main loop|main control loop]]) and not other, 'less conditional', sections (such as program initialization, termination and so on).

The interpreter does not need to be unduly complex, or produced by a programmer with the advanced knowledge of a compiler writer, and can be written just as any other application program - except that it is usually designed with efficiency in mind. Its primary function is to &quot;execute&quot; the table entries as a set of &quot;instructions&quot;. There need be no requirement for parsing of control table entries and these should therefore be designed, as far as possible, to be 'execution ready', requiring only the &quot;plugging in&quot; of variables from the appropriate columns to the already compiled generic code of the interpreter. The [[Instruction (computer science)|program instructions]] are, in theory, infinitely [[extensible]] and constitute (possibly arbitrary) values within the table that are meaningful only to the interpreter. The [[control flow]] of the interpreter is normally by sequential processing of each table row but may be modified by specific actions in the table entries.

These arbitrary values can thus be designed with [[algorithmic|efficiency]] in mind - by selecting values that can be used as direct indexes to data or [[function pointers]]. For particular platforms/[[computer language|language]], they can be specifically designed to minimize [[instruction path length]]s using [[branch table]] values or even, in some cases such as in [[Just-in-time compilation|JIT]] compilers, consist of directly executable [[machine code]] &quot;[[Snippet (programming)|snippets]]&quot; (or pointers to them).

The subroutines may be coded either in the same language as the interpreter itself or any other supported program language (provided that suitable inter-language 'Call' linkage mechanisms exist). The choice of language for the interpreter and/or subroutines will usually depend upon how portable it needs to be across various [[Platform (computing)|platform]]s. There may be several versions of the interpreter to enhance the [[Porting|portability]] of a control table. A subordinate control table pointer may optionally substitute for a subroutine pointer in the 'action' column(s) if the interpreter supports this construct, representing a conditional 'drop' to a lower logical level, mimicking a conventional [[Structured programming|structured program]] structure.

==Performance considerations==
At first sight, the use of control tables would appear to add quite a lot to a program's [[Computational overhead|overhead]], requiring, as it does, an interpreter process before the 'native' programming language statements are executed. This however is not always the case. By separating (or 'encapsulating') the executable coding from the logic, as expressed in the table, it can be more readily targeted to perform its function most efficiently. This may be experienced most obviously in a [[spreadsheet]] application - where the underlying spreadsheet software transparently converts complex logical 'formulae' in the most efficient manner it is able, in order to display its results.

The examples below have been chosen partly to illustrate potential performance gains that may not only ''compensate'' significantly for the additional tier of abstraction, but also ''improve'' upon - what otherwise might have been - less efficient, less maintainable and lengthier code. Although the examples given are for a 'low level' [[assembly language]] and for the [[C (language)|C language]], it can be seen, in both cases, that very few lines of code are required to implement the control table approach and yet can achieve very significant [[constant time]] performance improvements, reduce repetitive source coding and aid clarity, as compared with [[verbose]] conventional program language constructs. See also the [[Control table#Quotations|quotations]]by [[Donald Knuth]], concerning tables and the efficiency of [[multiway branch]]ing in this article.

==Examples of control tables==
The following examples are [[arbitrary]] (and based upon just a single input for simplicity), however the intention is merely to demonstrate how control flow can be effected via the use of tables instead of regular program statements. It should be clear that this technique can easily be extended to deal with multiple inputs, either by increasing the number of columns or utilizing multiple table entries (with optional and/or operator). Similarly, by using (hierarchical) 'linked' control tables, [[structured programming]] can be accomplished (optionally using indentation to help highlight subordinate control tables).

&quot;CT1&quot; is an example of a control table that is a simple [[lookup table]]. The first column represents the input value to be tested (by an implied 'IF input1 = x') and, if TRUE, the corresponding 2nd column (the 'action') contains a subroutine address to perform by a [[System call|call]] (or [[goto|jump]] to - similar to a [[Switch statement|SWITCH]] statement). It is, in effect, a [[multiway branch]] with return (a form of &quot;[[dynamic dispatch]]&quot;). The last entry is the default case where no match is found.

'''CT1'''
:{| class=&quot;wikitable&quot;
! input 1!! [[pointer (computer programming)|pointer]]
|-
| '''A''' || --&gt;Add
|-
| '''S''' || --&gt;Subtract
|-
| '''M''' || --&gt;Multiply
|-
| '''D''' || --&gt;Divide
|-
| '''?''' || --&gt;Default
|}
For programming languages that support pointers within [[data structure]]s alongside other data values, the above table (CT1) can be used to direct [[control flow]] to an appropriate [[subroutine]]s according to matching value from the table (without a column to indicate otherwise, equality is assumed in this simple case).

'''[[Assembly language]] example''' for [[IBM/360]] (maximum 16Mb address range) or [[Z/Architecture]]

No attempt is made to optimize the lookup in coding for this first example, and it uses instead a simple [[linear search]] technique - purely to illustrate the concept and demonstrate fewer source lines. To handle all 256 different input values, approximately 265 lines of source code would be required (mainly single line table entries) whereas multiple 'compare and branch' would have normally required around 512 source lines (the size of the [[binary file|binary]] is also approximately halved, each table entry requiring only 4 bytes instead of approximately 8 bytes for a series of 'compare immediate'/branch instructions (For larger input variables, the saving is even greater).

   * ------------------ interpreter --------------------------------------------*
            LM     R14,R0,=A(4,CT1,N)               Set R14=4, R15 --&gt; table, and R0 =no. of entries in table (N)
   TRY      CLC    INPUT1,0(R15)         *********  Found value in table entry ?
            BE     ACTION                * loop  *  YES, Load register pointer to sub-routine from table
            AR     R15,R14               *       *  NO, Point to next entry in CT1 by adding R14 (=4)
            BCT    R0,TRY                *********  Back until count exhausted, then drop through
   .             default action                          ... none of the values in table match, do something else
            LA     R15,4(R15)                       point to default entry (beyond table end)
   ACTION   L      R15,0(R15)                       get pointer into R15,from where R15 points
            BALR   R14,R15                          Perform the sub-routine (&quot;CALL&quot; and return)
            B      END                              go terminate this program
   * ------------------ control table -----------------------------------------*
   *                 | this column of allowable EBCDIC or ASCII values is tested '=' against variable 'input1'
   *                 |      | this column is the 3-byte address of the appropriate subroutine
   *                 v      v
   '''CT1'''      DC     C'A',AL3(ADD)                    START of Control Table (4 byte entry length)
            DC     C'S',AL3(SUBTRACT)
            DC     C'M',AL3(MULTIPLY)
            DC     C'D',AL3(DIVIDE)
   N        EQU    (*-CT1)/4                        number of valid entries in table (total length / entry length)
            DC     C'?',AL3(DEFAULT)                default entry - used on drop through to catch all
   INPUT1   DS     C                                input variable is in this variable
   * ------------------ sub-routines ------------------------------------------*
   ADD      CSECT                                   sub-routine #1 (shown as separate CSECT here but might
   .                                                                alternatively be in-line code)
   .            instruction(s) to add
            BR     R14                              return
   SUBTRACT CSECT                                   sub-routine #2
   .            instruction(s) to subtract
            BR     R14                              return
   . etc..
'''improving the performance of the interpreter in above example '''

:To make a selection in the example above, the average [[instruction path length]] (excluding the subroutine code) is '4n/2 +3', but can easily be reduced, where n = 1 to 64, to a [[Linear time|constant time]] &lt;math&gt;O(1)\,&lt;/math&gt; with a path length of '5' with ''zero comparisons'', if a 256 byte translate table is first utilized to create a ''direct'' index to CT1 from the raw EBCDIC data. Where n = 6, this would then be equivalent to just 3 sequential compare &amp; branch instructions. However, where n&lt;=64, on average it would need approximately 13 ''times'' less instructions than using multiple compares. Where n=1 to 256, on average it would use approximately 42 ''times'' less instructions - since, in this case, one additional instruction would be required (to multiply the index by 4).

'''Improved interpreter''' (up to '''26 times less executed instructions'''  than the above example on average, where n= 1 to 64 and up to 13 times less than would be needed using multiple comparisons).

To handle 64 different input values, approximately 85 lines of source code (or less) are required (mainly single line table entries) whereas multiple 'compare and branch' would require around 128 lines (the size of the [[binary file|binary]] is also almost halved - despite the additional 256 byte table required to extract the 2nd index).
   * ------------------ interpreter --------------------------------------------*
            SR     R14,R14               *********  Set R14=0
   CALC     IC     R14,INPUT1            * calc  *  put EBCDIC byte into lo order bits (24-31) of R14
            IC     R14,CT1X(R14)         *       *  use EBCDIC value as index on table 'CT1X' to get new index
   FOUND    L      R15,CT1(R14)          *********  get pointer to subroutine using index (0,4, 8 etc.)
            BALR   R14,R15                          Perform the sub-routine (&quot;CALL&quot; and return or Default)
            B      END                              go terminate this program
   * --------------- additional translate table (EBCDIC --&gt; pointer table INDEX)  256 bytes----*
   CT1X     DC     12AL1(00,00,00,00,00,00,00,00,00,00,00,00,00,00,00,00)   12 identical sets of 16 bytes of x'00
   *                                                                        representing X'00 - x'BF'
            DC     AL1(00,'''04''',00,00,'''16''',00,00,00,00,00,00,00,00,00,00,00)      ..x'C0' - X'CF'
            DC     AL1(00,00,00,00,'''12''',00,00,00,00,00,00,00,00,00,00,00)      ..x'D0' - X'DF'
            DC     AL1(00,00,'''08''',00,00,00,00,00,00,00,00,00,00,00,00,00)      ..x'E0' - X'EF'
            DC     AL1(00,00,00,00,00,00,00,00,00,00,00,00,00,00,00,00)      ..x'F0' - X'FF'
   * the assembler can be used to automatically calculate the index values and make the values more user friendly
   * (for e.g. '04' could be replaced with the symbolic expression 'PADD-CT1' in table CT1X above)
   * modified CT1 (added a default action when index = 00, single dimension, full 31 bit address)
   '''CT1'''      DC     A(DEFAULT)          index       =00      START of Control Table (4 byte address constants)
   PADD     DC     A(ADD)                          =04
   PSUB     DC     A(SUBTRACT)                     =08
   PMUL     DC     A(MULTIPLY)                     =12
   PDIV     DC     A(DIVIDE)                       =16
   * the rest of the code remains the same as first example

'''Further improved interpreter''' (up to '''21 times less executed instructions (where n&gt;=64)''' than the first example on average and up to 42 ''times'' less than would be needed using multiple comparisons).

To handle 256 different input values, approximately 280 lines of source code or less, would be required (mainly single line table entries), whereas multiple 'compare and branch' would require around 512 lines (the size of the [[binary file|binary]] is also almost halved once more).

   * ------------------ interpreter --------------------------------------------*
            SR     R14,R14               *********  Set R14=0
   CALC     IC     R14,INPUT1            * calc  *  put EBCDIC byte into lo order bits (24-31) of R14
            IC     R14,CT1X(R14)         *       *  use EBCDIC value as index on table 'CT1X' to get new index
            SLL    R14,2                 *       *  '''multiply index by 4 (additional instruction)'''
   FOUND    L      R15,CT1(R14)          *********  get pointer to subroutine using index (0,4, 8 etc.)
            BALR   R14,R15                          Perform the sub-routine (&quot;CALL&quot; and return or Default)
            B      END                              go terminate this program
   * --------------- additional translate table (EBCDIC --&gt; pointer table INDEX)  256 bytes----*
   CT1X     DC     12AL1(00,00,00,00,00,00,00,00,00,00,00,00,00,00,00,00)   12 identical sets of 16 bytes of x'00'
   *                                                                        representing X'00 - x'BF'
            DC     AL1(00,'''01''',00,00,'''04''',00,00,00,00,00,00,00,00,00,00,00)      ..x'C0' - X'CF'
            DC     AL1(00,00,00,00,'''03''',00,00,00,00,00,00,00,00,00,00,00)      ..x'D0' - X'DF'
            DC     AL1(00,00,'''02''',00,00,00,00,00,00,00,00,00,00,00,00,00)      ..x'E0' - X'EF'
            DC     AL1(00,00,00,00,00,00,00,00,00,00,00,00,00,00,00,00)      ..x'F0' - X'FF'
   * the assembler can be used to automatically calculate the index values and make the values more user friendly
   * (for e.g. '01' could be replaced with the symbolic expression 'PADD-CT1/4' in table CT1X above)
   * modified CT1 (index now based on 0,1,2,3,4  not 0,4,8,12,16 to allow all 256 variations)
   '''CT1'''      DC     A(DEFAULT)          index       =00      START of Control Table (4 byte address constants)
   PADD     DC     A(ADD)                          =01
   PSUB     DC     A(SUBTRACT)                     =02
   PMUL     DC     A(MULTIPLY)                     =03
   PDIV     DC     A(DIVIDE)                       =04
   * the rest of the code remains the same as the 2nd example

'''[[C (programming language)|C language]] example'''
This example in [[C (programming language)|C]] uses two tables, the first (CT1) is a simple [[linear search]] one-dimensional lookup table - to obtain an index by matching the input (x), and the second, associated table (CT1p), is a table of addresses of labels to jump to.
&lt;source lang=&quot;c&quot; enclose=&quot;div&quot;&gt;
 static const char  CT1[] = {  &quot;A&quot;,   &quot;S&quot;,        &quot;M&quot;,        &quot;D&quot; };                          /* permitted input  values */
 static const void *CT1p[] = { &amp;&amp;Add, &amp;&amp;Subtract, &amp;&amp;Multiply, &amp;&amp;Divide, &amp;&amp;Default};           /* labels to goto &amp; default*/
 for (int i = 0; i &lt; sizeof(CT1); i++)      /* loop thru ASCII values                                                    */
   {if (x==CT1[i]) goto *CT1p[i]; }       /* found --&gt; appropriate label                                               */
 goto *CT1p[i+1];                           /* not found --&gt; default label                                               */
&lt;/source&gt;
This can be made more efficient if a 256 byte table is used to translate the raw ASCII value (x) directly to a dense sequential index value for use in directly locating the branch address from CT1p (i.e. &quot;[[index mapping]]&quot; with a byte-wide array). It will then execute in [[constant time]] for all possible values of x (If CT1p contained the names of functions instead of labels, the jump could be replaced with a dynamic function call, eliminating the switch-like goto - but decreasing performance by the additional cost of function [[Housekeeping (computing)|housekeeping]]).
&lt;source lang=&quot;c&quot; enclose=&quot;div&quot;&gt;
 static const void *CT1p[] = {&amp;&amp;Default, &amp;&amp;Add, &amp;&amp;Subtract, &amp;&amp;Multiply, &amp;&amp;Divide};
 /* the 256 byte table, below, holds values (1,2,3,4), in corresponding ASCII positions (A,S,M,D), all others set to 0x00 */
 static const char CT1x[]={
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x01', '\x00', '\x00', '\x04', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x03', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x02', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x03', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00',
             '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00', '\x00'};
 /* the following code will execute in constant time, irrespective of the value of the input character (x)                    */
 i = CT1x(x);            /* extract the correct subroutine index from table CT1x using its ASCII value as an index initially  */
 goto *CT1p[i];          /* goto (Switch to) the label corresponding to the index (0=default,1= Add,2= Subtract,.) - see CT1p */
&lt;/source&gt;

The next example below illustrates how a similar effect can be achieved in languages that do '''not''' support pointer definitions in data structures but '''do''' support indexed branching to a subroutine - contained within a ([[zero-based|0-based]]) array of subroutine pointers. The table (CT2) is used to extract the index (from 2nd column) to the pointer array (CT2P). If pointer arrays are ''not'' supported, a SWITCH statement or equivalent can be used to alter the control flow to one of a sequence of program labels (e.g.: case0,case1,case2,case3,case4) which then either process the input directly, or else perform a call (with return) to the appropriate subroutine (default,Add,Subtract,Multiply or Divide,..) to deal with it.

'''CT2'''
:{| class=&quot;wikitable&quot;
! input 1!! '''subr #'''
|-
| '''A''' || '''1'''
|-
| '''S''' || '''2'''
|-
| '''M''' || '''3'''
|-
| '''D''' || '''4'''
|-
| '''?''' || '''0'''
|}
As in above examples, it is possible to very efficiently translate the potential [[ASCII]] input values (A,S,M,D or unknown) into a pointer array index without actually using a table lookup, but is shown here as a table for consistency with the first example.
::'''CT2P''' pointer array
::{| class=&quot;wikitable&quot;
! [[pointer (computer programming)|pointer]] [[Array data structure|array]]!!
|-
| '''--&gt;default'''
|-
| '''--&gt;Add'''
|-
| '''--&gt;Subtract'''
|-
| '''--&gt;Multiply'''
|-
| '''--&gt;Divide'''
|-
| '''--&gt;?other'''
|}

Multi-dimensional control tables can be constructed (i.e. customized) that can be 'more complex' than the above examples that might test for multiple conditions on multiple inputs or perform more than one 'action', based on some matching criteria. An 'action' can include a pointer to another subordinate control table. The simple example below has had an ''implicit'' 'OR' condition incorporated as an extra column (to handle lower case input, however in this instance, this could equally have been handled simply by having an extra entry for each of the lower case characters specifying the same subroutine identifier as the upper case characters). An extra column to count the actual run-time events for each input as they occur is also included.

'''CT3'''
:{| class=&quot;wikitable&quot;
! input 1!!alternate!! '''subr #'''!! '''count'''
|-
| '''A''' ||'''a'''|| '''1'''|| '''0'''
|-
| '''S''' ||'''s'''|| '''2'''|| '''0'''
|-
| '''M''' ||'''m'''|| '''3'''|| '''0'''
|-
| '''D''' ||'''d'''|| '''4'''|| '''0'''
|-
| '''?''' ||'''?'''|| '''0'''|| '''0'''
|}
The control table entries are then much more similar to conditional statements in [[procedural language]]s but, crucially, without the actual (language dependent) conditional statements (i.e. instructions) being present (the generic code is ''physically'' in the interpreter that processes the table entries, not in the table itself - which simply embodies the program logic via its structure and values).

In tables such as these, where a series of similar table entries defines the entire logic, a table entry number or pointer may effectively take the place of a [[program counter]] in more conventional programs and may be reset in an 'action', also specified in the table entry. The example below (CT4) shows how extending the earlier table, to include a 'next' entry (and/or including an 'alter flow' ([[Branch (computer science)|jump]]) subroutine) can create a [[Program loop|loop]] (This example is actually not the most efficient way to construct such a control table but, by demonstrating a gradual 'evolution' from the first examples above, shows how additional columns can be used to modify behaviour.) The fifth column demonstrates that more than one action can be initiated with a single table entry - in this case an action to be performed ''after'' the normal processing of each entry ('-' values mean 'no conditions' or 'no action').

[[Structured programming]] or [[Structured programming|&quot;Goto-less&quot; code]], (incorporating the equivalent of '[[Do while loop|DO WHILE]]' or '[[for loop]]' constructs), can also be accommodated with suitably designed and 'indented' control table structures.

'''CT4''' (a complete 'program' to read input1 and process,  repeating until 'E' encountered)
:{| class=&quot;wikitable&quot;
! input 1!!alternate!! '''subr #'''!! '''count'''!! '''jump'''
|-
| '''-''' ||'''-'''|| '''5'''|| '''0''' || '''-'''
|-
| '''E''' ||'''e'''|| '''7'''|| '''0''' || '''-'''
|-
| '''A''' ||'''a'''|| '''1'''|| '''0''' || '''-'''
|-
| '''S''' ||'''s'''|| '''2'''|| '''0''' || '''-'''
|-
| '''M''' ||'''m'''|| '''3'''|| '''0''' || '''-'''
|-
| '''D''' ||'''d'''|| '''4'''|| '''0''' || '''-'''
|-
| '''?''' ||'''?'''|| '''0'''|| '''0''' || '''-'''
|-
| '''-''' ||'''-'''|| '''6'''|| '''0''' || '''1'''
|}
::'''CT4P''' pointer array
::{| class=&quot;wikitable&quot;
! [[pointer (computer programming)|pointer]] [[Array data structure|array]]!!
|-
| '''--&gt;Default'''
|-
| '''--&gt;Add'''
|-
| '''--&gt;Subtract'''
|-
| '''--&gt;Multiply'''
|-
| '''--&gt;Divide'''
|-
| '''--&gt;Read Input1'''
|-
| '''--&gt;Alter flow'''
|-
| '''--&gt;End'''
|}

===Table-driven rating===
In the specialist field of [[telecommunications rating]] (concerned with the determining the cost of a particular call),
'''table-driven rating''' techniques illustrate the use of control tables in applications where the rules may change frequently because of market forces. The tables that determine the charges may be changed at short notice by non-programmers in many cases.&lt;ref&gt;Carl Wright, Service Level Corpo. (2002) ''[http://www.servicelevel.net/rating_matters/newsletters/issue12.htm Program Code Based vs. Table-driven vs. Rule-Based Rating]'', Rating Matters issue n. 12,  13 November 2002 ISSN: 1532-1886&lt;/ref&gt;&lt;ref&gt;Brian E. Clauser, Melissa J. Margolis, Stephen G. Clyman, Linette P. Ross (1997) ''[http://links.jstor.org/sici?sici=0022-0655%28199722%2934%3A2%3C141%3ADOASAF%3E2.0.CO%3B2-%23 Development of Automated Scoring Algorithms for Complex Performance Assessments: A Comparison of Two Approaches]'' Journal of Educational Measurement, Vol. 34, No. 2 (Summer, 1997), pp. 141-161&lt;/ref&gt;

If the algorithms are not pre-built into the interpreter (and therefore require additional runtime interpretation of an expression held in the table), it is known as &quot;Rule-based Rating&quot; rather than table-driven rating (and consequently consumes significantly more overhead).

===Spreadsheets===
A [[spreadsheet]] data sheet can be thought of as a two dimensional control table, with the non empty cells representing data to the underlying spreadsheet program (the interpreter). The cells containing formula are usually prefixed with an equals sign and simply designate a special type of data input that dictates the processing of other referenced cells - by altering the control flow within the interpreter. It is the externalization of formulae from the underlying interpreter that clearly identifies both spreadsheets, and the above cited &quot;rule based rating&quot; example as readily identifiable instances of the use of control tables by non programmers.

==Programming paradigm==
If the control tables technique could be said to belong to any particular [[programming paradigm]], the closest analogy might be [[Automata-based programming]] or [[Reflection (computer science)|&quot;reflective&quot;]] (a form of [[metaprogramming]] - since the table entries could be said to 'modify' the behaviour of the interpreter). The interpreter itself however, and the subroutines, can be programmed using any one of the available paradigms or even a mixture. The table itself can be essentially a collection of &quot;[[raw data]]&quot; values that do not even need to be compiled and could be read in from an external source (except in specific, platform dependent, implementations using memory pointers directly for greater efficiency).

==Analogy to bytecode / virtual machine instruction set==
A multi-dimensional control table has some conceptual similarities to [[bytecode]] operating on a [[virtual machine]], in that a [[platform dependent]] [[Interpreter (computing)|&quot;interpreter&quot;]] program is usually required to perform the actual execution (that is largely conditionally determined by the tables content). There are also some conceptual similarities to the recent [[Common Intermediate Language]] (CIL) in the aim of creating a common intermediate 'instruction set' that is independent of platform (but unlike CIL, no pretentions to be used as a common resource for other languages). [[p-code machine|P-code]] can also be considered a similar but earlier implementation with origins as far back as 1966.

==Instruction fetch==
When a multi-dimensional control table is used to determine program flow, the normal &quot;hardware&quot; [[Program Counter]] function is effectively simulated with either a [[Pointer (computer programming)|pointer]] to the first (or next) table entry or else an [[array index|index]] to it. &quot;Fetching&quot; the instruction involves decoding the ''data'' in that table entry - without necessarily copying all or some of the data within the entry first. Programming languages that are able to use [[Pointer (computer programming)|pointer]]s have the dual advantage that less [[Computational overhead|overhead]] is involved, both in accessing the contents and also advancing the counter to point to the next table entry after execution. Calculating the next 'instruction' address (i.e. table entry) can even be performed as an optional additional action of every individual table entry allowing [[Program loops|loops]] and or [[Branch (computer science)|jump]] instructions at any stage.

==Monitoring control table execution==
The interpreter program can optionally save the program counter (and other relevant details depending upon instruction type) at each stage to record a full or partial trace of the actual program flow for [[debugging]] purposes, [[Hot spot (computer science)|hot spot]] detection, [[code coverage]] analysis and [[Profiling (computer programming)|performance analysis]] (see examples CT3 &amp; CT4 above).

==Advantages==
* clarity - [[Table (information)|Information tables]] are [[Ubiquitous computing|ubiquitous]] and mostly [[inherently]] [[understanding|understood]] even by the [[general public]] (especially [[fault diagnosis|fault diagnostic]] tables in  [[User guide|product guides]])
* portability - can be designed to be 100% language independent (and platform independent - except for the interpreter)
* flexibility - ability to execute either [[Language primitive|primitives]] or [[subroutine]]s transparently and be custom designed to suit the problem
* compactness - table usually shows condition/action pairing side-by-side (without the usual platform/language implementation dependencies), often also resulting in
** [[binary file]] - reduced in size through less duplication of instructions
** [[Source code|source]] file - reduced in size through elimination of multiple conditional statements
** improved program load (or download) speeds
* maintainability - tables often reduce the number of source lines needed to be maintained v. multiple compares
* locality of reference - compact tables structures result in tables remaining in [[cache (computing)|cache]]
* code re-use - the &quot;interpreter&quot; is usually reusable. Frequently it can be easily adapted to new programming tasks using precisely the same technique and can grow 'organically' becoming, in effect, a [[standard library]] of tried and tested [[subroutines]], controlled by the table definitions.
* [[algorithmic efficiency|efficiency]] - systemwide optimization possible. Any performance improvement to the interpreter usually improves ''all'' applications using it (see examples in 'CT1' above).
* extensible - new 'instructions' can be added - simply by extending the interpreter
* interpreter can be written like an application program
Optionally:-
* the interpreter can be [[Introspection|introspective]] and &quot;self [[Optimization (computer science)|optimize]]&quot; using runtime [[Software metric|metrics]] collected within the table itself (see CT3 and CT4 - with entries that could be periodically sorted by descending count). The interpreter can also optionally choose the most efficient lookup technique dynamically from metrics gathered at run-time (e.g. size of array, range of values, sorted or unsorted)
* [[dynamic dispatch]] - common functions can be pre-loaded and less common functions fetched only on first encounter to reduce [[memory]] usage. In-table [[memoization]] can be employed to achieve this.
* The interpreter can have debugging, trace and monitor features built-in - that can then be switched on or off at will according to test or 'live' mode
* control tables can be built 'on-the-fly' (according to some user input or from parameters) and then executed by the interpreter (without building code literally).

==Disadvantages==
* training requirement - application programmers are not usually trained to produce generic solutions
The following mainly apply to their use in multi-dimensional tables, not the one-dimensional tables discussed earlier.
*  [[Computational overhead|overhead]] - some increase because of extra level of [[Indirection (programming)|indirection]] caused by virtual instructions having to be 'interpreted' (this however can usually be more than offset by a well designed generic interpreter taking full advantage of efficient direct translate, search and conditional testing techniques that may not otherwise have been utilized)
* Complex [[Expression (programming)|expression]]s cannot always be used ''directly'' in data table entries for comparison purposes
:(these 'intermediate values' can however be calculated beforehand instead within a subroutine and their values referred to in the conditional table entries. Alternatively, a subroutine can perform the complete complex conditional test (as an unconditional 'action') and, by setting a [[Truth bit|truth flag]] as its result, it can then be tested in the next table entry. See [[Structured program theorem]])

==Quotations==
{{cquote2|&quot;Multiway branching is an important programming technique which is all too often replaced by an inefficient sequence of if tests. [[Peter Naur]] recently wrote me that he considers the use of tables to control program flow as a basic idea of computer science that has been nearly forgotten; but he expects it will be ripe for rediscovery any day now. It is the key to efficiency in all the best compilers I have studied.&quot;|
: &quot;''Structured Programming with go to Statements''&quot; by [[Donald Knuth]]}}

{{cquote2|&quot;There is another way to look at a program written in interpretative language. It may be regarded as a series of subroutine calls, one after another. Such a program may in fact be expanded into a long sequence of calls on subroutines, and, conversely, such a sequence can usually be packed into a coded form that is readily interpreted. The advantage of interpretive techniques are the compactness of representation, the machine independence, and the increased diagnostic capability. An interpreter can often be written so that the amount of time spent in interpretation of the code itself and branching to the appropriate routine is negligible&quot;|

: &quot;''The Art of Computer Programming''&quot; Volume 1, 1997, page 202 by [[Donald Knuth]],
: renowned [[computer science|computer scientist]] and [[Emeritus|Professor Emeritus]] of the Art of Computer Programming&lt;ref&gt;[http://www-cs-faculty.stanford.edu/~knuth/ http://www-cs-faculty.stanford.edu/~knuth/].&lt;/ref&gt; at [[Stanford University]].}}

{{cquote2|&quot;The space required to represent a program can often be decreased by the use of interpreters in which common sequences of operations are represented compactly. A typical example is the use of a finite-state machine to encode a complex protocol or lexical format into a small table&quot;|
: &quot;''Writing Efficient Programs''[http://www.hipecc.wichita.edu/bentley.htm]&quot; by [[Jon Bentley]]}}

{{cquote2|&quot;Jump tables can be especially efficient if the range tests can be omitted. For example, if the control value is an enumerated type (or a character) then it can only contain a small fixed range of values and a range test is redundant provided the jump table is large enough to handle all possible values&quot;|
: &quot;''Compiler Code Generation for Multiway Branch Statements as a Static Search Problem''&quot; by David.A. SPULER}}
{{Refimprove|date=February 2009}}

==See also==
*[[Automata-based programming]]
*[[Database-centric architecture]]
*[[Data-driven testing]]
*[[Decision table]]
*[[Finite-state machine]]
*[[Keyword-driven testing]]
*[[Pointer (computer programming)]]
*[[Switch statement]] - [[multiway branch]]ing to one of a number of [[Label (programming language)|program label]]s, depending upon a single input variable
*[[Threaded code]]
*[[Threaded code#Token threading|Token threading]]

==Notes==
{{reflist}}

==References==
* [http://www.methodsandtools.com/archive/archive.php?id=39 Decision Table Based Methodology ]
* [http://pplab.snu.ac.kr/courses/adv_pl05/papers/p261-knuth.pdf Structured Programming with go to Statements]  by [[Donald Knuth]]
* [http://www.citeulike.org/user/derek_farn/article/2303550 Compiler code generation for multiway branch statements as a static search problem]  1I994, by David A. Spuler

==External links==
* [http://www.microsoft.com/technet/scriptcenter/resources/pstips/jan08/pstip0111.mspx Switch statement in Windows PowerShell] describes extensions to standard switch statement (providing some similar features to control tables)
* [http://www.cs.auckland.ac.nz/~j-hamer/07.211/C/C-Lesson/C-LESSON.4 Control Table example in &quot;C&quot; language using pointers], by Christopher Sawtell c1993, Department of Computer Science, [[University of Auckland]]
* [http://www.dkl.com/html/CMFiles/53table_driven_design.pdf Table driven design ] by Wayne Cunneyworth of Data Kinetics
* [http://commons.oreilly.com/wiki/index.php/From_Requirements_to_Tables_to_Code_and_Tests From Requirements to Tables to Code and Tests] By George Brooke
* [http://portal.acm.org/citation.cfm?id=362041.362195&amp;coll=GUIDE&amp;dl=GUIDE&amp;CFID=60499203&amp;CFTOKEN=67618699 Some comments on the use of ambiguous decision tables and their conversion to computer programs] by P. J. H. King and R. G. Johnson, Univ. of London, London, UK
* [http://portal.acm.org/citation.cfm?id=364113&amp;dl=GUIDE&amp;coll=GUIDE&amp;CFID=60499203&amp;CFTOKEN=67618699 Ambiguity in limited entry decision tables] by P. J. H. King
* [http://portal.acm.org/citation.cfm?id=365896&amp;dl=GUIDE&amp;coll=GUIDE&amp;CFID=60499203&amp;CFTOKEN=67618699 Conversion of decision tables to computer programs by rule mask techniques] by P. J. H. King
*[http://ols.fedoraproject.org/GCC/Reprints-2008/sayle-reprint.pdf A Superoptimizer Analysis of Multiway Branch Code Generation]  section 3.9, page 16 index mapping
* [http://www.netrino.com/node/137 Jump Tables via Function Pointer Arrays in C/C++] Jones, Nigel. &quot;Arrays of Pointers to Functions [http://www.rmbconsulting.us/Publications/PointerToFunction.pdf]&quot; Embedded Systems Programming, May 1999.
* [http://stats.grok.se/en/200912/control_table Page view statistics for this article for December 2009]
{{Use dmy dates|date=June 2011}}
* [http://is.ifmo.ru/download/modelingsoftwarewithfinitestatemachinesapracticalapproach.pdf Modelling software with finite state machines - a practical approach]
* [http://docs.google.com/viewer?a=v&amp;q=cache:dytF6h4pD-4J:lss.fnal.gov/archive/tm/TM-1508.pdf+%22finite+state+table%22&amp;hl=en&amp;gl=uk&amp;pid=bl&amp;srcid=ADGEESgaFAmVjB_QzcxIn2MmK7N6QeffWYyHfUGTZG7zfB0DKoTaJUc2m_uncWXIteXNu_oCi570cGtCCjJeHYFUURZNoGdZMgn62OgTldHus4dTTN6BNt_vSce0jBoayojMcIGOfhcj&amp;sig=AHIEtbQ_1D3cO9a-FChv2QDKZqlzZ4oT4Q Finite State Tables for General Computer Programming Applications January 1988] by Mark Leininger
* [http://msdn.microsoft.com/en-us/library/aa561913%28v=bts.20%29.aspx MSDN:Trigger-Based Event Processing]

[[Category:Control flow| ]]
[[Category:Data structures]]
[[Category:Compiler construction]]</text>
      <sha1>p8uj23jjzmjcp0wt0mvx5njlf9v8zgz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Disjoint-set data structure</title>
    <ns>0</ns>
    <id>1037551</id>
    <revision>
      <id>625342295</id>
      <parentid>621360262</parentid>
      <timestamp>2014-09-13T07:32:11Z</timestamp>
      <contributor>
        <ip>50.169.36.94</ip>
      </contributor>
      <comment>/* History */</comment>
      <text xml:space="preserve" bytes="15050">[[File:Dsu disjoint sets init.svg|thumb|360px|''MakeSet'' creates 8 singletons.]]
[[File:Dsu disjoint sets final.svg|thumb|360px|After some operations of ''Union'', some sets are grouped together.]]
In [[computing]], a '''disjoint-set data structure''', also called a '''union–find data structure''' or '''merge–find set''', is a [[data structure]] that keeps track of a [[Set (mathematics)|set]] of elements [[partition of a set|partitioned]] into a number of [[disjoint sets|disjoint]] (nonoverlapping) subsets. It supports two useful operations:

* ''Find'': Determine which subset a particular element is in. Find typically returns an item from this set that serves as its &quot;representative&quot;; by comparing the result of two Find operations, one can determine whether two elements are in the same subset.
* ''Union'': Join two subsets into a single subset.

The other important operation, ''MakeSet'', which makes a set containing only a given element (a [[singleton (mathematics)|singleton]]), is generally trivial. With these three operations, many practical [[partitioning problem]]s can be solved (see the ''Applications'' section).

In order to define these operations more precisely, some way of representing the sets is needed. One common approach is to select a fixed element of each set, called its ''representative'', to represent the set as a whole. Then, ''Find''(x) returns the representative of the set that ''x'' belongs to, and ''Union'' takes two set representatives as its arguments.

== Disjoint-set linked lists ==
A simple approach to creating a disjoint-set data structure is to create a [[linked list]] for each set. The element at the head of each list is chosen as its representative.

''MakeSet'' creates a list of one element. ''Union'' appends the two lists, a constant-time operation. The drawback of this implementation is that ''Find'' requires [[Big-O notation|Ω]](''n'') or linear time to traverse the list backwards from a given element to the head of the list.

This can be avoided by including in each linked list node a pointer to the head of the list; then ''Find'' takes constant time, since this pointer refers directly to the set representative. However, ''Union'' now has to update each element of the list being appended to make it point to the head of the new combined list, requiring [[Big-O notation|Ω]](''n'') time.

When the length of each list is tracked, the required time can be improved by always appending the smaller list to the longer. Using this ''weighted-union heuristic'', a sequence of ''m'' ''MakeSet'', ''Union'', and ''Find'' operations on ''n'' elements requires O(''m''&amp;nbsp;+&amp;nbsp;''n''log&amp;nbsp;''n'') time.&lt;ref name=&quot;IntroductionToAlgorithms&quot;&gt;{{Citation |first1=Thomas H. |last1=Cormen |author1-link=Thomas H. Cormen |first2=Charles E. |last2=Leiserson |author2-link=Charles E. Leiserson |first3=Ronald L. |last3=Rivest |author3-link=Ronald L. Rivest |first4=Clifford |last4=Stein |author4-link=Clifford Stein |title=[[Introduction to Algorithms]] |edition=Second |publisher=MIT Press |year=2001 |isbn=0-262-03293-7 |chapter=Chapter 21: Data structures for Disjoint Sets |pages=498&amp;ndash;524 }}&lt;/ref&gt; For asymptotically faster operations, a different data structure is needed.

=== Analysis of the naive approach ===
We now explain the bound &lt;math&gt;O(n \log(n))&lt;/math&gt; above.

Suppose you have a collection of lists and each node of each list contains an object, the name of the list to which it belongs, and the number of elements in that list. Also assume that the sum of the number of elements in all lists is &lt;math&gt;n&lt;/math&gt; (i.e. there are &lt;math&gt;n&lt;/math&gt; elements overall). We wish to be able to merge any two of these lists, and update all of their nodes so that they still contain the name of the list to which they belong. The rule for merging the lists &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; is that if &lt;math&gt;A&lt;/math&gt; is larger than &lt;math&gt;B&lt;/math&gt; then merge the elements of &lt;math&gt;B&lt;/math&gt; into &lt;math&gt;A&lt;/math&gt; and update the elements that used to belong to &lt;math&gt;B&lt;/math&gt;, and vice versa.

Choose an arbitrary element of list &lt;math&gt;L&lt;/math&gt;, say &lt;math&gt;x&lt;/math&gt;. We wish to count how many times in the worst case will &lt;math&gt;x&lt;/math&gt; need to have the name of the list to which it belongs updated. The element &lt;math&gt;x&lt;/math&gt; will only have its name updated when the list it belongs to is merged with another list of the same size or of greater size. Each time that happens, the size of the list to which &lt;math&gt;x&lt;/math&gt; belongs at least doubles. So finally, the question is &quot;how many times can a number double before it is the size of &lt;math&gt;n&lt;/math&gt;?&quot; (then the list containing &lt;math&gt;x&lt;/math&gt; will contain all &lt;math&gt;n&lt;/math&gt; elements). The answer is exactly &lt;math&gt;\log_2(n)&lt;/math&gt;. So for any given element of any given list in the structure described, it will need to be updated &lt;math&gt;\log_2(n)&lt;/math&gt; times in the worst case. Therefore updating a list of &lt;math&gt;n&lt;/math&gt; elements stored in this way takes &lt;math&gt;O(n \log(n))&lt;/math&gt; time in the worst case. A find operation can be done in &lt;math&gt;O(1)&lt;/math&gt; for this structure because each node contains the name of the list to which it belongs.

A similar argument holds for merging the trees in the data structures discussed below. Additionally, it helps explain the time analysis of some operations in the [[binomial heap]] and [[Fibonacci heap]] data structures.

== Disjoint-set forests ==
Disjoint-set forests are data structures where each set is represented by a [[tree data structure]], in which each node holds a [[reference]] to its parent node (see [[spaghetti stack]]). They were first described by [[Bernard A. Galler]] and [[Michael J. Fischer]] in 1964,&lt;ref&gt;{{Citation |first=Bernard A. |last=Galler |author1-link=Bernard A. Galler |first2=Michael J. |last2=Fischer |author2-link=Michael J. Fischer |title=An improved equivalence algorithm |journal=[[Communications of the ACM]] |volume=7 |issue=5 |date=May 1964 |pages=301–303 |url=http://portal.acm.org/citation.cfm?doid=364099.364331 |doi=10.1145/364099.364331}}. The paper originating disjoint-set forests.&lt;/ref&gt; although their precise analysis took years.

In a disjoint-set forest, the representative of each set is the root of that set's tree. ''Find'' follows parent nodes until it reaches the root. ''Union'' combines two trees into one by attaching the root of one to the root of the other. One way of implementing these might be:

  '''function''' ''MakeSet''(x)
      x.parent := x

  '''function''' ''Find''(x)
      if x.parent == x
         return x
      else
         return ''Find''(x.parent)

  '''function''' ''Union''(x, y)
      xRoot := ''Find''(x)
      yRoot := ''Find''(y)
      xRoot.parent := yRoot

In this naive form, this approach is no better than the linked-list approach, because the tree it creates can be highly unbalanced; however, it can be enhanced in two ways.

The first way, called ''union by rank'', is to always attach the smaller tree to the root of the larger tree. Since it is the depth of the tree that affects the running time, the tree with smaller depth gets added under the root of the deeper tree, which only increases the depth if the depths were equal.  In the context of this algorithm, the term ''rank'' is used instead of ''depth'' since it stops being equal to the depth if path compression (described below) is also used.  One-element trees are defined to have a rank of zero, and whenever two trees of the same rank ''r'' are united, the rank of the result is ''r''+1. Just applying this technique alone yields a worst-case running-time of &lt;math&gt;O(\log n)&lt;/math&gt; per ''MakeSet'', ''Union'', or ''Find'' operation. Pseudocode for the improved &lt;code&gt;MakeSet&lt;/code&gt; and &lt;code&gt;Union&lt;/code&gt;:

  '''function''' ''MakeSet''(x)
      x.parent := x
      x.rank   := 0

  '''function''' ''Union''(x, y)
      xRoot := ''Find''(x)
      yRoot := ''Find''(y)
      if xRoot == yRoot
          return
 
      &lt;u&gt;// x and y are not already in same set. Merge them.&lt;/u&gt;
      if xRoot.rank &lt; yRoot.rank
          xRoot.parent := yRoot
      else if xRoot.rank &gt; yRoot.rank
          yRoot.parent := xRoot
      else
          yRoot.parent := xRoot
          xRoot.rank := xRoot.rank + 1

The second improvement, called ''path compression'', is a way of flattening the structure of the tree whenever ''Find'' is used on it. The idea is that each node visited on the way to a root node may as well be attached directly to the root node; they all share the same representative. To effect this, as &lt;code&gt;Find&lt;/code&gt; recursively traverses up the tree, it changes each node's parent reference to point to the root that it found. The resulting tree is much flatter, speeding up future operations not only on these elements but on those referencing them, directly or indirectly. Here is the improved &lt;code&gt;Find&lt;/code&gt;:

  '''function''' ''Find''(x)
      if x.parent != x
         x.parent := ''Find''(x.parent)
      return x.parent

These two techniques complement each other; applied together, the [[amortized analysis|amortized]] time per operation is only &lt;math&gt;O(\alpha(n))&lt;/math&gt;, where &lt;math&gt;\alpha(n)&lt;/math&gt; is the [[inverse function|inverse of the function]] &lt;math&gt;n = f(x) = A(x,x)&lt;/math&gt;, and &lt;math&gt;A&lt;/math&gt; is the extremely fast-growing [[Ackermann function]]. Since &lt;math&gt;\alpha(n)&lt;/math&gt; is the inverse of this function, &lt;math&gt;\alpha(n)&lt;/math&gt; is less than 5 for all remotely practical values of &lt;math&gt;n&lt;/math&gt;. Thus, the amortized running time per operation is effectively a small constant.

In fact, this is [[asymptotically optimal]]: [[Michael Fredman|Fredman]] and Saks showed in 1989 that &lt;math&gt;\Omega(\alpha(n))&lt;/math&gt; words must be accessed by ''any'' disjoint-set data structure per operation on average.&lt;ref&gt;{{Citation |first=M. |last=Fredman |authorlink=Michael Fredman |first2=M. |last2=Saks |title=The cell probe complexity of dynamic data structures |journal=Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing |pages=345&amp;ndash;354 |date=May 1989 |quote=Theorem 5: Any CPROBE(log ''n'') implementation of the set union problem requires &amp;Omega;(''m'' &amp;alpha;(''m'', ''n'')) time to execute ''m'' Find's and ''n''&amp;minus;1 Union's, beginning with ''n'' singleton sets. }}&lt;/ref&gt;

== Applications ==
Disjoint-set data structures model the [[Partition of a set|partitioning of a set]], for example to keep track of the [[Connected component (graph theory)|connected components]] of an [[undirected graph]]. This model can then be used to determine whether two vertices belong to the same component, or whether adding an edge between them would result in a cycle.  The Union–Find algorithm is used in high-performance implementations of [[Unification (computer science)|unification]].&lt;ref&gt;{{cite journal |last1=Knight |first1=Kevin|year=1989 |title=Unification: A multidisciplinary survey |journal=ACM Computing Surveys|pages=93&amp;ndash;124 |url=http://portal.acm.org/citation.cfm?id=62030|doi=10.1145/62029.62030 |volume=21}}&lt;/ref&gt;

This data structure is used by the [[Boost Graph Library]] to implement its [http://www.boost.org/libs/graph/doc/incremental_components.html Incremental Connected Components] functionality. It is also used  for implementing [[Kruskal's algorithm]] to find the [[minimum spanning tree]] of a graph.

Note that the implementation as disjoint-set forests doesn't allow deletion of edges—even without path compression or the rank heuristic.

== History ==
While the ideas used in disjoint-set forests have long been familiar, [[Robert Tarjan]] was the first to prove the upper bound (and a restricted version of the lower bound) in terms of the inverse [[Ackermann function]], in 1975.&lt;ref&gt;{{cite journal |last1=Tarjan |first1=Robert Endre |author1-link=Robert E. Tarjan |year=1975 |title=Efficiency of a Good But Not Linear Set Union Algorithm |journal=Journal of the ACM |volume=22 |issue=2 |pages=215&amp;ndash;225 |url=http://portal.acm.org/citation.cfm?id=321884 |doi=10.1145/321879.321884 }}&lt;/ref&gt;
Until this time the best bound on the time per operation, proven by [[John Hopcroft|Hopcroft]] and [[Jeffrey Ullman|Ullman]],&lt;ref&gt;{{cite journal |last1=Hopcroft |first1=J. E. |author1-link=John Hopcroft |last2=Ullman |first2=J. D. |author2-link=Jeffrey Ullman |year=1973 |title=Set Merging Algorithms |journal=SIAM Journal on Computing |volume=2 |issue=4 |pages=294&amp;ndash;303 |doi=10.1137/0202024}}&lt;/ref&gt;
was [[Proof of O(log*n) time complexity of union–find|O(log&lt;sup&gt;*&lt;/sup&gt; n)]], the [[iterated logarithm]] of n, another slowly growing function (but not quite as slow as the inverse Ackermann function).

[[Robert E. Tarjan|Tarjan]] and [[Jan van Leeuwen|Van Leeuwen]] also developed one-pass ''Find'' algorithms that are more efficient in practice while retaining the same worst-case complexity.&lt;ref&gt;{{Citation |first=Robert E. |last=Tarjan |author1-link=Robert E. Tarjan |first2=Jan |last2=van Leeuwen |author2-link=Jan van Leeuwen |title=Worst-case analysis of set union algorithms |journal=Journal of the ACM |volume=31 |issue=2 |pages=245–281 |year=1984 |doi= 10.1145/62.2160}}&lt;/ref&gt;

In 2007, Sylvain Conchon and Jean-Christophe Filliâtre developed a [[persistent data structure|persistent]] version of the disjoint-set forest data structure, allowing previous versions of the structure to be efficiently retained, and formalized its correctness using the [[proof assistant]] [[Coq]].&lt;ref&gt;{{Citation |first=Sylvain |last=Conchon |first2=Jean-Christophe |last2=Filliâtre |contribution=A Persistent Union-Find Data Structure |title=ACM SIGPLAN Workshop on ML |location=Freiburg, Germany |date=October 2007|url=https://www.lri.fr/~filliatr/puf/}}&lt;/ref&gt; However, the implementation is only asymptotically if used ephemerally or when you will be consistently using the same version of the structure repeatedly with limited backtracking.

==See also==
*[[Partition refinement]], a different data structure for maintaining disjoint sets, with updates that split sets apart rather than merging them together

== References ==
{{reflist|30em}}

== External links ==
* [http://www.boost.org/libs/disjoint_sets/disjoint_sets.html C++ implementation], part of the [[Boost C++ libraries]]
* [http://www.lix.polytechnique.fr/~nielsen/Srmjava.java A Java implementation with an application to color image segmentation, Statistical Region Merging (SRM), IEEE Trans. Pattern Anal. Mach. Intell. 26(11): 1452–1458 (2004)]
* [http://www.cs.unm.edu/~rlpm/499/uf.html Java applet: A Graphical Union–Find Implementation], by Rory L. P. McGuire
* ''[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.8354 Wait-free Parallel Algorithms for the Union–Find Problem]'', a 1994 paper by Richard J. Anderson and Heather Woll describing a parallelized version of Union–Find that never needs to block
* [http://code.activestate.com/recipes/215912-union-find-data-structure/ Python implementation]
* [http://www.mathblog.dk/disjoint-set-data-structure/ Visual explanation and C# code]

[[Category:Data structures]]
[[Category:Search algorithms]]</text>
      <sha1>ftr3m5mj861gonwpkqfbydn4b0np71b</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Search data structure</title>
    <ns>0</ns>
    <id>24019691</id>
    <revision>
      <id>616701114</id>
      <parentid>615629432</parentid>
      <timestamp>2014-07-12T21:23:12Z</timestamp>
      <contributor>
        <username>Dalton Quinn</username>
        <id>21675110</id>
      </contributor>
      <minor/>
      <comment>/* Asymptotic amortized worst-case analysis */</comment>
      <text xml:space="preserve" bytes="4300">{{unreferenced|date=May 2012}}

In [[computer science]], a '''search data structure''' is any [[data structure]] that allows the efficient retrieval of specific items from a [[set (mathematics)|set]] of items, such as a specific [[record (computer science)|record]] from a [[database]].

The simplest, most general, and least efficient search structure is merely an unordered sequential [[list (computing)|list]] of all the items. Locating the desired item in such a list, by the [[linear search]] method, inevitably requires a number of operations proportional to the number ''n'' of items, in the [[worst case complexity|worst case]] as well as in the [[average case complexity|average case]].  Useful search data structures allow faster retrieval; however, they are limited to queries of some specific kind.  Moreover, since the cost of building such structures is at least proportional to ''n'', they only pay off if several queries are to be performed on the same database (or on a database that changes little between queries).

'''Static''' search structures are designed for answering many [[Information retrieval|queries]] on a fixed database; '''dynamic''' structures also allow insertion, deletion, or modification of items between successive queries. In the dynamic case, one must also consider the cost of fixing the search structure to account for the changes in the database.

==Classification==

The simplest kind of query is to locate a record that has a specific field (the ''key'') equal to a specified value ''v''.  Other common kinds of query are &quot;find the item with smallest (or largest) key value&quot;, &quot;find the item with largest key value not exceeding ''v''&quot;, &quot;find all items with key values between specified bounds ''v''&lt;sub&gt;min&lt;/sub&gt; and ''v''&lt;sub&gt;max&lt;/sub&gt;&quot;.

In certain databases the key values may be points in some [[dimension (mathematics)|multi-dimensional space]].  For example, the key may be a geographic position ([[latitude]] and [[longitude]]) on the [[Earth]].  In that case, common kinds of queries are ''find the record with a key closest to a given point ''v''&quot;, or &quot;find all items whose key lies at a given distance from ''v''&quot;, or &quot;find all items within a specified region ''R'' of the space&quot;.

A common special case of the latter are simultaneous range queries on two or more simple keys, such as &quot;find all employee records with salary between 50,000 and 100,000 and hired between 1995 and 2007&quot;.

===Single ordered keys===
*[[Array data structure|Array]] if the key values span a moderately compact interval.
*Priority-sorted list; see [[linear search]]
*Key-sorted array; see [[binary search]]
*[[Self-balancing binary search tree]]
*[[Hash table]]

===Finding the smallest element===
*[[Heap (data structure)|Heap]]

====Asymptotic amortized worst-case analysis====

In this table, the [[asymptotic analysis|asymptotic]] [[big-O notation|notation O(''f''(''n''))]] means &quot;not exceeding some fixed multiple of ''f''(''n'') in the worst case.&quot;

{| class=&quot;wikitable&quot;
|-
! 
! Insert
! Delete
! Balance
! Get at index
! Search
! Find minimum
! Find maximum
! Space usage
|-
| Unsorted [[Array data structure|array]]
| [[Linear time|O(''n'')]]
| O(''n'')
| N/A
| O(1)
| O(''n'')
| O(''n'')
| O(''n'')
| O(''n'')
|-
| [[Sorted array]]
| O(''n'')
| O(''n'')
| N/A
| O(1)
| O(log ''n'')
| O(1)
| O(1)
| O(''n'')
|-
| Unsorted [[linked list]]
| O(1)
| O(1)
| N/A
| O(''n'')
| O(''n'')
| O(''n'')
| O(''n'')
| O(''n'')
|-
| Sorted linked list
| O(1)
| O(1)
| N/A
| O(''n'')
| O(''n'')
| O(1)
| O(''n'')
| O(''n'')
|-
| [[Self-balancing binary tree]]
| O(1)
| O(1)
| O(log ''n'')
| N/A
| O(log ''n'')
| O(log ''n'')
| O(log ''n'')
| O(''n'')
|-
| [[Heap (data structure)|Heap]]
| O(1)
| O(1)
| O(log ''n'')
| N/A
| O(''n'')
| O(1)
| O(''n'')
| O(''n'')
|-
| [[Hash table]]
| O(1)
| O(1)
| O(''n'')
| N/A
| O(1)
| O(''n'')
| O(''n'')
| O(''n'') 
|-
| [[Trie]]
| O(''m'')
| O(''m'')
| N/A
| O(''n'')
| O(''m'')
| O(''m'')
| O(''m'')
| O(''n'')
|}

This table is only an approximate summary; for each data structure there are special situations and variants that may lead to different costs. Also two or more data structures can be combined to obtain lower costs.

== Footnotes ==
{{reflist}}

== See also ==
* [[List of data structures]]

[[Category:Data structures]]</text>
      <sha1>tp8ofi6200uxbs3wux4le35yycq7ho6</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Binary tree</title>
    <ns>0</ns>
    <id>4321</id>
    <revision>
      <id>624715303</id>
      <parentid>622807627</parentid>
      <timestamp>2014-09-08T20:22:05Z</timestamp>
      <contributor>
        <username>Mqtthiqs</username>
        <id>5533379</id>
      </contributor>
      <comment>/* Nodes and references */</comment>
      <text xml:space="preserve" bytes="32836">{{distinguish|B-tree}}
{{more footnotes|date=July 2014}}
[[Image:binary tree.svg|right|192px|thumb|A labeled binary tree of size 9 and height 4, with a root node whose value is 2. The above tree is unbalanced and not sorted.]]

In [[computer science]], a '''binary tree''' is a [[tree structure|tree]] [[data structure]] in which each node has at most two [[child node|children]], which are referred to as the ''left'' child and the ''right'' child. A [[recursive definition]] using just [[set theory]] notions is that a (non-empty) binary tree is a [[tuple|triple]] (''L'', ''S'', ''R''), where ''L'' and ''R'' are binary trees or the [[empty set]] and ''S'' is a [[singleton set]].&lt;ref name=&quot;GarnierTaylor2009&quot;&gt;{{cite book|author1=Rowan Garnier|author2=John Taylor|title=Discrete Mathematics: Proofs, Structures and Applications, Third Edition|url=http://books.google.com/books?id=WnkZSSc4IkoC&amp;pg=PA620|year=2009|publisher=CRC Press|isbn=978-1-4398-1280-8|page=620}}&lt;/ref&gt; Some authors allow the binary tree to be the empty set as well.&lt;ref name=&quot;Skiena2009&quot;&gt;{{cite book|author=Steven S Skiena|title=The Algorithm Design Manual|url=http://books.google.com/books?id=7XUSn0IKQEgC&amp;pg=PA77|year=2009|publisher=Springer Science &amp; Business Media|isbn=978-1-84800-070-4|page=77}}&lt;/ref&gt;

From a [[graph theory]] perspective, binary (and K-ary) trees as defined here are actually [[Arborescence (graph theory)|arborescence]]s.&lt;ref name=&quot;Knuth1997&quot;&gt;{{cite book|author=Knuth|title=The Art Of Computer Programming, Volume 1, 3/E|year=1997|publisher=Pearson Education|isbn=0-201-89683-4|page=363}}&lt;/ref&gt; A binary tree may thus be also called a '''bifurcating arborescence'''&lt;ref name=&quot;Knuth1997&quot;/&gt;—a term which actually appears in some very old programming books,&lt;ref name=&quot;Flores1971&quot;&gt;{{cite book|author=Iván Flores|title=Computer programming system/360|year=1971|publisher=Prentice-Hall|page=39}}&lt;/ref&gt; before the modern computer science terminology prevailed. It is also possible to interpret a binary tree as an [[undirected graph|undirected]], rather than a [[directed graph]], in which case a binary tree is an [[ordered tree|ordered]], [[rooted tree]].&lt;ref&gt;{{cite book|author=Kenneth Rosen|title=Discrete Mathematics and Its Applications, 7th edition|year=2011|publisher=McGraw-Hill Science|page=749|isbn=978-0-07-338309-5}}&lt;/ref&gt; Some authors use '''rooted binary tree''' instead of ''binary tree'' to emphasize the fact that the tree is rooted, but as defined above, a binary tree is always rooted.&lt;ref name=&quot;Mazur2010&quot;&gt;{{cite book|author=David R. Mazur|title=Combinatorics: A Guided Tour|url=http://books.google.com/books?id=yI4Jx5Obr08C&amp;pg=PA246|year=2010|publisher=Mathematical Association of America|isbn=978-0-88385-762-5|page=246}}&lt;/ref&gt; A binary tree is a special case of an ordered [[K-ary tree]], where ''k'' is 2. There is however a subtle difference between the binary tree data structure as defined here and the notions from graph theory or as K-ary tree is usually defined. As defined here, a binary tree node having a left child but no right child is not the same as a node having a right child but no left child, whereas an ordered/plane tree (or arborescence) from graph theory cannot tell these cases apart, and neither does ''k''-ary as usually understood as using a list of children.&lt;ref name=&quot;Aho1983&quot;&gt;{{cite book|author1=[[Alfred V. Aho]]|author2=[[John E. Hopcroft]]|author3=[[Jeffrey D. Ullman]]|title=Data Structures and Algorithms|year=1983|publisher=Pearson Education|isbn=978-81-7758-826-2|at=section 3.4: &quot;Binary trees&quot;}}&lt;/ref&gt; An actual generalization of binary tree would have to discern for example a case like having a first and third, but no second child; the [[trie]] data structure is actually the more appropriate generalization in this respect.&lt;ref name=&quot;Storer2001&quot;&gt;{{cite book|author=J.A. Storer|title=An Introduction to Data Structures and Algorithms|year=2002|publisher=Springer Science &amp; Business Media|isbn=978-1-4612-6601-3|page=127}}&lt;/ref&gt;

In computing, binary trees are seldom used solely for their structure. Much more typical is to define a labeling function on the nodes, which associates some value to each node.&lt;ref name=&quot;Makinson2009b&quot;&gt;{{cite book|author=David Makinson|title=Sets, Logic and Maths for Computing|year=2009|publisher=Springer Science &amp; Business Media|isbn=978-1-84628-845-6|page=199}}&lt;/ref&gt; Binary trees labelled this way are used to implement [[binary search tree]]s and [[binary heap]]s, and are used for efficient [[search algorithm|searching]] and [[Sorting algorithm|sorting]]. The designation of non-root nodes as left or right child even when there is only one child present matters in some of these applications, in particular it is significant in binary search trees.&lt;ref name=&quot;Gross2007&quot;&gt;{{cite book|author=Jonathan L. Gross|title=Combinatorial Methods with Computer Applications|url=http://books.google.com/books?id=hamtabmh0ZoC&amp;pg=PA248|year=2007|publisher=CRC Press|isbn=978-1-58488-743-0|page=248}}&lt;/ref&gt; In mathematics, what is termed ''binary tree'' can vary significantly from author to author. Some use the definition commonly used computer science,&lt;ref name=&quot;oem&quot;/&gt; but others define it as every non-leaf having exactly two children and don't necessarily order (as left/right) the children either.&lt;ref name=&quot;Foulds1992&quot;&gt;{{cite book|author=L.R. Foulds|title=Graph Theory Applications|url=http://books.google.com/books?id=IK7kreGl3vkC&amp;pg=PA32|year=1992|publisher=Springer Science &amp; Business Media|isbn=978-0-387-97599-3|page=32}}&lt;/ref&gt;

==Definitions==

===Recursive definition===
{{cleanup-rewrite|section|date=July 2014}}
Another way of defining a ''full'' binary tree is a [[recursive definition]]. A full binary tree is either:&lt;ref name=&quot;Rosen2011&quot;&gt;{{cite book|author=Kenneth Rosen|title=Discrete Mathematics and Its Applications 7th edition|year=2011|publisher=McGraw-Hill Science|pages=352–353|isbn=978-0-07-338309-5}}&lt;/ref&gt;
* A single vertex.
* A graph formed by taking two (full) binary trees, adding a vertex, and adding an edge directed from the new vertex to the root of each binary tree.
This also does not establish the order of children, but does fix a specific root node.

To actually define a binary tree in general, we must allow for the possibility that only one of children may be empty. An artifact, which in some textbooks is called an ''extended binary tree'' is needed for that purpose. An extended binary tree is thus recursively defined as:&lt;ref name=&quot;Rosen2011&quot;/&gt;
* the [[empty set]] is an extended binary tree
* if T&lt;sub&gt;1&lt;/sub&gt; and T&lt;sub&gt;2&lt;/sub&gt; are extended binary trees, then denote by T&lt;sub&gt;1&lt;/sub&gt; • T&lt;sub&gt;2&lt;/sub&gt; the extended binary tree obtained by adding a root ''r'' connected to the left to T&lt;sub&gt;1&lt;/sub&gt; and to the right to T&lt;sub&gt;2&lt;/sub&gt; by adding edges when these sub-trees are non-empty.

Another way of imagining this construction (and understanding the terminology) is to consider instead of the empty set a different type of node—for instance square nodes if the regular ones are circles.&lt;ref name=&quot;HuShing2002&quot;&gt;{{cite book|author1=Te Chiang Hu|author2=Man-tak Shing|title=Combinatorial Algorithms|year=2002|publisher=Courier Dover Publications|isbn=978-0-486-41962-6|page=162}}&lt;/ref&gt;

===Using graph theory concepts===
A binary tree is a [[rooted tree]] that is also an [[ordered tree]] (aka [[plane tree]]) in which every node has at most two children. A rooted tree naturally imparts a notion of levels (distance from the root), thus for every node a notion of children may be defined as the nodes connected to it a level below. Ordering of these children (e.g. by drawing them on a plane) makes possible to distinguish left child from right child.&lt;ref name=&quot;HsuLin2008&quot;&gt;{{cite book|author1=Lih-Hsing Hsu|author2=Cheng-Kuan Lin|title=Graph Theory and Interconnection Networks|url=http://books.google.com/books?id=vbxdqhDKOSYC&amp;pg=PA66|year=2008|publisher=CRC Press|isbn=978-1-4200-4482-9|page=66}}&lt;/ref&gt; But this still doesn't distinguish between a node with left but not a right child from a one with right but no left child.

The necessary distinction can be made by first partitioning the edges, i.e. defining the binary tree as triplet (V, E&lt;sub&gt;1&lt;/sub&gt;, E&lt;sub&gt;2&lt;/sub&gt;), where (V, E&lt;sub&gt;1&lt;/sub&gt; ∪ E&lt;sub&gt;2&lt;/sub&gt;) is a rooted tree (equivalently arborescence) and E&lt;sub&gt;1&lt;/sub&gt; ∩ E&lt;sub&gt;2&lt;/sub&gt; is empty, and also requiring that for all ''j'' ∈ { 1, 2 } every node has at most one E&lt;sub&gt;''j''&lt;/sub&gt; child.&lt;ref name=&quot;FlumGrohe2006&quot;&gt;{{cite book|author1=J. Flum|author2=M. Grohe|title=Parameterized Complexity Theory|year=2006|publisher=Springer|isbn=978-3-540-29953-0|page=245}}&lt;/ref&gt; A more informal way of making the distinction is to say, quoting the [[Encyclopedia of Mathematics]], that &quot;every node has a left child, a right child, neither, or both&quot; and to specify that these &quot;are all different&quot; binary trees.&lt;ref name=&quot;oem&quot;&gt;{{SpringerEOM|id=Binary_tree&amp;oldid=31607|title=Binary tree}} also in print as {{cite book|author=Michiel Hazewinkel|title=Encyclopaedia of Mathematics. Supplement I|url=http://books.google.com/books?id=3ndQH4mTzWQC&amp;pg=PA124|year=1997|publisher=Springer Science &amp; Business Media|isbn=978-0-7923-4709-5|page=124}}&lt;/ref&gt;

==Types of binary trees==
{{refimprove section|date=July 2014}}
Note that the terminology is by no means standardized in the literature.
* A '''rooted''' binary [[tree data structure|tree]] has a [[root node]] and every node has at most two children.
* In a '''full''' binary tree or '''proper''' binary tree &lt;ref&gt;{{cite web|url=http://xlinux.nist.gov/dads//HTML/fullBinaryTree.html | title=full binary tree | publisher = [[NIST]]}}&lt;/ref&gt; (sometimes '''2-tree''' or '''strictly''' binary tree) every node other than the leaves has two children. Physicists define a ''binary tree'' to mean a ''full binary tree''.&lt;ref&gt;''Unitary Symmetry'', James D. Louck, World Scientific Pub., 2008&lt;/ref&gt; [[File:Waldburg Ahnentafel.jpg|thumb|An [[ancestry chart]] which maps to a perfect depth-4 binary tree]]. A ''full tree'' is sometimes ambiguously defined as a ''perfect tree'' (see next).
* A '''{{visible anchor|perfect}}''' binary tree is a ''full binary tree'' in which all ''leaves'' have the same ''depth'' or same ''level''.&lt;ref&gt;{{cite web|url=http://www.nist.gov/dads/HTML/perfectBinaryTree.html|title=perfect binary tree | publisher = [[NIST]]}}&lt;/ref&gt; (This is ambiguously also called a ''complete binary tree'' (see next).) An example of a perfect binary tree is the [[ancestry chart]] of a person to a given depth, as each person has exactly two biological parents (one mother and one father); note that this reverses the usual parent/child tree convention, and these trees go in the opposite direction from usual (root at bottom).
* In a '''{{visible anchor|complete}}''' binary tree every level, ''except possibly the last'', is completely filled, and all nodes are as far left as possible. It can have between 1 and 2&lt;sup&gt;''h''&lt;/sup&gt; nodes, as far left as possible, at the last level h.&lt;ref name=&quot;complete binary tree&quot;&gt;{{cite web|url=http://www.nist.gov/dads/HTML/completeBinaryTree.html|title=complete binary tree| publisher = NIST}}&lt;/ref&gt; A binary tree is called an almost complete binary tree or nearly complete binary tree if mentioned exception holds, i.e. the last level is not completely filled. This type of binary tree is used as a specialized data structure called a [[Binary heap]].&lt;ref name=&quot;complete binary tree&quot;/&gt;
* An '''infinite complete''' binary tree has [[countably infinite]] number of levels, in which every node has two children, so that there are 2&lt;sup&gt;''d''&lt;/sup&gt; nodes at level ''d''. The set of all nodes is countably infinite, but the set of all infinite paths from the root is uncountable, having the [[cardinality of the continuum]]. These paths correspond by an order-preserving [[bijection]] to the points of the [[Cantor set]], or (using the example of a [[Stern–Brocot tree]]) to the set of positive [[irrational number]]s.
* A '''balanced''' binary tree has the minimum possible [[Binary_tree#Properties_of_binary_trees|maximum height]] (a.k.a. depth) for the leaf nodes, because for any given number of leaf nodes the leaf nodes are placed at the greatest height possible.

&lt;tt&gt;
: '''h'''&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;'''Balanced'''&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;'''Unbalanced, h = (n + 1)/2'''
: 1:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ABCDE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ABCDE
: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;/&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;\&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;/&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;\
: 2:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ABCD&amp;nbsp;&amp;nbsp;&amp;nbsp;E&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ABCD&amp;nbsp;&amp;nbsp;&amp;nbsp;E
: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;/&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;\&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;/&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;\
: 3:&amp;nbsp;&amp;nbsp;AB&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;CD&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ABC&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;D
: &amp;nbsp;&amp;nbsp;&amp;nbsp;/&amp;nbsp;&amp;nbsp;\&amp;nbsp;&amp;nbsp;/&amp;nbsp;&amp;nbsp;\&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;/&amp;nbsp;&amp;nbsp;&amp;nbsp;\
: 4:&amp;nbsp;A&amp;nbsp;&amp;nbsp;B&amp;nbsp;&amp;nbsp;C&amp;nbsp;&amp;nbsp;D&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;AB&amp;nbsp;&amp;nbsp;&amp;nbsp;C
: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;/&amp;nbsp;&amp;nbsp;\
: 5:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;A&amp;nbsp;&amp;nbsp;&amp;nbsp;B
&lt;/tt&gt;

: Perfect, complete, and Merkle&lt;ref&gt;[https://bitcoin.org/en/developer-guide#term-merkle-tree Bitcoin Developer Guide: Merkle tree]&lt;/ref&gt; binary trees are examples of balanced binary trees. A commonly mentioned structure which is balanced is the left and right subtrees of every node differ by 1 or less,&lt;ref&gt;Aaron M. Tenenbaum, et al. Data Structures Using C, Prentice Hall, 1990 ISBN 0-13-199746-7&lt;/ref&gt; although in general it is a binary tree where no leaf is much farther away from the root than any other leaf. (Different balancing schemes allow different definitions of &quot;much farther&quot;.&lt;ref&gt;Paul E. Black (ed.), entry for ''data structure'' in ''[[Dictionary of Algorithms and Data Structures]]. U.S. [[National Institute of Standards and Technology]]. 15 December 2004. [http://xw2k.nist.gov/dads//HTML/balancedtree.html Online version] Accessed 2010-12-19.&lt;/ref&gt;)
* A '''degenerate''' (or '''pathological''') tree is where each parent node has only one associated child node. This means that performance-wise, the tree will behave like a [[linked list]] data structure.

Note that this terminology often varies in the literature, especially with respect to the meaning of &quot;complete&quot; and &quot;full&quot;.

==Properties of binary trees==
* The number of nodes &lt;math&gt;n&lt;/math&gt; in a full binary tree, is at least &lt;math&gt;n = 2h - 1&lt;/math&gt; and at most &lt;math&gt;n = 2^h - 1&lt;/math&gt;, where &lt;math&gt;h&lt;/math&gt; is the maximum height (a.k.a. depth) traversed in nodes from any leaf node to the root inclusive of the root and the leaf node.
* The number of leaf nodes &lt;math&gt;l&lt;/math&gt; in a full binary tree, is &lt;math&gt;l = (n + 1) / 2&lt;/math&gt; because the number of non-leaf (a.k.a. internal) nodes &lt;math&gt;n - l = \sum_{k=0}^{log_2(l)-1} 2^k = 2^{log_2(l)} - 1 = l - 1&lt;/math&gt;.
* In a '''balanced''' full binary tree, &lt;math&gt;h = \lceil log_2(l)\rceil + 1 = \lceil log_2((n + 1) / 2)\rceil + 1 = \lceil log_2(n + 1)\rceil&lt;/math&gt; (see [[Floor and ceiling functions|ceiling function]]).
* In a '''perfect''' full binary tree, &lt;math&gt;l = 2^{h-1}&lt;/math&gt; thus &lt;math&gt;n = 2^h - 1&lt;/math&gt;.
* The maximum possible number of null links (i.e., absent children of the nodes) in a '''complete''' binary tree of ''n'' nodes is (''n''+1), where only 1 node exists in bottom-most level to the far left.
* The number of internal nodes in a '''complete''' binary tree of ''n'' nodes is &amp;lfloor; ''n''/2 &amp;rfloor;.
* For any non-empty binary tree with ''n''&lt;sub&gt;0&lt;/sub&gt; leaf nodes and ''n''&lt;sub&gt;2&lt;/sub&gt; nodes of degree 2, ''n''&lt;sub&gt;0&lt;/sub&gt; = ''n''&lt;sub&gt;2&lt;/sub&gt; + 1.&lt;ref&gt;{{cite book | last=Mehta | first=Dinesh |author2=[[Sartaj Sahni]] | title=Handbook of Data Structures and Applications|publisher=[[Chapman and Hall]]|isbn = 1-58488-435-5|year=2004 }}&lt;/ref&gt;

==Combinatorics==
{{unreferenced section|date=July 2014}}
In combinatorics one considers the problem of counting the number of full binary trees of a given size.  Here the trees have no values attached to their nodes (this would just multiply the number of possible trees by an easily determined factor), and trees are distinguished only by their structure; however the left and right child of any node are distinguished (if they are different trees, then interchanging them will produce a tree distinct from the original one). The size of the tree is taken to be the number ''n'' of internal nodes (those with two children); the other nodes are leaf nodes and there are {{nowrap|''n'' + 1}} of them. The number of such binary trees of size ''n'' is equal to the number of ways of fully parenthesizing a string of {{nowrap|''n'' + 1}} symbols (representing leaves) separated by ''n'' binary operators (representing internal nodes), so as to determine the argument subexpressions of each operator. For instance for {{nowrap|''n'' {{=}} 3}} one has to parenthesize a string like {{nowrap| &lt;math&gt;X*X*X*X&lt;/math&gt;}}, which is possible in five ways:
: &lt;math&gt;((X*X)*X)*X,\qquad (X*(X*X))*X,\qquad (X*X)*(X*X),\qquad X*((X*X)*X),\qquad X*(X*(X*X)).&lt;/math&gt;
The correspondence to binary trees should be obvious, and the addition of redundant parentheses (around an already parenthesized expression or around the full expression) is disallowed (or at least not counted as producing a new possibility).

There is a unique binary tree of size 0 (consisting of a single leaf), and any other binary tree is characterized by the pair of its left and right children; if these have sizes ''i'' and ''j'' respectively, the full tree has size {{nowrap|''i'' + ''j'' + 1}}. Therefore the number &lt;math&gt;C_n&lt;/math&gt; of binary trees of size ''n'' has the following recursive description &lt;math&gt;C_0=1&lt;/math&gt;, and &lt;math&gt;\textstyle C_n=\sum_{i=0}^{n-1}C_iC_{n-1-i}&lt;/math&gt; for any positive integer ''n''. It follows that &lt;math&gt;C_n&lt;/math&gt; is the [[Catalan number]] of index ''n''.

The above parenthesized strings should not be confused with the set of words of length 2''n'' in the [[Dyck language]], which consist only of parentheses in such a way that they are properly balanced. The number of such strings satisfies the same recursive description (each Dyck word of length 2''n'' is determined by the Dyck subword enclosed by the initial '(' and its matching ')' together with the Dyck subword remaining after that closing parenthesis, whose lengths 2''i'' and 2''j'' satisfy {{nowrap|''i'' + ''j'' + 1 {{=}} ''n''}}); this number is therefore also the Catalan number &lt;math&gt;C_n&lt;/math&gt;. So there are also five Dyck words of length 10:
: &lt;math&gt;()()(),\qquad ()(()),\qquad (())(),\qquad (()()),\qquad ((()))&lt;/math&gt;.
These Dyck words do not correspond in an obvious way to binary trees. A bijective correspondence can nevertheless be defined as follows: enclose the Dyck word in an extra pair of parentheses, so that the result can be interpreted as a  [[Lisp (programming language)|Lisp]] list expression (with the empty list () as only occurring atom); then the [[Lisp (programming language)#S-expressions represent lists|dotted-pair]] expression for that proper list is a fully parenthesized expression (with NIL as symbol and '.' as operator) describing the corresponding binary tree (which is in fact the internal representation of the proper list).

The ability to represent binary trees as strings of symbols and parentheses implies that binary trees can represent the elements of a [[free magma]] on a singleton set.

== Methods for storing binary trees ==&lt;!-- This section is linked from [[Ahnentafel]] --&gt;
Binary trees can be constructed from [[programming language]] primitives in several ways.

=== Nodes and references ===

In a language with [[record (computer science)|records]] and [[reference (computer science)|reference]]s, binary trees are typically constructed by having a tree node structure which contains some data and references to its left child and its right child. Sometimes it also contains a reference to its unique parent. If a node has fewer than two children, some of the child pointers may be set to a special null value, or to a special [[sentinel node]].

This method of storing binary trees wastes a fair bit of memory, as the pointers will be null (or point to the sentinel) more than half the time; a more conservative representation alternative is [[threaded binary tree]].&lt;ref name=&quot;Samanta2004&quot;&gt;{{cite book|author=D. Samanta|title=Classic Data Structures|year=2004|publisher=PHI Learning Pvt. Ltd.|isbn=978-81-203-1874-8|pages=264–265}}&lt;/ref&gt;

In languages with [[tagged union]]s such as [[ML (programming language)|ML]], a tree node is often a tagged union of two types of nodes, one of which is a 3-tuple of data, left child, and right child, and the other of which is a &quot;leaf&quot; node, which contains no data and functions much like the null value in a language with pointers. For example the following line of code in [[OCaml]] (an ML dialect) defines a binary tree that stores a character in each node.&lt;ref name=&quot;Scott2009&quot;&gt;{{cite book|author=Michael L. Scott|title=Programming Language Pragmatics|year=2009|publisher=Morgan Kaufmann|isbn=978-0-08-092299-7|page=347|edition=3rd}}&lt;/ref&gt;

&lt;!-- the source gives the example in Standard ML, which has &quot;datatype&quot; instead of &quot;type&quot;, but wikipedia's source tag doesn't support Standard ML. --&gt;
&lt;source lang=&quot;ocaml&quot;&gt;
type chr_tree = Empty | Node of char * chr_tree * chr_tree
&lt;/source&gt;

===Arrays===

Binary trees can also be stored in breadth-first order as an [[implicit data structure]] in [[array data structure|array]]s, and if the tree is a complete binary tree, this method wastes no space. In this compact arrangement, if a node has an index ''i'', its children are found at indices &lt;math&gt;2i + 1&lt;/math&gt; (for the left child) and &lt;math&gt;2i +2&lt;/math&gt; (for the right), while its parent (if any) is found at index ''&lt;math&gt;\left \lfloor \frac{i-1}{2} \right \rfloor&lt;/math&gt;'' (assuming the root has index zero). This method benefits from more compact storage and better [[locality of reference]], particularly during a preorder traversal. However, it is expensive to grow and wastes space proportional to 2&lt;sup&gt;''h''&lt;/sup&gt; - ''n'' for a tree of depth ''h'' with ''n'' nodes.

This method of storage is often used for [[binary heap]]s. No space is wasted because nodes are added in breadth-first order.

&lt;center&gt;[[Image:Binary tree in array.svg|300px|A small complete binary tree stored in an array]]&lt;/center&gt;

== Encodings ==

=== Succinct encodings ===
A [[succinct data structure]] is one which occupies close to minimum possible space, as established by [[information theory|information theoretical]] lower bounds. The number of different binary trees on &lt;math&gt;n&lt;/math&gt; nodes is &lt;math&gt;\mathrm{C}_{n}&lt;/math&gt;, the &lt;math&gt;n&lt;/math&gt;th [[Catalan number]] (assuming we view trees with identical ''structure'' as identical). For large &lt;math&gt;n&lt;/math&gt;, this is about &lt;math&gt;4^{n}&lt;/math&gt;; thus we need at least about &lt;math&gt;\log_{2}4^{n} = 2n&lt;/math&gt; bits to encode it. A succinct binary tree therefore would occupy &lt;math&gt;2n+o(n)&lt;/math&gt; bits.

One simple representation which meets this bound is to visit the nodes of the tree in preorder, outputting &quot;1&quot; for an internal node and &quot;0&quot; for a leaf. [http://theory.csail.mit.edu/classes/6.897/spring03/scribe_notes/L12/lecture12.pdf] If the tree contains data, we can simply simultaneously store it in a consecutive array in preorder. This function accomplishes this:

 '''function''' EncodeSuccinct(''node'' n, ''bitstring'' structure, ''array'' data) {
     '''if''' n = ''nil'' '''then'''
         append 0 to structure;
     '''else'''
         append 1 to structure;
         append n.data to data;
         EncodeSuccinct(n.left, structure, data);
         EncodeSuccinct(n.right, structure, data);
 }

The string ''structure'' has only &lt;math&gt;2n + 1&lt;/math&gt; bits in the end, where &lt;math&gt;n&lt;/math&gt; is the number of (internal) nodes; we don't even have to store its length. To show that no information is lost, we can convert the output back to the original tree like this:

 '''function''' DecodeSuccinct(''bitstring'' structure, ''array'' data) {
     remove first bit of ''structure'' and put it in ''b''
     '''if''' b = 1 '''then'''
         create a new node ''n''
         remove first element of data and put it in n.data
         n.left = DecodeSuccinct(structure, data)
         n.right = DecodeSuccinct(structure, data)
         '''return''' n
     '''else'''
         '''return''' nil
 }

More sophisticated succinct representations allow not only compact storage of trees but even useful operations on those trees directly while they're still in their succinct form.

=== Encoding general trees as binary trees ===
There is a one-to-one mapping between general ordered trees and binary trees, which in particular is used by [[Lisp (programming language)|Lisp]] to represent general ordered trees as binary trees. To convert a general ordered tree to binary tree, we only need to represent the general tree in left child-right sibling way. The result of this representation will be automatically binary tree, if viewed from a different perspective. Each node ''N'' in the ordered tree corresponds to a node ''N' '' in the binary tree; the ''left'' child of ''N' '' is the node corresponding to the first child of ''N'', and the ''right'' child of ''N' '' is the node corresponding to ''N'' 's next sibling --- that is, the next node in order among the children of the parent of ''N''. This binary tree representation of a general order tree is sometimes also referred to as a [[left child-right sibling binary tree]] (LCRS tree), or a [[doubly chained tree]], or a [[Filial-Heir chain]].

One way of thinking about this is that each node's children are in a [[linked list]], chained together with their ''right'' fields, and the node only has a pointer to the beginning or head of this list, through its ''left'' field.

For example, in the tree on the left, A has the 6 children {B,C,D,E,F,G}.  It can be converted into the binary tree on the right.

&lt;center&gt;
[[Image:N-ary to binary.svg|400x240px|An example of converting an n-ary tree to a binary tree]]
&lt;/center&gt;

The binary tree can be thought of as the original tree tilted sideways, with the black left edges representing ''first child'' and the blue right edges representing ''next sibling''.  The leaves of the tree on the left would be written in Lisp as:

:(((N O) I J) C D ((P) (Q)) F (M))

which would be implemented in memory as the binary tree on the right, without any letters on those nodes that have a left child.

== Common operations ==
{{original research|section|date=July 2014}}
[[File:BinaryTreeRotations.svg|thumb|300px|[[Tree rotation]]s are very common internal operations on [[Self-balancing binary search tree|self-balancing binary trees]].]]
There are a variety of different operations that can be performed on binary trees. Some are [[mutator method|mutator]] operations, while others simply return useful information about the tree.

=== Insertion ===
Nodes can be inserted into binary trees in between two other nodes or added after a [[leaf node]]. In binary trees, a node that is inserted is specified as to which child it is.

==== External nodes ====
Suppose that the external node being added onto is node A. To add a new node after node A, A assigns the new node as one of its children and the new node assigns node A as its parent.

==== Internal nodes ====
[[File:Insertion of binary tree node.svg|thumb|360px|The process of inserting a node into a binary tree]] 
Insertion on [[internal node]]s is slightly more complex than on external nodes. Say that the internal node is node A and that node B is the child of A. (If the insertion is to insert a right child, then B is the right child of A, and similarly with a left child insertion.) A assigns its child to the new node and the new node assigns its parent to A. Then the new node assigns its child to B and B assigns its parent as the new node.

=== Deletion ===
Deletion is the process whereby a node is removed from the tree. Only certain nodes in a binary tree can be removed unambiguously.&lt;ref name=&quot;rice&quot;&gt;{{cite web |url=http://www.clear.rice.edu/comp212/03-spring/lectures/22/|title=Binary Tree Structure|author=Dung X. Nguyen|year=2003|work= |publisher=rice.edu|accessdate=December 28, 2010}} 
&lt;/ref&gt;

==== Node with zero or one children ====
[[File:Deletion of internal binary tree node.svg|thumb|360px|The process of deleting an internal node in a binary tree]] 
Suppose that the node to delete is node A. If a node has no children ([[external node]]), deletion is accomplished by setting the child of A's parent to [[null pointer|null]]. If it has one child, set the parent of A's child to A's parent and set the child of A's parent to A's child.

==== Node with two children ====
In a binary tree, a node with two children cannot be deleted unambiguously.&lt;ref name=&quot;rice&quot;/&gt; However, in certain binary trees (including [[binary search tree]]s) these nodes ''can'' be deleted, though with a rearrangement of the tree structure.

=== Traversal ===
{{Main|Tree traversal}}

Pre-order, in-order, and post-order traversal visit each node in a tree by recursively visiting each node in the left and right subtrees of the root.

==== Depth-first order ====
In depth-first order, we always attempt to visit the node farthest from the root node that we can, but with the caveat that it must be a child of a node we have already visited. Unlike a depth-first search on graphs, there is no need to remember all the nodes we have visited, because a tree cannot contain cycles. Pre-order is a special case of this. See [[depth-first search]] for more information.

==== Breadth-first order ====
Contrasting with depth-first order is breadth-first order, which always attempts to visit the node closest to the root that it has not already visited. See [[breadth-first search]] for more information. Also called a ''level-order traversal''.

In a complete binary tree, a node's breadth-index (''i - (2&lt;sup&gt;d&lt;/sup&gt; - 1)'') can be used as traversal instructions from the root. Reading bitwise from left to right, starting at bit ''d - 1'', where ''d'' is the node's distance from the root (''d = floor(log2(i+1))'') and the node in question is not the root itself (''d &gt; 0''). When the breadth-index is masked at bit ''d - 1'', the bit values &lt;tt&gt;0&lt;/tt&gt; and &lt;tt&gt;1&lt;/tt&gt; mean to step either left or right, respectively. The process continues by successively checking the next bit to the right until there are no more. The rightmost bit indicates the final traversal from the desired node's parent to the node itself. There is a time-space trade-off between iterating a complete binary tree this way versus each node having pointer/s to its sibling/s.

==See also==
* [[2-3 tree]]
* [[2-3-4 tree]]
* [[AA tree]]
* [[AVL tree]]
* [[B-tree]]
* [[Binary space partitioning]]
* [[Huffman tree]]
* [[K-ary tree]]
* [[Kraft's inequality]]
* [[Optimal binary search tree]]
* [[Random binary tree]]
* [[Recursion (computer science)]]
* [[Red-black tree]]
* [[Rope (computer science)]]
* [[Self-balancing binary search tree]]
* [[Splay tree]]
* [[Strahler number]]
* [[Tree of primitive Pythagorean triples#Alternative methods of generating the tree]]
* [[Unrooted binary tree]]

== References ==

=== Citations ===
{{Reflist|33em}}

=== Bibliography ===
* [[Donald Knuth]]. ''The art of computer programming vol 1. Fundamental Algorithms'', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4. Section 2.3, especially subsections 2.3.1&amp;ndash;2.3.2 (pp.&amp;nbsp;318&amp;ndash;348).

==External links==
{{commons category|Binary trees}}
* [http://www.findstat.org/BinaryTrees binary trees] entry in the [http://www.findstat.org/ FindStat] database
*[http://www.gamedev.net/page/resources/_/technical/general-programming/trees-part-2-binary-trees-r1433 Gamedev.net introduction on binary trees]
*[http://www.brpreiss.com/books/opus4/html/page355.html Binary Tree Proof by Induction]
*[http://piergiu.wordpress.com/2010/02/21/balanced-binary-search-tree-on-array/ Balanced binary search tree on array How to create bottom-up an Ahnentafel list, or a balanced binary search tree on array]

{{CS-Trees}}

{{DEFAULTSORT:Binary Tree}}
[[Category:Binary trees| ]]
[[Category:Data structures]]</text>
      <sha1>6fsbelwlmyoxcjtqbf81vtvb6hxuqnk</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Search tree</title>
    <ns>0</ns>
    <id>844292</id>
    <revision>
      <id>604023351</id>
      <parentid>575774002</parentid>
      <timestamp>2014-04-13T15:47:01Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>search trees need not be binary (see [[B-tree]])</comment>
      <text xml:space="preserve" bytes="2283">{{multiple issues|
{{Unreferenced|date=December 2009}}
{{Technical|date=October 2012}}
}}

In [[computer science]], a '''search tree''' is a [[tree data structure]] in whose nodes data values are stored from some [[ordered set]], in such a way that [[in-order traversal]] of the tree visits the nodes in ascending order of the stored values. This means that for any internal node containing a value {{mvar|''v''}}, the values {{mvar|''x''}} stored in its left subtree satisfy {{math|''x'' ≤ ''v''}}, and the values {{mvar|''y''}} stored in its right subtree satisfy {{math|''v'' ≤ ''y''}}. Each [[subtree]] of a search tree is by itself again a search tree.

Search trees can implement the [[data type]] of (finite) [[multiset]]s. The advantage of using search trees is that the test for membership can be performed efficiently provided that the tree is reasonably balanced, that is, the leaves of the tree are at comparable depths. Various search-tree data structures exist, several of which also allow efficient insertion and deletion of elements, which operations then have to maintain tree balance. If the multiset being represented is immutable, this is not an issue.

Search trees can also implement [[associative array]]s by storing key–value pairs, where the ordering is based on the key part of these pairs.

In some kinds of search trees the data values are all stored in the leaves of the tree. In that case some additional information needs to be stored in the internal tree nodes to make efficient operations possible.

==Examples==
Some examples of search-tree data structures are:
*[[AVL tree]]s, [[Red-black tree]]s, [[splay tree]]s and [[Tango Trees]] which are instances of [[self-balancing binary search tree]]s;
*[[Ternary search tree]]s, in which each internal node has exactly three children;
*[[B tree]]s, commonly used in databases;
*[[B+ tree]]s, like B trees but with all data values stored in the leaves;
*[[van Emde Boas tree]]s, very efficient if the data values are fixed-size [[integer]]s.

==See also==
*[[Trie]], a tree data structure that allows searching for [[String (computer science)|strings]], which however does not store the actual data values in the nodes.

{{DEFAULTSORT:Search Tree}}
[[Category:Data structures]]


{{algorithm-stub}}</text>
      <sha1>nvdxa9makw9gq0ko7knlgcyebjf1l1p</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Concurrent data structure</title>
    <ns>0</ns>
    <id>25175778</id>
    <revision>
      <id>616045348</id>
      <parentid>553418470</parentid>
      <timestamp>2014-07-08T05:24:45Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>/* Further reading */ [[Hagit Attiya]]</comment>
      <text xml:space="preserve" bytes="8040">{{Refimprove|date=November 2009}} 

In [[computer science]], a '''concurrent data structure''' is a
particular way of storing and organizing [[data]] for access by
multiple computing [[Thread (computer science)|threads]] (or [[process (computing)|processes]]) on a [[computer]].

Historically, such data structures were used on [[uniprocessor]]
machines with [[operating systems]] that supported multiple
computing threads (or [[process (computing)|processes]]). The term [[concurrency (computer science)|concurrency]] captured the
[[multiplexing]]/interleaving of the threads' operations on the
data by the operating system, even though the processors never
issued two operations that accessed the data simultaneously.

Today, as [[multiprocessor]] computer architectures that provide
[[parallel computing|parallelism]] become the dominant computing platform (through the
proliferation of [[multi-core]] processors), the term has come to
stand mainly for data structures that can be accessed by multiple
threads which may actually access the data simultaneously because
they run on different processors that communicate with one another.
The concurrent data structure (sometimes also called a ''shared data structure'') is usually considered to reside in an abstract storage
environment called [[shared memory]], though this memory may be
physically implemented as either a &quot;tightly coupled&quot; or a
distributed collection of storage modules.

==Basic principles==

Concurrent data structures, intended for use in
parallel or distributed computing environments, differ from
&quot;sequential&quot; data structures, intended for use on a uni-processor
machine, in several ways 
.&lt;ref name=&quot;sahni&quot;&gt;
  {{cite book
  | author = Mark Moir and [[Nir Shavit]]
  | title = 'Handbook of Data Structures and Applications'
  | chapter = ''[http://www.cs.tau.ac.il/~shanir/concurrent-data-structures.pdf Concurrent Data Structures]''
  | edition = 1st
  | editor = Dinesh Metha and [[Sartaj Sahni]]
  | publisher = Chapman and Hall/CRC Press
  | year = 2007
  | pages = 47–14–47–30
  }}
&lt;/ref&gt; Most notably, in a sequential environment
one specifies the data structure's properties and checks that they
are implemented correctly, by providing '''safety properties'''. In
a concurrent environment, the specification must also describe
'''liveness properties''' which an implementation must provide.
Safety properties usually state that something bad never happens,
while liveness properties state that something good keeps happening.
These properties can be expressed, for example, using [[Linear Temporal Logic]].

The type of liveness requirements tend to define the data structure.
The [[method (computer science)|method]] calls can be [[Blocking (computing)|blocking]] or [[Non-blocking algorithm|non-blocking]]. Data structures are not
restricted to one type or the other, and can allow combinations
where some method calls are blocking and others are non-blocking
(examples can be found in the [[Java concurrency]] software
library).

The safety properties of concurrent data structures must capture their 
behavior given the many possible interleavings of methods
called by different threads. It is quite
intuitive to specify how abstract data structures
behave in a sequential setting in which there are no interleavings.
Therefore, many mainstream approaches for arguing the safety properties of a
concurrent data structure (such as [[serializability]], [[linearizability]], [[sequential consistency]], and
quiescent consistency &lt;ref name=&quot;sahni&quot; /&gt;) specify the structures properties
sequentially, and map its concurrent executions to
a collection of sequential ones. 

In order to guarantee the safety and liveness properties, concurrent
data structures must typically (though not always) allow threads to
reach [[consensus (computer science)|consensus]] as to the results
of their simultaneous data access and modification requests. To
support such agreement, concurrent data structures are implemented
using special primitive synchronization operations (see [[Synchronization (computer science)#Process_synchronization|synchronization primitives]])
available on modern [[multiprocessing|multiprocessor machine]]s
that allow multiple threads to reach consensus. This consensus can be reached achieved in a blocking manner by using [[Spinlock|locks]], or without locks, in which case it is [[Non-blocking algorithm|non-blocking]]. There is a wide body
of theory on the design of concurrent data structures (see
bibliographical references).

==Design and Implementation==

Concurrent data structures are significantly more difficult to design
and to verify as being correct than their sequential counterparts.

The primary source of this additional difficulty is concurrency, exacerbated by the fact that 
threads must be thought of as being completely asynchronous:
they are subject to operating system preemption, page faults,
interrupts, and so on. 

On today's machines, the layout of processors and
memory, the layout of data in memory, the communication load on the
various elements of the multiprocessor architecture all influence performance.
Furthermore, there is a tension  between correctness and performance: algorithmic enhancements that seek to improve performance often make it more difficult to design and verify a correct
data structure implementation.

A key measure for performance is scalability, captured by the [[speedup]] of the implementation. Speedup is a measure of how
effectively the application is utilizing the machine it is running
on. On a machine with P processors,  the speedup is the ratio of the structures execution time on a single processor to its execution time on T processors.  Ideally, we want linear speedup: we would like to achieve a
speedup of P when using P processors. Data structures whose
speedup grows with P are called '''scalable'''. The extent to which one can scale the performance of a concurrent data structure is captured by a formula known as [[Amdahl's law]] and 
more refined versions of it such as [[Gustafson's law]].

A key issue with the performance of concurrent data structures is the level of memory contention: the overhead in traffic to and from memory as a
result of multiple threads concurrently attempting to access the same
locations in memory. This issue is most acute with blocking implementations 
in which locks control access to memory.  In order to
acquire a lock, a thread must repeatedly attempt to modify that
location. On a [[Cache coherence|cache-coherent]]
multiprocessor (one in which processors have
local caches that are updated by hardware in order to keep them
consistent with the latest values stored) this results in long
waiting times for each attempt to modify the location, and is 
exacerbated by the additional memory traffic associated with
unsuccessful attempts to acquire the lock.

==See also==
* [[Java concurrency|Java concurrency (JSR 166)]]

==References==
{{reflist}}

==Further reading==
* [[Nancy Lynch]] &quot;Distributed Computing&quot;
* [[Hagit Attiya]] and Jennifer Welch &quot;Distributed Computing: Fundamentals, Simulations And Advanced Topics, 2nd Ed&quot;
* [[Doug Lea]], &quot;Concurrent Programming in Java: Design Principles and Patterns&quot;
* [[Maurice Herlihy]] and [[Nir Shavit]], &quot;The Art of Multiprocessor Programming&quot;
* Mattson, Sanders, and Massingil &quot;Patterns for Parallel Programming&quot;

==External links==
* [http://www.ibm.com/developerworks/aix/library/au-multithreaded_structures1/index.html Multithreaded data structures for parallel computing, Part 1] (Designing concurrent data structures) by Arpan Sen
* [http://www.ibm.com/developerworks/aix/library/au-multithreaded_structures2/index.html Multithreaded data structures for parallel computing: Part 2] (Designing concurrent data structures without mutexes) by Arpan Sen
* [http://libcds.sourceforge.net/ libcds] - C++ library of lock-free containers and safe memory reclamation schema

{{DEFAULTSORT:Concurrent Data Structure}}
[[Category:Data structures]]
[[Category:Concurrent computing]]</text>
      <sha1>1zbkuvmptnbrbu045rfl1efqsta7zer</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Partition refinement</title>
    <ns>0</ns>
    <id>31595644</id>
    <revision>
      <id>584482971</id>
      <parentid>584479975</parentid>
      <timestamp>2013-12-04T06:13:51Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <minor/>
      <comment>fix ref</comment>
      <text xml:space="preserve" bytes="9355">In the design of [[algorithm]]s, '''partition refinement''' is a technique for representing a [[partition of a set]] as a [[data structure]] that allows the partition to be refined by splitting its sets into a larger number of smaller sets. In that sense it is dual to the [[Disjoint-set data structure|union-find data structure]], which also maintains a partition into [[disjoint set]]s but in which the operations merge pairs of sets together. More specifically, a partition refinement algorithm maintains a family of disjoint sets {{math|''S''&lt;sub&gt;i&lt;/sub&gt;}}; at the start of the algorithm, this is just a single set containing all the elements in the data structure. At each step of the algorithm, a set {{mvar|X}} is presented to the algorithm, and each set {{math|''S''&lt;sub&gt;i&lt;/sub&gt;}} that contains members of {{mvar|X}} is replaced by two sets, the [[Intersection (set theory)|intersection]] {{math|''S''&lt;sub&gt;i&lt;/sub&gt; &amp;cap; ''X''}} and the [[Complement (set theory)|difference]] {{math|''S''&lt;sub&gt;i&lt;/sub&gt; \ ''X''}}. Partition refinement forms a key component of several efficient algorithms on [[graph theory|graphs]] and [[finite automaton|finite automata]].&lt;ref&gt;{{citation
 | last1 = Paige | first1 = Robert
 | last2 = Tarjan | first2 = Robert E.
 | doi = 10.1137/0216062
 | mr = 917035
 | issue = 6
 | journal = SIAM Journal on Computing
 | pages = 973–989
 | title = Three partition refinement algorithms
 | volume = 16
 | year = 1987}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last1 = Habib | first1 = Michel
 | last2 = Paul | first2 = Christophe
 | last3 = Viennot | first3 = Laurent
 | doi = 10.1142/S0129054199000125
 | mr = 1759929
 | issue = 2
 | journal = International Journal of Foundations of Computer Science
 | pages = 147–170
 | title = Partition refinement techniques: an interesting algorithmic tool kit
 | volume = 10
 | year = 1999}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last1 = Habib | first1 = Michel
 | last2 = Paul | first2 = Christophe
 | last3 = Viennot | first3 = Laurent
 | contribution = A synthesis on partition refinement: a useful routine for strings, graphs, Boolean matrices and automata
 | doi = 10.1007/BFb0028546
 | mr = 1650757
 | pages = 25–38
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | title = STACS 98 (Paris, 1998)
 | volume = 1373
 | year = 1998}}.&lt;/ref&gt;

==Data structure==
A partition refinement algorithm may be implemented by maintaining an object for each set that stores a [[Collection (computing)|collection]] of its elements, in a form such as a [[doubly linked list]] that allows for rapid deletion, and an object for each element that points to the set containing it. Alternatively, element identifiers may be stored in an array, ordered by the sets they belong to, and sets may be represented by start and end indices into this array.&lt;ref&gt;{{cite conference
 | first1 = Antti | last1 = Valmari
 | first2 = Petri | last2 = Lehtinen
 |  title = Efficient minimization of DFAs with partial transition functions
 | booktitle = 25th International Symposium on Theoretical Aspects of Computer Science (STACS 2008)
 | pages = 645–656
 | series = Leibniz International Proceedings in Informatics (LIPIcs)
 | isbn = 978-3-939897-06-4
 | issn = 1868-8969
 | year = 2008
 | volume = 1
 | editor1-first = Susanne | editor1-last = Albers
 | editor2-first = Pascal | editor2-last = Weil
 | publisher = Schloss Dagstuhl: Leibniz-Zentrum fuer Informatik
 | location = Dagstuhl, Germany
 | url = http://drops.dagstuhl.de/opus/volltexte/2008/1328
 | doi = 10.4230/LIPIcs.STACS.2008.1328}}.&lt;/ref&gt;&lt;ref&gt;{{cite journal
 | last1 = Knuutila | first1 = Timo
 | doi = 10.1016/S0304-3975(99)00150-4
 | title = Re-describing an algorithm by Hopcroft
 | journal = Theoretical Computer Science
 | volume = 250
 | year = 2001
 | issue = 1-2
 | issn = 0304-3975
 | url = http://www.sciencedirect.com/science/article/pii/S0304397599001504
 | pages = 333–363}}&lt;/ref&gt; With either of these representations, each set should also have an [[instance variable]] that may point to a second set into which it is being split.

To perform a refinement operation, loop through the elements of {{mvar|X}}. For each element {{mvar|x}}, find the set {{math|''S''&lt;sub&gt;''i''&lt;/sub&gt;}} containing {{mvar|x}}, and check whether a second set for {{math|''S''&lt;sub&gt;''i''&lt;/sub&gt;}} has already been formed. If not, create the second set and add  {{math|''S''&lt;sub&gt;''i''&lt;/sub&gt;}} to a list {{mvar|L}} of the sets that are split by the operation.
Then, regardless of whether a new second set was formed, remove {{mvar|x}} from {{math|''S''&lt;sub&gt;''i''&lt;/sub&gt;}} and add it to the second set. In the representation in which all elements are stored in a single array, moving {{mvar|x}} from one set to another may be performed by swapping {{mvar|x}} with the final element of {{math|''S''&lt;sub&gt;''i''&lt;/sub&gt;}} and then decrementing the end index of {{math|''S''&lt;sub&gt;''i''&lt;/sub&gt;}} and the start index of the new set. Finally, after all elements of {{mvar|X}} have been processed in this way, loop through {{mvar|L}}, separating each current set {{math|''S''&lt;sub&gt;''i''&lt;/sub&gt;}} from the second set that has been split from it, and report both of these sets as newly formed sets from the refinement operation.

The time to perform the refinement operations in this way is {{math|''O''({{pipe}}''X''{{pipe}})}}, independent of the number of elements or the total number of sets in the data structure.

==Applications==
Possibly the first application of partition refinement was in an algorithm by {{harvtxt|Hopcroft|1971}} for [[DFA minimization]]. In this problem, one is given as input a [[deterministic finite automaton]], and must find an equivalent automaton with as few states as possible. The algorithm maintains a partition of the states of the input automaton into subsets, with the property that any two states in different subsets must be mapped to different states of the output automaton; initially, there are two subsets, one containing all the accepting states and one containing the remaining states. At each step one of the subsets {{math|''S&lt;sub&gt;i&lt;/sub&gt;''}} and one of the input symbols {{mvar|x}} of the automaton are chosen, and the subsets of states are refined into states for which a transition labeled {{mvar|x}} would lead to {{math|''S&lt;sub&gt;i&lt;/sub&gt;''}}, and states for which an {{mvar|x}}-transition would lead somewhere else. When a set {{math|''S&lt;sub&gt;i&lt;/sub&gt;''}} that has already been chosen is split by a refinement, only one of the two resulting sets (the smaller of the two) needs to be chosen again; in this way, each state participates in the sets {{mvar|X}} for {{math|''O''(''s'' log ''n'')}} refinement steps and the overall algorithm takes time {{math|''O''(''ns'' log ''n'')}}, where {{mvar|n}} is the number of initial states and {{mvar|s}} is the size of the alphabet.&lt;ref&gt;{{citation
 | last = Hopcroft | first = John | authorlink = John Hopcroft
 | contribution = An {{math|''n'' log ''n''}} algorithm for minimizing states in a finite automaton
 | mr = 0403320
 | location = New York
 | pages = 189–196
 | publisher = Academic Press
 | title = Theory of machines and computations (Proc. Internat. Sympos., Technion, Haifa, 1971)
 | year = 1971}}.&lt;/ref&gt;

Partition refinement was applied  by {{harvtxt|Sethi|1976}} in an efficient implementation of the [[Coffman–Graham algorithm]] for parallel scheduling. Sethi showed that it could be used to construct a [[lexicographic order|lexicographically ordered]] [[topological sort]] of a given [[directed acyclic graph]] in linear time; this lexicographic topological ordering is one of the key steps of the Coffman–Graham algorithm. In this application, the elements of the disjoint sets are vertices of the input graph and the sets {{mvar|X}} used to refine the partition are sets of neighbors of vertices. Since the total number of neighbors of all vertices is just the number of edges in the graph, the algorithm takes time linear in the number of edges, its input size.&lt;ref&gt;{{citation
 | last = Sethi | first = Ravi | authorlink = Ravi Sethi
 | doi = 10.1137/0205005
 | mr = 0398156
 | issue = 1
 | journal = SIAM Journal on Computing
 | pages = 73–82
 | title = Scheduling graphs on two processors
 | volume = 5
 | year = 1976}}.&lt;/ref&gt;

Partition refinement also forms a key step in [[lexicographic breadth-first search]], a graph search algorithm with applications in the recognition of [[chordal graph]]s and several other important classes of graphs. Again, the disjoint set elements are vertices and the set {{mvar|X}} represent [[neighborhood (graph theory)|sets of neighbors]], so the algorithm takes linear time.&lt;ref&gt;{{citation|first1=D. J.|last1=Rose|first2=R. E.|last2=Tarjan|author2-link=Robert Tarjan|first3=G. S.|last3=Lueker|year=1976|title=Algorithmic aspects of vertex elimination on graphs|journal=[[SIAM Journal on Computing]]|volume=5|pages=266–283|issue=2|doi=10.1137/0205021}}.&lt;/ref&gt;&lt;ref&gt;{{citation|first=Derek G.|last=Corneil|authorlink=Derek Corneil|contribution=Lexicographic breadth first search – a survey|title=Graph-Theoretic Methods in Computer Science|publisher=Springer-Verlag|series=Lecture Notes in Computer Science|volume=3353|year=2004|pages=1–19|url=http://www.springerlink.com/content/nwddhwtbgafm6jaa/}}.&lt;/ref&gt;

==See also==
* [[Refinement (sigma algebra)]]

==References==
{{reflist}}

[[Category:Data structures]]</text>
      <sha1>km4062ebnlad3pz2rei1lpp29t8pnx0</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Succinct data structure</title>
    <ns>0</ns>
    <id>10122951</id>
    <revision>
      <id>618289519</id>
      <parentid>615795401</parentid>
      <timestamp>2014-07-24T16:05:25Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* References */Task 5: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <text xml:space="preserve" bytes="11007">In [[computer science]], a '''succinct data structure''' is a [[data structure]] which uses an amount of space that is &quot;close&quot; to the [[information-theoretic]] lower bound, but (unlike other compressed representations) still allows for efficient query operations.  The concept was originally introduced by Jacobson &lt;ref name=&quot;jacobson1988succinct&quot;/&gt; to encode [[bit vector]]s, (unlabeled) [[tree (data structure)|trees]], and [[planar graph]]s. Unlike general [[lossless data compression]] algorithms, succinct data structures retain the ability to use them in-place, without decompressing them first. A related notion is that of a [[compressed data structure]], in which the size of the data structure depends upon the particular data being represented.

Suppose that &lt;math&gt;Z&lt;/math&gt; is the information-theoretical optimal number of bits needed to store some data. A representation of this data is called

* ''[[implicit data structure|implicit]]'' if it takes &lt;math&gt;Z + O(1)&lt;/math&gt; bits of space,
* ''succinct'' if it takes &lt;math&gt;Z + o(Z)&lt;/math&gt; bits of space, and
* ''compact'' if it takes &lt;math&gt;O(Z)&lt;/math&gt; bits of space.

Implicit structures are thus usually reduced to storing information using some permutation of the input data; the most well-known example of this is the [[Heap_(data_structure)|heap]].

==Succinct dictionaries==

Succinct indexable dictionaries, also called ''rank/select'' dictionaries, form the basis of a number of succinct representation techniques, including [[binary trees]], &lt;math&gt;k&lt;/math&gt;-ary trees and [[multisets]],&lt;ref name=&quot;raman2002succinct&quot;/&gt; as well as [[suffix tree]]s and [[suffix array|arrays]].&lt;ref name=&quot;sadakane2006squeezing&quot;/&gt; The basic problem is to store a subset &lt;math&gt;S&lt;/math&gt; of a universe &lt;math&gt;U =
[0 \dots n) = \{0, 1, \dots, n - 1\}&lt;/math&gt;, usually represented as a bit array &lt;math&gt;B[0 \dots n)&lt;/math&gt; where &lt;math&gt;B[i] = 1&lt;/math&gt; iff &lt;math&gt;i \in S&lt;/math&gt;. An indexable dictionary supports the usual methods on dictionaries (queries, and insertions/deletions in the dynamic case) as well as the following operations:

* &lt;math&gt;\mathbf{rank}_q(x) = |\{ k \in [0 \dots x] : B[k] = q \}|&lt;/math&gt;
* &lt;math&gt;\mathbf{select}_q(x)= \min \{k \in [0 \dots n) : \mathbf{rank}_q(k) = x\}&lt;/math&gt;

for &lt;math&gt;q \in \{0, 1\}&lt;/math&gt;.

In other words, &lt;math&gt;\mathbf{rank}_q(x)&lt;/math&gt; returns the number of elements equal to &lt;math&gt;q&lt;/math&gt; up to position &lt;math&gt;x&lt;/math&gt; while  &lt;math&gt;\mathbf{select}_q(x)&lt;/math&gt; returns the position of the &lt;math&gt; x&lt;/math&gt;-th occurrence of &lt;math&gt; q &lt;/math&gt;.

There is a simple representation &lt;ref name=&quot;jacobson1989space&quot;/&gt; which uses &lt;math&gt;n + o(n)&lt;/math&gt; bits of storage space (the original bit array and an &lt;math&gt;o(n)&lt;/math&gt; auxiliary structure) and supports '''rank''' and '''select''' in constant time. It uses an idea similar to that for [[Range Minimum Query|range-minimum queries]]; there are a constant number of recursions before stopping at a subproblem of a limited size. The bit array &lt;math&gt;B&lt;/math&gt; is partitioned into ''large blocks'' of size &lt;math&gt;l = \lg^2 n&lt;/math&gt; bits and ''small blocks'' of size &lt;math&gt;s = \lg n / 2&lt;/math&gt; bits. For each large block, the rank of its first bit is stored in a separate table &lt;math&gt;R_l[0 \dots n/l)&lt;/math&gt;; each such entry takes &lt;math&gt;\lg n&lt;/math&gt; bits for a total of &lt;math&gt;(n/l) \lg n = n / \lg n&lt;/math&gt; bits of storage. Within a large block, another directory &lt;math&gt;R_s[0 \dots l/s)&lt;/math&gt; stores the rank of each of the &lt;math&gt;l/s = 2 \lg n&lt;/math&gt; small blocks it contains. The difference here is that it only needs &lt;math&gt;\lg l = \lg \lg^2 n = 2 \lg \lg n&lt;/math&gt; bits for each entry, since only the differences from the rank of the first bit in the containing large block need to be stored. Thus, this table takes a total of &lt;math&gt;(n/s) \lg l = 4 n \lg \lg n / \lg n&lt;/math&gt; bits. A lookup table &lt;math&gt;R_p&lt;/math&gt; can then be used that stores the answer to every possible rank query on a bit string of length &lt;math&gt;s&lt;/math&gt; for &lt;math&gt;i \in [0, s)&lt;/math&gt;; this requires &lt;math&gt;2^s s \lg s = O(\sqrt{n} \lg n \lg \lg n)&lt;/math&gt; bits of storage space. Thus, since each of these auxiliary tables take &lt;math&gt;o(n)&lt;/math&gt; space, this data structure supports rank queries in &lt;math&gt;O(1)&lt;/math&gt; time and &lt;math&gt;n + o(n)&lt;/math&gt; bits of space.

To answer a query for &lt;math&gt;\mathbf{rank}_1(x)&lt;/math&gt; in constant time, a constant time algorithm computes

&lt;math&gt;\mathbf{rank}_1(x) = R_l[\lfloor x / l \rfloor] + R_s[\lfloor x / s\rfloor] + R_p[x \lfloor x / s\rfloor, x \text{ mod } s]&lt;/math&gt;

In practice, the lookup table &lt;math&gt;R_p&lt;/math&gt; can be replaced by bitwise operations and smaller tables to perform find the number of bits set in the small blocks. This is often beneficial, since succinct data structures find their uses in large data sets, in which case cache misses become much more frequent and the chances of the lookup table being evicted from closer CPU caches becomes higher.&lt;ref name=&quot;gonzález2005practical&quot; /&gt; Select queries can be easily supported by doing a binary search on the same auxiliary structure used for '''rank'''; however, this takes &lt;math&gt;O(\lg n)&lt;/math&gt; time in the worst case. A more complicated structure using &lt;math&gt;3n/\lg \lg n + O(\sqrt{n} \lg n \lg \lg n) = o(n)&lt;/math&gt; bits of additional storage can be used to support '''select''' in constant time.&lt;ref name=&quot;clark1998compact&quot; /&gt; In practice, many of these solutions have hidden constants in the &lt;math&gt;O(\cdot)&lt;/math&gt; notation which dominate before any asymptotic advantage becomes apparent; implementations using broadword operations and word-aligned blocks often perform better in practice.&lt;ref name=&quot;vigna2008broadword&quot; /&gt;

===Entropy-compressed dictionaries===

The &lt;math&gt;n + o(n)&lt;/math&gt; space approach can be improved by noting that there are &lt;math&gt;\textstyle \binom{n}{m}&lt;/math&gt; distinct &lt;math&gt;m&lt;/math&gt;-subsets of &lt;math&gt;[n)&lt;/math&gt; (or binary strings of length &lt;math&gt;n&lt;/math&gt; with exactly &lt;math&gt;m&lt;/math&gt; 1’s), and thus &lt;math&gt;\textstyle \mathcal{B}(m,n) = \lceil \lg \binom{n}{m} \rceil&lt;/math&gt; is an information theoretic lower bound on the number of bits needed to store &lt;math&gt;B&lt;/math&gt;. There is a succinct (static) dictionary which attains this bound, namely using &lt;math&gt;\mathcal{B}(m,n) + o(\mathcal{B}(m,n))&lt;/math&gt; space.&lt;ref name=&quot;brodnik1999membership&quot; /&gt; This structure can be extended to support '''rank''' and '''select''' queries and takes &lt;math&gt;\mathcal{B}(m,n) + O(m + n \lg \lg n / \lg n)&lt;/math&gt; space.&lt;ref name=&quot;raman2002succinct&quot; /&gt; This bound can be reduced to a space/time tradeoff by reducing the storage space of the dictionary to &lt;math&gt;\mathcal{B}(m,n) + O(n t^t / \lg^t n + n^{3/4})&lt;/math&gt; with queries taking &lt;math&gt;O(t)&lt;/math&gt; time.&lt;ref name=&quot;patrascu2008succincter&quot; /&gt;

==Examples==

When a sequence of variable-length items needs to be encoded, the items can simply be placed one after another, with no delimiters. A separate binary string consisting of 1s in the positions where an item begins, and 0s every where else is encoded along with it. Given this string, the &lt;math&gt;select&lt;/math&gt; function can quickly determine where each item begins, given its index.&lt;ref&gt;{{cite web|url=http://cmph.sourceforge.net/papers/esa09.pdf |title=Hash, displace, and compress|last=Belazzougui|first=Djamal}}&lt;/ref&gt;

Another example is the representation of a [[binary tree]]: an arbitrary binary tree on &lt;math&gt;n&lt;/math&gt; nodes can be represented in &lt;math&gt;2n + o(n)&lt;/math&gt; bits while supporting a variety of operations on any node, which includes finding its parent, its left and right child, and returning the size of its subtree, each in constant time. The number of different binary trees on &lt;math&gt;n&lt;/math&gt; nodes is &lt;math&gt;{\tbinom{2n}{n}}&lt;/math&gt;&lt;math&gt;/(n+1)&lt;/math&gt;. For large &lt;math&gt;n&lt;/math&gt;, this is about &lt;math&gt;4^n&lt;/math&gt;; thus we need at least about &lt;math&gt;\log_2(4^n)=2n&lt;/math&gt; bits to encode it. A succinct binary tree therefore would occupy only &lt;math&gt;2&lt;/math&gt; bits per node.

==References==
&lt;references&gt;
&lt;ref name=&quot;jacobson1988succinct&quot;&gt;
{{Cite journal
| last = Jacobson
| first = G. J
| title = Succinct static data structures
| year = 1988
}}
&lt;/ref&gt;
&lt;ref name=&quot;raman2002succinct&quot;&gt;
{{Cite conference
| isbn = 0-89871-513-X
| pages = 233–242
| last = Raman
| first = R.
|author2=V. Raman |author3=S. S Rao
 | title = Succinct indexable dictionaries with applications to encoding k-ary trees and multisets
| booktitle = Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms
| date = 2002
| url = http://www.cs.cmu.edu/afs/cs.cmu.edu/project/aladdin/wwwlocal/hash/RaRaRa02.pdf
}}
&lt;/ref&gt;
&lt;ref name=&quot;sadakane2006squeezing&quot;&gt;
{{Cite conference
| isbn = 0-89871-605-5
| pages = 1230–1239
| last = Sadakane
| first = K.
|author2=R. Grossi
 | title = Squeezing succinct data structures into entropy bounds
| booktitle = Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm
| date = 2006
| url = http://www.dmi.unisa.it/people/cerulli/www/WSPages/WSFiles/Abs/S3/S33_abs_Grossi.pdf
}}
&lt;/ref&gt;
&lt;ref name=&quot;jacobson1989space&quot;&gt;
{{Cite journal
| last = Jacobson
| first = G.
| title = Space-efficient static trees and graphs
| year = 1989
| url = http://www.cs.cmu.edu/afs/cs/project/aladdin/wwwlocal/compression/00063533.pdf
}}
&lt;/ref&gt;
&lt;ref name=&quot;gonzález2005practical&quot;&gt;
{{Cite conference
| pages = 27–38
| last = González
| first = R.
|author2=S. Grabowski |author3=V. Mäkinen |author4=G. Navarro
 | title = Practical implementation of rank and select queries
| booktitle = Poster Proceedings Volume of 4th Workshop on Efficient and Experimental Algorithms (WEA)
| date = 2005
| url = http://www.dcc.uchile.cl/~gnavarro/algoritmos/ps/wea05.pdf
}}
&lt;/ref&gt;
&lt;ref name=&quot;clark1998compact&quot;&gt;
{{Cite journal
| last = Clark
| first = D.
| title = Compact pat trees
| year = 1998
| url = https://uwspace.uwaterloo.ca/bitstream/10012/64/1/nq21335.pdf
}}
&lt;/ref&gt;
&lt;ref name=&quot;vigna2008broadword&quot;&gt;
{{Cite journal
| pages = 154–168
| last = Vigna
| first = S.
| title = Broadword implementation of rank/select queries
| journal = Experimental Algorithms
| year = 2008
| url = http://sux.dsi.unimi.it/paper.pdf
| doi = 10.1007/978-3-540-68552-4_12
| series = Lecture Notes in Computer Science
| isbn = 978-3-540-68548-7
| volume = 5038
}}
&lt;/ref&gt;
&lt;ref name=&quot;brodnik1999membership&quot;&gt;
{{Cite journal
| volume = 28
| issue = 5
| pages = 1627–1640
| last = Brodnik
| first = A.
|author2=J. I Munro
 | title = Membership in constant time and almost-minimum space
| journal = SIAM J. Comput.
| year = 1999
| url = http://www.cs.cmu.edu/afs/cs.cmu.edu/project/aladdin/wwwlocal/compression/BM99.pdf
| doi = 10.1137/S0097539795294165
}}
&lt;/ref&gt;
&lt;ref name=&quot;patrascu2008succincter&quot;&gt;
{{Cite conference
| pages = 305–313
| last = Pătraşcu
| first = M. | authorlink = Mihai Pătraşcu
| title = Succincter
| booktitle = Foundations of Computer Science, 2008. FOCS'08. IEEE 49th Annual IEEE Symposium on
| date = 2008
| url = http://people.csail.mit.edu/mip/papers/succinct/succinct.pdf
}}
&lt;/ref&gt;
&lt;/references&gt;

{{DEFAULTSORT:Succinct Data Structure}}
[[Category:Data structures]]</text>
      <sha1>tv0trfl0tzhpho67jzpujggbqv6delv</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Data structure</title>
    <ns>0</ns>
    <id>8519</id>
    <revision>
      <id>626672948</id>
      <parentid>626595987</parentid>
      <timestamp>2014-09-22T20:23:55Z</timestamp>
      <contributor>
        <username>Richard Yin</username>
        <id>21253570</id>
      </contributor>
      <comment>Reorganizing. I have decided to skip talk page discussion because as far as I can tell the article is edited infrequently and the talk page more so. Feel free to revert and message me if you think this is wrong</comment>
      <text xml:space="preserve" bytes="8896">[[Image:Hash table 3 1 1 0 1 0 0 SP.svg|thumb|315px|right|A [[hash table]]]]
In [[computer science]], a '''data structure''' is a particular way of organizing [[data (computing)|data]] in a computer so that it can be used [[algorithmic efficiency|efficiently]].&lt;ref&gt;Paul E. Black (ed.), entry for ''data structure'' in ''[[Dictionary of Algorithms and Data Structures]]. U.S. [[National Institute of Standards and Technology]]. 15 December 2004. [http://xlinux.nist.gov/dads/HTML/datastructur.html Online version] Accessed May 21, 2009.''&lt;/ref&gt;&lt;ref&gt;Entry ''data structure'' in the [[Encyclopædia Britannica]] (2009) [http://www.britannica.com/EBchecked/topic/152190/data-structure Online entry] accessed on May 21, 2009.&lt;/ref&gt;

Different kinds of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, [[B-tree]]s are particularly well-suited for implementation of databases, while [[compiler]] implementations usually use [[hash table]]s to look up identifiers.

Data structures provide a means to manage large amounts of data efficiently, such as large [[database]]s and [[web indexing|internet indexing services]]. Usually, efficient data structures are a key in designing efficient [[algorithm]]s. Some formal design methods and [[programming language]]s emphasize data structures, rather than algorithms, as the key organizing factor in software design. Storing and retrieving can be carried out on data stored in both [[main memory]] and in [[secondary memory]].

==Overview==
Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by an address{{snd}} a bit string that can be itself stored in memory and manipulated by the program. Thus, the [[Array data structure|array]] and [[record (computer science)|record]] data structures are based on computing the addresses of data items with [[arithmetic operations]]; while the [[linked data structure]]s are based on storing addresses of data items within the structure itself. Many data structures use both principles, sometimes combined in non-trivial ways (as in [[XOR linked list|XOR linking]]).

The implementation of a data structure usually requires writing a set of [[subroutine|procedures]] that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an [[abstract data type]], a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).

==Examples==
{{main|List of data structures}}
There are numerous types of data structures:
*An [[array data structure|''array'']] is a number of elements in a specific order. They are accessed using an integer to specify which element is required (although the elements may be of almost any type). Typical implementations allocate contiguous memory words for the elements of arrays (but this is not always a necessity). Arrays may be fixed-length or expandable.
*[[Record (computer science)|''Records'']] (also called ''tuples'' or ''structs'') are among the simplest data structures. A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. The elements of records are usually called ''fields'' or ''members''.
*An ''[[associative array]]'' (also called a ''dictionary'' or ''map'') is a more flexible variation on a record, in which [[name-value pair]]s can be added and deleted freely. A [[hash table]] is a common implementation of an associative array.
*A [[Union (computer science)|''union'']] type specifies which of a number of permitted primitive types may be stored in its instances, e.g. &quot;float or long integer&quot;. Contrast with a [[record (computer science)|record]], which could be defined to contain a float ''and'' an integer; whereas, in a union, there is only one value at a time. Enough space is allocated to contain the widest member datatype.
*A ''[[tagged union]]'' (also called a [[variant type|''variant'']], ''variant record'', ''discriminated union'', or ''disjoint union'') contains an additional field indicating its current type, for enhanced type safety.
*A [[set (abstract data type)|''set'']] is an [[abstract data structure]] that can store specific values, without any particular [[sequence|order]], and with no repeated values. Values themselves are not retrieved from sets, rather one tests a value for membership to obtain a boolean &quot;in&quot; or &quot;not in&quot;.
*[[Graph (abstract data type)|''Graphs'']] and [[tree (data structure)|''trees'']] are [[linked data structure|linked]] [[abstract data type|abstract data structures]] composed of [[node (computer science)|''nodes'']]. Each node contains a value and also one or more [[pointer (computer programming)|pointers]] to other nodes. Graphs can be used to represent networks, while variants of trees can be used for [[sorting algorithm|sorting]] and [[search algorithm|searching]], having their nodes arranged in some relative order based on their values.
*An [[object (computer science)|''object'']] contains data fields, like a record, as well as various [[Method (computer programming)|methods]]. In the context of [[object-oriented programming]], records are known as [[plain old data structures]] to distinguish them from objects.

==Language support==
Most [[assembly language]]s and some low-level languages, such as [[BCPL]] (Basic Combined Programming Language), lack support for data structures.  On the other hand, many [[high-level programming language]]s and some higher-level assembly languages, such as [[MASM]], have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the [[C (programming language)|C]] and [[Pascal (programming language)|Pascal]] languages support [[struct]]s and records, respectively, in addition to vectors (one-dimensional [[array data type|arrays]]) and multi-dimensional arrays.

Most programming languages feature some sort of [[Library (computing)|library]] mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the [[C++]] [[Standard Template Library]], the [[Java Collections Framework]], and [[Microsoft]]'s [[.NET Framework]].

Modern languages also generally support [[modular programming]], the separation between the [[interface (computing)|interface]] of a library module and its implementation. Some provide [[opaque data type]]s that allow clients to hide implementation details. [[Object-oriented programming language]]s, such as [[C++]], [[Java (programming language)|Java]] and [[Smalltalk]] may use [[classes (computer science)|classes]] for this purpose.

Many known data structures have [[concurrent data structure|concurrent]] versions that allow multiple computing threads to access the data structure simultaneously.

==See also==
{{Wikipedia books|Data structures}}
{{Div col||25em}}
* [[List of data structures]]
* [[Plain old data structure]]
* [[Concurrent data structure]]
* [[Data model]]
* [[Dynamization]]
* [[Linked data structure]]
* [[Persistent data structure]]
{{Div col end}}

==References==
{{Reflist}}

==Further reading==
* Peter Brass, ''Advanced Data Structures'', [[Cambridge University Press]], 2014
* [[Donald Knuth]], ''[[The Art of Computer Programming]]'', vol. 1. [[Addison-Wesley]], 3rd edition, 1997.
* Dinesh Mehta and [[Sartaj Sahni]] ''Handbook of Data Structures and Applications'', [[Chapman and Hall]]/[[CRC Press]], 2007.
* [[Niklaus Wirth]], ''Algorithms and Data Structures'', [[Prentice Hall]], 1985.
* Diane Zak, Introduction to programming with c++, copyright 2011 Cengage Learning Asia Pte Ltd

==External links==
{{Sister project links|wikt=data structure|commons=Category:Data structures|b=Data Structures|v=Topic:Data structures|n=no}}
* [http://scanftree.com/Data_Structure/  course on data structures ] 
* [http://scanftree.com/programs/operation/data-structure/  Data structures Programs Examples in c,java]
* [http://academicearth.org/computer-science/ UC Berkeley video course on data structures]
* [http://nist.gov/dads/ Descriptions] from the [[Dictionary of Algorithms and Data Structures]]
* [http://www.cs.auckland.ac.nz/software/AlgAnim/ds_ToC.html Data structures course]
* [http://msdn.microsoft.com/en-us/library/aa289148(VS.71).aspx An Examination of Data Structures from .NET perspective]
* [http://people.cs.vt.edu/~shaffer/Book/C++3e20110915.pdf Schaffer, C. ''Data Structures and Algorithm Analysis'']

{{Data structures}}
{{Data types}}
{{Data model}}

{{DEFAULTSORT:Data Structure}}
[[Category:Data structures| ]]</text>
      <sha1>suqjytu9pf5g4vgrlmm6u65xdk6wes4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Symbol table</title>
    <ns>0</ns>
    <id>475294</id>
    <revision>
      <id>619319590</id>
      <parentid>618760518</parentid>
      <timestamp>2014-07-31T17:50:15Z</timestamp>
      <contributor>
        <ip>142.76.31.138</ip>
      </contributor>
      <comment>/* Implementation */ spaces after periods</comment>
      <text xml:space="preserve" bytes="5931">{{norefs|date=November 2012}}

In [[computer science]], a '''symbol table''' is a [[data structure]] used by a language translator such as a [[compiler]] or [[interpreter (computing)|interpreter]], where each [[identifier]] in a program's [[source code]] is associated with information relating to its declaration or appearance in the source, such as its [[data type|type]], [[scope (programming)|scope]] level and sometimes its [[Memory address|location]].

==Implementation==
A common implementation technique is to use a [[hash table]]. A compiler may use one large symbol table for all symbols or use separated, hierarchical symbol tables for different [[Scope (programming)|scopes]]. There are also trees, linear lists and [[self-organizing list]]s which can be used to implement symbol table. It also simplifies the classification of literals in tabular format. The symbol table is accessed by most phases of a compiler, beginning with the lexical analysis to optimization.

==Uses==
An [[object file]] will contain a symbol table of the identifiers it contains that are externally visible.  During the linking of different object files, a [[Linker (computing)|linker]] will use these symbol tables to resolve any unresolved references.

A symbol table may only exist during the translation process, or it may be embedded in the output of that process for later exploitation, for example, during an interactive [[debugger|debugging session]], or as a resource for formatting a diagnostic report during or after [[execution (computers)|execution]] of a program.

While [[reverse engineering]] an executable, many tools refer to the symbol table to check what addresses have been assigned to global variables and known functions. If the symbol table has been [[strip (Unix)|stripped]] or cleaned out before being converted into an executable, tools will find it harder to determine addresses or understand anything about the program.

At that time of accessing variables and allocating memory dynamically, a compiler should perform many works and as such the extended stack model requires the '''symbol table'''.

==Example==

Consider the following program written in [[C (programming language)|C]]:
&lt;source lang=&quot;C&quot;&gt;
// Declare an external function
extern double bar(double x);

// Define a public function
double foo(int count)
{
    double  sum = 0.0;

    // Sum all the values bar(1) to bar(count)
    for (int i = 1;  i &lt;= count;  i++)
        sum += bar((double) i);
    return sum;
}
&lt;/source&gt;

A C compiler that parses this code will contain at least the following symbol table entries:
{| class=&quot;wikitable&quot;
|- style=&quot;background:silver&quot;
!style=&quot;width:8em&quot; | Symbol name
!style=&quot;width:10em&quot; | Type
!style=&quot;width:10em&quot; | Scope
|-
| &lt;code&gt;bar&lt;/code&gt; || function, double || extern
|-
| &lt;code&gt;x&lt;/code&gt; || double || function parameter
|-
| &lt;code&gt;foo&lt;/code&gt; || function, double || global
|-
| &lt;code&gt;count&lt;/code&gt; || int || function parameter
|-
| &lt;code&gt;sum&lt;/code&gt; || double || block local
|-
| &lt;code&gt;i&lt;/code&gt; || int || for-loop statement
|-
|}

In addition, the symbol table will also contain entries generated by the compiler for intermediate expression values (e.g., the expression that casts the &lt;code&gt;i&lt;/code&gt; loop variable into a &lt;code&gt;double&lt;/code&gt;, and the return value of the call to function &lt;code&gt;bar()&lt;/code&gt;), statement labels, and so forth.

As another example, the symbol table of a small program is listed below. The table itself was generated using the [[GNU Binary Utilities|GNU binutils']] [[nm (Unix)|nm]] utility. There is one data symbol, (noted by the &quot;D&quot; type), and many functions (self defined as well as from the standard library). The first column is where the symbol is located in the memory, the second is &quot;[http://sourceware.org/binutils/docs-2.17/binutils/nm.html#nm The symbol type]&quot; and the third is the name of the symbol. By passing suitable parameters, the symbol table was made to sort on basis of address.
{| class=&quot;wikitable&quot; style=&quot;text-align:center&quot;
|+Example table
|-
! Address !! Type !! Name
|-
|00000020 || a || T_BIT
|-
|00000040 || a || F_BIT
|-
|00000080 || a || I_BIT
|-
|20000004 || t || irqvec
|-
|20000008 || t || fiqvec
|-
|2000000c || t || InitReset
|-
|20000018 || T || _main
|-
|20000024 || t || End
|-
|20000030 || T || AT91F_US3_CfgPIO_useB
|-
|2000005c || t || AT91F_PIO_CfgPeriph
|-
|200000b0 || T || main
|-
|20000120 || T || AT91F_DBGU_Printk
|-
|20000190 || t || AT91F_US_TxReady
|-
|200001c0 || t || AT91F_US_PutChar
|-
|200001f8 || T || AT91F_SpuriousHandler
|-
|20000214 || T || AT91F_DataAbort
|-
|20000230 || T || AT91F_FetchAbort
|-
|2000024c || T || AT91F_Undef
|-
|20000268 || T || AT91F_UndefHandler
|-
|20000284 || T || AT91F_LowLevelInit
|-
|200002e0 || t || AT91F_DBGU_CfgPIO
|-
|2000030c || t || AT91F_PIO_CfgPeriph
|-
|20000360 || t || AT91F_US_Configure
|-
|200003dc || t || AT91F_US_SetBaudrate
|-
|2000041c || t || AT91F_US_Baudrate
|-
|200004ec || t || AT91F_US_SetTimeguard
|-
|2000051c || t || AT91F_PDC_Open
|-
|2000059c || t || AT91F_PDC_DisableRx
|-
|200005c8 || t || AT91F_PDC_DisableTx
|-
|200005f4 || t || AT91F_PDC_SetNextTx
|-
|20000638 || t || AT91F_PDC_SetNextRx
|-
|2000067c || t || AT91F_PDC_SetTx
|-
|200006c0 || t || AT91F_PDC_SetRx
|-
|20000704 || t || AT91F_PDC_EnableRx
|-
|20000730 || t || AT91F_PDC_EnableTx
|-
|2000075c || t || AT91F_US_EnableTx
|-
|20000788 || T || __aeabi_uidiv
|-
|20000788 || T || __udivsi3
|-
|20000884 || T || __aeabi_uidivmod
|-
|2000089c || T || __aeabi_idiv0
|-
|2000089c || T || __aeabi_ldiv0
|-
|2000089c || T || __div0
|-
|200009a0 || D || _data
|-
|200009a0 || A || _etext
|-
|200009a0 || D || holaamigosh
|-
|200009a4 || A || __bss_end__
|-
|200009a4 || A || __bss_start
|-
|200009a4 || A || __bss_start__
|-
|200009a4 || A || _edata
|-
|200009a4 || A || _end
|}

==See also==
*[[Debug symbol]]

[[Category:Compiler construction]]
[[Category:Compiler structures]]
[[Category:Data structures]]</text>
      <sha1>tq3uwaevz18c97ol9vnb7gt1ph5bpvy</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Synonym (database)</title>
    <ns>0</ns>
    <id>25753044</id>
    <revision>
      <id>605785253</id>
      <parentid>597201989</parentid>
      <timestamp>2014-04-25T18:33:01Z</timestamp>
      <contributor>
        <ip>180.215.72.81</ip>
      </contributor>
      <comment>/* Public synonyms */</comment>
      <text xml:space="preserve" bytes="5352">{{cleanup|date=January 2010}}
A '''synonym''' is an alias or alternate name for a [[Table (database)|table]], [[View (database)|view]], [[sequence]], or other [[Database schema|schema]] object. They are used mainly to make it easy for users to access database objects owned by other users. They hide the underlying object's identity and make it harder for a malicious program or user to target the underlying object. Because a synonym is just an alternate name for an object, it requires no storage other than its definition. When an application uses a synonym, the [[DBMS]] forwards the request to the synonym's underlying base object. By coding your programs to use synonyms instead of database object names, you insulate yourself from any changes in the name, ownership, or object locations. If you frequently refer to a database object that has a long name, you might appreciate being able to refer to it with a shorter name without having to rename it and alter the code referring to it.

Synonyms are very powerful from the point of view of allowing users access to objects that do not lie within their [[Database schema|schema]]. All synonyms have to be created explicitly with the CREATE SYNONYM command and the underlying objects can be located in the same [[database]] or in other databases that are connected by {{clarify span|database links|date=September 2012}}.

There are two major uses of synonyms:
* '''Object invisibility''': Synonyms can be created to keep the original object hidden from the user.
* '''Location invisibility''': Synonyms can be created as aliases for tables and other objects that are not part of the local database.

When you create a table or a procedure, it is created in your schema, and other users can access it only by using your schema name as a prefix to the object's name.  The way around for this is for the schema owner creates a synonym with the same name as the table name.

== Public synonyms ==

Public synonyms are owned by special schema in the [[Oracle Database]] called PUBLIC. As mentioned earlier, public synonyms can be referenced by all users in the database. Public synonyms are usually created by the application owner for the tables and other objects such as procedures and packages so the users of the application can see the objects.

The following code shows how to create a public synonym for the employee table:

&lt;pre&gt;

CREATE PUBLIC SYNONYM employees for hr.employees;

&lt;/pre&gt;

Now any user can see the table by just typing the original table name. If you wish, you could provide a different table name for that table in the CREATE SYNONYM statement. Remember that the DBA must public synonyms. Just because you can see a table through public (or private) synonym doesn’t mean that you can also perform SELECT, INSERT, UPDATE or DELETE  operations on the table. To be able to perform those operations, a user needs specific privileges for the underlying object, either directly or through  roles from the application owner.

== Private synonyms ==

A private synonym is a synonym within a database schema that a developer typically uses to mask the true name of a table, view stored procedure, or other database object in an application schema.

Private synonyms, unlike public synonyms, can be referenced only by the schema that owns the table or object. You may want to create private synonyms when you want to refer to the same table by different contexts. Private synonym overrides public synonym definitions. You create private synonyms the same way you create public synonyms, but you omit the PUBLIC keyword in the CREATE statement.

The following example shows how to create a private synonym called addresses for the locations table. Note that once you create the private synonym, you can refer to the synonym exactly as you would the original table name.

&lt;pre&gt;

CREATE SYNONYM addresses FOR hr.locations;

&lt;/pre&gt;

== Drop a synonym ==

Synonyms, both private and public, are dropped in the same manner by using the DROP SYNONYM command, but there is one important difference. If  you are dropping a public synonym; you need to add the keyword PUBLIC after the keyword DROP.

&lt;pre&gt;

DROP SYNONYM addresses;

&lt;/pre&gt;

The ALL_SYNONYMS (or DBA_SYNONYMS) view provides information on all synonyms in your database.

== References ==

* {{Cite book
|last=Palinski 
|first=John Adolph 
|authorlink=
|coauthors=
|year=2002
|title=Oracle SQL and PL/SQL Handbook: A Guide for Data Administrators, Developers, and Business Analysts
|edition=
|publisher=[[Addison–Wesley]]
|location=
|id=
|isbn=978-0-201-75294-6
}}
* {{Cite book
|last=Gennick
|first=Jonathan
|authorlink=
|coauthors=
|year=2004
|title=Oracle SQL*Plus: the definitive guide
|publisher=[[O'Reilly Media]]
|location=
|id=
|isbn=978-0-596-00746-1
}}
* {{Cite book
|last=Alapati
|first=Sam R
|authorlink=
|coauthors=
|year=2005
|title=Expert Oracle Database 10g Administration
|publisher=[[Apress]]
|location=
|id=
|isbn=978-1-59059-451-3
}}
* {{Cite book
|last=Bobrowski
|first=Steve
|authorlink=
|coauthors=
|year=
|title=Hands-on Oracle Database 10g Express Edition for Windows
|publisher=[[McGraw-Hill]]
|location=
|id=
|isbn=978-0-07-226331-2
}}

{{Databases}}

[[Category:Relational model]]
[[Category:Data structures]]
[[Category:Data modeling]]
[[Category:Database management systems]]
[[Category:Databases]]</text>
      <sha1>byd0gi2gbh9jxin1ne3bsj3ujan0g9m</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Container (abstract data type)</title>
    <ns>0</ns>
    <id>5170615</id>
    <revision>
      <id>584140063</id>
      <parentid>582482358</parentid>
      <timestamp>2013-12-02T00:49:54Z</timestamp>
      <contributor>
        <username>Gmelli</username>
        <id>205383</id>
      </contributor>
      <minor/>
      <comment>/* See also */</comment>
      <text xml:space="preserve" bytes="6349">{{For|the abstract notion of containers in [[type theory]]|Container (type theory)}}
{{Cleanup|reason=text is clunky|date=March 2012}}
In [[computer science]], a '''container''' is a [[Class (computer science)|class]], a [[data structure]],&lt;ref&gt;Paul E. Black (ed.), entry for ''data structure'' in ''[[Dictionary of Algorithms and Data Structures]]. US [[National Institute of Standards and Technology]].15 December 2004. Accessed on Oct 04, 2011.&lt;/ref&gt;&lt;ref&gt;Entry ''data structure'' in the [[Encyclopædia Britannica]] (2009) [http://www.britannica.com/EBchecked/topic/152190/data-structure Online entry] Accessed on Oct 04, 2011.&lt;/ref&gt; or an [[abstract data type]] (ADT) whose instances are collections of other objects. In other words; they are used for storing objects in an organized way following specific access rules. The size of the container depends on the number of the objects (elements) it contains.The underlying implementation of various types of containers may vary in space and time complexity allowing for flexibility in choosing the right implementation for a given scenario.

==Overview==
Containers can be studied under three points of views.

# ''Access'' : It means accessing the container elements. In the case of arrays, accessing is done with the array index. For stacks, access of elements is done using [[LIFO (computing)|LIFO]] (Last In First Out) &lt;ref name = &quot;investopedia&quot;&gt;[http://www.investopedia.com/terms/l/lifo.asp#axzz1ZoGPLx59 LIFO(investopedia.com)]&lt;/ref&gt; (alternative name '''FILO''' (First In Last Out) and in queues it is done using [[FIFO (computing)|FIFO]] (First In First Out).&lt;ref name = &quot;investopedia&quot;&gt;[http://www.investopedia.com/terms/f/fifo.asp#axzz1ZoGPLx59 FIFO(investopedia.com)]&lt;/ref&gt;&lt;ref name = &quot;businessdictionary&quot;&gt;[http://www.businessdictionary.com/definition/first-in-first-out-FIFO.html FIFO(businessdictionary.com)]&lt;/ref&gt;
# ''Storage'' : It includes storing of items of containers. Some containers are finite containers and some are infinite containers.
# ''Traversal'' : It includes how the item can be traversed.

Container classes are expected to implement methods to do the following:
* create a new empty container (constructor),
* report the number of objects it stores (size),
* delete all the objects in the container (clear),
* insert new objects into the container,
* remove objects from it,
* provide access to the stored objects.

Containers are sometimes implemented in conjunction with [[iterator]]s.

==Types==
Containers can be divided into two groups:
# ''Value based containers''
# ''Reference based containers''

===Value based containers===
Store copies of objects. If we access an object, the object returns a copy of it. If an external object is changed after it has been inserted in the container it will not affect the content of the container.

===Reference based containers===
Store [[Pointer (computer programming)|pointer]]s or [[Reference (computer science)|reference]]s to the object. If we access an object, the object returns a reference to it. If an external object is changed after it has been inserted in the container, it affects the content of the container.

==Single or associative==
A container may be:
# ''Single value''
# ''[[Associative array|Associative]]''

===Single value containers===
Each object is stored independently in the container and it is accessed directly or with an [[iterator]].

===Associative containers===
[[Associative array]], map, or dictionary is a container composed of (key,value) pairs, such that each key appears at most once in the container. The key is used to find the value, the object, if it is stored in the container.

==Examples of containers==
Containers are divided in the [[Standard Template Library]] into [[associative container]]s and standard [[sequence container]]s. Besides this two types, so-called [[Standard Template Library#Containers|container adaptor]]s exist. Data structures that are implemented by containers include [[array data structure|array]]s, [[List (computing)|list]]s, [[Associative array|map]]s, [[Queue (data structure)|queue]]s, [[Set (computer science)|set]]s, [[Stack (data structure)|stack]]s, [[table (information)|table]]s, [[tree (data structure)|tree]]s, and [[Vector (C++)|vector]]s.

==Graphic containers==
[[Widget toolkit]]s use special [[Widget (computing)|widgets]] also called ''Containers'' to group the other widgets together ([[Window (computing)|windows]], [[Panel (computer software)|panels]], ...). Apart from their graphical properties, they have the same type of behavior as '''container classes''', as they keep a list of their child [[Widget (computing)|widgets]], and allow to add, remove, or retrieve [[Widget (computing)|widgets]] amongst their children.

==Implementations==
{{Expand section|date=May 2007}}
* .NET: [http://msdn.microsoft.com/en-us/library/system.collections.aspx System.Collections (MSDN)]
* ActionScript3: [http://sibirjak.com/osflash/projects/as3commons-collections/ AS3Commons Collections Framework]
* C++: [[C++ Standard Library]] (SC++L) or the obsolete [[Standard Template Library]] (STL)
* Java: [[Java collections framework]] (JCF)
* [[Objective-C]]: part of the [[Foundation Kit]]
* [[PL/SQL]] Collections&lt;ref&gt;{{Cite web
| title = PL/SQL Collections and Records
| accessdate = 2013-04-20
| url = http://docs.oracle.com/cd/E11882_01/appdev.112/e25519/composites.htm#CIHIEBJC
}}&lt;/ref&gt;
* [[Scala (programming language)|Scala]] Mutable and Immutable Collections in the packages &lt;code&gt;scala.collection.mutable&lt;/code&gt; and &lt;code&gt;scala.collection.immutable&lt;/code&gt;

==See also==
*[[List of data structures]]
*[[Standard Template Library#Containers]]
*[[Collection (abstract data type)]]
*[[Stack (data structure)|Stack data structure]]

==References==
{{Reflist}}

==External links==
*[http://help.sap.com/saphelp_40b/helpdata/en/c5/e4b14a453d11d189430000e829fbbd/content.htm Container Data Structure Declaration and Initialization]
*[http://www.acornarcade.com/articles/Building_the_Dream_1_-_Container_data_structures/index1162.html Container data structures]
{{Data structures}}
{{Data types}}
{{Use dmy dates|date=March 2012}}

{{DEFAULTSORT:Container (Data Structure)}}
[[Category:Abstract data types]]
[[Category:Object-oriented programming]]
[[Category:Data structures]]

[[eo:Ujo]]
[[pt:Container (programação)]]
[[qu:Wisina]]</text>
      <sha1>q5qovggp750lkrd4q7py8w2qc4bye6x</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Sorted array</title>
    <ns>0</ns>
    <id>15844857</id>
    <revision>
      <id>623382846</id>
      <parentid>605680449</parentid>
      <timestamp>2014-08-30T00:24:25Z</timestamp>
      <contributor>
        <ip>94.175.90.208</ip>
      </contributor>
      <text xml:space="preserve" bytes="4985">{{Infobox data structure
|name=Sorted array
|type=Array
|invented_by=[[John von Neumann]]
|invented_year=1945
|space_avg=O(n)
|space_worst=O(n)
|search_avg=O(log n)
|search_worst=O(log n)
|insert_avg=O(n)
|insert_worst=O(n)
|delete_avg=O(n)
|delete_worst=O(n)
}}

A '''sorted array''' is an [[array data structure]] in which each element is sorted in numerical, alphabetical, or some other order, and placed at equally spaced addresses in computer memory. It is typically used in [[computer science]] to implement [[static and dynamic data structures|static]] [[lookup table]]s to hold multiple values which have the same [[data type]]. Sorting an array is useful in organising [[data]] in ordered form and recovering them rapidly.

==Methods==
There are many well-known methods by which an array can be sorted, which include, but are not limited to including:  [[selection sort]], [[bubble sort]], [[insertion sort]], [[merge sort]], [[quicksort]], [[heapsort]], and [[counting sort]].{{Citation needed|date=November 2011}}  These sorting techniques have different algorithms associated with them, and there are therefore different advantages to using each method.

==Overview==
Sorted arrays are the most space-efficient data structure with the best [[locality of reference]] for sequentially stored data.{{Citation needed|date=November 2011}}

Elements within a sorted array are found using a [[binary search]], in O(log ''n''); thus sorted arrays are suited for cases when one needs to be able to look up elements quickly, e.g. as a [[Set (computer science)|set]] or [[Set (computer science)#Multiset|multiset]] [[data structure]]. This complexity for lookups is the same as for [[self-balancing binary search tree]]s.

In some data structures, an array of structures is used. In such cases, the same sorting methods can be used to sort the structures according to some key as a structure element; for example, sorting records of students according to roll numbers or names or grades.

If one is using a sorted [[dynamic array]], then it is possible to insert and delete elements. The insertion and deletion of elements in a sorted array executes at O(''n''), due to the need to shift all the elements following the element to be inserted or deleted; in comparison a self-balancing binary search tree inserts and deletes at O(log ''n''). In the case where elements are deleted or inserted at the end, a sorted dynamic array can do this in [[Amortized analysis|amortized]] O(1) time while a self-balancing binary search tree always operates at O(log ''n'').

Elements in a sorted array can be looked up by their index ([[random access]]) at O(1) time, an operation taking O(log ''n'') or O(''n'') time for more complex data structures.

==History==
[[John von Neumann]] wrote the first array sorting program ([[merge sort]]) in 1945, when the [[EDVAC|first stored-program computer]] was still being built.&lt;ref&gt;Donald Knuth, ''The Art of Computer Programming'', vol. 3. Addison-Wesley&lt;/ref&gt;

==Applications of sorted arrays==
'''1) Commercial Computing:'''&lt;ref&gt;http://algs4.cs.princeton.edu/25applications/&lt;/ref&gt;

Government organisations, private companies and many web based applications have to deal with huge amounts of data. The data will often have to be accessed multiple times. Keeping the data in a sorted format allows for quick and easy recovery of data.

'''2) In [[discrete mathematics]]:'''
Sorted arrays can be used to implement [[Dijkstra's algorithm]] or [[Prim's algorithm]].  Also, algorithms like [[Kruskal's Algorithm]] for finding minimal spanning trees.

'''3) In priority scheduling''':  At the [[operating system]] level many processes are pending at a time, but CPU can handle only one process at a single instance in time.  Therefore, priorities are associated to each process.Then the processes are sent to CPU according to the highest priority by using sorted array of process ID's. Here, processes got sorted depending upon their priorities and then CPU is allocated to them. The process having the highest priority takes first position in sorted array. Hence priority-wise system processes scheduling is done.&lt;ref&gt;{{cite book|title=Operating System Concepts by Peter B. Galvin|publisher=WILEY-INDIA Pvt. limited|isbn=978-81-265-2051-0}}&lt;/ref&gt;

'''4) In Shortest-Job-First Scheduling''': This is the special case of priority scheduling. Here, Processes get sorted according to burst time of the processes. The process requiring the shortest time will be allocated CPU first. Hence, Processes are being sent to CPU according to their burst time.
[[File:Priority scheduling.pdf|right|300px||]]
{| class=&quot;wikitable&quot;
|-
! Process !! Burst time
|-
| P1 ||  3 
|-
| P2 || 4
|-
| P3 || 1
|-
| P4 || 8
|-
| P5 || 6
|}

==See also==
* [[array data structure]]
* [[insertion sort]]
* [[sorting algorithm]]
* [[binary search algorithm]]
* [[bubble sort]]
* [[Kruskal's Algorithm]]
* [[Prim's algorithm]].

==References==
{{reflist|2}}

[[Category:Data structures]]</text>
      <sha1>6ufs134h5q6arz8e8xhhrzt5dhosi5h</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Collection (abstract data type)</title>
    <ns>0</ns>
    <id>12119816</id>
    <revision>
      <id>621641714</id>
      <parentid>598420042</parentid>
      <timestamp>2014-08-17T16:06:12Z</timestamp>
      <contributor>
        <username>Chmarkine</username>
        <id>15398482</id>
      </contributor>
      <minor/>
      <comment>/* Implementations */change to https if the server sends [[HTTP Strict Transport Security|HSTS]] header, replaced: http://docs.python.org → https://docs.python.org using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="8802">In [[computer science]], a '''collection''' or '''container''' is a grouping of some variable number of data items (possibly zero) that have some shared significance to the problem being solved and need to be operated upon together in some controlled fashion.  Generally, the data items will be of the same type or, in languages supporting inheritance, derived from some common ancestor type. A collection is a concept applicable to [[abstract data type]]s, and does not prescribe a specific implementation as a concrete [[data structure]], though often there is a conventional choice; see [[container (type theory)]] for [[type theory]] discussion.

Some different kinds of collections are [[List (abstract data type)|lists]], [[Set (computer science)|sets]], [[Multiset|bags]] (or multisets), [[Tree (data structure)|trees]] and [[Graph (data structure)|graphs]].  An [[enumerated type]] may be either a list or a set.

A fixed-size table (or array) is usually not considered a collection because it holds a fixed number of items, although tables/arrays commonly play a role in the implementation of collections. Variable-sized arrays are generally considered collections, and fixed-size arrays may likewise considered a collection, albeit with limitations.

==Linear collections==
{{also|Linear data structure}}
Many collections behave as if they are storing data in a line, ordered in some way, with access to one or both ends. The actual data structure implementing such a collection need not be linear – for example, a priority queue is often implemented as a heap, which is a kind of tree. Important such collections include:
* [[List (abstract data type)|List]]
* [[Array data type|Array]]
* [[Stack (abstract data type)|Stack]] (FILO, LIFO)
* [[Queue (abstract data type)|Queue]] (FIFO, LILO)
* [[Priority queue]] (often implemented as a [[Heap (data structure)|heap]])
* [[Double-ended queue]] (deque)
* [[Double-ended priority queue]] (DEPQ)

===Lists===

In a '''list''', the order of data items is significant.  Duplicate data items are permitted.  Examples of operations on lists are searching for an item in the list and determining its location (if it is present), removing an item from the list, adding an item at a specific location, etc.  If the principal operations on the list are to be the addition of items at one end and the removal of items at the other, it will generally be called a [[Queue (data structure)|queue]] or [[FIFO]].  If the principal operations are the addition and removal of items at just one end, it will be called a [[Stack (data structure)|stack]] or [[LIFO (computing)|LIFO]].  In both cases, items are maintained within the collection in the same order (unless they are removed and re-inserted somewhere else) and so these are special cases of the list collection.  Other specialized operations on lists include sorting, where, again, the order of items is of great importance.

===Priority queues===
Also called heaps, keep track of the 'minimum' or 'maximum' element in the collection, according to some ordering criterion. The ordering of other elements does not matter. One may think of a priority queue as a list that always keeps the minimum or maximum at the head, while the remaining elements are kept in a bag.

==Associative collections==
Other collections can instead be interpreted as sort of function: given an input &quot;key&quot;, the collection yields an output value. Important examples are sets, multisets, and associative arrays. A set can be interpreted as a specialized multiset, which in turn is a specialized map, in each case by limiting the possible values – considering a set as represented by its [[indicator function]].

===Sets===
In a [[Set (computer science)|set]], the order of data items is of no consequence, but duplicate items are not permitted.  Examples of operations on sets are the addition and removal of items and searching for an item in the set.  Some languages support sets directly.  In others, sets can be implemented by a [[hash table]] with dummy values; only the keys are used in representing the set.

===Multisets===
{{main|Multiset (abstract data type)}}
A &quot;bag&quot; or [[Multiset (abstract data type)|multiset]], is like a set – the order of data items is of no consequence.  But in this case, duplicate items are permitted.  Examples of operations on bags are the addition and removal of items and determining how many of a particular item are present in the bag.  Bags can be transformed into lists by the action of sorting.

===Associative arrays===
{{Main|Associative array}}
An [[associative array]] (&quot;map&quot;, &quot;dictionary&quot;, &quot;lookup table&quot;) acts like a dictionary, providing a &quot;value&quot; (like a definition) in response to a lookup on a &quot;key&quot; (like a word).  The &quot;value&quot; might be a reference to a compound data structure.  A [[hash table]] is usually an efficient implementation, and thus this data type is often known as a &quot;hash&quot;.

==Graphs==
{{Main|Graph (abstract data type)|l1=Graph}}

In a '''graph''', data items have associations with one or more other data items in the collection and are somewhat like trees without the concept of a root or the parent-child relationship so that all data items are peers.  Examples of operations on graphs are traversals and searches which explore the associations of data items looking for some specific property.  Graphs are frequently used to model real-world situations and to solve related problems.  An example is the [[Spanning tree protocol]], which creates a graph (or mesh) representation of a data network and figures out which associations between switching nodes need to be broken to turn it into a tree and thus prevent data going around in loops.

===Trees===
{{main|Tree (data structure)|l1=Tree}}

A special kind of graph is a tree. In a '''tree''', a 'root' data item has associated with it some number of data items which in turn have associated with them some number of other items in what is frequently viewed as parent-child relationships. Every item (other than the root) has a single parent (the root has no parent) and some number of children, possibly zero.  Examples of operations on trees are the addition of data items so as to maintain a specific property of the tree to perform sorting, etc. and traversals to visit data items in a specific sequence.

Tree collections can be used to naturally store hierarchical data, which is presented in a tree-like manner, such as menu systems and files in directories on a data storage system.

Specialized trees are used in various algorithms. For example, the [[heap sort]] uses a kind of tree called a [[Heap (data structure)|heap]].

==Abstract concept vs. implementation==

As described here, a collection and the various kinds of collections are abstract concepts.  There exists in the literature considerable confusion between the abstract concepts of computer science and their specific implementations in various languages or kinds of languages.  Assertions that collections, lists, sets, trees, etc. are data structures, abstract data types or classes must be read with this in mind.  Collections are first and foremost abstractions that are useful in formulating solutions to computing problems.  Viewed in this light, they retain important links to underlying mathematical concepts which can be lost when the focus is on the implementation.

For example, a priority queue is often implemented as a heap, while an associative array is often implemented as a hash table, so these abstract types are often referred to by this preferred implementation, as a &quot;heap&quot; or a &quot;hash&quot;, though this is not strictly correct.

== Implementations ==
Some collections may be [[primitive data type]]s in a language, such as lists, while more complex collections are implemented as [[composite data type]]s in libraries, sometimes in the [[standard library]]. Examples include:
* C++: known as [[Container (abstract data type)|Container]] (see article for details), implemented in [[C++ Standard Library]] and earlier [[Standard Template Library]]
* Java: implemented in the [[Java collections framework]]
* Python: some built-in, others implemented in the [https://docs.python.org/3/library/collections.html collections] library

== External links ==
* [http://commons.apache.org/collections/ Apache Commons Collections]
* [http://sibirjak.com/osflash/projects/as3commons-collections/ AS3Commons Collections Framework] ActionScript3 implementation of the most common collections.
* [http://www.collectionspy.com CollectionSpy] &amp;mdash; A profiler for Java's Collections Framework.
* [http://code.google.com/p/guava-libraries/ Guava]
* [http://jezuk.co.uk/cgi-bin/view/mango Mango Java library]

{{data structures}}

{{DEFAULTSORT:Collection}}
[[Category:Abstract data types]]
[[Category:Data structures]]</text>
      <sha1>hfkg6xvoi4fdc1aynuybdnsax4zvbsh</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Piece table</title>
    <ns>0</ns>
    <id>35516996</id>
    <revision>
      <id>610269673</id>
      <parentid>490339179</parentid>
      <timestamp>2014-05-26T22:44:30Z</timestamp>
      <contributor>
        <username>Carriearchdale</username>
        <id>5860794</id>
      </contributor>
      <minor/>
      <comment>/* References */clean up using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="581">{{context|date=May 2012}}
A '''piece table''' is a [[data structure]] typically used to represent a series of edits on a (potentially) read-only text document. An initial reference (or 'span') to the whole of the original file is created, with subsequent inserts and deletes being created as combinations of one, two, or three references to sections of either the original document or of the spans associated with earlier inserts.

== References ==
* http://www.cs.unm.edu/~crowley/papers/sds/node15.html#SECTION00064000000000000000

[[Category:Data structures]]


{{compsci-stub}}</text>
      <sha1>ahrz3xyoc3c5afeacv2jk4ojtxe4kal</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Retroactive data structures</title>
    <ns>0</ns>
    <id>35579271</id>
    <revision>
      <id>604468567</id>
      <parentid>587017306</parentid>
      <timestamp>2014-04-16T16:34:12Z</timestamp>
      <contributor>
        <ip>140.153.24.26</ip>
      </contributor>
      <comment>/* Partially retroactive */</comment>
      <text xml:space="preserve" bytes="7686">{{one source|date=April 2012}}
In [[computer science]] a '''retroactive data structure''' is [[data structure]] which supports efficient modifications to a sequence of operations that have been performed on the structure. These modifications can take the form of retroactive insertion, deletion or updating an operation that was performed at some time in the past.&lt;ref name=retroDS /&gt;

== Some applications of retroactive data structures ==
In the real world there are many cases where one would like to modify a past operation from a sequence of operations. Listed below are some of the possible applications:
* ''Error correction'': Incorrect input of data. The data should be corrected and all the secondary effects of the incorrect data be removed.
* ''Bad data'': When dealing with large systems, particular those involving a large amount of automated data transfer, it is not uncommon. For example suppose one of the sensors for a weather network malfunctions and starts to report garbage data or incorrect data. The ideal solution would be to remove all the data that the sensor produced since it malfunctioned along with all the effects the bad data had on the overall system.
* ''Recovery'': Suppose that a hardware sensor was damaged but is now repaired and data is able to be read from the sensor. We would like to be able to insert the data back into the system as if the sensor was never damaged in the first place.
* ''Manipulation of the past'': Changing the past can be helpful in the cases of damage control and retroactive data structures are designed for intentional manipulation of the past.

== Time as a spatial dimension ==
It is not possible to consider time as an additional spatial dimension. To illustrate this suppose we map the dimension of time onto an axis of space. The data structure we will use to add the spatial time dimension is a min-heap. Let the y axis represent the key values of the items within the heap and the x axis is the spatial time dimension. After several insertions and delete-min operations (all done non-retroactively) our min-heap would appear like in figure 1. Now suppose we retroactively insert zero to the beginning of the operation list. Our min-heap would appear like in figure 2. Notice how the single operation produces a cascading effect which affects the entire data structure. Thus we can see that while time can be drawn as a spatial dimension, operations with time involved produces dependence which have a ripple when modifications are made with respect to time.
[[File:Wiki min heap ex1.png|thumb|Figure 1. Min-Heap with timeline.]]
[[File:Wiki min heap ex2.png|thumb|Figure 2. Min-Heap and timeline after retroactive operation.]]

== Comparison to persistence ==
At first glance the notion of a retroactive data structures seems very similar to [[persistent data structures]] since they both take into account the dimension of time. The key difference between persistent data structures and retroactive data structures is ''how'' they handle the element of time. A persistent data structure maintains several versions of a data structure and operations can be performed on one version to produce another version of the data structure. Since each operation produces a new version, each version thus becomes an archive that cannot be changed (only new versions can be spawned from it). Since each version does not change, the dependence between each version also does not change. In retroactive data structures we allow changes to be made directly to previous versions. Since each version is now interdependent, a single change can cause a ripple of changes of all later versions. Figures 1 and 2 show an example of this rippling effect.

== Definition ==
Any data structure can be reformulated in a retroactive setting. In general the data structure involves a series of updates and queries made over some period of time. Let U = [u&lt;sub&gt;t&lt;sub&gt;1&lt;/sub&gt;&lt;/sub&gt;, u&lt;sub&gt;t&lt;sub&gt;2&lt;/sub&gt;&lt;/sub&gt;, u&lt;sub&gt;t&lt;sub&gt;3&lt;/sub&gt;&lt;/sub&gt;, ..., u&lt;sub&gt;t&lt;sub&gt;m&lt;/sub&gt;&lt;/sub&gt;] be the sequence of update operations from t&lt;sub&gt;1&lt;/sub&gt; to t&lt;sub&gt;m&lt;/sub&gt; such that t&lt;sub&gt;1&lt;/sub&gt; &lt; t&lt;sub&gt;2&lt;/sub&gt; &lt; ... &lt; t&lt;sub&gt;m&lt;/sub&gt;. The assumption here is that at most one operation can be performed for a given time t.

=== Partially retroactive ===
We define the data structure to be partially retroactive if it can perform update and query operations at the current time ''and'' support insertion and deletion operations in the past. Thus for partially retroactive we are interested in the following operations:
* Insert(t, u): Insert a new operation u into the list U at time t.
* Delete(t): Delete the operation at time t from the list U.
Given the above retroactive operations, a standard insertion operation would now the form of Insert(t, &quot;insert(x)&quot;). All retroactive changes on the operational history of the data structure can potentially affect all the operations at the time of the operation to the present. For example if we have t&lt;sub&gt;i-1&lt;/sub&gt; &lt; t &lt; t&lt;sub&gt;i+1&lt;/sub&gt;, then Insert(t, insert(x)) would be place a new operation, ''op'', between the operations ''op&lt;sub&gt;i-1&lt;/sub&gt;'' and ''op&lt;sub&gt;i+1&lt;/sub&gt;''. The current state of the data structure (i.e.: the data structure at the present time) would then be in a state such the operations ''op&lt;sub&gt;i-1&lt;/sub&gt;'', ''op'' and ''op&lt;sub&gt;i+1&lt;/sub&gt;'' all happened in a sequence, as if the
operation ''op'' was always there. See figure 1 and 2 for a visual example.

=== Fully retroactive ===
We define the data structure to be fully retroactive if in addition to the partially retroactive operations we also allow for one to perform queries about the past. Similar to how the standard operation insert(x) becomes Insert(t, &quot;insert(x)&quot;) in the partially retroactive model, the operation query(x) in the fully retroactive model now has the form Query(t, &quot;query(x)&quot;).

=== Retroactive running times ===
The running time of retroactive data structures are based on the number of operations, ''m'', performed on the structure, the number of operations ''r'' that were performed before the retroactive operation is performed, and the maximum number of elements ''n'' in the structure at any single time.

=== Automatic retro-activity ===
The main question regarding automatic retro-activity with respect to data structures is whether or not there is a general technique which can convert any data structure into an efficient retroactive counterpart. A simple approach is to perform a roll-back on all the changes made to the structure prior to the retroactive operation that is to be applied. Once we have rolled back the data structure to the appropriate state we can then apply the retroactive operation to make the change we wanted. Once the change is made we must then reapply all the changes we rolled back before to put the data structure into its new state. While this can work for any data structure, it is often inefficient and wasteful especially once the number of changes we need to roll-back is large. To create an ''efficient'' retroactive data structure we must take a look at the properties of the structure itself to determine where speed ups can be realized. Thus there is no general way to convert any data structure into an efficient retroactive counterpart. Erik D. Demaine, John Iacono and Stefan Langerman prove this.&lt;ref name =&quot;retroDS&quot;&gt;{{cite journal|last=Demaine|first=Erik D|coauthors=John Iacono  and Stefan Langerman|title=Retroactive data structures|journal=ACM Transactions on Algorithms|year=2007|volume=3|url=http://doi.acm.org/10.1145/1240233.1240236|accessdate=21 April 2012}}&lt;/ref&gt;

==See also==
* [[Persistent data structure]]

==References==
{{Reflist}}

[[Category:Data structures]]</text>
      <sha1>nrsj9cb4dz1ajsav82pfgs52p8jbosx</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Wavelet Tree</title>
    <ns>0</ns>
    <id>36038546</id>
    <revision>
      <id>546355551</id>
      <parentid>533501556</parentid>
      <timestamp>2013-03-22T18:49:38Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q2552862]]</comment>
      <text xml:space="preserve" bytes="3940">[[File:Wavelet tree.png|frame|A wavelet tree on the string &quot;abracadabra&quot;. At each node the symbols of the string are projected onto two partitions of the alphabet, and a bitvector denotes to which partition each symbol belongs. Note that only the bitvectors are stored; the strings in the nodes are only for illustratory purposes.]]

The '''Wavelet Tree''' is a [[succinct data structure]] to store strings in compressed space. It generalizes the &lt;math&gt;\mathbf{rank}_q&lt;/math&gt; and &lt;math&gt;\mathbf{select}_q&lt;/math&gt; operations defined on [[Succinct_data_structure#Succinct_dictionaries|bitvectors]] to arbitrary alphabets. 

Originally introduced to represent [[compressed suffix array]]s,&lt;ref name=&quot;GGV03&quot;/&gt; it has found application in several contexts.&lt;ref name=&quot;FGM09&quot;/&gt;&lt;ref name=&quot;Navarro12&quot;/&gt; The tree is defined by recursively partitioning the alphabet into pairs of subsets; the leaves correspond to individual symbols of the alphabet, and at each node a [[Succinct_data_structure#Succinct_dictionaries|bitvector]] stores whether a symbol of the string belongs to one subset or the other. 

The name derives from an analogy with the [[wavelet transform]] for signals, which recursively decomposes a signal into low-frequency and high-frequency components.

== Properties ==
Let &lt;math&gt;\Sigma&lt;/math&gt; be a finite alphabet with &lt;math&gt;\sigma=|\Sigma|&lt;/math&gt;. By using [[Succinct_data_structure#Succinct_dictionaries|succinct dictionaries]] in the nodes, a string &lt;math&gt;s \in \Sigma^*&lt;/math&gt; can be stored in &lt;math&gt;nH_0(s) + o(|s|\log \sigma)&lt;/math&gt;, where &lt;math&gt;H_0(s)&lt;/math&gt; is the order-0 [[Entropy (information theory)|empirical entropy]] of &lt;math&gt;s&lt;/math&gt;.

If the tree is balanced, the operations &lt;math&gt;\mathbf{access}&lt;/math&gt;, &lt;math&gt;\mathbf{rank}_q&lt;/math&gt;, and &lt;math&gt;\mathbf{select}_q&lt;/math&gt; can be supported in &lt;math&gt;O(\log \sigma)&lt;/math&gt; time.

== Extensions ==
Several extensions to the basic structure have been presented in the literature. To reduce the height of the tree, multiary nodes can be used instead of binary.&lt;ref name=&quot;FGM09&quot; /&gt; The data structure can be made dynamic, supporting insertions and deletions at arbitrary points of the string; this feature enables the implementation of dynamic [[FM-index]]es.&lt;ref name=&quot;CHLK07&quot;/&gt; This can be further generalized, allowing the update operations to change the underlying alphabet: the Wavelet Trie&lt;ref name=&quot;GO12&quot;/&gt; exploits the [[trie]] structure on an alphabet of strings to enable dynamic tree modifications.

== Further reading ==
* [http://www.alexbowe.com/wavelet-trees Wavelet Trees]. A blog post describing the construction of a wavelet tree, with examples.

== References ==
&lt;references&gt;
&lt;ref name=&quot;GGV03&quot;&gt;
R. Grossi, A. Gupta, and J. S. Vitter, [http://www.di.unipi.it/~grossi/PAPERS/sodaconf03FINAL-LATEST.pdf High-order entropy-compressed text indexes], ''Proceedings of the 14th Annual SIAM/ACM Symposium on Discrete Algorithms (SODA)'', January 2003, 841-850.
&lt;/ref&gt;
&lt;ref name=&quot;FGM09&quot;&gt;
P. Ferragina, R. Giancarlo, G. Manzini, [http://www.ittc.ku.edu/~jsv/Papers/FGM09.wavelettrees.pdf The myriad virtues of Wavelet Trees], ''Information and Computation'', Volume 207, Issue 8, August 2009, Pages 849-866
&lt;/ref&gt;
&lt;ref name=&quot;Navarro12&quot;&gt;
G. Navarro, [http://www.dcc.uchile.cl/~gnavarro/ps/cpm12.pdf Wavelet Trees for All], ''Proceedings of 23rd Annual Symposium on Combinatorial Pattern Matching (CPM)'', 2012
&lt;/ref&gt;
&lt;ref name=&quot;CHLK07&quot;&gt;
H.-L. Chan, W.-K. Hon, T.-W. Lam, and K. Sadakane, Compressed Indexes for
dynamic text collections, ''ACM Transactions on Algorithms'', 3(2), 2007
&lt;/ref&gt;
&lt;ref name=&quot;GO12&quot;&gt;

R. Grossi and G. Ottaviano, [http://arxiv.org/abs/1204.3581 The Wavelet Trie: maintaining an indexed sequence of strings in compressed space], ''In Proceedings of the 31st Symposium on the Principles of Database Systems (PODS)'', 2012
&lt;/ref&gt;
&lt;/references&gt;

[[Category:Trees (data structures)]]
[[Category:Data structures]]
[[Category:String data structures]]</text>
      <sha1>i0f592zwstr5a1ey2sogljyr2el4wff</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hash table</title>
    <ns>0</ns>
    <id>13833</id>
    <revision>
      <id>626070041</id>
      <parentid>626031605</parentid>
      <timestamp>2014-09-18T11:15:25Z</timestamp>
      <contributor>
        <username>Johnuniq</username>
        <id>6036800</id>
      </contributor>
      <comment>Undid revision 626031605 by [[Special:Contributions/69.204.204.85|69.204.204.85]] ([[User talk:69.204.204.85|talk]]) original correct</comment>
      <text xml:space="preserve" bytes="50173">{{distinguish|Hash list|Hash tree (disambiguation){{!}}Hash tree}}
{{Use mdy dates|date=January 2013}}
&lt;!--THE UNDERSCORE &quot;_&quot; IS SIGNIFICANT, DO NOT CHANGE IT TO SPACE--&gt;
{{Infobox data structure
|name=Hash table
|type=Unordered [[associative array]]
|invented_by=
|invented_year=1953
|space_avg=O(''n'')&lt;ref name=&quot;Cormen et al&quot;&gt;
  {{cite book
  |last1=Cormen |first1=Thomas H. |author1-link=Thomas H. Cormen
  |last2=Leiserson |first2=Charles E. |author2-link=Charles E. Leiserson
  |last3=Rivest |first3=Ronald L. |author3-link=Ronald L. Rivest
  |last4=Stein |first4=Clifford |author4-link=Clifford Stein
  | title = [[Introduction to Algorithms]]
  | edition = 3rd
  | publisher = Massachusetts Institute of Technology
  | year = 2009
  | isbn = 978-0-262-03384-8
  | pages = 253–280
  }}
&lt;/ref&gt;

|space_worst=O(''n'')
|search_avg=O(1)
|search_worst=O(''n'')
|insert_avg=O(1)
|insert_worst=O(''n'')
|delete_avg=O(1)
|delete_worst=O(''n'')
}}
[[File:Hash table 3 1 1 0 1 0 0 SP.svg|thumb|315px|right|A small phone book as a hash table]]

In [[computing]], a '''hash table''' (also '''hash map''') is a [[data structure]] used to implement an [[associative array]], a structure that can map [[Unique key|keys]] to [[Value (computer science)|values]]. A hash table uses a [[hash function]] to compute an ''index'' into an array of ''buckets'' or ''slots'', from which the correct value can be found.

Ideally, the hash function will assign each key to a unique bucket, but this situation is rarely achievable in practice (usually some keys will hash to the same bucket). Instead, most hash table designs assume that ''[[Collision (computer science)|hash collisions]]''—different keys that are assigned by the hash function to the same bucket—will occur and must be accommodated in some way.

In a well-dimensioned hash table, the average cost (number of [[instruction (computer science)|instructions]]) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at ([[amortized analysis|amortized]]&lt;ref name=&quot;leiser&quot;&gt;[[Charles E. Leiserson]], [http://videolectures.net/mit6046jf05_leiserson_lec13/ ''Amortized Algorithms, Table Doubling, Potential Method''] Lecture 13, course MIT 6.046J/18.410J Introduction to Algorithms—Fall 2005
&lt;/ref&gt;) constant average cost per operation.&lt;ref name=&quot;knuth&quot;&gt;
  {{cite book
  | first=Donald |last=Knuth |author1-link=Donald Knuth
  | title = 'The Art of Computer Programming'
  | volume = 3: ''Sorting and Searching''
  | edition = 2nd
  | publisher = Addison-Wesley
  | year = 1998
  | isbn = 0-201-89685-0
  | pages = 513–558
  }}
&lt;/ref&gt;&lt;ref name=&quot;cormen&quot;/&gt;

In many situations, hash tables turn out to be more efficient than [[search tree]]s or any other [[table (computing)|table]] lookup structure. For this reason, they are widely used in many kinds of computer [[software]], particularly for associative arrays, [[database index]]ing, [[cache (computing)|caches]], and [[set (abstract data type)|sets]].

==Hashing==
{{Main|Hash function}}

The idea of hashing is to distribute the entries (key/value pairs) across an array of ''buckets''. Given a key, the algorithm computes an ''index'' that suggests where the entry can be found:

 index = f(key, array_size)

Often this is done in two steps:

 hash = hashfunc(key)
 index = hash % array_size

In this method, the ''hash'' is independent of the array size, and it is then ''reduced'' to an index (a number between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;array_size&amp;nbsp;−&amp;nbsp;1&lt;/code&gt;) using the [[Modulo operation|modulo operator]] (&lt;code&gt;%&lt;/code&gt;).

In the case that the array size is a [[power of two]], the remainder operation is reduced to [[Mask (computing)|masking]], which improves speed, but can increase problems with a poor hash function.

===Choosing a good hash function===
A good hash function and implementation algorithm are essential for good hash table performance, but may be difficult to achieve.

A basic requirement is that the function should provide a [[Uniform distribution (discrete)|uniform distribution]] of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g. a [[Pearson's chi-squared test#Discrete uniform distribution|Pearson's chi-squared test]] for discrete uniform distributions.&lt;ref name=&quot;chernoff&quot;&gt;
  {{Cite news
  | first=Karl |last=Pearson |author1-link=Karl Pearson
  | year = 1900
  | title = On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling
  | journal = Philosophical Magazine, Series 5
  | volume = 50
  | number = 302
  | pages = 157–175
  | doi=10.1080/14786440009463897
  }}
&lt;/ref&gt;&lt;ref name=&quot;plackett&quot;&gt;
  {{Cite news
  |first=Robin |last=Plackett |author1-link=Robin Plackett
  | year = 1983
  | title = Karl Pearson and the Chi-Squared Test
  | journal = International Statistical Review (International Statistical Institute (ISI))
  | volume = 51
  | number = 1
  | pages = 59–72
  | doi=10.2307/1402731
  }}
&lt;/ref&gt;

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size ''s'', then the hash function needs to be uniform only when ''s'' is a [[power of two]]. On the other hand, some hashing algorithms provide uniform hashes only when ''s'' is a [[prime number]].&lt;ref name=&quot;twang1&quot;&gt;Thomas Wang (1997),  [http://www.concentric.net/~Ttwang/tech/primehash.htm Prime Double Hash Table]. Retrieved April 27, 2012&lt;/ref&gt;

For [[open addressing]] schemes, the hash function should also avoid ''clustering'', the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash&lt;ref name=&quot;knuth&quot;/&gt; is claimed to have particularly poor clustering behavior.&lt;ref name=&quot;twang1&quot;/&gt;

[[Cryptographic hash function]]s are believed to provide good hash functions for any table size ''s'', either by [[modulo operation|modulo]] reduction or by [[Mask (computing)|bit masking]]{{Citation Needed|date=July 2014}}. They may also be appropriate if there is a risk of malicious users trying to [[denial of service attack|sabotage]] a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret [[salt (cryptography)|salt]] to the data, or using a [[universal hash function]]).

===Perfect hash function===
If all keys are known ahead of time, a [[perfect hash function]] can be used to create a perfect hash table that has no collisions. If [[Perfect hash function#Minimal perfect hash function|minimal perfect hashing]] is used, every location in the hash table can be used as well.

Perfect hashing allows for [[constant time]] lookups in the worst case. This is in contrast to most chaining and open addressing methods, where the time for lookup is low on average, but may be very large (proportional to the number of entries) for some sets of keys.

==Key statistics==
A critical statistic for a hash table is called the ''load factor''. This is simply the number of entries divided by the number of buckets, that is, ''n''/''k'' where ''n'' is the number of entries and ''k'' is the number of buckets.

If the load factor is kept reasonable, the hash table should perform well, provided the hashing is good. If the load factor grows too large, the hash table will become slow, or it may fail to work (depending on the method used). The expected [[constant time]] property of a hash table assumes that the load factor is kept below some bound. For a ''fixed'' number of buckets, the time for a lookup grows with the number of entries and so does not achieve the desired constant time.

Second to that, one can examine the variance of number of entries per bucket. For example, two tables both have 1000 entries and 1000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.

==Collision resolution==
Hash [[Collision (computer science)|collisions]] are practically unavoidable when hashing a random subset of a large set of possible keys. For example, if 2,450 keys are hashed into a million buckets, even with a perfectly uniform random distribution, according to the [[birthday problem]] there is approximately a 95% chance of at least two of the keys being hashed to the same slot.

Therefore, most hash table implementations have some collision resolution strategy to handle such events. Some common strategies are described below. All these methods require that the keys (or pointers to them) be stored in the table, together with the associated values.

===Separate chaining===
[[File:Hash table 5 0 1 1 1 1 1 LL.svg|thumb|450px|right|Hash collision resolved by separate chaining.]]

In the method known as ''separate chaining'', each bucket is independent, and has some sort of [[List (abstract data type)|list]] of entries with the same index. The time for hash table operations is the time to find the bucket (which is constant) plus the time for the list operation. (The technique is also called ''open hashing'' or ''closed addressing''.)

In a good hash table, each bucket has zero or one entries, and sometimes two or three, but rarely more than that. Therefore, structures that are efficient in time and space for these cases are preferred. Structures that are efficient for a fairly large number of entries are not needed or desirable. If these cases happen often, the hashing is not working well, and this needs to be fixed.

====Separate chaining with linked lists====
Chained hash tables with [[linked list]]s are popular because they require only basic data structures with simple algorithms, and can use simple hash functions that are unsuitable for other methods.

The cost of a table operation is that of scanning the entries of the selected bucket for the desired key. If the distribution of keys is [[SUHA|sufficiently uniform]], the ''average'' cost of a lookup depends only on the average number of keys per bucket—that is, on the load factor.

Chained hash tables remain effective even when the number of table entries ''n'' is much higher than the number of slots. Their performance [[graceful degradation|degrades more gracefully]] (linearly) with the load factor. For example, a chained hash table with 1000 slots and 10,000 stored keys (load factor 10) is five to ten times slower than a 10,000-slot table (load factor 1); but still 1000 times faster than a plain sequential list, and possibly even faster than a balanced search tree.{{Citation Needed|date=January 2014}}

For separate-chaining, the worst-case scenario is when all entries are inserted into the same bucket, in which case the hash table is ineffective and the cost is that of searching the bucket data structure. If the latter is a linear list, the lookup procedure may have to scan all its entries, so the worst-case cost is proportional to the number ''n'' of entries in the table.

The bucket chains are often implemented as [[Sequence|ordered lists]], sorted by the key field; this choice approximately halves the average cost of unsuccessful lookups, compared to an unordered list{{Citation needed|date=April 2011}}. However, if some keys are much more likely to come up than others, an unordered list with [[move-to-front heuristic]] may be more effective. More sophisticated data structures, such as balanced search trees, are worth considering only if the load factor is large (about 10 or more), or if the hash distribution is likely to be very non-uniform, or if one must guarantee good performance even in a worst-case scenario. However, using a larger table and/or a better hash function may be even more effective in those cases.

Chained hash tables also inherit the disadvantages of linked lists. When storing small keys and values, the space overhead of the &lt;code&gt;next&lt;/code&gt; pointer in each entry record can be significant. An additional disadvantage is that traversing a linked list has poor [[Locality of reference|cache performance]], making the processor cache ineffective.

====Separate chaining with list head cells====

[[File:Hash table 5 0 1 1 1 1 0 LL.svg|thumb|right|500px|Hash collision by separate chaining with head records in the bucket array.]]

Some chaining implementations store the first record of each chain in the slot array itself.&lt;ref name=&quot;cormen&quot;&gt;{{cite book
 |last1=Cormen |first1=Thomas H. |author1-link=Thomas H. Cormen
 |last2=Leiserson |first2=Charles E. |author2-link=Charles E. Leiserson
 |last3=Rivest |first3=Ronald L. |author3-link=Ronald L. Rivest
 |last4=Stein |first4=Clifford |author4-link=Clifford Stein
 | title = [[Introduction to Algorithms]]
 | publisher = MIT Press and McGraw-Hill
 | year= 2001
 | isbn = 978-0-262-53196-2
 | edition = 2nd
 | pages=221–252
 | nopp = Chapter 11: Hash Tables
 }}&lt;/ref&gt;
The number of pointer traversals is decreased by one for most cases. The purpose is to increase cache efficiency of hash table access.

The disadvantage is that an empty bucket takes the same space as a bucket with one entry. To save space, such hash tables often have about as many slots as stored entries, meaning that many slots have two or more entries.

====Separate chaining with other structures====
Instead of a list, one can use any other data structure that supports the required operations. For example, by using a [[self-balancing binary search tree|self-balancing tree]], the theoretical worst-case time of common hash table operations (insertion, deletion, lookup) can be brought down to [[Big O notation|O(log ''n'')]] rather than O(''n''). However, this approach is only worth the trouble and extra memory cost if long delays must be avoided at all costs (e.g. in a real-time application), or if one must guard against many entries hashed to the same slot (e.g. if one expects extremely non-uniform distributions, or in the case of web sites or other publicly accessible services, which are vulnerable to malicious key distributions in requests).

The variant called [[array hash table]] uses a [[dynamic array]] to store all the entries that hash to the same slot.&lt;ref&gt;
  {{Cite book
  | title=Cache-conscious Collision Resolution in String Hash Tables
  | first1=Nikolas
  | last1=Askitis
  | first2=Justin
  | last2=Zobel
  |date=October 2005
  | isbn=978-3-540-29740-6
  | url=http://www.springerlink.com/content/b61721172558qt03/
  | pages=91–102
  | journal=Proceedings of the 12th International Conference, String Processing and Information Retrieval (SPIRE 2005)
  | doi=10.1007/11575832_11
  | volume=3772/2005}}
&lt;/ref&gt;&lt;ref&gt;
  {{Cite journal
  | title=Engineering scalable, cache and space efficient tries for strings
  | first1=Nikolas
  | last1=Askitis
  | first2=Ranjan
  | last2=Sinha
  | year=2010
  | issn=1066-8888
  | doi=10.1007/s00778-010-0183-9
  | url=http://www.springerlink.com/content/86574173183j6565/
  | journal=The VLDB Journal
  | volume=17
  | issue=5
  | pages=633–660
  }}
&lt;/ref&gt;&lt;ref&gt;
  {{Cite book
  | title=Fast and Compact Hash Tables for Integer Keys
  | first1=Nikolas
  | last1=Askitis
  | year=2009
  | isbn=978-1-920682-72-9
  | url=http://crpit.com/confpapers/CRPITV91Askitis.pdf
  | pages=113–122
  | journal=Proceedings of the 32nd Australasian Computer Science Conference (ACSC 2009)
  | volume=91
  }}
&lt;/ref&gt; Each newly inserted entry gets appended to the end of the dynamic array that is assigned to the slot. The dynamic array is resized in an ''exact-fit'' manner, meaning it is grown only by as many bytes as needed. Alternative techniques such as growing the array by block sizes or ''pages'' were found to improve insertion performance, but at a cost in space. This variation makes more efficient use of [[CPU cache|CPU caching]] and the [[translation lookaside buffer]] (TLB), because slot entries are stored in sequential memory positions. It also dispenses with the &lt;code&gt;next&lt;/code&gt; pointers that are required by linked lists, which saves space. Despite frequent array resizing, space overheads incurred by operating system such as memory fragmentation, were found to be small.

An elaboration on this approach is the so-called [[dynamic perfect hashing]],&lt;ref&gt;Erik Demaine, Jeff Lind. 6.897: Advanced Data Structures. MIT Computer Science and Artificial Intelligence Laboratory. Spring 2003. http://courses.csail.mit.edu/6.897/spring03/scribe_notes/L2/lecture2.pdf&lt;/ref&gt; where a bucket that contains ''k'' entries is organized as a perfect hash table with ''k''&lt;sup&gt;2&lt;/sup&gt; slots. While it uses more memory (''n''&lt;sup&gt;2&lt;/sup&gt; slots for ''n'' entries, in the worst case and ''n*k'' slots in the average case), this variant has guaranteed constant worst-case lookup time, and low amortized time for insertion.

===Open addressing===
[[File:Hash table 5 0 1 1 1 1 0 SP.svg|thumb|380px|right|Hash collision resolved by open addressing with linear probing (interval=1). Note that &quot;Ted Baker&quot; has a unique hash, but nevertheless collided with &quot;Sandra Dee&quot;, that had previously collided with &quot;John Smith&quot;.]]
In another strategy, called [[open addressing]], all entry records are stored in the bucket array itself. When a new entry has to be inserted, the buckets are examined, starting with the hashed-to slot and proceeding in some ''probe sequence'', until an unoccupied slot is found. When searching for an entry, the buckets are scanned in the same sequence,  until either the target record is found, or an unused array slot is found, which indicates that there is no such key in the table.&lt;ref name=&quot;tenenbaum90&quot;&gt;
  {{Cite book
  | title=Data Structures Using C
  | first1=Aaron M.
  | last1=Tenenbaum
  | first2=Yedidyah
  | last2=Langsam
  | first3=Moshe J.
  | last3=Augenstein
  | publisher=Prentice Hall
  | year=1990
  | isbn=0-13-199746-7
  | pages=456–461, p. 472
  }}
&lt;/ref&gt; The name &quot;open addressing&quot; refers to the fact that the location (&quot;address&quot;) of the item is not determined by its hash value. (This method is also called  '''closed hashing'''; it should not be confused with &quot;open hashing&quot; or &quot;closed addressing&quot; that usually mean separate chaining.)

Well-known probe sequences include:
* [[Linear probing]], in which the interval between probes is fixed (usually 1)
* [[Quadratic probing]], in which the interval between probes is increased by adding the successive outputs of a quadratic polynomial to the starting value given by the original hash computation
* [[Double hashing]], in which the interval between probes is computed by another hash function

A drawback of all these open addressing schemes is that the number of stored entries cannot exceed the number of slots in the bucket array. In fact, even with good hash functions, their performance dramatically degrades when the load factor grows beyond 0.7 or so. Thus a more aggressive resize scheme is needed. Separate linking works correctly with any load factor, although performance is likely to be reasonable if it is kept below 2 or so. For many applications, these restrictions mandate the use of dynamic resizing, with its attendant costs.

Open addressing schemes also put more stringent requirements on the hash function: besides distributing the keys more uniformly over the buckets, the function must also minimize the clustering of hash values that are consecutive in the probe order. Using separate chaining, the only concern is that too many objects map to the ''same'' hash value; whether they are adjacent or nearby is completely irrelevant.

Open addressing only saves memory if the entries are small (less than four times the size of a pointer) and the load factor is not too small. If the load factor is close to zero (that is, there are far more buckets than stored entries), open addressing is wasteful even if each entry is just two words.

[[File:Hash table average insertion time.png|thumb|right|362px|This graph compares the average number of cache misses required to look up elements in tables with chaining and linear probing. As the table passes the 80%-full mark, linear probing's performance drastically degrades.]]

Open addressing avoids the time overhead of allocating each new entry record, and can be implemented even in the absence of a memory allocator. It also avoids the extra indirection required to access the first entry of each bucket (that is, usually the only one). It also has better [[locality of reference]], particularly with linear probing. With small record sizes, these factors can yield better performance than chaining, particularly for lookups. 
Hash tables with open addressing are also easier to [[serialization|serialize]], because they do not use pointers.

On the other hand, normal open addressing is a poor choice for large elements, because these elements fill entire [[CPU cache]] lines (negating the cache advantage), and a large amount of space is wasted on large empty table slots. If the open addressing table only stores references to elements (external storage), it uses space comparable to chaining even for large records but loses its speed advantage.

Generally speaking, open addressing is better used for hash tables with small records that can be stored within the table (internal storage) and fit in a cache line. They are particularly suitable for elements of one word or less. If the table is expected to have a high load factor, the records are large, or the data is variable-sized, chained hash tables often perform as well or better.

Ultimately, used sensibly, any kind of hash table algorithm is usually fast ''enough''; and the percentage of a calculation spent in hash table code is low. Memory usage is rarely considered excessive. Therefore, in most cases the differences between these algorithms are marginal, and other considerations typically come into play.{{Citation needed|date=March 2012}}

===Coalesced hashing===
A hybrid of chaining and open addressing, [[coalesced hashing]] links together chains of nodes within the table itself.&lt;ref name=&quot;tenenbaum90&quot; /&gt;  Like open addressing, it achieves space usage and (somewhat diminished) cache advantages over chaining. Like chaining, it does not exhibit clustering effects; in fact, the table can be efficiently filled to a high density. Unlike chaining, it cannot have more elements than table slots.

===Cuckoo hashing===
Another alternative open-addressing solution is [[cuckoo hashing]], which ensures constant lookup time in the worst case, and constant amortized time for insertions and deletions. It uses two or more hash functions, which means any key/value pair could be in two or more locations. For lookup, the first hash function is used; if the key/value is not found, then the second hash function is used, and so on. If a collision happens during insertion, then the key is re-hashed with the second hash function to map it to another bucket. If all hash functions are used and there is still a collision, then the key it collided with is removed to make space for the new key, and the old key is re-hashed with one of the other hash functions, which maps it to another bucket. If that location also results in a collision, then the process repeats until there is no collision or the process traverses all the buckets, at which point the table is resized. By combining multiple hash functions with multiple cells per bucket, very high space utilisation can be achieved.

===Robin Hood hashing===
&lt;!-- We must describe ordered hash tables--&gt;
One interesting variation on double-hashing collision resolution is Robin Hood hashing.&lt;ref&gt;
  {{Cite techreport
  | year=1986
  | last=Celis
  | first=Pedro
  | authorlink= Pedro Celis
  | title=Robin Hood hashing
  | number=CS-86-14
  | institution=Computer Science Department, University of Waterloo
| url=https://cs.uwaterloo.ca/research/tr/1986/CS-86-14.pdf
  }}&lt;/ref&gt;&lt;ref&gt;
  {{Cite web
  | year=2013
  | last=Goossaert
  | first=Emmanuel
  | title=Robin Hood hashing
| url=http://codecapsule.com/2013/11/11/robin-hood-hashing/
  }}
&lt;/ref&gt; The idea is that a new key may displace a key already inserted, if its probe count is larger than that of the key at the current position. The net effect of this is that it reduces worst case search times in the table. This is similar to ordered hash tables&lt;ref&gt;{{cite journal |last= Amble |first= Ole |last2= Knuth |first2= Don|date=1974 |title= Ordered hash tables|journal= Computer Journal|volume= 17|issue= 2 |pages=135 |doi=10.1093/comjnl/17.2.135}}&lt;/ref&gt; except that the criterion for bumping a key does not depend on a direct relationship between the keys. Since both the worst case and the variation in the number of probes is reduced dramatically, an interesting variation is to probe the table starting at the expected successful probe value and then expand from that position in both directions.&lt;ref&gt;
{{cite journal
  | last = Viola
  | first = Alfredo
  | title =Exact distribution of individual displacements in linear probing hashing
  | journal =Transactions on Algorithms (TALG)
  | volume = 1
  | issue = 2,
  | date = October 2005
  | pages = 214–242
  | publisher = ACM
  | url =
  | doi = 10.1145/1103963.1103965| nopp =true }}&lt;/ref&gt;
External Robin Hashing is an extension of this algorithm where the table is stored in an external file and each table position corresponds to a fixed-sized page or bucket with ''B'' records.&lt;ref&gt;
  {{Cite techreport
  | year= March 1988
  | last=Celis |first=Pedro
  | authorlink=Pedro Celis
  | title=External Robin Hood Hashing
  | number=TR246
  | institution=Computer Science Department, Indiana University
  }}&lt;/ref&gt;

===2-choice hashing===
[[2-choice hashing]] employs 2 different hash functions, ''h&lt;sub&gt;1&lt;/sub&gt;''(''x'') and ''h&lt;sub&gt;2&lt;/sub&gt;''(''x''), for the hash table. Both hash functions are used to compute two table locations. When an object is inserted in the table, then it is placed in the table location that contains fewer objects (with the default being the ''h&lt;sub&gt;1&lt;/sub&gt;''(''x'') table location if there is equality in bucket size). 2-choice hashing employs the principle of the [[power of two choices]].

===Hopscotch hashing===
Another alternative open-addressing solution is [[hopscotch hashing]],&lt;ref&gt;{{cite conference
| last1=Herlihy |first1=Maurice |last2=Shavit |first2=Nir |last3=Tzafrir |first3=Moran
| title = Hopscotch Hashing
| booktitle = DISC '08: Proceedings of the 22nd international symposium on Distributed Computing
| year = 2008
| pages = 350–364
| location = Arcachon, France
| publisher = Springer-Verlag
| location = Berlin, Heidelberg
}}&lt;/ref&gt; which combines the approaches of [[cuckoo hashing]] and [[linear probing]], yet seems in general to avoid their limitations. In particular it works well even when the load factor grows beyond 0.9. The algorithm is well suited for implementing a resizable [[concurrent hash table]].

The hopscotch hashing algorithm works by defining a neighborhood of buckets near the original hashed bucket, where a given entry is always found. Thus, search is limited to the number of entries in this neighborhood, which is logarithmic in the worst case, constant on average, and with proper alignment of the neighborhood typically requires one cache miss. When inserting an entry, one first attempts to add it to a bucket in the neighborhood. However, if all buckets in this neighborhood are occupied, the algorithm traverses buckets in sequence until an open slot (an unoccupied bucket) is found (as in linear probing). At that point, since the empty bucket is outside the neighborhood, items are repeatedly displaced in a sequence of hops. (This is similar to cuckoo hashing, but with the difference that in this case the empty slot is being moved into the neighborhood, instead of items being moved out with the hope of eventually finding an empty slot.) Each hop brings the open slot closer to the original neighborhood, without invalidating the neighborhood property of any of the buckets along the way. In the end, the open slot has been moved into the neighborhood, and the entry being inserted can be added to it.

==Dynamic resizing==
The good functioning of a hash table depends on the fact that the table size is proportional to the number of entries. With a fixed size, and the common structures, it is similar to linear search, except with a better constant factor. In some cases, the number of entries may be definitely known in advance, for example keywords in a language. More commonly, this is not known for sure, if only due to later changes in code and data. It is one serious, although common, mistake to not provide ''any'' way for the table to resize. A general-purpose hash table &quot;class&quot; will almost always have some way to resize, and it is good practice even for simple &quot;custom&quot; tables. An implementation should check the load factor, and do something if it becomes too large (this needs to be done only on inserts, since that is the only thing that would increase it).

To keep the load factor under a certain limit, e.g. under 3/4, many table implementations expand the table when items are inserted. For example, in [[Java (programming language)|Java's]] &lt;code&gt;HashMap&lt;/code&gt; class the default load factor threshold for table expansion is 0.75 and in [[Python (programming language)|Python's]] &lt;code&gt;dict&lt;/code&gt;, table size is resized when load factor is greater than 2/3.

Since buckets are usually implemented on top of a [[dynamic array]] and any constant proportion for resizing greater than 1 will keep the load factor under the desired limit, the exact choice of the constant is determined by the same [[space-time tradeoff]] as for [[dynamic array]]s.

Resizing is accompanied by a full or incremental table '''rehash''' whereby existing items are mapped to new bucket locations.

To limit the proportion of memory wasted due to empty buckets, some implementations also shrink the size of the table—followed by a rehash—when items are deleted. From the point of [[space-time tradeoff]]s, this operation is similar to the deallocation in dynamic arrays.

===Resizing by copying all entries===
A common approach is to automatically trigger a complete resizing when the load factor exceeds some threshold ''r''&lt;sub&gt;max&lt;/sub&gt;. Then a new larger table is [[dynamic memory allocation|allocated]], all the entries of the old table are removed and inserted into this new table, and the old table is returned to the free storage pool. Symmetrically, when the load factor falls below a second threshold ''r''&lt;sub&gt;min&lt;/sub&gt;, all entries are moved to a new smaller table.

If the table size increases or decreases by a fixed percentage at each expansion, the total cost of these resizings, [[amortized analysis|amortized]] over all insert and delete operations, is still a constant, independent of the number of entries ''n'' and of the number ''m'' of operations performed.

For example, consider a table that was created with the minimum possible size and is doubled each time the load ratio exceeds some threshold. If ''m'' elements are inserted into that table, the total number of extra re-insertions that occur in all dynamic resizings of the table is at most ''m''&amp;nbsp;−&amp;nbsp;1. In other words, dynamic resizing roughly doubles the cost of each insert or delete operation.

===Incremental resizing===
Some hash table implementations, notably in [[real-time system]]s, cannot pay the price of enlarging the hash table all at once, because it may interrupt time-critical operations. If one cannot avoid dynamic resizing, a solution is to perform the resizing gradually:
* During the resize, allocate the new hash table, but keep the old table unchanged.
* In each lookup or delete operation, check both tables.
* Perform insertion operations only in the new table.
* At each insertion also move ''r'' elements from the old table to the new table.
* When all elements are removed from the old table, deallocate it.
To ensure that the old table is completely copied over before the new table itself needs to be enlarged, it
is necessary to increase the size of the table by a factor of at least (''r'' + 1)/''r'' during resizing.

===Monotonic keys===
If it is known that key values will always increase (or decrease) [[Monotonic function|monotonically]], then a variation of [[consistent hashing]] can be achieved by keeping a list of the single most recent key value at each hash table resize operation. Upon lookup, keys that fall in the ranges defined by these list entries are directed to the appropriate hash function—and indeed hash table—both of which can be different for each range. Since it is common to grow the overall number of entries by doubling, there will only be O(lg(N)) ranges to check, and binary search time for the redirection would be O(lg(lg(N))). As with consistent hashing, this approach guarantees that any key's hash, once issued, will never change, even when the hash table is later grown.

===Other solutions===
[[Linear hashing]]&lt;ref&gt;{{cite conference | first=Witold | last=Litwin | title=Linear hashing: A new tool for file and table addressing | year=1980 | pages=212–223 | booktitle=Proc. 6th Conference on Very Large Databases}}&lt;/ref&gt; is a hash table algorithm that permits incremental hash table expansion. It is implemented using a single hash table, but with two possible look-up functions.

Another way to decrease the cost of table resizing is to choose a hash function in such a way that the hashes of most values do not change when the table is resized. This approach, called [[consistent hashing]], is prevalent in disk-based and distributed hashes, where rehashing is prohibitively costly.

==Performance analysis==
In the simplest model, the hash function is completely unspecified and the table does not resize. For the best possible choice of hash function, a table of size ''k'' with open addressing has no collisions and holds up to ''k'' elements, with a single comparison for successful lookup, and a table of size ''k'' with chaining and ''n'' keys has the minimum max(0, ''n''-''k'') collisions and O(1 + ''n''/''k'') comparisons for lookup. For the worst choice of hash function, every insertion causes a collision, and hash tables degenerate to linear search, with Ω(''n'') amortized comparisons per insertion and up to ''n'' comparisons for a successful lookup.

Adding rehashing to this model is straightforward. As in a [[dynamic array]], geometric resizing by a factor of ''b'' implies that only ''n''/''b''&lt;sup&gt;''i''&lt;/sup&gt; keys are inserted ''i'' or more times, so that the total number of insertions is bounded above by ''bn''/(''b''-1), which is O(''n''). By using rehashing to maintain ''n'' &lt; ''k'', tables using both chaining and open addressing can have unlimited elements and perform successful lookup in a single comparison for the best choice of hash function.

In more realistic models, the hash function is a [[random variable]] over a probability distribution of hash functions, and performance is computed on average over the choice of hash function. When this distribution is [[Uniform distribution (discrete)|uniform]], the assumption is called &quot;simple uniform hashing&quot; and it can be shown that hashing with chaining requires Θ(1 + ''n''/''k'') comparisons on average for an unsuccessful lookup, and hashing with open addressing requires Θ(1/(1 - ''n''/''k'')).&lt;ref&gt;Doug Dunham. [http://www.duluth.umn.edu/~ddunham/cs4521s09/notes/ch11.txt CS 4521 Lecture Notes]. University of Minnesota Duluth. Theorems 11.2, 11.6. Last modified April 21, 2009.&lt;/ref&gt; Both these bounds are constant, if we maintain ''n''/''k'' &lt; ''c'' using table resizing, where ''c'' is a fixed constant less than 1.

==Features==

===Advantages===
The main advantage of hash tables over other table data structures is speed. This advantage is more apparent when the number of entries is large. Hash tables are particularly efficient when the maximum number of entries can be predicted in advance, so that the bucket array can be allocated once with the optimum size and never resized.

If the set of key-value pairs is fixed and known ahead of time (so insertions and deletions are not allowed), one may reduce the average lookup cost by a careful choice of the hash function, bucket table size, and internal data structures. In particular, one may be able to devise a hash function that is collision-free, or even perfect (see below). In this case the keys need not be stored in the table.

===Drawbacks===
Although operations on a hash table take constant time on average, the cost of a good hash function can be significantly higher than the inner loop of the lookup algorithm for a sequential list or search tree. Thus hash tables are not effective when the number of entries is very small. (However, in some cases the high cost of computing the hash function can be mitigated by saving the hash value together with the key.)

For certain string processing applications, such as [[spell checker|spell-checking]], hash tables may be less efficient than [[trie]]s, [[finite automata]], or [[Judy array]]s. Also, if each key is represented by a small enough number of bits, then, instead of a hash table, one may use the key directly as the index into an array of values. Note that there are no collisions in this case.

The entries stored in a hash table can be enumerated efficiently (at constant cost per entry), but only in some pseudo-random order. Therefore, there is no efficient way to locate an entry whose key is ''nearest'' to a given key. Listing all ''n'' entries in some specific order generally requires a separate sorting step, whose cost is proportional to log(''n'') per entry. In comparison, ordered search trees have lookup and insertion cost proportional to log(''n''), but allow finding the nearest key at about the same cost, and ''ordered'' enumeration of all entries at constant cost per entry.

If the keys are not stored (because the hash function is collision-free), there may be no easy way to enumerate the keys that are present in the table at any given moment.

Although the ''average'' cost per operation is constant and fairly small, the cost of a single operation may be quite high. In particular, if the hash table uses [[#Dynamic resizing|dynamic resizing]], an insertion or deletion operation may occasionally take time proportional to the number of entries. This may be a serious drawback in real-time or interactive applications.

Hash tables in general exhibit poor [[locality of reference]]—that is, the data to be accessed is distributed seemingly at random in memory. Because hash tables cause access patterns that jump around, this can trigger [[CPU cache|microprocessor cache]] misses that cause long delays. Compact data structures such as arrays searched with [[linear search]] may be faster, if the table is relatively small and keys are compact. The optimal performance point varies from system to system.

Hash tables become quite inefficient when there are many collisions. While extremely uneven hash distributions are extremely unlikely to arise by chance, a [[Black hat hacking|malicious adversary]] with knowledge of the hash function may be able to supply information to a hash that creates worst-case behavior by causing excessive collisions, resulting in very poor performance, e.g. a [[denial of service attack]].&lt;ref&gt;Alexander Klink and Julian Wälde's ''[http://events.ccc.de/congress/2011/Fahrplan/attachments/2007_28C3_Effective_DoS_on_web_application_platforms.pdf Efficient Denial of Service Attacks on Web Application Platforms]'', December 28, 2011, 28th Chaos Communication Congress. Berlin, Germany.&lt;/ref&gt; In critical applications, [[universal hashing]] can be used; a data structure with better worst-case guarantees may be preferable.&lt;ref&gt;Crosby and Wallach's ''[http://www.cs.rice.edu/~scrosby/hash/CrosbyWallach_UsenixSec2003.pdf Denial of Service via Algorithmic Complexity Attacks]''.&lt;/ref&gt;

==Uses==
{{unreferenced section|date=July 2013}}

===Associative arrays===
Hash tables are commonly used to implement many types of in-memory tables. They are used to implement [[associative array]]s (arrays whose indices are arbitrary [[string (computing)|strings]] or other complicated objects), especially in [[interpreter (computer science)|interpreted]] [[programming language]]s like [[AWK]], [[Perl]], and [[PHP]].

When storing a new item into a [[multimap]] and a hash collision occurs, the multimap unconditionally stores both items.

When storing a new item into a typical associative array and a hash collision occurs, but the actual keys themselves are different, the associative array likewise stores both items. However, if the key of the new item exactly matches the key of an old item, the associative array typically erases the old item and overwrites it with the new item, so every item in the table has a unique key.

===Database indexing===
Hash tables may also be used as [[disk drive|disk]]-based data structures and [[index (database)|database indices]] (such as in [[dbm]]) although [[B-tree]]s are more popular in these applications.

===Caches===
Hash tables can be used to implement [[cache (computing)|caches]], auxiliary data tables that are used to speed up the access to data that is primarily stored in slower media. In this application, hash collisions can be handled by discarding one of the two colliding entries—usually erasing the old item that is currently stored in the table and overwriting it with the new item, so every item in the table has a unique hash value.

===Sets===
Besides recovering the entry that has a given key, many hash table implementations can also tell whether such an entry exists or not.

Those structures can therefore be used to implement a [[set data structure]], which merely records whether a given key belongs to a specified set of keys. In this case, the structure can be simplified by eliminating all parts that have to do with the entry values. Hashing can be used to implement both static and dynamic sets.

===Object representation===
Several dynamic languages, such as [[Perl]], [[Python (programming language)|Python]], [[JavaScript]], and [[Ruby (programming language)|Ruby]], use hash tables to implement objects. In this representation, the keys are the names of the members and methods of the object, and the values are pointers to the corresponding member or method.

===Unique data representation===
Hash tables can be used by some programs to avoid creating multiple character strings with the same contents. For that purpose, all strings in use by the program are stored in a single ''string pool'' implemented as a hash table, which is checked whenever a new string has to be created. This technique was introduced in [[Lisp (programming language)|Lisp]] interpreters under the name [[hash consing]], and can be used with many other kinds of data ([[expression tree]]s in a symbolic algebra system, records in a database, files in a file system, binary decision diagrams, etc.)

===String interning===
{{main|String interning}}

==Implementations==

===In programming languages===
Many programming languages provide hash table functionality, either as built-in associative arrays or as standard [[library (computing)|library]] modules. In [[C++11]], for example, the &lt;code&gt;[[unordered map (C++)|unordered_map]]&lt;/code&gt; class provides hash tables for keys and values of arbitrary type.

In [[PHP]] 5, the Zend 2 engine uses one of the hash functions from [[Daniel J. Bernstein]] to generate the hash values used in managing the mappings of data pointers stored in a hash table. In the PHP source code, it is labelled as &lt;code&gt;DJBX33A&lt;/code&gt; (Daniel J. Bernstein, Times 33 with Addition).

[[Python (programming language)|Python]]'s built-in hash table implementation, in the form of the &lt;code&gt;dict&lt;/code&gt; type, as well as [[Perl]]'s hash type (%) are used internally to implement namespaces and therefore need to pay more attention to security, i.e. collision attacks.

In the [[.NET Framework]], support for hash tables is provided via the non-generic &lt;code&gt;Hashtable&lt;/code&gt; and generic &lt;code&gt;Dictionary&lt;/code&gt; classes, which store key-value pairs, and the generic &lt;code&gt;HashSet&lt;/code&gt; class, which stores only values.

===Independent packages===
* [http://code.google.com/p/sparsehash/ SparseHash] (formerly Google SparseHash) An extremely memory-efficient hash_map implementation, with only 2 bits/entry of overhead. The SparseHash library has several C++ hash map implementations with different performance characteristics,  including one that optimizes for memory use and another that optimizes for speed.
* [http://www.sunrisetel.net/software/devtools/sunrise-data-dictionary.shtml SunriseDD] An open source C library for hash table storage of arbitrary data objects with lock-free lookups, built-in reference counting and guaranteed order iteration. The library can participate in external reference counting systems or use its own built-in reference counting. It comes with a variety of hash functions and allows the use of runtime supplied hash functions via callback mechanism. Source code is well documented.
* [http://uthash.sourceforge.net/ uthash] This is an easy-to-use hash table for C structures.

==History==
The idea of hashing arose independently in different places. In January 1953, H. P. Luhn wrote an internal IBM memorandum that used hashing with chaining.&lt;ref name=&quot;hashhist&quot;/&gt; [[Gene Amdahl|G. N. Amdahl]], E. M. Boehme, [[Nathaniel Rochester (computer scientist)|N. Rochester]], and [[Arthur Samuel]] implemented a program using hashing at about the same time. Open addressing with linear probing (relatively prime stepping) is credited to Amdahl, but Ershov (in Russia) had the same idea.&lt;ref name=&quot;hashhist&quot;&gt;{{cite book|title=Handbook of Datastructures and Applications|pages=9–15|isbn=1-58488-435-5|first1=Dinesh P. |last1=Mehta |first2=Sartaj |last2=Sahni | author2-link = Sartaj Sahni}}&lt;/ref&gt;

==See also==
* [[Rabin–Karp string search algorithm]]
* [[Stable hashing]]
* [[Consistent hashing]]
* [[Extendible hashing]]
* [[Lazy deletion]]
* [[Pearson hashing]]

===Related data structures===
There are several data structures that use hash functions but cannot be considered special cases of hash tables:
* [[Bloom filter]], memory efficient data-structure designed for constant-time approximate lookups; uses hash function(s) and can be seen as an approximate hash table.
* [[Distributed hash table]] (DHT), a resilient dynamic table spread over several nodes of a network.
* [[Hash array mapped trie]], a [[trie]] structure, similar to the [[array mapped trie]], but where each key is hashed first.

==References==
{{reflist|colwidth=30em}}

==Further reading==
*{{cite book |last=Tamassia |first=Roberto |title=Data structures and algorithms in Java : [updated for Java 5.0] |year=2006 |publisher=Wiley |location=Hoboken, NJ |isbn=0-471-73884-0 |pages=369–418 |edition=4th |first2=Michael T. |last2=Goodrich |chapter=Chapter Nine: Maps and Dictionaries}}

*{{cite journal|last=McKenzie|first=B. J. |first2=R. |last2=Harries |first3=T. |last3=Bell |title=Selecting a hashing algorithm |journal=Software Practice &amp; Experience |date=Feb 1990 |volume=20 |issue=2 |pages=209–224 |doi=10.1002/spe.4380200207}}

==External links==
{{Commons category|Hash tables}}
* [http://www.burtleburtle.net/bob/hash/doobs.html A Hash Function for Hash Table Lookup] by Bob Jenkins.
* [http://www.sparknotes.com/cs/searching/hashtables/summary.html Hash Tables] by SparkNotes—explanation using C
* [http://www.azillionmonkeys.com/qed/hash.html Hash functions] by Paul Hsieh
* [http://blog.griddynamics.com/2011/03/ultimate-sets-and-maps-for-java-part-i.html Design of Compact and Efficient Hash Tables for Java] link not working
* [http://libhashish.sourceforge.net/ Libhashish] hash library
* [[NIST]] entry on [http://www.nist.gov/dads/HTML/hashtab.html hash tables]
* Open addressing hash table removal algorithm from [[ICI programming language]], ''ici_set_unassign'' in [http://ici.cvs.sourceforge.net/ici/ici/set.c?view=markup set.c] (and other occurrences, with permission).
* [http://www.relisoft.com/book/lang/pointer/8hash.html A basic explanation of how the hash table works by Reliable Software]
* [http://compgeom.cs.uiuc.edu/~jeffe/teaching/373/notes/06-hashing.pdf Lecture on Hash Tables]
* [http://task3.cc/308/hash-maps-with-linear-probing-and-separate-chaining/ Hash-tables in C]—two simple and clear examples of hash tables implementation in C with linear probing and chaining
* [http://opendatastructures.org/versions/edition-0.1e/ods-java/5_Hash_Tables.html Open Data Structures - Chapter 5 - Hash Tables]
* [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-7-hashing-hash-functions/ MIT's Introduction to Algorithms: Hashing 1] MIT OCW lecture Video
* [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-8-universal-hashing-perfect-hashing/ MIT's Introduction to Algorithms: Hashing 2] MIT OCW lecture Video
* [http://www.lampos.net/sort-hashmap How to sort a HashMap (Java) and keep the duplicate entries]
* [http://www.laurentluce.com/posts/python-dictionary-implementation/ How python dictionary works]

{{Data structures}}

{{DEFAULTSORT:Hash Table}}
[[Category:Articles with example C code]]
[[Category:Hashing|*]]
[[Category:Search algorithms]]
[[Category:Data structures]]</text>
      <sha1>lacz3ruhxdja0re4mws2of5l39ix0gg</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>List of data structures</title>
    <ns>0</ns>
    <id>177318</id>
    <revision>
      <id>626498456</id>
      <parentid>626492897</parentid>
      <timestamp>2014-09-21T17:01:08Z</timestamp>
      <contributor>
        <username>Mindmatrix</username>
        <id>160367</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/27.251.39.83|27.251.39.83]] ([[User talk:27.251.39.83|talk]]) to last version by Discospinster</comment>
      <text xml:space="preserve" bytes="6187">This is a list of [[data structure]]s. For a wider list of terms, see [[list of terms relating to algorithms and data structures]]. For a comparison of running time of subset of this list see [[comparison of data structures]].

==Data types==

=== [[Primitive type]]s ===
*[[Boolean data type|Boolean]], true or false
*[[Character (computing)|Character]]
*[[Floating point|Floating-point]], single-precision [[real number]] values
*[[Double precision|Double]], a wider floating-point size
*[[Integer (computer science)|Integer]], integral or fixed-precision values
*[[Enumerated type]], a small set of uniquely named values

=== [[Composite type]]s ===

(Sometimes also referred to as [[Plain old data structure]]s.)

*[[Array data type|Array]]
*[[Record (computer science)|Record]] (also called [[tuple]] or [[struct]])
*[[Union (computer science)|Union]]
*[[Tagged union]] (also called a [[Variant type|variant]], variant record, discriminated union, or disjoint union)

=== [[Abstract data types]] ===
*Array
*[[Container (data structure)|Container]]
*[[Associative array|Map/Associative array/Dictionary]]
*[[Multimap]]
*[[List (abstract data type)|List]]
*[[Set (computer science)|Set]]
*[[Multiset (abstract data type)|Multiset]]
*[[Priority queue]]
*[[Queue (data structure)|Queue]]
*[[Deque]]
*[[Stack (data structure)|Stack]]
*[[String (computer science)|String]]
*[[Tree (computer science)|Tree]]
*[[Graph (data structure)|Graph]]
Some properties of abstract data types:

{| class=&quot;wikitable&quot;
!Structure
!Stable
!Unique
!Cells per Node
|-
|Bag ([[Multiset (abstract data type)|multiset]])
|no
|no
|1
|-
|[[Set (computer science)|Set]]
|no
|yes
|1
|-
|[[List (computing)|List]]
|yes
|no
|1
|-
|[[Associative array|Map]]
|no
|yes
|2
|}

&quot;Stable&quot; means that input order is retained.

Other structures such as &quot;linked list&quot; and &quot;stack&quot; cannot easily be defined this way because there are specific operations associated with them.

== Linear data structures ==

=== Arrays ===
*[[Array data structure|Array]]
*[[Bidirectional map]]
*[[Bit array]]
*[[Bit field]]
*[[Bitboard]]
*[[Bitmap]]
*[[Circular buffer]]
*[[Control table]]
*[[System image|Image]]
*[[Dynamic array]]
*[[Gap buffer]]
*[[Hashed array tree]]
*[[Heightmap]]
*[[Lookup table]]
*[[Matrix (computer science)|Matrix]]
*[[Parallel array]]
*[[Sorted array]]
*[[Sparse array]]
*[[Sparse matrix]]
*[[Iliffe vector]]
*[[Variable-length array]]

=== Lists ===
*[[Doubly linked list]]
*[[Array list]]
*[[Linked list]]
*[[Self-organizing list]]
*[[Skip list]]
*[[Unrolled linked list]]
*[[VList]]
*[[Xor linked list]]
*[[Zipper (data structure)|Zipper]]
*[[Doubly connected edge list]]
*[[Difference list]]

== Trees ==
{{main|Tree (data structure)}}

=== Binary trees ===
*[[AA tree]]
*[[AVL tree]]
*[[Binary search tree]]
*[[Binary tree]]
*[[Cartesian tree]]
*[[Order statistic tree]]
*[[Pagoda (data structure)|Pagoda]]
*[[Randomized binary search tree]]
*[[Red-black tree]]
*[[Rope (computer science)|Rope]]
*[[Scapegoat tree]]
*[[Self-balancing binary search tree]]
*[[Splay tree]]
*[[T-tree]]
*[[Tango tree]]
*[[Threaded binary tree]]
*[[Top tree]]
*[[Treap]]
*[[Weight-balanced tree]]
*[[Binary data structure]]

=== B-trees ===
*[[B-tree]]
*[[B+ tree]]
*[[B*-tree]]
*[[B sharp tree]]
*[[Dancing tree]]
*[[2-3 tree]]
*[[2-3-4 tree]]
*[[Queap]]
*[[Fusion tree]]
*[[Bx-tree Moving Object Index|Bx-tree]]
*[[AList]]

=== Heaps ===
*[[Heap (data structure)|Heap]]
*[[Binary heap]]
*[[Weak heap]]
*[[Binomial heap]]
*[[Fibonacci heap]]
**[[AF-heap]]
*[[2-3 heap]]
*[[Soft heap]]
*[[Pairing heap]]
*[[Leftist tree|Leftist heap]]
*[[Treap]]
*[[Beap]]
*[[Skew heap]]
*[[Ternary heap]]
*[[D-ary heap]]
*[[Brodal queue]]

=== Tries ===
In these data structures each tree node compares a bit slice of key values.
*[[Trie]]
*[[Radix tree]]
*[[Suffix tree]]
*[[Suffix array]]
*[[Compressed suffix array]]
*[[FM-index]]
*[[Generalised suffix tree]]
*[[B-trie]]
*[[Judy array]]
*[[X-fast trie]]
*[[Y-fast trie]]
*[[Ctrie]]

=== Multiway trees ===
*[[Ternary tree]]
*[[K-ary tree]]
*[[And–or tree]]
*[[(a,b)-tree]]
*[[Link/cut tree]]
*[[SPQR-tree]]
*[[Spaghetti stack]]
*[[Disjoint-set data structure]]
*[[Fusion tree]]
*[[Enfilade (Xanadu)|Enfilade]]
*[[Exponential tree]]
*[[Fenwick tree]]
*[[Van Emde Boas tree]]
*[[Rose tree]]

=== Space-partitioning trees ===
These are data structures used for [[space partitioning]] or [[binary space partitioning]].
*[[Segment tree]]
*[[Interval tree]]
*[[Range tree]]
*[[Bin (computational geometry)|Bin]]
*[[Kd-tree]]
*[[Implicit kd-tree]]
*[[Min/max kd-tree]]
*[[Adaptive k-d tree]]
*[[Quadtree]]
*[[Octree]]
*[[Linear octree]]
*[[Z-order (curve)|Z-order]]
*[[UB-tree]]
*[[R-tree]]
*[[R+ tree]]
*[[R* tree]]
*[[Hilbert R-tree]]
*[[X-tree]]
*[[Metric tree]]
*[[Cover tree]]
*[[M-tree]]
*[[VP-tree]]
*[[BK-tree]]
*[[Bounding interval hierarchy]]
*[[BSP tree]]
*[[Rapidly exploring random tree]]

=== Application-specific trees ===
*[[Abstract syntax tree]]
*[[Parse tree]]
*[[Decision tree]]
*[[Alternating decision tree]]
*[[minmax|Minimax tree]]
*[[Expectiminimax tree]]
*[[Finger tree]]
*[[Expression tree]]

== Hashes ==
*[[Bloom filter]]
*[[Count-Min sketch]]
*[[Distributed hash table]]
*[[Double Hashing]]
*[[Dynamic perfect hashing|Dynamic perfect hash table]]
*[[Hash array mapped trie]]
*[[Hash list]]
*[[Hash table]]
*[[Hash tree (disambiguation)|Hash tree]]
*[[Hash trie]]
*[[Koorde]]
*[[Prefix hash tree]]
*[[Rolling hash]]
*[[MinHash]]
*[[Quotient filter]]
*[[Ctrie]]

== Graphs ==
*[[Graph (data structure)|Graph]]
*[[Adjacency list]]
*[[Adjacency matrix]]
*[[Graph-structured stack]]
*[[Scene graph]]
*[[Binary decision diagram]]
*[[Zero-suppressed decision diagram]]
*[[And-inverter graph]]
*[[Directed graph]]
*[[Directed acyclic graph]]
*[[Propositional directed acyclic graph]]
*[[Multigraph]]
*[[Hypergraph]]

== Other ==
*[[Lightmap]]
*[[Winged edge]]
*[[Doubly connected edge list]]
*[[Quad-edge]]
*[[Routing table]]
*[[Symbol table]]

{{Data structures}}

==External links==
*[http://tommyds.sourceforge.net/doc/benchmark.html Tommy Benchmarks] Comparison of several data structures.

[[Category:Data structures|*]]
[[Category:Computing-related lists|Data Structures]]</text>
      <sha1>3p6z4vcvnyiqv7kx5mnqv38239nzgu9</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Randomized meldable heap</title>
    <ns>0</ns>
    <id>39033432</id>
    <revision>
      <id>575168493</id>
      <parentid>571182892</parentid>
      <timestamp>2013-09-30T17:38:44Z</timestamp>
      <contributor>
        <ip>128.61.34.124</ip>
      </contributor>
      <comment>/* Meld */</comment>
      <text xml:space="preserve" bytes="5149">In computer science, a '''randomized meldable heap''' (also '''Meldable''' [[Heap (data structure)|Heap]] or '''Randomized Meldable''' [[Priority Queue]]) is a priority queue based [[data structure]] in which the underlying structure is also a heap-ordered [[binary tree]].  However, there are no restrictions on the shape of the underlying binary tree.

This approach has a number of advantages over similar data structures.  It offers greater simplicity:  all operations for the randomized meldable heap are easy to implement and the constant factors in their complexity bounds are small.  There is also no need to preserve balance conditions and no satellite information within the nodes is necessary.  Lastly, this structure has good worst-case time efficiency.  The execution time of each individual operation is at most logarithmic with high probability.&lt;ref name=&quot;Gambin&quot;&gt;A. Gambin and A. Malinowski. 1998. Randomized Meldable Priority Queues. In Proceedings of the 25th Conference on Current Trends in Theory and Practice of Informatics: Theory and Practice of Informatics (SOFSEM '98), Branislav Rovan (Ed.). Springer-Verlag, London, UK, UK, 344-349.&lt;/ref&gt;

==Operations==
The randomized meldable heap supports a number of common operations.  These are insertion, deletion, and a searching operation, findMin.  The insertion and deletion operations are implemented in terms of an additional operation specific to the meldable heap, Meld(Q1, Q2).

===Meld===
The basic goal of the meld (also called merge) operation is to take two heaps (by taking each heaps root nodes), Q1 and Q2, and merges them, returning a single heap node as a result.  This heap node is the root node of a heap containing all elements from the two subtrees rooted at Q1 and Q2.

A nice feature of this meld operation is that it can be defined recursively.  If either heaps are null, then the merge is taking place with an empty set and the method simply returns the root node of the non-empty heap.  If both Q1 and Q2 are not nil, check if Q1 &gt; Q2.  If it is, swap the two.  It is therefore ensured that Q1 &lt; Q2 and that the root node of the merged heap will contain Q1.  We then recursively merge Q2 with Q1.left or Q1.right.  This step is where the randomization comes in as this decision of which side to merge with is determined by a coin toss.

 '''function''' Meld('''Node''' ''Q''1, '''Node''' ''Q''2)
  '''if''' ''Q''1 is nil =&gt; '''return''' ''Q''2
  '''if''' ''Q''2 is nil =&gt; '''return''' ''Q''1
  '''if''' ''Q1'' &gt; ''Q''2 =&gt; swap ''Q''1 and ''Q''2
  '''if''' coin_toss is 0 =&gt; ''Q''1.''left'' = Meld(''Q''1.''left'', ''Q''2)
  '''else''' ''Q''1.''right'' = Meld(''Q''1.''right'', ''Q''2)
  '''return ''Q''1'''

===Insert===
With the meld operation complete, inserting into the meldable heap is easy.  First, a new node, u, is created containing the value x.  This new node is then simply melded with the heaps root node.

 '''function''' Insert(x)
  '''Node''' u = '''new Node'''
  u.x = x
  root = Meld(u, root)
  r.parent = nil
  increment node count

===Remove===
Similarly easy to the insert operation, Remove() uses the Meld operation to eliminate the root node from the heap.  This is done by simply melding the two children of the root node and making the returned node the new root.

 '''function''' Remove()
  root = Meld(root.left, root.right)
  '''if''' root is not nil =&gt; root.parent = nil
  decrement node count

===FindMin===
Possibly the easiest operation for the randomized meldable heap, FindMin() simply returns the element currently stored in the heap's root node.

===Additional Operations===
Some additional operations that can be implemented for the meldable heap that also have ''O''(log''n'') worst-case efficiency are:
*Remove(u) - Remove the node u and its key from the heap.
*Absorb(Q) - Add all elements of the meldable heap Q to this heap, emptying Q in the process.
*DecreaseKey(u, y) - Decreases the key in node u to y (pre-condition: y &lt;= u.x).

==Efficiency Analysis==
As all non-constant-time operations are defined in terms of the Meld operation, the efficiency of these operations can be determined through analysis of the complexity of melding two randomized heaps.

The result of this analysis is that the expected time of any meldable priority queue operation on a n-node randomized heap is ''O''(log''n'').&lt;ref name=&quot;Gambin&quot; /&gt;&lt;ref name=&quot;Morin&quot;&gt;P. Morin, [http://opendatastructures.org/ods-java.pdf] Open Data Structures, p. 191-&lt;/ref&gt;

{| class=&quot;wikitable&quot;
|-
! Operation !! Worst-Case Time Efficiency
|-
| Meld(Q1, Q2) || ''O''(log''n'')
|-
| Insert(x) || ''O''(log''n'')
|-
| Remove() || ''O''(log''n'')
|-
| FindMin() || ''O''(log''n'')
|-
| Remove(x) || ''O''(log''n'')
|-
| Absorb(Q) || ''O''(log''n'')
|}

==History==
The meldable heap appears to have first been proposed in 1998 by Gambin and Malinowski.&lt;ref name=&quot;Gambin&quot; /&gt;

==Variants==
While the randomized meldable heap is the simplest form of a meldable heap implementation, others do exist.  These are:
*[[Leftist heap]]
*[[Binomial heap]]
*[[Fibonacci Heap]]
*[[Pairing heap]]
*[[Skew heap]]

==References==
{{reflist}}



[[Category:Data structures]]</text>
      <sha1>sdfy65dwyfhj1qz9e61ydjfhfs02ejg</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Order-maintenance problem</title>
    <ns>0</ns>
    <id>39047079</id>
    <revision>
      <id>607413041</id>
      <parentid>590139808</parentid>
      <timestamp>2014-05-07T00:40:14Z</timestamp>
      <contributor>
        <username>January2009</username>
        <id>8705583</id>
      </contributor>
      <comment>add link to Athanasios Tsakalidis</comment>
      <text xml:space="preserve" bytes="20909">{{Orphan|date=April 2013}}

In [[Computer Science]] the '''Order-Maintenance Problem''' involves
maintaining a totally ordered set supporting the following operations:

* &lt;code&gt;insert(X, Y)&lt;/code&gt;, which inserts X immediately after Y in the total order;
* &lt;code&gt;order(X, Y)&lt;/code&gt;, which determines if X precedes Y in the total order; and
* &lt;code&gt;delete(X)&lt;/code&gt;, which removes X from the set.

The first data structure for order-maintenance was given by Dietz in
1982.&lt;ref name=&quot;Dietz&quot;&gt;{{citation
 | last = Dietz | first = Paul F.
 | contribution = Maintaining order in a linked list
 | doi = 10.1145/800070.802184
 | location = New York, NY, USA
 | pages = 122–127
 | publisher = ACM
 | title = [[Symposium on Theory of Computing|Proceedings of the 14th Annual ACM Symposium on Theory of Computing (STOC '82)]]
 | year = 1982}}.&lt;/ref&gt; This data
structure supports &lt;code&gt;insert(X, Y)&lt;/code&gt; in &lt;math&gt;O(\log n)&lt;/math&gt;
amortized time and &lt;code&gt;order(X, Y)&lt;/code&gt; in constant time but does
not support deletion. [[Athanasios Tsakalidis|Tsakalidis]] published a data structure in 1984
based on BB[α] trees with the same performance bounds that supports
deletion in &lt;math&gt;O(\log n)&lt;/math&gt; and showed how indirection can be
used to improve insertion and deletion performance to
&lt;math&gt;O(1)&lt;/math&gt; amortized time.&lt;ref name=&quot;Tsakalidis&quot;&gt;{{citation
 | last = Tsakalidis | first = Athanasios K.
 | doi = 10.1007/BF00289142
 | issue = 1
 | journal = [[Acta Informatica]]
 | mr = 747173
 | pages = 101–112
 | title = Maintaining order in a generalized linked list
 | volume = 21
 | year = 1984}}.&lt;/ref&gt; In 1987 Dietz and Sleator published the first data
structure supporting all operations in worst-case constant time.&lt;ref
name=&quot;DietzSleator&quot;&gt;{{citation
 | last1 = Dietz | first1 = P.
 | last2 = Sleator | first2 = D. | author2-link= Daniel Sleator
 | contribution = Two algorithms for maintaining order in a list
 | doi = 10.1145/28395.28434
 | location = New York, NY, USA
 | pages = 365–372
 | publisher = ACM
 | title = [[Symposium on Theory of Computing|Proceedings of the 19th Annual ACM Symposium on Theory of Computing (STOC '87)]]
 | year = 1987}}. Full version,
[http://www.cs.cmu.edu/~sleator/papers/maintaining-order.html Tech. Rep. CMU-CS-88-113], Carnegie Mellon
University, 1988.&lt;/ref&gt;

Efficient data structures for order-maintenance have applications in
many areas, including data structure persistence,&lt;ref
name=&quot;Driscoll&quot;&gt;{{citation
 | last1 = Driscoll | first1 = James R.
 | last2 = Sarnak | first2 = Neil
 | last3 = Sleator | first3 = Daniel D. | author3-link = Daniel Sleator
 | last4 = Tarjan | first4 = Robert E. | author4-link = Robert Tarjan
 | doi = 10.1016/0022-0000(89)90034-2
 | issue = 1
 | journal = [[Journal of Computer and System Sciences]]
 | mr = 990051
 | pages = 86–124
 | title = Making data structures persistent
 | volume = 38
 | year = 1989}}.&lt;/ref&gt; graph algorithms&lt;ref
name=&quot;Eppstein&quot;&gt;{{citation
 | last1 = Eppstein | first1 = David | author1-link = David Eppstein
 | last2 = Galil | first2 = Zvi | author2-link = Zvi Galil
 | last3 = Italiano | first3 = Giuseppe F.
 | last4 = Nissenzweig | first4 = Amnon
 | doi = 10.1145/265910.265914
 | issue = 5
 | journal = [[Journal of the ACM]]
 | mr = 1492341
 | pages = 669–696
 | title = Sparsification—a technique for speeding up dynamic graph algorithms
 | volume = 44
 | year = 1997}}.&lt;/ref&gt;&lt;ref name=&quot;Katriel&quot;&gt;{{citation
 | last1 = Katriel | first1 = Irit
 | last2 = Bodlaender | first2 = Hans L. | author2-link = Hans L. Bodlaender
 | doi = 10.1145/1159892.1159896
 | issue = 3
 | journal = ACM Transactions on Algorithms
 | mr = 2253786
 | pages = 364–379
 | title = Online topological ordering
 | volume = 2
 | year = 2006}}.&lt;/ref&gt; and fault-tolerant data structures.&lt;ref
name=&quot;Aumann&quot;&gt;{{citation
 | last1 = Aumann | first1 = Yonatan
 | last2 = Bender | first2 = Michael A.
 | contribution = Fault tolerant data structures
 | doi = 10.1109/SFCS.1996.548517
 | pages = 580–589
 | title = [[Symposium on Foundations of Computer Science|Proceedings of the 37th Annual Symposium on Foundations of Computer Science (FOCS 1996)]]
 | year = 1996}}.&lt;/ref&gt;

==List-labeling==

A problem related to the order-maintenance problem is the
''list-labeling problem'' in which instead of the &lt;code&gt;order(X,
Y)&lt;/code&gt; operation the solution must maintain an assignment of labels
from a universe of integers &lt;math&gt;\{1, 2, \ldots, u\}&lt;/math&gt; to the
elements of the set such that X precedes Y in the total order if and
only if X is assigned a lesser label than Y. It must also support an
operation &lt;code&gt;label(X)&lt;/code&gt; returning the label of any node X.
Note that &lt;code&gt;order(X, Y)&lt;/code&gt; can be implemented simply by
comparing &lt;code&gt;label(X)&lt;/code&gt; and &lt;code&gt;label(Y)&lt;/code&gt; so that any
solution to the list-labeling problem immediately gives one to the
order-maintenance problem. In fact, most solutions to the
order-maintenance problem are solutions to the list-labeling problem
augmented with a level of data structure indirection to improve
performance. We will see an example of this below.

For efficiency, it is desirable for the size &lt;math&gt;u&lt;/math&gt; of the
universe be bounded in the number of elements &lt;math&gt;n&lt;/math&gt; stored in
the data structure. In the case where &lt;math&gt;u&lt;/math&gt; is required to be
linear in &lt;math&gt;n&lt;/math&gt; the problem is known as the ''packed-array maintenance'' or ''dense sequential file maintenance'' problem.
Consider the elements as entries in a file and the labels as giving
the addresses where each element is stored. Then an efficient solution
to the packed-array maintenance problem would allow efficient
insertions and deletions of entries into the middle of a file with no
asymptotic space overhead.&lt;ref name=&quot;Itai&quot;&gt;{{citation
 | last1 = Itai | first1 = Alon
 | last2 = Konheim | first2 = Alan
 | last3 = Rodeh | first3 = Michael
 | contribution = A sparse table implementation of priority queues
 | doi = 10.1007/3-540-10843-2_34
 | pages = 417–431
 | series = Lecture Notes in Computer Science
 | title = Automata, Languages and Programming: Eighth Colloquium Acre (Akko), Israel July 13–17, 1981
 | volume = 115
 | year = 1981}}.&lt;/ref&gt;&lt;ref name=&quot;WillardIns&quot;&gt;{{citation
 | last = Willard | first = Dan E. | author-link = Dan Willard
 | publisher = Bell Laboratories
 | series = Technical Report TM81-45193-5
 | title = Inserting and deleting records in blocked sequential files
 | year = 1981}}.&lt;/ref&gt;&lt;ref
name=&quot;WillardGood&quot;&gt;{{citation
 | last = Willard | first = Dan E. | author-link = Dan Willard
 | contribution = Maintaining dense sequential files in a dynamic environment (Extended Abstract)
 | doi = 10.1145/800070.802183
 | location = New York, NY, USA
 | pages = 114–121
 | publisher = ACM
 | title = Proceedings of the 14th ACM Symposium on Theory of Computing (STOC '82)
 | year = 1982}}.&lt;/ref&gt;&lt;ref name=&quot;WillardADensity&quot;&gt;{{citation
 | last = Willard | first = Dan E. | author-link = Dan Willard
 | contribution = Good worst-case algorithms for inserting and deleting records in dense sequential files
 | doi = 10.1145/16894.16879
 | location = New York, NY, USA
 | pages = 251–260
 | publisher = ACM
 | series = SIGMOD Record 15(2)
 | title = Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data (SIGMOD '86)
 | year = 1986}}.&lt;/ref&gt;&lt;ref
name=&quot;WillardMaint&quot;&gt;{{citation
 | last = Willard | first = Dan E. | author-link = Dan Willard
 | doi = 10.1016/0890-5401(92)90034-D
 | issue = 2
 | journal = [[Information and Computation]]
 | pages = 150–204
 | title = A density control algorithm for doing insertions and deletions in a sequentially ordered file in a good worst-case time
 | volume = 97
 | year = 1992}}.&lt;/ref&gt; This usage has recent applications in
cache-oblivious data structures&lt;ref name=&quot;BenderCacheObl&quot;&gt;{{citation
 | last1 = Bender | first1 = Michael A.
 | last2 = Demaine | first2 = Erik D. | author2-link = Erik Demaine
 | last3 = Farach-Colton | first3 = Martin
 | doi = 10.1137/S0097539701389956
 | issue = 2
 | journal = [[SIAM Journal on Computing]]
 | mr = 2191447
 | pages = 341–358
 | title = Cache-oblivious B-trees
 | url = http://erikdemaine.org/papers/CacheObliviousBTrees_SICOMP/paper.pdf
 | volume = 35
 | year = 2005}}.&lt;/ref&gt;
and optimal worst-case in-place sorting.&lt;ref
name=&quot;Franceschini&quot;&gt;{{citation
 | last1 = Franceschini | first1 = Gianni
 | last2 = Geffert | first2 = Viliam
 | doi = 10.1145/1082036.1082037
 | issue = 4
 | journal = [[Journal of the ACM]]
 | mr = 2164627
 | pages = 515–537
 | title = An in-place sorting with O(''n''&amp;nbsp;log&amp;nbsp;''n'') comparisons and O(''n'') moves
 | volume = 52
 | year = 2005}}.&lt;/ref&gt;

However, under some restrictions, &lt;math&gt;\Omega(\log^2 n)&lt;/math&gt; lower
bounds have been found on the performance of insertion in solutions of
the list-labeling problem with linear universes&lt;ref
name=&quot;DietzZhang&quot;&gt;{{citation
 | last1 = Dietz | first1 = Paul F.
 | last2 = Zhang | first2 = Ju
 | contribution = Lower bounds for monotonic list labeling
 | doi = 10.1007/3-540-52846-6_87
 | location = Berlin
 | mr = 1076025
 | pages = 173–180
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = SWAT 90 (Bergen, 1990)
 | volume = 447
 | year = 1990}}.&lt;/ref&gt; whereas we
will see below that a solution with a polynomial universe can perform
insertions in &lt;math&gt;O(\log n)&lt;/math&gt; time.

===List-labeling and binary search trees===

[[File:An example of path labels as described in the order-maintenance page.svg|thumb|alt=An example of path labels as described in the order-maintenance page.| The path label of X in this binary tree is 0221, which is the base 3 representation of the integer 25.]]

List-labeling in a universe of size polynomial in the number
&lt;math&gt;n&lt;/math&gt; of elements in the total order is connected to the
problem of maintaining balance in a [[binary search tree]]. Note that
every node X of a binary search tree is implicitly labeled with an
integer that corresponds to its path from the root of the tree. We
call this integer the ''path label'' of X and define it as follows.
Let &lt;math&gt;\sigma&lt;/math&gt; be the sequence of left and right descents in
this path. For example, &lt;math&gt;\sigma = (\text{left}, \text{right},
\text{right})&lt;/math&gt; if X is the right child of the right child of the
left child of the root of the tree. Then X is labeled with the integer
whose [[radix|base]] 3 representation is given by replacing every
occurrence of &lt;math&gt;\text{left}&lt;/math&gt; in &lt;math&gt;\sigma&lt;/math&gt; with 0,
replacing every occurrence of &lt;math&gt;\text{right}&lt;/math&gt; in
&lt;math&gt;\sigma&lt;/math&gt; with 2 and appending 1 to the end of the resulting
string. Then in the previous example, the path label of X is
0221&lt;sub&gt;3&lt;/sub&gt; which is equal to 25 in base 10. Note that path
labels preserve tree-order and so can be used to answer order queries
in constant time.
&lt;!-- XXX need a figure illustrating path labels! --&gt;

If the tree has height &lt;math&gt;h&lt;/math&gt; then these integers come from
the universe &lt;math&gt;\{1, 2, \ldots, 3^{h+1}-2\}&lt;/math&gt;. Then if the
tree is [[self-balancing binary search tree|self-balancing]] so that
it maintains a height no greater than &lt;math&gt;c\log n&lt;/math&gt; then the
size of the universe is polynomial in &lt;math&gt;n&lt;/math&gt;.

Therefore, the list-labeling problem can be solved by maintaining a
balanced binary search tree on the elements in the list augmented with
path labels for each node. However, since every rebalancing operation
of the tree would have to also update these path labels, not every
self-balancing tree data structure is suitable for this application.
Note, for example, that rotating a node with a subtree of size
&lt;math&gt;k&lt;/math&gt;, which can be done in constant time under usual
circumstances, requires &lt;math&gt;\Omega(k)&lt;/math&gt; path label updates. In
particular, if the node being rotated is the root then the rotation
would take time linear in the size of the whole tree. With that much
time the entire tree could be rebuilt. We will see below that there
are self-balancing binary search tree data structures that cause an
appropriate number of label updates during rebalancing.

==Data structure==

The list-labeling problem can be solved with a universe of size
polynomial in the number of elements &lt;math&gt;n&lt;/math&gt; by augmenting a
[[scapegoat tree]] with the path labels described above. Scapegoat
trees do all of their rebalancing by rebuilding subtrees. Since it
takes &lt;math&gt;\Omega(k)&lt;/math&gt; time to rebuild a subtree of size
&lt;math&gt;k&lt;/math&gt;, relabeling that subtree in &lt;math&gt;O(k)&lt;/math&gt; time
after it is rebuilt does not change the asymptotic performance of the
rebalancing operation.

===Order===

Given two nodes X and Y, &lt;code&gt;order(X, Y)&lt;/code&gt; determines their
order by comparing their path labels.

===Insert===

Given a new node for X and a pointer to the node Y, &lt;code&gt;insert(X,
Y)&lt;/code&gt; inserts X immediately after Y in the usual way. If a
rebalancing operation is required, the appropriate subtree is rebuilt,
as usual for a scapegoat tree. Then that subtree is traversed depth
first and the path labels of each of its nodes is updated. As
described above, this asymptotically takes no longer than the usual
rebuilding operation.

===Delete===

Deletion is performed similarly to insertion. Given the node X to be
deleted, &lt;code&gt;delete(X)&lt;/code&gt; removes X as usual. Any subsequent
rebuilding operation is followed by a relabeling of the rebuilt
subtree.

==Analysis==

It follows from the argument above about the rebalancing performance
of a scapegoat tree augmented with path labels that the insertion and
deletion performance of this data structure are asymptotically no
worse than in a regular scapegoat tree. Then, since insertions and
deletions take &lt;math&gt;O(\log n)&lt;/math&gt; amortized time in scapegoat
trees, the same holds for this data structure.

Furthermore, since a scapegoat tree with parameter α maintains a
height of at most &lt;math&gt;\log_{1/\alpha}n + 1&lt;/math&gt;, the path labels
have size at most &lt;math&gt;3^{\log_{1/\alpha}n + 2}-2 \le
9n^{\frac{1}{\log_3(1/\alpha)}}&lt;/math&gt;. For the typical value of
&lt;math&gt;\alpha=\tfrac{2}{3}&lt;/math&gt; then the labels are at most
&lt;math&gt;9n^3&lt;/math&gt;.

Of course, the order of two nodes can be determined in constant time
by comparing their labels. A closer inspection shows that this holds
even for large &lt;math&gt;n&lt;/math&gt;. Specifically, if the word size of the
machine is &lt;math&gt;w&lt;/math&gt; bits, then typically it can address at most
&lt;math&gt;2^w&lt;/math&gt; nodes so that &lt;math&gt;n &lt; 2^w&lt;/math&gt;. It follows that
the universe has size at most &lt;math&gt;9\cdot 2^{3w}&lt;/math&gt; and so that
the labels can be represented with a constant number of (at most &lt;math&gt;4 +
3w&lt;/math&gt;) bits.

===Lower bound on list-labeling===

It has been shown that any solution to the list-labeling problem with
a universe polynomial in the number of elements will have insertion
and deletion performance no better than &lt;math&gt;\Omega(\log
n)&lt;/math&gt;.&lt;ref name=&quot;DietzPaul&quot;&gt;{{citation
 | last1 = Dietz | first1 = Paul F.
 | last2 = Seiferas | first2 = Joel I.
 | last3 = Zhang | first3 = Ju
 | contribution = A tight lower bound for on-line monotonic list labeling
 | doi = 10.1007/3-540-58218-5_12
 | location = Berlin
 | mr = 1315312
 | pages = 131–142
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Algorithm theory—SWAT '94 (Aarhus, 1994)
 | volume = 824
 | year = 1994}}.&lt;/ref&gt; Then, for list-labeling, the above solution is
asymptotically optimal. Incidentally, this also proves an
&lt;math&gt;\Omega(\log n)&lt;/math&gt; lower bound on the amortized rebalancing
time of an insertion or deletion in a scapegoat tree.

However, this lower bound does not apply to the order-maintenance
problem and, as stated above, there are data structures that give
worst-case constant time performance on all order-maintenance
operations.

==O(1) amortized insertion via indirection==

Indirection is a technique used in data structures in which a problem
is split into multiple levels of a data structure in order to improve
efficiency. Typically, a problem of size &lt;math&gt;n&lt;/math&gt; is split into
&lt;math&gt;n/\log n&lt;/math&gt; problems of size &lt;math&gt;\log n&lt;/math&gt;. For
example, this technique is used in [[y-fast trie]]s. This
strategy also works to improve the insertion and deletion performance
of the data structure described above to constant amortized time. In
fact, this strategy works for any solution of the list-labeling
problem with &lt;math&gt;O(\log n)&lt;/math&gt; amortized insertion and deletion
time.

[[File:Depiction of indirection in a tree based solution to the order-maintenance problem.svg|thumb|alt=Depiction of indirection in a tree based solution to the order-maintenance problem.| The order-maintenance data structure with indirection.  The total order elements are stored in &lt;math&gt;O(N/\log N)&lt;/math&gt; contiguous sublists of size &lt;math&gt;O(\log N)&lt;/math&gt;, each of which has a representative in the scapegoat tree.]]

The new data structure is completely rebuilt whenever it grows too
large or too small. Let &lt;math&gt;N&lt;/math&gt; be the number of elements of
the total order when it was last rebuilt. The data structure is
rebuilt whenever the invariant &lt;math&gt;\tfrac{N}{3}\le n\le 2N&lt;/math&gt; is
violated by an insertion or deletion. Since rebuilding can be done in
linear time this does not affect the amortized performance of
insertions and deletions.

During the rebuilding operation, the &lt;math&gt;N&lt;/math&gt; elements of the
total order are split into &lt;math&gt;O(N/\log N)&lt;/math&gt; contiguous
sublists, each of size &lt;math&gt;\Omega(\log N)&lt;/math&gt;. A path-labeled
scapegoat tree is constructed on a set of nodes representing each of
the sublists in their original list order. For each sublist a
doubly-linked of its elements is built storing with each element a
pointer to its representative in the tree as well as a local integer
label. These local labels are independent of the labels used in the
tree and are used to compare any two elements of the same sublist. The
elements of a sublist are given the local labels &lt;math&gt;\log N, 2\log
N, \ldots, \log^2 N, \ldots&lt;/math&gt;, in their original list order.

===Order===

Given the sublist nodes X and Y, &lt;code&gt;order(X, Y)&lt;/code&gt; can be
answered by first checking if the two nodes are in the same
sublist. If so, their order can be determined by comparing their local
labels. Otherwise the path labels of their representatives in the tree
are compared. This takes constant time.

===Insert===

Given a new sublist node for X and a pointer to the sublist node Y,
&lt;code&gt;insert(X, Y)&lt;/code&gt; inserts X immediately after Y in the sublist
of Y. If X is inserted at the end of the sublist, it is given the
local label &lt;math&gt;\ell(Y) + \log N&lt;/math&gt;, where &lt;math&gt;\ell(Y)&lt;/math&gt;
is the local label of Y; otherwise, if possible, it is labeled with
the floor of the average of the local labels of its two neighbours.
This easy case takes constant time.

In the hard case, the neighbours of X have contiguous local labels and
the sublist has to be rebuilt.  However, in this case at least
&lt;math&gt;\log N - 1&lt;/math&gt; elements have been added to the sublist since
it was first built.  Then we can afford to spend a linear amount of
time in the size of the sublist to rebuild it and possibly split it
into smaller sublists without affecting the amortized cost of
insertion by any more than a constant.

If the sublist has size &lt;math&gt;k&lt;/math&gt; then we split it into
&lt;math&gt;O(k/\log N)&lt;/math&gt; contiguous sublists of size &lt;math&gt;O(\log
N)&lt;/math&gt;, locally labeling each new sublist as described above and
pointing each element of a sublist to a new representative node to be
inserted into the tree. It takes &lt;math&gt;O(k)&lt;/math&gt; time to construct
the sublists. Since we do not allow empty sublists, there are at most
&lt;math&gt;n\le 2N&lt;/math&gt; of them and so a representative can be inserted
into the tree in &lt;math&gt;O(\log N)&lt;/math&gt; time. Hence, it takes
&lt;math&gt;O(k)&lt;/math&gt; time to insert all of the new representatives into
the tree.

===Delete===

Given a sublist node X to be deleted, &lt;code&gt;delete(X)&lt;/code&gt; simply
removes X from its sublist in constant time. If this leaves the
sublist empty, then we need to remove the representative of the
sublist from the tree. Since at least &lt;math&gt;\Omega(\log N)&lt;/math&gt;
elements were deleted from the sublist since it was first built we can afford to spend the &lt;math&gt;O(\log N)&lt;/math&gt; time it takes to remove the representative without affecting the amortized cost of deletion by any more than a constant.

==See also==
* [[Scapegoat tree]]

==References==
{{reflist}}

==External links==
* [http://dx.doi.org/10.1007/3-540-45749-6_17 Two simplified algorithms for maintaining order in a list.] - This paper presents a list-labeling data structure with amortized performance that does not explicitly store a tree.  The analysis given is simpler than the one given by (Dietz and Sleator, 1987) for a similar data structure.
* [http://opendatastructures.org/ods-java/8_Scapegoat_Trees.html Open Data Structures - Chapter 8 - Scapegoat trees]

[[Category:Data structures]]</text>
      <sha1>6a599a16yac7jr1i9bnaxi5dc0hnf71</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Anatree</title>
    <ns>0</ns>
    <id>39065734</id>
    <revision>
      <id>624080820</id>
      <parentid>568822310</parentid>
      <timestamp>2014-09-03T22:59:30Z</timestamp>
      <contributor>
        <username>Trappist the monk</username>
        <id>10289486</id>
      </contributor>
      <minor/>
      <comment>Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated date parameter errors]] using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="6357">{{more footnotes|date=July 2013}}

[[File: Image of an anatree1.png|frame|250px|x150px|Anatree]]
[[File: Image of a mixed anatree.png|frame|250px|x150px|Mixed Anatree]]
An '''Anatree''' &lt;ref&gt;{{cite journal|last=Reams|first=Charles|title=Anatree: a fast data structure for anagrams|journal=Journal of Experimental Algorithmics (JEA)|date=March 2012 |volume=17|issue=1 |pages=2012 |doi=10.1145/2133803.2133804}}&lt;/ref&gt; is a [[data structure]] designed to solve [[anagrams]]. Solving an anagram is the problem of finding a word from a given list of letters. These problems are commonly encountered in word games like wordwheel, [[scrabble]] or [[crossword]] puzzles that we find in newspapers. The problem for the wordwheel also has the condition that the central letter appear in all the words framed with the given set. Some other conditions may be introduced regarding the frequency (number of appearances) of each of the letters in the given input string. These problems are classified as [[Constraint satisfaction problem]] in computer science literature.

An anatree is represented as a directed [[tree]] which contains a set of words (W) encoded as strings in some [[alphabet]]. The internal vertices are labelled with some letter in the alphabet and the leaves contain words. The edges are labelled with non-negative integers. An anatree has the property that the sum of the edge labels from the root to the leaf is the length of the word stored at the leaf. If the internal vertices are labelled as &lt;math&gt;\alpha_1&lt;/math&gt;, &lt;math&gt;\alpha_2&lt;/math&gt; ... &lt;math&gt;\alpha_l&lt;/math&gt;, and the edge labels are &lt;math&gt;n_1&lt;/math&gt;, &lt;math&gt;n_2&lt;/math&gt; ... &lt;math&gt;n_l&lt;/math&gt;, then the path from the root to the leaf along these vertices and edges are a list of words that contain &lt;math&gt;n_1&lt;/math&gt; &lt;math&gt;\alpha_1&lt;/math&gt; s, &lt;math&gt;n_2&lt;/math&gt; &lt;math&gt;\alpha_2&lt;/math&gt; s and so on. Anatrees are intended to be read only data structures with all the words available at construction time. 

A '''Mixed Anatree''' is an anatree where the internal vertices also store words. A mixed anatree can have words of varying lengths, where as in a regular anatree, all words are of the same length.

== Data Structures for solving Anagrams ==
A number of data structures have been proposed to solve anagrams in constant time. Two of the most commonly used data structures are the '''Alphabetic map''' and the '''Frequency map'''. The Alphabetic map maintains a [[hash table]] of all the possible words that can be in the language (this is referred to as the [[lexicon]]). For a given input sting, sort the letters in alphabetic order. This sorted string maps onto a word in the hash table. Hence finding the anagram requires sorting the letters and a looking up the word in the hash table. The sorting can be done in linear time by the [[counting sort]] and hash table look up can be done in constant time. 

For example for the word ANATREE, the alphabetic map would produce a mapping of &lt;math&gt; \{AAEENRT -&gt; \{''anatree''\}\}&lt;/math&gt;.

A Frequency map also stores the list of all possible words in the lexicon in a hash table. For a given input string, the frequency map maintains the frequencies (number of appearances) of all the letters and uses this count to perform a look up in the hash table. The worst case execution time is found to be linear in size of the lexicon. 

For example for the word ANATREE, the alphabetic map would produce a mapping of &lt;math&gt; f(A)-&gt;2, f(E)-&gt;2, f(N)-&gt;1, f(R)-&gt;1, f(T)-&gt;2&lt;/math&gt;. The words that do not appear in the string are not written in the map.

== Construction of an Anatree ==
The construction of an Anatree begins by selecting a label for the root and partitioning words based on the label chosen for the root. This process is repeated recursively for all the labels of the tree. Anatree construction is non-canonical for a given set of words, depending on the label chosen for the root, the anatree will differ accordingly. The performance of the anatree is greatly impacted by the choice of labels. 

The following are some heuristics for choosing labels:
* Start labeling vertices in alphabetical order from the root. This approach reduces construction overhead
* Start labeling vertices based on the relative frequency. A probabilistic approach is used to assign labels to vertices. If &lt;math&gt;W_n^\alpha&lt;/math&gt; is the set of words that contain &lt;math&gt;n \alpha s&lt;/math&gt;, then we label the vertex with &lt;math&gt;\alpha&lt;/math&gt; if it maximizes the expected distance to the leaf. This approach has the most frequently appearing characters (like E) labeled at the root and the least frequently appearing characters labeled at the leaves. The following equation is maximized &lt;math&gt; D_\alpha = \sum_n n \frac{|W_n^\alpha|}{|W|} &lt;/math&gt;. This approach prevents long sequences of zero labeled edges since they do not contribute letters to the words generated by the anatree.

== Finding Anagrams in an Anatree ==
To find a word in an anatree, start at the root, depending on the frequency of the label in the given input string, follow the edge that has that frequency till the leaf. The leaf contains the required word. For example, consider the anatree in the figure, to find the word &lt;math&gt;dog&lt;/math&gt;, the given string may be &lt;math&gt;ogd&lt;/math&gt;. Start at the root and follow the edge that has &lt;math&gt;1&lt;/math&gt; as the label. We follow this label since the given input string has &lt;math&gt;1&lt;/math&gt; &lt;math&gt;d&lt;/math&gt;. Traverse this edge until the leaf is encountered. That gives the required word.

== Space and Time Requirements ==
A lexicon that stores &lt;math&gt;w&lt;/math&gt; words (each word can be &lt;math&gt;l&lt;/math&gt; characters long) in an alphabet &lt;math&gt;A&lt;/math&gt; has the following space requirements. 
{| class=&quot;wikitable&quot;
|-
! Data Structure !! Space Requirements
|-
| Alphabetic Map || &lt;math&gt; O(wl(log |A|) &lt;/math&gt;
|-
| Frequency Map || &lt;math&gt; O(w(|A|log l + l log |A|)) &lt;/math&gt;
|-
| Anatree || &lt;math&gt; O(w|A|(log |A| + l)) &lt;/math&gt;
|}

The worst case execution time of an anatree is &lt;math&gt; O(|w| (l + w |A|^2)) &lt;/math&gt;

==See also==
* Anagram &lt;ref&gt;{{cite web|url=http://anagram.sourceforge.net/|title=anagram|accessdate=7 April 2013}}&lt;/ref&gt;
* Damn Cool Algorithms Part 3 - Anagram Trees &lt;ref&gt;{{cite web|url=http://blog.notdot.net/2007/10/Damn-Cool-Algorithms-Part-3-Anagram-Trees|title=Nick's Blog}}&lt;/ref&gt;
* [[Anagrams]]

==References==
{{Reflist}}

[[Category:Data structures]]</text>
      <sha1>3e87279thwj8yt7jqka2feo4hk5b7qg</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Java collections framework</title>
    <ns>0</ns>
    <id>4294832</id>
    <revision>
      <id>626782378</id>
      <parentid>626741658</parentid>
      <timestamp>2014-09-23T16:34:24Z</timestamp>
      <contributor>
        <username>Seaphoto</username>
        <id>1594127</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/180.211.100.146|180.211.100.146]] ([[User talk:180.211.100.146|talk]]) to last revision by ClueBot NG ([[WP:HG|HG]])</comment>
      <text xml:space="preserve" bytes="20600">[[File:Map Classes.jpg|thumb|class- and interface hierarchy of java.util.Map]]
The '''[[Java (programming language)|Java]] collections framework''' ('''JCF''') is a set of [[class (computer science)|classes]] and [[interface (java)|interfaces]] that implement commonly reusable collection [[data structure]]s.&lt;ref&gt;{{cite web
| url=http://download.oracle.com/javase/tutorial/collections/intro/index.html
| title=Lesson: Introduction to Collections
| publisher=[[Oracle Corporation]]
| accessdate=2010-12-22}}&lt;/ref&gt;

Although referred to as a [[Software framework|framework]], it works in a manner of a '''library'''. The JCF provides both interfaces that define various collections and classes that implement them.

==History==
Collection implementations in pre-[[J2SE 1.2|JDK 1.2]] versions of the Java platform included few data structure classes, but did not contain a collections framework.&lt;ref name=&quot;ibm&quot;&gt;{{cite web
| url=http://www.digilife.be/quickreferences/PT/Java%20Collections%20Framework.pdf
| title=Java Collections Framework
| publisher=[[IBM]]
| accessdate=2011-01-01}}&lt;/ref&gt; The standard methods for grouping Java objects were via the array, the '''{{Javadoc:SE|java/util|Vector}}''', and the '''{{Javadoc:SE|java/util|Hashtable}}''' classes, which unfortunately were not easy to extend, and did not implement a standard member interface.&lt;ref&gt;{{cite web
| url=http://www.javaworld.com/jw-11-1998/jw-11-collections.html
| title=Get started with the Java Collections Framework
| publisher=[[JavaWorld]]
| quote=''Before Collections made its most welcome debut, the standard methods for grouping Java objects were via the array, the Vector, and the Hashtable. All three of these collections have different methods and syntax for accessing members: arrays use the square bracket ([]) symbols, Vector uses the elementAt method, and Hashtable uses &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;put&lt;/code&gt; methods.''
| date=1998-01-11
| accessdate=2011-01-01}}&lt;/ref&gt;

To address the need for reusable collection [[data structure]]s, several independent frameworks were developed,&lt;ref name=&quot;ibm&quot;/&gt; the most used being [[Doug Lea]]'s ''Collections package'',&lt;ref name=&quot;douglea&quot;&gt;{{cite web
| url=http://gee.cs.oswego.edu/dl/classes/collections/index.html
| title=Overview of the collections Package 
| author=[[Doug Lea]]
| quote=''The Sun Java Development Kit JDK1.2 finally includes a standard set of collection classes. While there are some design and implementation differences, the JDK1.2 package contains most of the same basic abstractions, structure, and functionality as this package. For this reason, this collections package will NOT be further updated''
| accessdate=2011-01-01}}&lt;/ref&gt; and [[Recursion Software|ObjectSpace]] ''Generic Collection Library'' (JGL),&lt;ref&gt;{{cite web
| title=Generic Collection Library for Java™
| url=http://www.stanford.edu/group/coursework/docsTech/jgl/
| accessdate=2011-01-01}}&lt;/ref&gt; whose main goal was consistency with the [[C++]] [[Standard Template Library]] (STL).&lt;ref&gt;{{cite web
| title=Generic Collection Library
| publisher=[[Recursion Software|ObjectSpace]]
| quote=''Version 1 of JGL was made free to the Java community in June 1996''
| url=http://www.javaworld.com/javaworld/jw-06-1997/jw-06-jgl.html
| title=Need a good set of abstract data structures? ObjectSpace's JGL packs a punch!
| quote=''As with Java itself, the Java Generic Library borrows heavily from the C++ camp: It takes the best from C++'s STL, while leaving the C++ warts behind. Most C++ programmers today will know of their STL, but few are managing to exploit its potential.''
| publisher=[[JavaWorld]]
| date=1997-01-06
| accessdate=2011-01-01}}&lt;/ref&gt;

The collections framework was designed and developed primarily by [[Joshua Bloch]], and was introduced in [[J2SE 1.2|JDK 1.2]]. It reused many ideas and classes from Doug Lea's ''Collections package'', which was deprecated as a result.&lt;ref name=&quot;douglea&quot; /&gt; Sun chose not to use the ideas of JGL, because they wanted a compact framework, and consistency with C++ was not one of their goals.&lt;ref&gt;{{cite web
| url=http://www.javaworld.com/javaworld/jw-01-1999/jw-01-jglvscoll.html
| title=The battle of the container frameworks: which should you use?
| quote=''Comparing ObjectSpace Inc.'s JGL and Sun's Collections Framework turns out to be like comparing apples and kiwi fruits. At first sight, the two frameworks seem to be competing for the same developers, but after a closer inspection it is clear that the two cannot be compared fairly without acknowledging first that the two frameworks have different goals. If, like Sun's documentation states, Collections is going to homogenize Sun's own APIs (core API, extensions, etc.), then clearly Collections has to be great news, and a good thing, even to the most fanatic JGL addict. Provided Sun doesn't break its promise in this area, I'll be happy to invest my resources in adopting Collections in earnest. ''
| publisher=[[JavaWorld]]
| date=1999-01-01
| accessdate=2011-01-01}}&lt;/ref&gt;

Doug Lea later developed a [[Concurrency (computer science)|concurrency]] [[Java package|package]], comprising new Collection-related classes.&lt;ref name=&quot;douglea2&quot;&gt;{{cite web
| url=http://gee.cs.oswego.edu/dl/classes/EDU/oswego/cs/dl/util/concurrent/intro.html
| title=Overview of package util.concurrent Release 1.3.4
| author=[[Doug Lea]]
| quote=''Note: Upon release of J2SE 5.0, this package enters maintenance mode: Only essential corrections will be released. J2SE5 package java.util.concurrent includes improved, more efficient, standardized versions of the main components in this package.''
| accessdate=2011-01-01}}&lt;/ref&gt; An updated version of these concurrency utilities was included in [[J2SE 5.0|JDK 5.0]] as of [[Java concurrency|JSR 166]].

==Architecture==
Almost all collections in Java are derived from the '''{{Javadoc:SE|package=java.util|java/util|Collection}}''' interface. Collection defines the basic parts of all collections. The interface states the add() and remove() methods for adding to and removing from a collection respectively. Also required is the toArray() method, which converts the collection into a simple array of all the elements in the collection. Finally, the contains() method checks if a specified element is in the collection. The Collection interface is a subinterface of '''{{Javadoc:SE|package=java.lang|java/lang|Iterable}}''', so any Collection may be the target of a for-each statement. (The Iterable interface provides the iterator() method used by for-each statements.) All collections have an iterator that goes through all of the elements in the collection. Additionally, Collection is a generic. Any collection can be written to store any class. For example, Collection&lt;String&gt; can hold strings, and the elements from the collection can be used as strings without any casting required.&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/lang/Iterable.html |title=Iterable (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06 |accessdate=2013-08-16}}&lt;/ref&gt;

===List interface===
Lists are implemented in the JCF via the '''{{Javadoc:SE|package=java.util|java/util|List}}''' interface. It defines a list as essentially a more flexible version of an array. Elements have a specific order, and duplicate elements are allowed. Elements can be placed in a specific position. They can also be searched for within the list. Two concrete classes implement List. The first is '''{{Javadoc:SE|package=java.util|java/util|ArrayList}}''', which implements the list as an array. Whenever functions specific to a list are required, the class moves the elements around within the array in order to do it. The other implementation is '''{{Javadoc:SE|package=java.util|java/util|LinkedList}}'''. This class stores the elements in nodes that each have a pointer to the previous and next nodes in the list. The list can be traversed by following the pointers, and elements can be added or removed simply by changing the pointers around to place the node in its proper place.&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/util/List.html |title=List (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06 |accessdate=2013-08-16}}&lt;/ref&gt;

===Stack interfaces===
Stacks are implemented using '''{{Javadoc:SE|package=java.util|java/util|Stack}}'''. The stack offers to put new object on the stack (method push()) and to get objects from the stack (method pop()). A stack returns the object according to last-in-first-out (LIFO), e.g. the object which was placed latest on the stack is returned first. Java provides a standard implementation of a stack in java.util.Stack. The Stack class represents a last-in-first-out (LIFO) stack of objects. It extends class Vector with five operations that allow a vector to be treated as a stack. The usual push and pop operations are provided, as well as a method to peek at the top item on the stack, a method to test for whether the stack is empty, and a method to search the stack for an item and discover how far it is from the top.  When a stack is first created, it contains no items.....

===Queue interfaces===
The '''{{Javadoc:SE|package=java.util|java/util|Queue}}''' interface defines the queue data structure, which stores elements in the order in which they are inserted. New additions go to the end of the line, and elements are removed from the front. It creates a first-in first-out system. This interface is implemented by '''{{Javadoc:SE|package=java.util|java/util|LinkedList}}''', '''{{Javadoc:SE|package=java.util|java/util|ArrayDeque}}''', and '''{{Javadoc:SE|package=java.util|java/util|PriorityQueue}}'''. LinkedList, of course, also implements the List interface and can also be used as one. But it also has the Queue methods. ArrayDeque implements the queue as an array. Both LinkedList and ArrayDeque also implement the '''{{Javadoc:SE|package=java.util|java/util|Deque}}''' interface, giving it more flexibility.&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/util/Queue.html |title=Queue (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06 |accessdate=2013-08-16}}&lt;/ref&gt;

'''{{Javadoc:SE|package=java.util|java/util|Queue}}''' can be used more flexibly with its subinterface, '''{{Javadoc:SE|package=java.util.concurrent|java/util/concurrent|BlockingQueue}}'''. The BlockingQueue interface works like a regular queue, but additions to and removals from the queue are blocking. If remove is called on an empty queue, it can be set to wait either a specified time or indefinitely for an item to appear in the queue. Similarly, adding an item is subject to an optional capacity restriction on the queue, and the method can wait for space to become available in the queue before returning.&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/BlockingQueue.html |title=BlockingQueue (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06 |accessdate=2013-08-16}}&lt;/ref&gt;

'''{{Javadoc:SE|package=java.util|java/util|PriorityQueue}}''' implements '''{{Javadoc:SE|package=java.util|java/util|Queue}}''', but also alters it. Instead of elements being ordered by the order in which they are inserted, they are ordered by priority. The method used to determine priority is either the compareTo() method in the elements or a method given in the constructor. The class creates this by using a heap to keep the items sorted.&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/util/PriorityQueue.html |title=PriorityQueue (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06 |accessdate=2013-08-16}}&lt;/ref&gt;

===Double-ended queue (deque) interfaces===
The '''{{Javadoc:SE|package=java.util|java/util|Queue}}''' interface is expanded by the '''{{Javadoc:SE|package=java.util|java/util|Deque}}''' subinterface. Deque creates a double-ended queue. While a regular queue only allows insertions at the rear and removals at the front, the deque allows insertions or removals to take place both at the front and the back. A deque is like a queue that can be used forwards or backwards, or both at once. Additionally, both a forwards and a backwards iterator can be generated. The Deque interface is implemented by '''{{Javadoc:SE|package=java.util|java/util|ArrayDeque}}''' and '''{{Javadoc:SE|package=java.util|java/util|LinkedList}}'''.&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/util/Deque.html |title=Deque (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06 |accessdate=2013-08-16}}&lt;/ref&gt;

The '''{{Javadoc:SE|package=java.util.concurrent|java/util/concurrent|BlockingDeque}}''' interface works similarly to '''{{Javadoc:SE|package=java.util.concurrent|java/util/concurrent|BlockingQueue}}'''. The same methods for insertion and removal with time limits for waiting for the insertion or removal to become possible are provided. However, the interface also provides the flexibility of a deque. Insertions and removals can take place at both ends. The blocking function is combined with the deque function.&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/BlockingDeque.html |title=BlockingDeque (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06 |accessdate=2013-08-16}}&lt;/ref&gt;

===Set interfaces===
Java's '''{{Javadoc:SE|package=java.util|java/util|Set}}''' interface defines the set. A set can't have any duplicate elements in it. Additionally, the set has no set order. As such, elements can't be found by index. Set is implemented by '''{{Javadoc:SE|package=java.util|java/util|HashSet}}''', '''{{Javadoc:SE|package=java.util|java/util|LinkedHashSet}}''', and '''{{Javadoc:SE|package=java.util|java/util|TreeSet}}'''. HashSet uses a hash table. More specifically, it uses a '''{{Javadoc:SE|package=java.util|java/util|HashMap}}''' to store the hashes and elements and to prevent duplicates. '''{{Javadoc:SE|package=java.util|java/util|LinkedHashSet}}''' extends this by creating a doubly linked list that links all of the elements by their insertion order. This ensures that the iteration order over the set is predictable. '''{{Javadoc:SE|package=java.util|java/util|TreeSet}}''' uses a [[red-black tree]] implemented by a '''{{Javadoc:SE|package=java.util|java/util|TreeMap}}'''. The red-black tree makes sure that there are no duplicates. Additionally, it allows TreeSet to implement '''{{Javadoc:SE|package=java.util|java/util|SortedSet}}'''.&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/util/Set.html |title=Set (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06 |accessdate=2013-08-16}}&lt;/ref&gt;

The '''{{Javadoc:SE|package=java.util|java/util|Set}}''' interface is extended by the '''{{Javadoc:SE|package=java.util|java/util|SortedSet}}''' interface. Unlike a regular set, the elements in a sorted set are sorted, either by the element's compareTo() method, or a method provided to the constructor of the sorted set. The first and last elements of the sorted set can be retrieved, and subsets can be created via minimum and maximum values, as well as beginning or ending at the beginning or ending of the sorted set. The SortedSet interface is implemented by '''{{Javadoc:SE|package=java.util|java/util|TreeSet}}'''&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/util/SortedSet.html |title=SortedSet (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06 |accessdate=2013-08-16}}&lt;/ref&gt;

'''{{Javadoc:SE|package=java.util|java/util|SortedSet}}''' is extended further via the '''{{Javadoc:SE|package=java.util|java/util|NavigableSet}}''' interface. It's similar to SortedSet, but there are a few additional methods. The floor(), ceiling(), lower(), and higher() methods find an element in the set that's close to the parameter. Additionally, a descending iterator over the items in the set is provided. As with SortedSet, '''{{Javadoc:SE|package=java.util|java/util|TreeSet}}''' implements NavigableSet.&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/util/NavigableSet.html |title=NavigableSet (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06}}&lt;/ref&gt;

===Map interfaces===
Maps are defined by the '''{{Javadoc:SE|package=java.util|java/util|Map}}''' interface in Java. Maps are simple data structures that associate a key with a value. The element is the value. This lets the map be very flexible. If the key is the hash code of the element, the map is essentially a set. If it's just an increasing number, it becomes a list. Maps are implemented by '''{{Javadoc:SE|package=java.util|java/util|HashMap}}''', '''{{Javadoc:SE|package=java.util|java/util|LinkedHashMap}}''', and '''{{Javadoc:SE|package=java.util|java/util|TreeMap}}'''. HashMap uses a hash table. The hashes of the keys are used to find the values in various buckets. LinkedHashMap extends this by creating a doubly linked list between the elements. This allows the elements to be accessed in the order in which they were inserted into the map. TreeMap, in contrast to HashMap and LinkedHashMap, uses a red-black tree. The keys are used as the values for the nodes in the tree, and the nodes point to the values in the map.&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/util/Map.html |title=Map (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06 |accessdate=2013-08-16}}&lt;/ref&gt;

The '''{{Javadoc:SE|package=java.util|java/util|Map}}''' interface is extended by its subinterface, '''{{Javadoc:SE|package=java.util|java/util|SortedMap}}'''. This interface defines a map that's sorted by the keys provided. Using, once again, the compareTo() method or a method provided in the constructor to the sorted map, the key-value pairs are sorted by the keys. The first and last keys in the map can be called. Additionally, submaps can be created from minimum and maximum keys. SortedMap is implemented by '''{{Javadoc:SE|package=java.util|java/util|TreeMap}}'''.&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/util/SortedMap.html |title=SortedMap (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06 |accessdate=2013-08-16}}&lt;/ref&gt;

The '''{{Javadoc:SE|package=java.util|java/util|NavigableMap}}''' interface extends '''{{Javadoc:SE|package=java.util|java/util|SortedMap}}''' in various ways. Methods can be called that find the key or map entry that's closest to the given key in either direction. The map can also be reversed, and an iterator in reverse order can be generated from it. It's implemented by '''{{Javadoc:SE|package=java.util|java/util|TreeMap}}'''.&lt;ref&gt;{{cite web|url=http://docs.oracle.com/javase/7/docs/api/java/util/NavigableMap.html |title=NavigableMap (Java Platform SE 7 ) |publisher=Docs.oracle.com |date=2013-06-06 |accessdate=2013-08-16}}&lt;/ref&gt;

==Extensions to the Java collections framework==
Java collections framework is extended by the [[Apache Commons]] Collections library, which adds collection types such as a bag and bidirectional map, as well utilities for creating unions and intersections.&lt;ref&gt;{{cite web|url=http://commons.apache.org/proper/commons-collections/ |title=Collections - Home |publisher=Commons.apache.org |date=2013-07-04 |accessdate=2013-08-16}}&lt;/ref&gt;

Google has released its own collections libraries as part of the [[Google Guava|guava libraries]].

==See also==
* [[Container (data structure)]]
* [[Standard Template Library]]
* [[Java concurrency]]

==References==
{{Reflist|2}}

==External links==
{{wikibooks|Java Programming|Collections}}
* [http://javalessons.com/cgi-bin/fun/java-tutorials-main.cgi?sub=adv&amp;ses=ao789 Collections Lessons]
* [http://www.collectionspy.com CollectionSpy] &amp;mdash; A profiler for Java's Collections Framework.
* [http://www.onjava.com/pub/a/onjava/excerpt/javaian5_chap04/ Generic Types]
* [http://tutorials.jenkov.com/java-collections/index.html Java 6 Collection Tutorial] &amp;mdash; By Jakob Jenkov, Kadafi Kamphulusa
* [http://www.onjava.com/pub/a/onjava/excerpt/javagenerics_chap05/ Java Generics and Collections]
* [http://www-128.ibm.com/developerworks/java/library/j-tiger07195/ Taming Tiger: The Collections Framework]
* [http://docs.oracle.com/javase/7/docs/technotes/guides/collections/index.html 'The Collections Framework'] (Oracle Java SE 7 documentation)
* [http://docs.oracle.com/javase/tutorial/collections/ 'The Java Tutorials - Collections' by Josh Bloch]
* [http://www.janeve.me/articles/which-java-collection-to-use 'Which Java Collection to use?'] &amp;mdash; by Janeve George

[[Category:Java programming language]]
[[Category:Data structures]]</text>
      <sha1>89vqj9bfnhezj9wj6l83or0cctz7t44</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Data descriptor</title>
    <ns>0</ns>
    <id>38237049</id>
    <revision>
      <id>604141390</id>
      <parentid>567757423</parentid>
      <timestamp>2014-04-14T10:56:22Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor/>
      <comment>fixed [[Help:CS1 errors#bad_date|CS1 errors: dates]] to meet [[MOS:DATEFORMAT]] (also [[WP:AWB/GF|General fixes]]) using [[Project:AWB|AWB]] (10069)</comment>
      <text xml:space="preserve" bytes="4887">In [[computing]], a '''data descriptor''' is a structure containing information that describes data.

Data descriptors may be used in [[compiler]]s,&lt;ref&gt;{{cite journal|last=Holt|first=Richard C|title=Data descriptors: a compile-time model of data and addressing|journal=ACM Transactions on Programming Languages and Systems (TOPLAS)|date=July 1987|volume=9|issue=3|pages=367–389|doi=10.1145/24039.24051}}&lt;/ref&gt;  as a software structure at run time in languages like [[Ada (programming language)|Ada]]&lt;ref&gt;{{cite web|last=Schonberg|first=Ed|title=Ada Compared with C++|url=http://archive.adaic.com/intro/ada-vs-c/ada-vs-c.html|work=The Advantages of Ada 95|accessdate=January 15, 2013}}&lt;/ref&gt;  or [[PL/I]], or as a hardware structure in some computers such as [[Burroughs large systems]].

Data descriptors are typically used at run-time to pass argument information to called [[subroutines]].  HP [[OpenVMS]]&lt;ref&gt;{{cite web|last=Hewlett-Packard|title=Chapter 7 OpenVMS Argument Descriptors|url=http://h71000.www7.hp.com/doc/82final/5973/5973pro_019.html#arg_desc_formats_chap|work=HP OpenVMS Systems Documentation|accessdate=January 15, 2013}}&lt;/ref&gt; and [[Multics]]&lt;ref&gt;{{cite book|last=Honeywell, Inc.|title=Multics Programmers' Manual &amp;ndash; Subsystem Writers' Guide|year=1979|pages=2&amp;ndash;13-2&amp;ndash;18|url=http://bitsavers.informatik.uni-stuttgart.de/pdf/honeywell/multics/AK92-2_MPM_SubsysWrtGud_Mar79.pdf}}&lt;/ref&gt;  have system-wide language-independent standards for argument descriptors.  Descriptors are also used to hold information about data that is only fully known at run-time, such as a [[dynamic memory allocation|dynamically allocated array]].

Unlike a ''[[dope vector]]'', a data descriptor does not contain address information.

==Examples==
The following descriptor is used by IBM ''Enterprise PL/I'' to describe a [[String (computer science)|character string]]:&lt;ref&gt;{{cite book|last=IBM Corporation|title=Enterprise PL/I for z/OSProgramming Guide|year=2006|pages=385|url=http://pic.dhe.ibm.com/infocenter/pdthelp/v1r1/topic/com.ibm.entpli.doc_3.6/ibm3pg50.pdf}}&lt;/ref&gt;
&lt;pre&gt;
      +--------+--------+--------+--------+
      |  desc  | string |        | flags  |
      |  type  |  type  | (res)  |        |
      +--------+--------+--------+--------+
      |       maximum string length       |
      |                                   |
      +--------+--------+--------+--------+
 byte         0        1        2        3
&lt;/pre&gt;
* 'desc type' is 2 to indicate that this is an element descriptor rather than an array or structure descriptor.
* 'string type' indicates that this is a character or a bit string, with varying or nonvarying length.  2 indicates a nonvarying (fixed-length) character string.
*  '(res)' is a reserved byte not used for character strings.
* 'flags' indicate the encoding of the string, [[EBCDIC]] or [[ASCII]], and the encoding of the length of varying strings.
* 'maximum string length' is the actual length of the string for nonvarying strings, or the maximum length for varying strings.

Here is the source of an [[Array data structure|array]] descriptor from Multics.&lt;ref&gt;{{cite web|last=MIT/Honeywell|title=array.incl.pl1|url=http://web.mit.edu/multics-history/source/Multics/ldd/include/array.incl.pl1|work=Multics/ldd/include|accessdate=January 20, 2012}}&lt;/ref&gt;  The definitions include a structure for the base array information and a structure for each dimension.  (Multics ran on systems with 36-bit words).

&lt;source lang=PLI&gt;
dcl	1 array			        based    aligned,
	2 node_type		        bit(9)   unaligned,
	2 reserved		        bit(34)  unaligned,
	2 number_of_dimensions	        fixed(7) unaligned,
	2 own_number_of_dimensions	fixed(7) unaligned,
	2 element_boundary		fixed(3) unaligned,
	2 size_units		        fixed(3) unaligned,
	2 offset_units		        fixed(3) unaligned,
	2 interleaved		        bit(1)   unaligned,
	2 c_element_size		fixed(24),
	2 c_element_size_bits	        fixed(24),
	2 c_virtual_origin		fixed(24),
	2 element_size		        ptr unaligned,
	2 element_size_bits		ptr unaligned,
	2 virtual_origin		ptr unaligned,
	2 symtab_virtual_origin	        ptr unaligned,
	2 symtab_element_size	        ptr unaligned,
	2 bounds			ptr unaligned,
	2 element_descriptor	        ptr unaligned;

dcl	1 bound			        based aligned,
	2 node_type		        bit(9),
	2 c_lower			fixed(24),
	2 c_upper			fixed(24),
	2 c_multiplier		        fixed(24),
	2 c_desc_multiplier		fixed(24),
	2 lower			        ptr unaligned,
	2 upper			        ptr unaligned,
	2 multiplier		        ptr unaligned,
	2 desc_multiplier		ptr unaligned,
	2 symtab_lower		        ptr unaligned,
	2 symtab_upper		        ptr unaligned,
	2 symtab_multiplier		ptr unaligned,
	2 next			        ptr unaligned;
&lt;/source&gt;

==See also==
*[[Burroughs large systems descriptors]]

==References==
{{Reflist}}

[[Category:Data structures]]


{{compu-prog-stub}}</text>
      <sha1>tvzf4krdh87woveqdud7lq6buvu73eu</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Device tree</title>
    <ns>0</ns>
    <id>40312837</id>
    <revision>
      <id>612688873</id>
      <parentid>612688477</parentid>
      <timestamp>2014-06-12T21:41:42Z</timestamp>
      <contributor>
        <username>Dsimic</username>
        <id>6479630</id>
      </contributor>
      <comment>/* top */ Layout improvement</comment>
      <text xml:space="preserve" bytes="3032">{{Incomplete|date=August 2013}}

The '''device tree''' is a [[data structure]] for describing hardware, which originated from [[Open Firmware]].  The data structure can hold any kind of data as internally it is a [[Tree structure|tree]] of named nodes and [[Property (programming)|properties]].  Nodes contain properties and child nodes, while properties are [[Attribute–value pair|name–value pairs]].

As an example, [[Das U-Boot]] and [[kexec]] include support for device trees.

==Usage in Linux==
Given the correct device tree, the same compiled kernel can support different hardware configurations within a wider architecture family. The [[Linux kernel]] can read device tree information in the [[ARM architecture|ARM]], [[x86]], [[MicroBlaze]], [[PowerPC]], and [[SPARC]] architectures. For ARM, use of device trees has become mandatory for all new [[System on chip|SoC]]s.&lt;ref&gt;{{cite web|title=ARM soc Linux support checklist|url=http://www.elinux.org/images/a/ad/Arm-soc-checklist.pdf}}&lt;/ref&gt; This can be seen as a remedy to the vast number of forks (of Linux and Das U-boot) that has historically been created to support (marginally) different ARM boards. Allegedly, the purpose is to move a significant part of the hardware description out of the kernel binary, and into the compiled device tree blob, which is handed to the kernel by the bootloader, replacing a range of board specific [[C (programming language)|C]] [[source code|source files]] and compile time options in the kernel.&lt;ref&gt;{{cite web|title=arm soc linux support checklist|url=http://www.elinux.org/images/a/ad/Arm-soc-checklist.pdf}}&lt;/ref&gt;

It has been customary of ARM-based [[Linux distribution]]s to include a bootloader, that necessarily was customised for specific boards, for example [[Raspberry Pi]] or Hackberry A10. This has created problems for the creators of Linux distributions as some part of the operating system must be compiled specifically for every board variant, or updated to support new boards. However, some modern SoCs (for example, Freescale i.MX6) have a vendor provided bootloader with device tree on a separate chip from the operating system.&lt;ref&gt;{{cite web|title=u-boot update for Boundary Devices' boards|url=http://boundarydevices.com/u-boot-2013-10-release/}}&lt;/ref&gt;

A proprietary configuration file format used for similar purposes, the FEX file format,&lt;ref&gt;{{cite web|url=http://linux-sunxi.org/Fex_Guide |title=Fex Guide |publisher=linux-sunxi.org |date=2014-05-30 |accessdate=2014-06-12}}&lt;/ref&gt; is a [[de-facto standard]] among [[Allwinner]] SoCs.

==See also==
{{Portal|Computer science}}
* [[Differentiated System Description Table]]{{snd}} part of ACPI firmware that describes power events to an OS.

==References==
{{Reflist}}

== External links ==
* http://www.devicetree.org/
* http://elinux.org/Device_Trees
* http://omappedia.org/wiki/Device_Tree
* [http://www.power.org/resources/downloads/Power_ePAPR_APPROVED_v1.0.pdf Specification]

{{Data structures}}

[[Category:Data structures]]


{{Comp-sci-stub}}</text>
      <sha1>g4r05qh904r49bevlnwlgzu7mejs9hh</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Doubly linked face list</title>
    <ns>0</ns>
    <id>12745973</id>
    <revision>
      <id>573947564</id>
      <parentid>573947070</parentid>
      <timestamp>2013-09-21T19:03:49Z</timestamp>
      <contributor>
        <username>Senator2029</username>
        <id>13054498</id>
      </contributor>
      <minor/>
      <comment>Added {{[[Template:expert-subject|expert-subject]]}} and {{[[Template:unreferenced|unreferenced]]}} tags to article ([[WP:TW|TW]])</comment>
      <text xml:space="preserve" bytes="600">{{expert-subject|1=Mathematics|date=September 2013}}
{{unreferenced|date=September 2013}}
In [[applied mathematics]], a '''doubly linked face list''' ('''DLFL''') is an efficient [[data structure]] for storing [[Topological manifold|2-manifold]] mesh data.  The structure stores [[linked list]]s for a 3D mesh's faces, edges, vertices, and corners. The structure guarantees the preservation of the [[manifold]] property.

This data structure is used by the 3D modeling software [[TopMod]].

[[Category:3D imaging]]
[[Category:Applied mathematics]]
[[Category:Data structures]]


{{Applied-math-stub}}</text>
      <sha1>jen73akwzmdtyoqm6r356faeg5kt1v9</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Process Environment Block</title>
    <ns>0</ns>
    <id>28137693</id>
    <revision>
      <id>615913677</id>
      <parentid>615913605</parentid>
      <timestamp>2014-07-07T06:37:55Z</timestamp>
      <contributor>
        <username>Codename Lisa</username>
        <id>16847332</id>
      </contributor>
      <comment>removed [[Category:Windows architecture]]; added [[Category:Windows NT architecture]] using [[WP:HC|HotCat]]</comment>
      <text xml:space="preserve" bytes="8169">In [[computing]] the '''Process Environment Block''' (abbreviated '''PEB''') is a data structure in the [[Windows NT]] operating system family.  It is an [[opaque data type|opaque data structure]] that is used by the operating system internally, most of whose fields are not intended for use by anything other than the operating system.&lt;ref name=Nagar1997 /&gt;  Microsoft notes, in its [[MSDN Library]] documentation — which documents only a few of the fields — that the structure &quot;may be altered in future versions of Windows&quot;.&lt;ref name=MSDN1 /&gt;  The PEB contains data structures that apply across a whole [[process (computing)|process]], including global context, startup parameters, data structures for the program image loader, the program image base address, and synchronization objects used to provide [[mutual exclusion]] for process-wide data structures.&lt;ref name=Nagar1997 /&gt;

The PEB is closely associated with the [[kernel mode]] &lt;source lang=cpp enclose=none&gt;EPROCESS&lt;/source&gt; data structure, as well as with per-process data structures managed within the address space of the [[Client/Server Runtime Subsystem|Client-Server Runtime Sub-System]] process.  However, (like the CSRSS data structures) the PEB is not a kernel mode data structure itself.  It resides in the application mode address space of the process that it relates to.  This is because it is designed to be used by the application-mode code in the operating system libraries, such as [[ntdll.dll|NTDLL]], that executes outside of kernel mode, such as the code for the program image loader and the heap manager.&lt;ref name=Internals5 /&gt;

In [[WinDbg]], the command that dumps the contents of a PEB is the &lt;tt&gt;!peb&lt;/tt&gt; command, which is passed the address of the PEB within a process' application address space. That information, in turn, is obtained by the &lt;tt&gt;!process&lt;/tt&gt; command, which displays the information from the &lt;source lang=cpp enclose=none&gt;EPROCESS&lt;/source&gt; data structure, one of whose fields is the address of the PEB.&lt;ref name=Internals5 /&gt;

{| class=&quot;wikitable sortable&quot;
|+ Fields of the PEB that are documented by Microsoft&lt;ref name=MSDN1 /&gt;
|-
! Field !! meaning !! notes
|-
| &lt;code&gt;BeingDebugged&lt;/code&gt; || Whether the process is being debugged || Microsoft recommends not using this field but using the official Win32 &lt;source lang=cpp enclose=none&gt;CheckRemoteDebuggerPresent()&lt;/source&gt; library function instead.&lt;ref name=MSDN1 /&gt;
|-
| Ldr || A pointer to a &lt;source lang=cpp enclose=none&gt;PEB_LDR_DATA&lt;/source&gt; structure providing information about loaded modules || Contains the base address of [[kernel32]] and [[ntdll]].
|-
| ProcessParameters || A pointer to a &lt;source lang=cpp enclose=none&gt;RTL_USER_PROCESS_PARAMETERS&lt;/source&gt; structure providing information about loaded modules || The &lt;source lang=cpp enclose=none&gt;RTL_USER_PROCESS_PARAMETERS&lt;/source&gt; structure is also mostly opaque and not guaranteed to be consistent across multiple versions of Windows.&lt;ref name=MSDN3 /&gt;
|-
| PostProcessInitRoutine || A pointer to a callback function called after DLL initialization but before the main executable code is invoked || This callback function is used on [[Windows 2000]], but is not guaranteed to be used on later versions of Windows NT.&lt;ref name=MSDN1 /&gt;
|-
| SessionId || The session ID of the Terminal Services session that the process is part of || The &lt;source lang=cpp enclose=none&gt;NtCreateUserProcess()&lt;/source&gt; system call initializes this by calling the kernel's internal &lt;source lang=cpp enclose=none&gt;MmGetSessionId()&lt;/source&gt; function.&lt;ref name=Internals5 /&gt;
|}

The contents of the PEB are initialized by the &lt;source lang=cpp enclose=none&gt;NtCreateUserProcess()&lt;/source&gt; system call, the [[Native API]] function that implements part of, and underpins, the Win32 &lt;source lang=cpp enclose=none&gt;CreateProcess()&lt;/source&gt;, &lt;source lang=cpp enclose=none&gt;CreateProcessAsUser()&lt;/source&gt;, &lt;source lang=cpp enclose=none&gt;CreateProcessWithTokenW()&lt;/source&gt;, and &lt;source lang=cpp enclose=none&gt;CreateProcessWithLogonW()&lt;/source&gt; library functions that are in [[Microsoft Windows library files|the kernel32.dll and advapi32.dll libraries]] as well as underpinning the &lt;code&gt;[[fork (computing)|fork()]]&lt;/code&gt; function in the [[Microsoft POSIX subsystem|Windows NT POSIX]] library, posix.dll.&lt;ref name=Internals5 /&gt;

For Windows NT POSIX processes, the contents of a new process' PEB are initialized by &lt;source lang=cpp enclose=none&gt;NtCreateUserProcess()&lt;/source&gt; as simply a direct copy of the parent process' PEB, in line with how the &lt;source lang=cpp enclose=none&gt;fork()&lt;/source&gt; function operates.  For Win32 processes, the initial contents of a new process' PEB are mainly taken from global variables maintained within the kernel.  However, several fields may instead be taken from information provided within the process' image file, in particular information provided in the &lt;source lang=cpp enclose=none&gt;IMAGE_OPTIONAL_HEADER32&lt;/source&gt; data structure within the [[Portable Executable|PE]] file format (PE+ or PE32+ in 64 bit executable images).&lt;ref name=Internals5 /&gt;

{| class=&quot;wikitable sortable&quot;
|+ Fields from a PEB that are initialized from kernel global variables&lt;ref name=Internals5 /&gt;
|-
! Field !! is initialized from !! overridable by PE information?
|-
| &lt;code&gt;NumberOfProcessors&lt;/code&gt; || &lt;code&gt;KeNumberOfProcessors&lt;/code&gt; || {{no}}
|-
| &lt;code&gt;NtGlobalFlag&lt;/code&gt; || &lt;code&gt;NtGlobalFlag&lt;/code&gt; || {{no}}
|-
| &lt;code&gt;CriticalSectionTimeout&lt;/code&gt; || &lt;code&gt;MmCriticalSectionTimeout&lt;/code&gt; || {{no}}
|-
| &lt;code&gt;HeapSegmentReserve&lt;/code&gt; || &lt;code&gt;MmHeapSegmentReserve&lt;/code&gt; || {{no}}
|-
| &lt;code&gt;HeapSegmentCommit&lt;/code&gt; || &lt;code&gt;MmHeapSegmentCommit&lt;/code&gt; || {{no}}
|-
| &lt;code&gt;HeapDeCommitTotalFreeThreshold&lt;/code&gt; || &lt;code&gt;MmHeapDeCommitTotalFreeThreshold&lt;/code&gt; || {{no}}
|-
| &lt;code&gt;HeapDeCommitFreeBlockThreshold&lt;/code&gt; || &lt;code&gt;MmHeapDeCommitFreeBlockThreshold&lt;/code&gt; || {{no}}
|-
| &lt;code&gt;MinimumStackCommit&lt;/code&gt; || &lt;code&gt;MmMinimumStackCommitInBytes&lt;/code&gt; || {{no}}
|-
| &lt;code&gt;ImageProcessAffinityMask&lt;/code&gt; || &lt;code&gt;KeActiveProcessors&lt;/code&gt; || {{Yes|&lt;source lang=cpp enclose=none&gt;ImageLoadConfigDirectory.ProcessAffinityMask&lt;/source&gt;}}
|-
| &lt;code&gt;OSMajorVersion&lt;/code&gt; || &lt;code&gt;NtMajorVersion&lt;/code&gt; || {{Yes|&lt;source lang=cpp enclose=none&gt;OptionalHeader.Win32VersionValue &amp; 0xFF&lt;/source&gt;}}
|-
| &lt;code&gt;OSMinorVersion&lt;/code&gt; || &lt;code&gt;NtMinorVersion&lt;/code&gt; || {{Yes|&lt;source lang=cpp enclose=none&gt;(OptionalHeader.Win32VersionValue &gt;&gt; 8) &amp; 0xFF&lt;/source&gt;}}
|-
| &lt;code&gt;OSBuildNumber&lt;/code&gt; || &lt;source lang=cpp enclose=none&gt;NtBuildNumber &amp; 0x3FFF&lt;/source&gt; combined with &lt;code&gt;CmNtCSDVersion&lt;/code&gt; || {{Yes|&lt;source lang=cpp enclose=none&gt;(OptionalHeader.Win32VersionValue &gt;&gt; 16) &amp; 0x3FFF&lt;/source&gt;  combined with &lt;source lang=cpp enclose=none&gt;ImageLoadConfigDirectory.CmNtCSDVersion&lt;/source&gt;}}
|-
| &lt;code&gt;OSPlatformId&lt;/code&gt; || &lt;source lang=cpp enclose=none&gt;VER_PLATFORM_WIN32_NT&lt;/source&gt; || {{Yes|&lt;source lang=cpp enclose=none&gt;(OptionalHeader.Win32VersionValue &gt;&gt; 30) ^ 0x2&lt;/source&gt;}}
|}

== References ==
&lt;references&gt;
&lt;ref name=Nagar1997&gt;{{cite book|title=Windows NT file system internals: a developer's guide|series=O'Reilly Series|author=Rajeev Nagar|publisher=O'Reilly|year=1997|isbn=9781565922495|pages=129}}&lt;/ref&gt;
&lt;ref name=MSDN1&gt;{{cite web|work=[[MSDN Library]]|publisher=[[Microsoft]]|title=Process and Thread structures: PEB Structure|url=http://msdn.microsoft.com/library/aa813706(VS.85).aspx|date=2010-07-15|accessdate=2010-07-15}}&lt;/ref&gt;
&lt;ref name=MSDN3&gt;{{cite web|work=[[MSDN Library]]|publisher=[[Microsoft]]|title=Process and Thread structures: RTL_USER_PROCESS_PARAMETERS Structure|url=http://msdn.microsoft.com/library/aa813741(VS.85).aspx|date=2010-07-15|accessdate=2010-07-15}}&lt;/ref&gt;
&lt;ref name=Internals5&gt;{{cite book|title=Windows internals|series=Microsoft Press Series|author=[[Mark Russinovich|Mark E. Russinovich]], [[David A. Solomon]], and Alex Ionescu|edition=5th|publisher=Microsoft Press|year=2009|isbn=9780735625303|pages=335–336,341–342,348,357–358}}&lt;/ref&gt;
&lt;/references&gt;

{{data types}}

[[Category:Data types]]
[[Category:Object-oriented programming]]
[[Category:Windows NT architecture]]
[[Category:Data structures]]</text>
      <sha1>7t125yibolx8lyc7435iap8dsakdxqm</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>I/O request packet</title>
    <ns>0</ns>
    <id>10998227</id>
    <revision>
      <id>588209195</id>
      <parentid>535430853</parentid>
      <timestamp>2013-12-29T13:30:32Z</timestamp>
      <contributor>
        <ip>223.18.59.19</ip>
      </contributor>
      <text xml:space="preserve" bytes="2057">'''I/O request packets (IRPs)''' are kernel mode structures that are used by [[Windows Driver Model]] (WDM) and [[Windows NT]] [[device driver]]s to communicate with each other and with the [[operating system]]. They are data structures that describe I/O requests, and can be equally well thought of as &quot;I/O request descriptors&quot; or similar. Rather than passing a large number of small arguments (such as buffer address, buffer size, I/O function type, etc.) to a driver, all of these parameters are passed via a single pointer to this persistent data structure. The IRP with all of its parameters can be put on a queue if the I/O request cannot be performed immediately. I/O completion is reported back to the I/O manager by passing its address to a routine for that purpose, IoCompleteRequest. The IRP may be repurposed as a special kernel APC object if such is required to report completion of the I/O to the requesting thread. 

IRPs are typically created by the I/O Manager in response to I/O requests from user mode. However, IRPs are sometimes created by the plug-and-play manager, power manager, and other system components, and can also be created by drivers and then passed to other drivers. 

The I/O request packet mechanism is also used by [[Digital Equipment Corporation]]'s [[OpenVMS|VMS]] operating system, and was used by Digital's [[RSX-11]] family of operating systems before that.

==See also==
* [[Architecture of Windows NT]]

==References and external links==
* [http://www.microsoft.com/technet/archive/winntas/training/ntarchitectoview/ntarc_6.mspx?mfr=true Whitepaper on Windows I/O model]
* [http://technet.microsoft.com/en-us/library/cc776371%28WS.10%29.aspx How Device Drivers work]
* [http://msdn.microsoft.com/en-us/library/windows/hardware/ff550694(v=vs.85).aspx IRP (Windows Drivers)]

{{Windows Components}}

{{DEFAULTSORT:I O request packet}}
[[Category:Windows NT architecture]]
[[Category:Device drivers]]
[[Category:Data structures]]
[[Category:Windows NT kernel]]

{{Windows-stub}}

[[de:IRP]]
[[ru:I/O request packet]]</text>
      <sha1>0vevcs1oom5hqk0vqtyx4lvki419ekm</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>System Service Dispatch Table</title>
    <ns>0</ns>
    <id>27341950</id>
    <revision>
      <id>614126211</id>
      <parentid>588209302</parentid>
      <timestamp>2014-06-23T18:37:32Z</timestamp>
      <contributor>
        <username>Codename Lisa</username>
        <id>16847332</id>
      </contributor>
      <comment>removed [[Category:Microsoft Windows]]; added [[Category:Windows technology]] using [[WP:HC|HotCat]]</comment>
      <text xml:space="preserve" bytes="1424">The '''System Service Descriptor Table''' ('''SSDT''') is an internal [[dispatch table]] within [[Microsoft Windows]].

[[Hooking]] SSDT calls is often used as a technique in both Windows [[rootkit]]s and [[antivirus software]].&lt;ref&gt;{{Cite web|url=http://www.symantec.com/connect/articles/windows-rootkits-2005-part-one|title= Windows rootkits of 2005, part one|work=Symantec|year=2005}}&lt;/ref&gt;&lt;ref name=&quot;ZDNET2010&quot;&gt;{{Cite web|url=http://www.zdnet.co.uk/news/security-threats/2010/05/11/attack-defeats-most-antivirus-software-40088896/ |year=2010|title=Attack defeats 'most' antivirus software|work=ZD Net UK}}&lt;/ref&gt;

In 2010, many computer security products which relied on hooking SSDT calls were shown to be vulnerable to [[Exploit (computer security)|exploits]] using [[race condition]]s to attack the products' security checks.&lt;ref name=&quot;ZDNET2010&quot;/&gt;
==Structure of the SSDT==
&lt;source lang=&quot;cpp&quot;&gt;
typedef struct _KSERVICE_DESCRIPTOR_TABLE
{
    PULONG ServiceTableBase; 
    PULONG ServiceCounterTableBase; 
    ULONG NumberOfServices; 
    PUCHAR ParamTableBase; 
}KSERVICE_DESCRIPTOR_TABLE,*PKSERVICE_DESCRIPTOR_TABLE;
&lt;/source&gt;
The pointer to this structure is '''KeServiceDescriptorTable''', exported by [[ntoskrnl.exe]].
== References ==
{{Reflist}}

[[Category:Windows technology]]
[[Category:Computer security]]
[[Category:Rootkits|*]]
[[Category:Windows NT kernel]]
[[Category:Data structures]]

{{Windows-stub}}</text>
      <sha1>nqpj2hezz17z2i8255n697zfwgh8cek</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Processor Control Region</title>
    <ns>0</ns>
    <id>41949052</id>
    <revision>
      <id>626691715</id>
      <parentid>626691617</parentid>
      <timestamp>2014-09-22T23:12:23Z</timestamp>
      <contributor>
        <ip>71.74.134.87</ip>
      </contributor>
      <comment>grammar fixes</comment>
      <text xml:space="preserve" bytes="872">'''Processor Control Region (PCR)''' is a [[Microsoft Windows|Windows]] kernel mode [[data structure]] that contains information about the current [[Processor (computing)|processor]]. It can be accessed via the fs [[segment register]].

==Structure==
In [[C (programming language)|C]] and [[C++]], the '''PCR''' is known as '''KPCR'''. It contains information about the current processor.

==Processor Control Block==
The PCR contains a substructure called '''Processor Control Block (KPRCB)''', which contains information such as CPU step and a pointer to the thread object of the current thread.

==References==
*http://www.nirsoft.net/kernel_struct/vista/KPCR.html
*http://www.nirsoft.net/kernel_struct/vista/KPRCB.html
==See also==
*[[Process Environment Block]]
*[[Process control block]]

{{Windows-stub}}

[[Category:Windows NT kernel]]
[[Category:Data structures]]</text>
      <sha1>afvqdkgcwvu1jpvep7d0pg0fv7zvufl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Finger search</title>
    <ns>0</ns>
    <id>42439522</id>
    <revision>
      <id>616973046</id>
      <parentid>611543751</parentid>
      <timestamp>2014-07-14T22:34:41Z</timestamp>
      <contributor>
        <username>Gankro</username>
        <id>7745758</id>
      </contributor>
      <comment>/* Treaps */</comment>
      <text xml:space="preserve" bytes="6859">{{orphan|date=April 2014}}

In [[computer science]], a '''finger search''' on a [[data structure]] is an extension of any search operation that structure supports, where a reference (finger) to an element in the data structure is given along with the query. While the search time for an element is most frequently expressed as a function of the number of elements in a data structure, finger search times are a function of the distance between the element and the finger.

In a set of ''n'' elements, the distance ''d''(''x'',''y'') (or simply ''d'' when unambiguous) between two elements ''x'' and ''y'' is there difference in rank. If ''x'' and ''y'' are the ''i''th and ''j''th largest elements in the structure, then the difference in rank is |''i'' - ''j''|. If a normal search in some structure would normally take O(f(''n'')) time, then a finger search for ''x'' with finger ''y'' should ideally take O(f(''d'')) time. Remark that since ''d'' ≤ ''n'', it follows that in the worst case, finger search is only as bad as normal search. However, in practice these degenerate finger searches do more work than normal searches. For instance, if f(''n'') is log(''n''), and finger search does twice as many comparisons as normal search in the worst case, it follows that finger search is slower when ''d'' &gt; √''n''. Therefore, finger search should only be used when one can reasonably expect the target to actually be close to the finger.

==Implementations==

Some popular data structures support finger search with no additional changes to the actual structure. In structures where searching for an element ''x'' is accomplished by narrowing down an interval over which ''x'' can be found, finger search from ''y'' is typically accomplished by reversing the search process from ''y'' until the search interval is large enough to contain ''x'', at which point search proceeds as normal.

===Sorted Linked Lists===
In a [[linked list]], one normally searches linearly for an element by walking from one end to the other. If the linked list is sorted, and we have a reference to some node containing ''y'', then we may find ''x'' in O(''d'') time by starting our search from ''y''.

===Sorted Arrays===

In a [[sorted array]] ''A'', one normally searches for an element ''x'' in ''A'' with a [[binary search]]. Finger search is accomplished by performing a [[Binary search algorithm#One-sided search|one-sided search]] from ''A''[''j''] = ''y''. While binary search halves the search space after each comparison, one-sided search doubles the search space after each comparison. Specifically, on the ''k''th iteration of one-sided search (assuming ''x &gt; y''), the interval under consideration is ''A''[''j'', ''j''+2&lt;sup&gt;''k''-1&lt;/sup&gt;]. Expansion halts as soon as ''A''[''j'' + 2&lt;sup&gt;''k''-1&lt;/sup&gt;] ≥ ''x'', at which point this interval is binary searched for ''x''.

If one-sided search takes ''k'' iterations to find an interval that contains ''x'', then it follows that ''d'' &gt; 2&lt;sup&gt;''k''-2&lt;/sup&gt;. Binary searching this range will also take another ''k'' iterations. Therefore, finger search for ''x'' from ''y'' takes O(''k'') = O(log''d'') time.

===Skip Lists===

In a [[skip list]], one can finger search for ''x'' from a node containing the element ''y'' by simply continuing the search from this point. Note that if ''x &lt; y'', then search proceeds backwards, and if ''x &gt; y'', then search proceeds forwards. The backwards case is symmetric to normal search in a skip list, but the forward case is actually more complex. Normally, search in a skiplist is expected to be fast because the sentinel at the start of the list is as tall as the tallest node. However, our finger could be a node of height 1. Because of this, we may occasionally climb while trying to search; something which never occurs normally. However, even with this complication, we can achieve O(log''d'') expected search time.&lt;ref name=&quot;brodal&quot;&gt;{{cite web|title=Randomized Splay Trees: Theoretical and Experimental Results|url=http://www.cs.au.dk/~gerth/papers/finger05.pdf}}&lt;/ref&gt;

===Treaps===
[[File:Performing finger searches on treaps.svg|right|thumb|Example of finger search on treaps.]]

A [[treap]] is a randomized [[binary search tree]] (BST). Searching in a treap is the same as searching for an element in any other BST. Treaps however have the property that the expected path length between two elements of distance ''d'' is O(log''d''). Thus, to finger search from the node containing ''y'' for ''x'', one can climb the tree from ''y'' until an ancestor of ''x'' is found, at which point normal BST search proceeds as usual. While determining if a node is the ancestor of another is non-trivial, one may augment the tree to support queries of this form to give expected O(log''d'') finger search time.&lt;ref name=&quot;brodal&quot;/&gt;

==Generalizations==

If one can perform finger search in an iterative manner in O(f(''d'')) time, where each iteration takes O(1) time, then by providing ''c'' different fingers, one can perform finger search in O(''c'' min{''d''(''x'', ''y''&lt;sub&gt;1&lt;/sub&gt;), ..., ''d''(''x'', ''y''&lt;sub&gt;''c''&lt;/sub&gt;)}) time. This is accomplished by starting finger search for all ''c'' fingers, and iterating them forward one step each until the first one terminates.

Given any sequence ''A'' = [''a&lt;sub&gt;1&lt;/sub&gt;'', ..., ''a&lt;sub&gt;m&lt;/sub&gt;''] of ''m'' accesses, a structure is said to have the ''static finger property'' for a fixed finger ''f'', if the time to perform ''A'' is &lt;math&gt;O(\sum_{i=1}^{m}\log d(f,a_i))&lt;/math&gt;. [[Splay tree]]s have this property for any choice of ''f'' with no additional processing on sufficiently large sequences of accesses.&lt;ref&gt;{{cite web|title=John Iacono. Key independent optimality. Algorithmica, 42(1):3-10, 2005.|url=http://john2.poly.edu/papers/algo04/paper.pdf}}&lt;/ref&gt;

==Applications==

Finger search can be used to re-use work already done in previous searches. For instance, one way to iterate over the elements in a data structure is to simply finger search for them in order, where the finger for one query is the location of the result of the last. One may optimize their data structure by doing this internally if it is known that searches are frequently near the last search.

A [[finger search tree]] is a type of data structure that allows fingers to be specified such that all or some of their supported operations are faster when they access or modify a location near a finger. In contrast to the finger searches described in this article, these fingers are not provided at query time, but are separately specified so that all future operations use these fingers. One benefit of this is that the user needs not manipulate or store internal references to the structure, they may simply specify an element in the structure.

==References==
{{reflist}}

[[Category:Data structures]]
[[Category:Search algorithms]]</text>
      <sha1>826giuepz5mxusnbbv4oaz9w1pcx238</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Key-independent optimality</title>
    <ns>0</ns>
    <id>42673906</id>
    <revision>
      <id>610271670</id>
      <parentid>607366873</parentid>
      <timestamp>2014-05-26T23:05:52Z</timestamp>
      <contributor>
        <username>Edemaine</username>
        <id>1773</id>
      </contributor>
      <comment>correct first paragraph</comment>
      <text xml:space="preserve" bytes="1918">Key-independent optimality is a property of some [[binary search tree ]] data structures in [[computer science]]
proposed by John Iacono.&lt;ref&gt;{{cite web|title=John Iacono. Key independent optimality. Algorithmica, 42(1):3-10, 2005.|url=http://john2.poly.edu/papers/algo04/paper.pdf}}&lt;/ref&gt;
Suppose that [[Attribute–value pair|key-value pairs]] are stored in a data
structure, and that the keys have no relation to their paired values. 
A data structure has **key-independent optimality** if, when randomly assigning the keys, the expected performance of the data structure is within a constant factor of the optimal data structure.  Key-independent optimality is related to [[Optimal binary search tree|dynamic optimality]]. 

==Definitions==

There are many binary search tree algorithms that 
can look up a sequence of &lt;math&gt;m&lt;/math&gt;
keys &lt;math&gt;X = x_1, x_2, \cdots, x_m&lt;/math&gt;, where each &lt;math&gt;x_i&lt;/math&gt;
is a number between &lt;math&gt;1&lt;/math&gt; and &lt;math&gt;n&lt;/math&gt;.
For each sequence &lt;math&gt;X&lt;/math&gt;, let &lt;math&gt;OPT(X)&lt;/math&gt; 
be the fastest binary search tree algorithm that looks up the elements in &lt;math&gt;X&lt;/math&gt; in order. 
Let &lt;math&gt;b&lt;/math&gt; be one of the
&lt;math&gt;n!&lt;/math&gt; possible
permutation of the sequence &lt;math&gt;1, 2, \cdots, n&lt;/math&gt;, chosen at random,
where
&lt;math&gt;b(i)&lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;th entry of &lt;math&gt;b&lt;/math&gt;.
Let &lt;math&gt;b(X) = b(x_1), b(x_2), \cdots ,b(x_m)&lt;/math&gt;. 
Iacono defined, for a sequence &lt;math&gt;X&lt;/math&gt;, that &lt;math&gt;KIOPT(X) =
E[OPT(b(X))]&lt;/math&gt;. 

A data structure has key-independent optimality 
if it can lookup the elements in &lt;math&gt;X&lt;/math&gt; in time
&lt;math&gt;O(KIOPT(X))&lt;/math&gt;.

==Relationship with other bounds==

Key-independent optimality has been proved to be asymptotically equivalent to
the
[[Splay tree#Performance theorems|working set theorem]].
[[Splay tree|Splay trees]] are known to have key-independent optimality.

==References==
{{reflist}}

[[Category:Data structures]]</text>
      <sha1>o51xgygqk5zdpi6vkby51p9v2yz9cfj</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Disruptor (software pattern)</title>
    <ns>0</ns>
    <id>42963753</id>
    <revision>
      <id>625936611</id>
      <parentid>625935969</parentid>
      <timestamp>2014-09-17T12:12:02Z</timestamp>
      <contributor>
        <username>Frap</username>
        <id>612852</id>
      </contributor>
      <comment>Added categories</comment>
      <text xml:space="preserve" bytes="4806">{{Orphan|date=June 2014}}

'''Disruptor''' is a [[data structure]] and concurrency software pattern for low latency and high throughput for asynchronous event processing architectures. It ensures that any data is owned by only one thread for write access, therefore eliminating write contention.

All memory for the [[Circular buffer|ring buffer]] is pre-allocated on start up. This pre-allocation of entries eliminates issues in programming languages that support [[Garbage collection (computer science)|garbage collection]], since the entries will be re-used and live for the duration of the Disruptor instance.

On creation of the ring buffer the Disruptor utilises the [[abstract factory pattern]] to pre-allocate the entries. When an entry is claimed, a producer can copy its data into the pre-allocated structure.

In most common usages of the Disruptor there is usually only one producer. Typical producers are file readers or network listeners. In cases where there is a single producer there is no contention on sequence/entry allocation.

== Code example ==
&lt;source lang=&quot;java&quot;&gt;
// Callback handler which can be implemented by consumers
final BatchHandler&lt;ValueEntry&gt; batchHandler = new BatchHandler&lt;ValueEntry&gt;()
{
    public void onAvailable(final ValueEntry entry) throws Exception
    {
        // Process a new entry as it becomes available.
    }

    public void onEndOfBatch() throws Exception
    {
        // Useful for flushing results to an IO device if necessary.
    }

    public void onCompletion()
    {
        // Do any necessary clean up before shutdown.
    }
};

RingBuffer&lt;ValueEntry&gt; ringBuffer = 
    new RingBuffer&lt;ValueEntry&gt;(ValueEntry.ENTRY_FACTORY, SIZE,
                               ClaimStrategy.Option.SINGLE_THREADED,
                               WaitStrategy.Option.YIELDING);
ConsumerBarrier&lt;ValueEntry&gt; consumerBarrier = ringBuffer.createConsumerBarrier();
BatchConsumer&lt;ValueEntry&gt; batchConsumer = 
    new BatchConsumer&lt;ValueEntry&gt;(consumerBarrier, batchHandler);
ProducerBarrier&lt;ValueEntry&gt; producerBarrier = ringBuffer.createProducerBarrier(batchConsumer);

// Each consumer can run on a separate thread
EXECUTOR.submit(batchConsumer);

// Producers claim entries in sequence
ValueEntry entry = producerBarrier.nextEntry();

// copy data into the entry container

// make the entry available to consumers
producerBarrier.commit(entry);
&lt;/source&gt;

=== C# ===
&lt;source lang=&quot;csharp&quot;&gt;
using System;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using Disruptor;
using Disruptor.Dsl;

namespace DisruptorTest
{
    public sealed class ValueEntry
    {
        public long Value { get; set; }

        public ValueEntry()
        {
            Console.WriteLine(&quot;New ValueEntry created&quot;);
        }
    }

    public class ValueAdditionHandler : IEventHandler&lt;ValueEntry&gt;
    {
        public void OnNext(ValueEntry data, long sequence, bool endOfBatch)
        {
            Console.WriteLine(&quot;Event handled: Value = {0} (processed event {1})&quot;, data.Value, sequence);
        }
    }

    class Program
    {
        private static readonly Random _random = new Random();
        private static readonly int _ringSize = 16;  // Must be multiple of 2

        static void Main(string[] args)
        {
            var disruptor = new Disruptor.Dsl.Disruptor&lt;ValueEntry&gt;(() =&gt; new ValueEntry(), _ringSize, TaskScheduler.Default);

            disruptor.HandleEventsWith(new ValueAdditionHandler());

            var ringBuffer = disruptor.Start();

            while (true)
            {
                long sequenceNo = ringBuffer.Next();

                ValueEntry entry = ringBuffer[sequenceNo];

                entry.Value = _random.Next();

                ringBuffer.Publish(sequenceNo);

                Console.WriteLine(&quot;Published entry {0}, value {1}&quot;, sequenceNo, entry.Value);

                Thread.Sleep(250);
            }
        }
    }
}
&lt;/source&gt;

== See also ==
{{Portal|Computer programming}}
* [[Concurrent data structure]]

== References ==
{{Citation
| last      = Thompson
| first     = Martin
| last2     = Farley
| first2    = Dave
| last3     = Barker
| first3    = Michael
| last4     = Gee
| first4    = Patricia
| last5     = Stewart
| first5    = Andrew
| title     = Disruptor: High performance alternative to bounded queues for exchanging data between concurrent threads
| url       = https://disruptor.googlecode.com/files/Disruptor-1.0.pdf
| year      = 2011
}}

== External links ==
* http://lmax-exchange.github.io/disruptor/
* http://disruptor.googlecode.com/files/Disruptor-1.0.pdf
* https://github.com/LMAX-Exchange/disruptor/wiki/Introduction

{{Data structures}}

[[Category:Concurrent computing]]
[[Category:Data structures]]
[[Category:Software design patterns]]

{{Compu-sci-stub}}</text>
      <sha1>g3j1bpq0v72xbfj5oqkzhwhan4gt00h</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Linked list</title>
    <ns>0</ns>
    <id>18167</id>
    <revision>
      <id>626554544</id>
      <parentid>625369917</parentid>
      <timestamp>2014-09-22T01:12:17Z</timestamp>
      <contributor>
        <ip>24.4.170.134</ip>
      </contributor>
      <comment>/* Basic concepts and nomenclature */  typo</comment>
      <text xml:space="preserve" bytes="50565">{{Refimprove|date=February 2014}}

In [[computer science]], a '''linked list''' is a [[data structure]] consisting of a group of [[node (computer science)|node]]s which together represent a sequence. Under the simplest form, each node is composed of a data and a [[reference (computer science)|reference]] (in other words, a ''link'') to the next node in the sequence; more complex variants add additional links. This structure allows for efficient insertion or removal of elements from any position in the sequence.

&lt;div class=&quot;center&quot;&gt;[[File:Singly-linked-list.svg]]&lt;br&gt;&lt;small&gt;''A linked list whose nodes contain two fields: an integer value and a link to the next node. The last node is linked to a terminator used to signify the end of the list.''&lt;/small&gt;&lt;/div&gt;&lt;!--NEEDED: A VARIANT FIGURE WITHOUT THE FINAL NULL--&gt;

Linked lists are among the simplest and most common data structures. They can be used to implement several other common [[abstract data type]]s, including [[List (abstract data type)|lists]] (the abstract data type), [[Stack (abstract data type)|stacks]], [[Queue (abstract data type)|queues]], [[associative array]]s, and [[S-expression]]s, though it is not uncommon to implement the other data structures directly without using a list as the basis of implementation.

The principal benefit of a linked list over a conventional [[array data structure|array]] is that the list elements can easily be inserted or removed without reallocation or reorganization of the entire structure because the data items need not be stored contiguously in memory or on disk. Linked lists allow insertion and removal of nodes at any point in the list, and can do so with a constant number of operations if the link previous to the link being added or removed is maintained during list traversal.

On the other hand, simple linked lists by themselves do not allow [[random access]] to the data, or any form of efficient indexing.  Thus, many basic operations&amp;nbsp;— such as obtaining the last node of the list (assuming that the last node is not maintained as separate node reference in the list structure), or finding a node that contains a given datum, or locating the place where a new node should be inserted&amp;nbsp;— may require scanning most or all of the list elements. The advantages and disadvantages of using linked lists are as follows:-

Advantages:
* Linked lists are a dynamic data structure, allocating the needed memory when the program is initiated.
* Insertion and deletion node operations are easily implemented in a linked list.
* Linear data structures such as stacks and queues are easily executed with a linked list.
* They can reduce access time and may expand in real time without memory overhead.  

Disadvantages:
* They have a tendency to waste memory due to [[pointer (computer science)|pointers]] requiring extra storage space.
* Nodes in a linked list must be read in order from the beginning as linked lists are inherently [[sequential access]].
* Nodes are stored incontiguously, greatly increasing the time required to access individual elements within the list.
* Difficulties arise in linked lists when it comes to reverse traversing. Singly linked lists are extremely difficult to navigate backwards, and while doubly linked lists are somewhat easier to read, memory is wasted in allocating space for a back pointer.

==History==
Linked lists were developed in 1955–1956 by [[Allen Newell]], [[Cliff Shaw]] and [[Herbert A. Simon]] at [[RAND Corporation]] as the primary [[data structure]] for their [[Information Processing Language]]. IPL was used by the authors to develop several early [[artificial intelligence]] programs, including the Logic Theory Machine, the [[General Problem Solver]], and a computer chess program. Reports on their work appeared in IRE Transactions on Information Theory in 1956, and several conference proceedings from 1957 to 1959, including Proceedings of the Western Joint Computer Conference in 1957 and 1958, and Information Processing (Proceedings of the first [[UNESCO]] International Conference on Information Processing) in 1959. The now-classic diagram consisting of blocks representing list nodes with arrows pointing to successive list nodes appears in &quot;Programming the Logic Theory Machine&quot; by Newell and Shaw in Proc. WJCC, February 1957. Newell and Simon were recognized with the ACM [[Turing Award]] in 1975 for having &quot;made basic contributions to artificial intelligence, the psychology of human cognition, and list processing&quot;.
The problem of [[machine translation]] for [[natural language]] processing led [[Victor Yngve]] at [[Massachusetts Institute of Technology]] (MIT) to use linked lists as data structures in his COMIT programming language for computer research in the field of [[linguistics]]. A report on this language entitled &quot;A programming language for mechanical translation&quot; appeared in Mechanical Translation in 1958.

[[LISP]], standing for list processor, was created by [[John McCarthy (computer scientist)|John McCarthy]] in 1958 while he was at MIT and in 1960 he published its design in a paper in the [[Communications of the ACM]], entitled &quot;Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I&quot;. One of LISP's major data structures is the linked list.
By the early 1960s, the utility of both linked lists and languages which use these structures as their primary data representation was well established. Bert Green of the [[MIT Lincoln Laboratory]] published a review article entitled &quot;Computer languages for symbol manipulation&quot; in IRE Transactions on Human Factors in Electronics in March 1961 which summarized the advantages of the linked list approach. A later review article, &quot;A Comparison of list-processing computer languages&quot; by Bobrow and Raphael, appeared in Communications of the ACM in April 1964.

Several operating systems developed by [[Technical Systems Consultants]] (originally of West Lafayette Indiana, and later of Chapel Hill, North Carolina) used singly linked lists as file structures. A directory entry pointed to the first sector of a file, and succeeding portions of the file were located by traversing pointers. Systems using this technique included Flex (for the [[Motorola 6800]] CPU), mini-Flex (same CPU), and Flex9 (for the Motorola 6809 CPU). A variant developed by TSC for and marketed by Smoke Signal Broadcasting in California, used doubly linked lists in the same manner.

The TSS/360 operating system, developed by IBM for the System 360/370 machines, used a double linked list for their file system catalog. The directory structure was similar to Unix, where a directory could contain files and/or other directories and extend to any depth. A utility flea was created to fix file system problems after a crash, since modified portions of the file catalog were sometimes in memory when a crash occurred. Problems were detected by comparing the forward and backward links for consistency. If a forward link was corrupt, then if a backward link to the infected node was found, the forward link was set to the node with the backward link. A humorous comment in the source code where this utility was invoked stated &quot;Everyone knows a flea collar gets rid of bugs in cats&quot;.

==Basic concepts and nomenclature==
Each record of a linked list is often called an '''element''' or '''[[node (computer science)|node]]'''.

The field of each node that contains the address of the next node is usually called the '''''next'' link''' or '''''next'' pointer'''.  The remaining fields are known as the '''data''', '''information''', '''value''', '''cargo''', or '''payload''' fields.

The '''head''' of a list is its first node. The '''tail''' of a list may refer either to the rest of the list after the head, or to the last node in the list. In [[Lisp (programming language)|Lisp]] and some derived languages, the next node may be called the '''[[CAR and CDR|cdr]]''' (pronounced ''could-er'') of the list, while the payload of the head node may be called the '''[[CAR and CDR|car]]'''.

===Singly linked list===
Singly linked lists contain nodes which have a data field as well as a '''''next''''' field, which points to the next node in line of nodes. Operations that can be performed on singly linked lists include insertion, deletion and traversal.

&lt;div class=&quot;center&quot;&gt;[[Image:Singly-linked-list.svg]]&lt;br&gt;&lt;small&gt;''A singly linked list whose nodes contain two fields: an integer value and a link to the next node''&lt;/small&gt;&lt;/div&gt;

===Doubly linked list===
{{main|Doubly linked list}}
In a '''doubly linked list''', each node contains, besides the next-node link, a second link field pointing to the ''previous'' node in the sequence.  The two links may be called '''forward'''('''s''') and '''backwards''', or '''next''' and '''prev'''('''previous''').

&lt;div class=&quot;center&quot;&gt;[[Image:Doubly-linked-list.svg]]&lt;br&gt;&lt;small&gt;''A doubly linked list whose nodes contain three fields: an integer value, the link forward to the next node, and the link backward to the previous node''&lt;/small&gt;&lt;/div&gt;

A technique known as [[XOR linked list|XOR-linking]] allows a doubly linked list to be implemented using a single link field in each node. However, this technique requires the ability to do bit operations on addresses, and therefore may not be available in some high-level languages.

===Multiply linked list===
In a '''multiply linked list''', each node contains two or more link fields, each field being used to connect the same set of data records in a different order (e.g., by name, by department, by date of birth, etc.). While doubly linked lists can be seen as special cases of multiply linked list, the fact that the two orders are opposite to each other leads to simpler and more efficient algorithms, so they are usually treated as a separate case.

===Circular Linked list===
In the last [[node (computer science)|node]] of a list, the link field often contains a [[Null pointer#Null pointer|null]] reference, a special value used to indicate the lack of further nodes. A less common convention is to make it point to the first node of the list; in that case the list is said to be 'circular' or 'circularly linked'; otherwise it is said to be 'open' or 'linear'.

&lt;div class=&quot;center&quot;&gt;[[Image:Circularly-linked-list.svg]]&lt;br&gt;&lt;small&gt;''A circular linked list''&lt;/small&gt;&lt;/div&gt;

In the case of a circular doubly linked list, the only change that occurs is that the end, or &quot;tail&quot;, of the said list is linked back to the front, or &quot;head&quot;, of the list and vice versa.

===Sentinel nodes===
{{Main|Sentinel node}}
In some implementations, an extra '''sentinel''' or '''dummy''' node may be added before the first data record and/or after the last one.  This convention simplifies and accelerates some list-handling algorithms, by ensuring that all links can be safely dereferenced and that every list (even one that contains no data elements) always has a &quot;first&quot; and &quot;last&quot; node.

===Empty lists===
An empty list is a list that contains no data records. This is usually the same as saying that it has zero nodes. If sentinel nodes are being used, the list is usually said to be empty when it has only sentinel nodes.

===Hash linking===
The link fields need not be physically part of the nodes.  If the data records are stored in an array and referenced by their indices, the link field may be stored in a separate array with the same indices as the data records.

===List handles===
Since a reference to the first node gives access to the whole list, that reference is often called the '''address''', '''pointer''', or '''handle''' of the list.  Algorithms that manipulate linked lists usually get such handles to the input lists and return the handles to the resulting lists. In fact, in the context of such algorithms, the word &quot;list&quot; often means &quot;list handle&quot;.  In some situations, however, it may be convenient to refer to a list by a handle that consists of two links, pointing to its first and last nodes.

===Combining alternatives===
The alternatives listed above may be arbitrarily combined in almost every way, so one may have circular doubly linked lists without sentinels, circular singly linked lists with sentinels, etc.

==Tradeoffs==
As with most choices in computer programming and design, no method is well suited to all circumstances. A linked list data structure might work well in one case, but cause problems in another. This is a list of some of the common tradeoffs involving linked list structures.

===Linked lists vs. dynamic arrays===

{{List data structure comparison}}
A ''[[dynamic array]]'' is a data structure that allocates all elements contiguously in memory, and keeps a count of the current number of elements. If the space reserved for the dynamic array is exceeded, it is reallocated and (possibly) copied, an expensive operation.

Linked lists have several advantages over dynamic arrays. Insertion or deletion of an element at a specific point of a list, assuming that we have indexed a pointer to the node (before the one to be removed, or before the insertion point) already, is a constant-time operation (otherwise without this reference it is O(n)), whereas insertion in a dynamic array at random locations will require moving half of the elements on average, and all the elements in the worst case.  While one can &quot;delete&quot; an element from an array in constant time by somehow marking its slot as &quot;vacant&quot;, this causes [[fragmentation (computer)|fragmentation]] that impedes the performance of iteration.

Moreover, arbitrarily many elements may be inserted into a linked list, limited only by the total memory available; while a dynamic array will eventually fill up its underlying array data structure and will have to reallocate&amp;nbsp;— an expensive operation, one that may not even be possible if memory is fragmented, although the cost of reallocation can be averaged over insertions, and the cost of an insertion due to reallocation would still be [[Amortized analysis|amortized]] O(1). This helps with appending elements at the array's end, but inserting into (or removing from) middle positions still carries prohibitive costs due to data moving to maintain contiguity. An array from which many elements are removed may also have to be resized in order to avoid wasting too much space.

On the other hand, dynamic arrays (as well as fixed-size [[array data structure]]s) allow constant-time [[random access]], while linked lists allow only [[sequential access]] to elements. Singly linked lists, in fact, can be easily traversed in only one direction. This makes linked lists unsuitable for applications where it's useful to look up an element by its index quickly, such as [[heapsort]]. Sequential access on arrays and dynamic arrays is also faster than on linked lists on many machines, because they have optimal [[locality of reference]] and thus make good use of data caching.

Another disadvantage of linked lists is the extra storage needed for references, which often makes them impractical for lists of small data items such as [[character (computing)|characters]] or [[boolean value]]s, because the storage overhead for the links may exceed by a factor of two or more the size of the data. In contrast, a dynamic array requires only the space for the data itself (and a very small amount of control data).&lt;ref group=&quot;note&quot;&gt;The amount of control data required for a dynamic array is usually of the form &lt;math&gt;K+B*n&lt;/math&gt;, where &lt;math&gt;K&lt;/math&gt; is a per-array constant, &lt;math&gt;B&lt;/math&gt; is a per-dimension constant, and &lt;math&gt;n&lt;/math&gt; is the number of dimensions.  &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are typically on the order of 10 bytes.&lt;/ref&gt;  It can also be slow, and with a naïve allocator, wasteful, to allocate memory separately for each new element, a problem generally solved using [[memory pool]]s.

Some hybrid solutions try to combine the advantages of the two representations.  [[Unrolled linked list]]s store several elements in each list node, increasing cache performance while decreasing memory overhead for references. [[CDR coding]] does both these as well, by replacing references with the actual data referenced, which extends off the end of the referencing record.

A good example that highlights the pros and cons of using dynamic arrays vs. linked lists is by implementing a program that resolves the [[Josephus problem]]. The Josephus problem is an election method that works by having a group of people stand in a circle. Starting at a predetermined person, you count around the circle ''n'' times. Once you reach the ''n''th person, take them out of the circle and have the members close the circle. Then count around the circle the same ''n'' times and repeat the process, until only one person is left. That person wins the election. This shows the strengths and weaknesses of a linked list vs. a dynamic array, because if you view the people as connected nodes in a circular linked list then it shows how easily the linked list is able to delete nodes (as it only has to rearrange the links to the different nodes). However, the linked list will be poor at finding the next person to remove and will need to search through the list until it finds that person. A dynamic array, on the other hand, will be poor at deleting nodes (or elements) as it cannot remove one node without individually shifting all the elements up the list by one. However, it is exceptionally easy to find the ''n''th person in the circle by directly referencing them by their position in the array.

The [[list ranking]] problem concerns the efficient conversion of a linked list representation into an array. Although trivial for a conventional computer, solving this problem by a [[parallel algorithm]] is complicated and has been the subject of much research.

A [[self-balancing binary search tree|balanced tree]] has similar memory access patterns and space overhead to a linked list while permitting much more efficient indexing, taking O(log n) time instead of O(n) for a random access. However, insertion and deletion operations are more expensive due to the overhead of tree manipulations to maintain balance.  Schemes exist for trees to automatically maintain themselves in a balanced state: [[AVL tree]]s or [[red-black tree]]s.

===Singly linked linear lists vs. other lists===

While doubly linked and/or circular lists have advantages over singly linked linear lists, linear lists offer some advantages that make them preferable in some situations.

A singly linked linear list is a [[recursion|recursive]] data structure, because it contains a pointer to a ''smaller'' object of the same type.  For that reason, many operations on singly linked linear lists (such as [[Merge algorithm|merging]] two lists, or enumerating the elements in reverse order) often have very simple recursive algorithms, much simpler than any solution using [[iteration|iterative command]]s.  While those recursive solutions can be adapted for doubly linked and circularly linked lists, the procedures generally need extra arguments and more complicated base cases.

Linear singly linked lists also allow [[tail-sharing]], the use of a common final portion of sub-list as the terminal portion of two different lists. In particular, if a new node is added at the beginning of a list, the former list remains available as the tail of the new one&amp;nbsp;— a simple example of a [[persistent data structure]]. Again, this is not true with the other variants: a node may never belong to two different circular or doubly linked lists.

In particular, end-sentinel nodes can be shared among singly linked non-circular lists.  The same end-sentinel node may be used for ''every'' such list.  In [[Lisp programming language|Lisp]], for example, every proper list ends with a link to a special node, denoted by &lt;code&gt;nil&lt;/code&gt; or &lt;code&gt;()&lt;/code&gt;, whose &lt;code&gt;[[CAR and CDR|CAR]]&lt;/code&gt; and &lt;code&gt;CDR&lt;/code&gt; links point to itself.  Thus a Lisp procedure can safely take the &lt;code&gt;CAR&lt;/code&gt; or &lt;code&gt;CDR&lt;/code&gt; of ''any'' list.

The advantages of the fancy variants are often limited to the complexity of the algorithms, not in their efficiency.  A circular list, in particular, can usually be emulated by a linear list together with two variables that point to the first and last nodes, at no extra cost.

===Doubly linked vs. singly linked===
Double-linked lists require more space per node (unless one uses [[XOR linked list|XOR-linking]]), and their elementary operations are more expensive; but they are often easier to manipulate because they allow fast and easy sequential access to the list in both directions. In a doubly linked list, one can insert or delete a node in a constant number of operations given only that node's address. To do the same in a singly linked list, one must have the ''address of the pointer'' to that node, which is either the handle for the whole list (in case of the first node) or the link field in the ''previous'' node. Some algorithms require access in both directions. On the other hand, doubly linked lists do not allow tail-sharing and cannot be used as [[persistent data structure]]s.

===Circularly linked vs. linearly linked===

A circularly linked list may be a natural option to represent arrays that are naturally circular, e.g. the corners of a [[polygon]], a pool of [[data buffer|buffers]] that are used and released in [[FIFO (computing)|FIFO]] (&quot;first in, first out&quot;) order, or a set of processes that should be [[time sharing|time-shared]] in [[round-robin scheduling|round-robin order]]. In these applications, a pointer to any node serves as a handle to the whole list.

With a circular list, a pointer to the last node gives easy access also to the first node, by following one link. Thus, in applications that require access to both ends of the list (e.g., in the implementation of a queue), a circular structure allows one to handle the structure by a single pointer, instead of two.

A circular list can be split into two circular lists, in constant time, by giving the addresses of the last node of each piece. The operation consists in swapping the contents of the link fields of those two nodes.  Applying the same operation to any two nodes in two distinct lists joins the two list into one. This property greatly simplifies some algorithms and data structures, such as the [[quad-edge data structure|quad-edge]] and [[face-edge data structure|face-edge]].

The simplest representation for an empty ''circular'' list (when such a thing makes sense) is a null pointer, indicating that the list has no nodes.  Without this choice, many algorithms have to test for this special case, and handle it separately.  By contrast, the use of null to denote an empty ''linear'' list is more natural and often creates fewer special cases.

===Using sentinel nodes===
Sentinel node may simplify certain list operations, by ensuring that the next and/or previous nodes exist for every element, and that even empty lists have at least one node.  One may also use a sentinel node at the end of the list, with an appropriate data field, to eliminate some end-of-list tests.  For example, when scanning the list looking for a node with a given value ''x'', setting the sentinel's data field to ''x'' makes it unnecessary to test for end-of-list inside the loop.  Another example is the merging two sorted lists: if their sentinels have data fields set to +∞, the choice of the next output node does not need special handling for empty lists.

However, sentinel nodes use up extra space (especially in applications that use many short lists), and they may complicate other operations (such as the creation of a new empty list).

However, if the circular list is used merely to simulate a linear list, one may avoid some of this complexity by adding a single sentinel node to every list, between the last and the first data nodes.  With this convention, an empty list consists of the sentinel node alone, pointing to itself via the next-node link.  The list handle should then be a pointer to the last data node, before the sentinel, if the list is not empty; or to the sentinel itself, if the list is empty.

The same trick can be used to simplify the handling of a doubly linked linear list, by turning it into a circular doubly linked list with a single sentinel node.  However, in this case, the handle should be a single pointer to the dummy node itself.&lt;ref&gt;Ford, William and Topp, William ''Data Structures with C++ using STL Second Edition'' (2002). Prentice-Hall. ISBN 0-13-085850-1, pp. 466-467&lt;/ref&gt;

==Linked list operations==

When manipulating linked lists in-place, care must be taken to not use values that you have invalidated in previous assignments. This makes algorithms for inserting or deleting linked list nodes somewhat subtle. This section gives [[pseudocode]] for adding or removing nodes from singly, doubly, and circularly linked lists in-place. Throughout we will use ''null'' to refer to an end-of-list marker or [[sentinel value|sentinel]], which may be implemented in a number of ways.

===Linearly linked lists===

====Singly linked lists====

Our node data structure will have two fields.  We also keep a variable ''firstNode'' which always points to the first node in the list, or is ''null'' for an empty list.

  '''record''' ''Node''
  {
     data; ''// The data being stored in the node''
     ''Node'' next ''// A [[reference (computer science)|reference]] to the next node, null for last node''
  }

  '''record''' ''List''
  {
      ''Node'' firstNode ''// points to first node of list; null for empty list''
  }

Traversal of a singly linked list is simple, beginning at the first node and following each ''next'' link until we come to the end:

  node := list.firstNode
  '''while''' node not null
      ''(do something with node.data)''
      node := node.next

The following code inserts a node after an existing node in a singly linked list. The diagram shows how it works. Inserting a node before an existing one cannot be done directly; instead, one must keep track of the previous node and insert a node after it.

  [[File:CPT-LinkedLists-addingnode.svg|center]]

  '''function''' insertAfter(''Node'' node, ''Node'' newNode) ''// insert newNode after node''
      newNode.next := node.next
      node.next    := newNode

Inserting at the beginning of the list requires a separate function. This requires updating ''firstNode''.

  '''function''' insertBeginning(''List'' list, ''Node'' newNode) ''// insert node before current first node''
      newNode.next   := list.firstNode
      list.firstNode := newNode

Similarly, we have functions for removing the node ''after'' a given node, and for removing a node from the beginning of the list. The diagram demonstrates the former. To find and remove a particular node, one must again keep track of the previous element.

[[File:CPT-LinkedLists-deletingnode.svg|center]]

  '''function''' removeAfter(''Node'' node) ''// remove node past this one''
      obsoleteNode := node.next
      node.next := node.next.next
      destroy obsoleteNode

  '''function''' removeBeginning(''List'' list) ''// remove first node''
      obsoleteNode := list.firstNode
      list.firstNode := list.firstNode.next ''// point past deleted node''
      destroy obsoleteNode

Notice that &lt;code&gt;removeBeginning()&lt;/code&gt; sets &lt;code&gt;list.firstNode&lt;/code&gt; to &lt;code&gt;null&lt;/code&gt; when removing the last node in the list.

Since we can't iterate backwards, efficient &lt;code&gt;insertBefore&lt;/code&gt; or &lt;code&gt;removeBefore&lt;/code&gt; operations are not possible.

Appending one linked list to another can be inefficient unless a reference to the tail is kept as part of the List structure, because we must traverse the entire first list in order to find the tail, and then append the second list to this.  Thus, if two linearly linked lists are each of length &lt;math&gt;n&lt;/math&gt;, list appending has [[asymptotic time complexity]] of &lt;math&gt;O(n)&lt;/math&gt;.  In the Lisp family of languages, list appending is provided by the &lt;code&gt;[[append]]&lt;/code&gt; procedure.

Many of the special cases of linked list operations can be eliminated by including a dummy element at the front of the list.  This ensures that there are no special cases for the beginning of the list and renders both &lt;code&gt;insertBeginning()&lt;/code&gt; and &lt;code&gt;removeBeginning()&lt;/code&gt; unnecessary. In this case, the first useful data in the list will be found at &lt;code&gt;list.'''firstNode'''.next&lt;/code&gt;.

===Circularly linked list===

In a circularly linked list, all nodes are linked in a continuous circle, without using ''null.'' For lists with a front and a back (such as a queue), one stores a reference to the last node in the list. The ''next'' node after the last node is the first node. Elements can be added to the back of the list and removed from the front in constant time.

Circularly linked lists can be either singly or doubly linked.

Both types of circularly linked lists benefit from the ability to traverse the full list beginning at any given node. This often allows us to avoid storing ''firstNode'' and ''lastNode'', although if the list may be empty we need a special representation for the empty list, such as a ''lastNode'' variable which points to some node in the list or is ''null'' if it's empty; we use such a ''lastNode'' here.  This representation significantly simplifies adding and removing nodes with a non-empty list, but empty lists are then a special case.

====Algorithms====
Assuming that ''someNode'' is some node in a non-empty circular singly linked list, this code iterates through that list starting with ''someNode'':

  '''function''' iterate(someNode)
    '''if''' someNode ≠ '''null'''
      node := someNode
      '''do'''
        do something with node.value
        node := node.next
      '''while''' node ≠ someNode

Notice that the test &quot;'''while''' node ≠ someNode&quot; must be at the end of the loop. If the test was moved to the beginning of the loop, the procedure would fail whenever the list had only one node.

This function inserts a node &quot;newNode&quot; into a circular linked list after a given node &quot;node&quot;.  If &quot;node&quot; is null, it assumes that the list is empty.

  '''function''' insertAfter(''Node'' node, ''Node'' newNode)
      '''if''' node = '''null'''
        newNode.next := newNode
      '''else'''
        newNode.next := node.next
        node.next := newNode

Suppose that &quot;L&quot; is a variable pointing to the last node of a circular linked list (or null if the list is empty).  To append &quot;newNode&quot; to the ''end'' of the list, one may do

  insertAfter(L, newNode)
  L := newNode

To insert &quot;newNode&quot; at the ''beginning'' of the list, one may do

  insertAfter(L, newNode)
  '''if''' L = '''null'''
    L := newNode
&lt;!--PLEASE FIX THIS
As in doubly linked lists, &quot;removeAfter&quot; and &quot;removeBefore&quot; can be implemented with &quot;remove(list, node.prev)&quot; and &quot;remove(list, node.next)&quot;.

example:
node 'tom' next pointing to node 'jerry'...till the last point at node 'fix'..
data in node 'fix' previous to node 'tom'...looping instruction..
--&gt;

==Linked lists using arrays of nodes==

Languages that do not support any type of [[reference (computer science)|reference]] can still create links by replacing pointers with array indices. The approach is to keep an [[array data type|array]] of [[record (computer science)|record]]s, where each record has integer fields indicating the index of the next (and possibly previous) node in the array. Not all nodes in the array need be used. If records are also not supported, [[parallel array]]s can often be used instead.

As an example, consider the following linked list record that uses arrays instead of pointers:

  '''record''' ''Entry'' {
     ''integer'' next; ''// index of next entry in array''
     ''integer'' prev; ''// previous entry (if double-linked)''
     ''string'' name;
     ''real'' balance;
  }

By creating an array of these structures, and an integer variable to store the index of the first element, a linked list can be built:

 ''integer'' listHead
 ''Entry'' Records[1000]

Links between elements are formed by placing the array index of the next (or previous) cell into the Next or Prev field within a given element.  For example:

{| class=&quot;wikitable&quot;
|-
! Index
! Next
! Prev
! Name
! Balance
|-
| 0
| 1
| 4
| Jones, John
| 123.45
|-
| 1
| -1
| 0
| Smith, Joseph
| 234.56
|-
| 2 (listHead)
| 4
| -1
| Adams, Adam
| 0.00
|-
| 3
| 
| 
| Ignore, Ignatius
| 999.99
|-
| 4
| 0
| 2
| Another, Anita
| 876.54
|-
| 5
| 
| 
| 
| 
|-
| 6
| 
| 
| 
| 
|-
| 7
| 
| 
| 
|
|}

In the above example, &lt;code&gt;ListHead&lt;/code&gt; would be set to 2, the location of the first entry in the list.  Notice that entry 3 and 5 through 7 are not part of the list.  These cells are available for any additions to the list.  By creating a &lt;code&gt;ListFree&lt;/code&gt; integer variable, a [[free list]] could be created to keep track of what cells are available.  If all entries are in use, the size of the array would have to be increased or some elements would have to be deleted before new entries could be stored in the list.

The following code would traverse the list and display names and account balance:
 i := listHead
 '''while''' i ≥ 0 ''// loop through the list''
      print i, Records[i].name, Records[i].balance ''// print entry''
      i := Records[i].next

When faced with a choice, the advantages of this approach include:
* The linked list is relocatable, meaning it can be moved about in memory at will, and it can also be quickly and directly [[serialization|serialized]] for storage on disk or transfer over a network.
* Especially for a small list, array indexes can occupy significantly less space than a full pointer on many architectures.
* [[Locality of reference]] can be improved by keeping the nodes together in memory and by periodically rearranging them, although this can also be done in a general store.
* Naïve [[dynamic memory allocation|dynamic memory allocators]] can produce an excessive amount of overhead storage for each node allocated; almost no allocation overhead is incurred per node in this approach.
* Seizing an entry from a pre-allocated array is faster than using dynamic memory allocation for each node, since dynamic memory allocation typically requires a search for a free memory block of the desired size.

This approach has one main disadvantage, however: it creates and manages a private memory space for its nodes. This leads to the following issues:
* It increases complexity of the implementation.
* Growing a large array when it is full may be difficult or impossible, whereas finding space for a new linked list node in a large, general memory pool may be easier.
* Adding elements to a dynamic array will occasionally (when it is full) unexpectedly take linear ([[Big-O notation|O]](n)) instead of constant time (although it's still an [[amortized analysis|amortized]] constant).
* Using a general memory pool leaves more memory for other data if the list is smaller than expected or if many nodes are freed.
For these reasons, this approach is mainly used for languages that do not support dynamic memory allocation. These disadvantages are also mitigated if the maximum size of the list is known at the time the array is created.

==Language support==

Many [[programming language]]s such as [[Lisp programming language|Lisp]] and [[Scheme (programming language)|Scheme]] have singly linked lists built in. In many [[functional programming language|functional languages]], these lists are constructed from nodes, each called a ''[[cons]]'' or ''cons cell''. The cons has two fields: the ''[[car and cdr|car]]'', a reference to the data for that node, and the ''[[car and cdr|cdr]]'', a reference to the next node. Although cons cells can be used to build other data structures, this is their primary purpose.

In languages that support [[abstract data type]]s or templates, linked list ADTs or templates are available for building linked lists.  In other languages, linked lists are typically built using [[reference (computer science)|reference]]s together with [[record (computer science)|record]]s.

==Internal and external storage==

When constructing a linked list, one is faced with the choice of whether to store the data of the list directly in the linked list nodes, called ''internal storage'', or merely to store a reference to the data, called ''external storage''. Internal storage has the advantage of making access to the data more efficient, requiring less storage overall, having better [[locality of reference]], and simplifying memory management for the list (its data is allocated and deallocated at the same time as the list nodes).

External storage, on the other hand, has the advantage of being more generic, in that the same data structure and machine code can be used for a linked list no matter what the size of the data is. It also makes it easy to place the same data in multiple linked lists. Although with internal storage the same data can be placed in multiple lists by including multiple ''next'' references in the node data structure, it would then be necessary to create separate routines to add or delete cells based on each field.  It is possible to create additional linked lists of elements that use internal storage by using external storage, and having the cells of the additional linked lists store references to the nodes of the linked list containing the data.

In general, if a set of data structures needs to be included in multiple linked lists, external storage is the best approach.  If a set of data structures need to be included in only one linked list, then internal storage is slightly better, unless a generic linked list package using external storage is available.  Likewise, if different sets of data that can be stored in the same data structure are to be included in a single linked list, then internal storage would be fine.

Another approach that can be used with some languages involves having different data structures, but all have the initial fields, including the ''next'' (and ''prev'' if double linked list) references in the same location.  After defining separate structures for each type of data, a generic structure can be defined that contains the minimum amount of data shared by all the other structures and contained at the top (beginning) of the structures.  Then generic routines can be created that use the minimal structure to perform linked list type operations, but separate routines can then handle the specific data.  This approach is often used in message parsing routines, where several types of messages are received, but all start with the same set of fields, usually including a field for message type.  The generic routines are used to add new messages to a queue when they are received, and remove them from the queue in order to process the message.  The message type field is then used to call the correct routine to process the specific type of message.

===Example of internal and external storage===

Suppose you wanted to create a linked list of families and their members.  Using internal storage, the structure might look like the following:

  '''record''' ''member'' { ''// member of a family''
      ''member'' next;
      ''string'' firstName;
      ''integer'' age;
  }
  '''record''' ''family'' { ''// the family itself''
      ''family'' next;
      ''string'' lastName;
      ''string'' address;
      ''member'' members ''// head of list of members of this family''
  }

To print a complete list of families and their members using internal storage, we could write:

  aFamily := Families ''// start at head of families list''
  '''while''' aFamily ≠ '''null''' ''// loop through list of families''
      print information about family
      aMember := aFamily.members ''// get head of list of this family's members''
      '''while''' aMember ≠ '''null''' ''// loop through list of members''
          print information about member
          aMember := aMember.next
      aFamily := aFamily.next

Using external storage, we would create the following structures:

  '''record''' ''node'' { ''// generic link structure''
      ''node'' next;
      ''pointer'' data ''// generic pointer for data at node''
  }
  '''record''' ''member'' { ''// structure for family member''
      ''string'' firstName;
      ''integer'' age
  }
  '''record''' ''family'' { ''// structure for family''
      ''string'' lastName;
      ''string'' address;
      ''node'' members ''// head of list of members of this family''
  }

To print a complete list of families and their members using external storage, we could write:

  famNode := Families ''// start at head of families list''
  '''while''' famNode ≠ '''null''' ''// loop through list of families''
      aFamily := (family) famNode.data ''// extract family from node''
      print information about family
      memNode := aFamily.members ''// get list of family members''
      '''while''' memNode ≠ '''null''' ''// loop through list of members''
          aMember := (member)memNode.data ''// extract member from node''
          print information about member
          memNode := memNode.next
      famNode := famNode.next

Notice that when using external storage, an extra step is needed to extract the record from the node and cast it into the proper data type.  This is because both the list of families and the list of members within the family are stored in two linked lists using the same data structure (''node''), and this language does not have parametric types.

As long as the number of families that a member can belong to is known at compile time, internal storage works fine. If, however, a member needed to be included in an arbitrary number of families, with the specific number known only at run time, external storage would be necessary.

===Speeding up search===

Finding a specific element in a linked list, even if it is sorted, normally requires O(''n'') time ([[linear search]]).  This is one of the primary disadvantages of linked lists over other data structures.  In addition to the variants discussed above, below are two simple ways to improve search time.

In an unordered list, one simple heuristic for decreasing average search time is the ''move-to-front heuristic'', which simply moves an element to the beginning of the list once it is found. This scheme, handy for creating simple caches, ensures that the most recently used items are also the quickest to find again.

Another common approach is to &quot;[[index (database)|index]]&quot; a linked list using a more efficient external data structure. For example, one can build a [[red-black tree]] or [[hash table]] whose elements are references to the linked list nodes. Multiple such indexes can be built on a single list. The disadvantage is that these indexes may need to be updated each time a node is added or removed (or at least, before that index is used again).

===Random access lists===
A [[random access list]] is a list with support for fast random access to read or modify any element in the list.&lt;ref name=&quot;okasaki&quot;/&gt; One possible implementation is a [[skew binary random access list]] using the [[skew binary number system]], which involves a list of trees with special properties; this allows worst-case constant time head/cons operations, and worst-case logarithmic time random access to an element by index.&lt;ref name=&quot;okasaki&quot;/&gt; Random access lists can be implemented as [[persistent data structure]]s.&lt;ref name=&quot;okasaki&quot;/&gt;

Random access lists can be viewed as immutable linked lists in that they likewise support the same O(1) head and tail operations.&lt;ref name=&quot;okasaki&quot;/&gt;

A simple extension to random access lists is the [[min-list]], which provides an additional operation that yields the minimum element in the entire list in constant time (without{{clarify|r=should be &quot;disregarding&quot;?|date=October 2011}} mutation complexities).&lt;ref name=&quot;okasaki&quot;&gt;C Okasaki, &quot;[http://cs.oberlin.edu/~jwalker/refs/fpca95.ps Purely Functional Random-Access Lists]&quot;&lt;/ref&gt;

==Related data structures==

Both [[stack (data structure)|stacks]] and [[queue (data structure)|queues]] are often implemented using linked lists, and simply restrict the type of operations which are supported.

The [[skip list]] is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer.  This process continues down to the bottom layer, which is the actual list.

A [[binary tree]] can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node.

An [[unrolled linked list]] is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list.

A [[hash table]] may use linked lists to store the chains of items that hash to the same position in the hash table.

A [[heap (data structure)|heap]] shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index.

A [[self-organizing list]] rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list.

==Notes==
{{reflist|group=note}}

==Footnotes==
{{reflist}}

==References==
{{More footnotes|date=March 2012}}
*{{Cite web|last=Juan|first=Angel|title=Ch20 –Data Structures; ID06 - PROGRAMMING with JAVA (slide part of the book &quot;Big Java&quot;, by CayS. Horstmann)|url=http://www.uoc.edu/in3/emath/docs/java/ch20.pdf|format=PDF|page=3|year=2006|postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;{{inconsistent citations}}}}
*{{cite web|url=http://nist.gov/dads/HTML/linkedList.html|title=Definition of a linked list|date=2004-08-16|publisher=[[National Institute of Standards and Technology]]|accessdate=2004-12-14}}
*{{cite book|last=Antonakos|first=James L.|coauthors=Mansfield, Kenneth C., Jr.|title=Practical Data Structures Using C/C++|publisher=Prentice-Hall|year=1999|pages=165–190|isbn=0-13-280843-9}}
*{{cite book|last=Collins|first=William J.|title=Data Structures and the Java Collections Framework|publisher=McGraw Hill|location=New York|year=2005|pages=239–303|isbn=0-07-282379-8|origyear=2002}}
*{{cite book|last=Cormen|first=Thomas H.|coauthors=[[Charles E. Leiserson]]; [[Ronald L. Rivest]]; [[Stein, Clifford|Clifford Stein]]|title=[[Introduction to Algorithms]]|publisher=MIT Press|year=2003|pages=205–213 &amp; 501–505|isbn=0-262-03293-7}}
*{{cite book|last=Cormen|first=Thomas H.|coauthors=[[Charles E. Leiserson]]; [[Ronald L. Rivest]]; [[Stein, Clifford|Clifford Stein]]|title=[[Introduction to Algorithms]]|publisher=MIT Press|year=2001|pages=204–209|isbn=0-262-03293-7|chapter=10.2: Linked lists|authorlink=Thomas H. Cormen|edition=2md}}
*{{cite journal|last=Green|first=Bert F. Jr.|year=1961|title=Computer Languages for Symbol Manipulation|journal=IRE Transactions on Human Factors in Electronics|issue=2|pages=3–8|doi=10.1109/THFE2.1961.4503292}}
*{{cite journal|last=McCarthy|first=John|year=1960|title=Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I|journal=[[Communications of the ACM]]|url=http://www-formal.stanford.edu/jmc/recursive.html|authorlink=John McCarthy (computer scientist)|doi=10.1145/367177.367199|volume=3|issue=4|pages=184}}
*{{cite book|last=Knuth|first=Donald|title=Fundamental Algorithms|publisher=Addison-Wesley|year=1997|edition=3rd|pages=254–298|chapter=2.2.3-2.2.5|isbn=0-201-89683-4|authorlink=Donald Knuth}}
*{{cite journal|last=Newell|first=Allen|author2=Shaw, F. C. |year=1957|title=Programming the Logic Theory Machine|journal=Proceedings of the Western Joint Computer Conference|pages=230–240|authorlink=Allen Newell}}
*{{cite web|url=http://cslibrary.stanford.edu/103/LinkedListBasics.pdf|title=Linked list basics|last=Parlante|first=Nick|year=2001|publisher=Stanford University|accessdate=2009-09-21}}
*{{cite book|last=Sedgewick|first=Robert|title=Algorithms in C|publisher=Addison Wesley|year=1998|pages=90–109|isbn=0-201-31452-5|authorlink=Robert Sedgewick (computer scientist)}}
*{{cite book|last=Shaffer|first=Clifford A.|title=A Practical Introduction to Data Structures and Algorithm Analysis|publisher=Prentice Hall|location=New Jersey|year=1998|pages=77–102|isbn=0-13-660911-2}}
*{{cite journal|last=Wilkes|first=Maurice Vincent|year=1964|title=An Experiment with a Self-compiling Compiler for a Simple List-Processing Language|journal=Annual Review in Automatic Programming|publisher=Pergamon Press|volume=4|issue=1|authorlink=Maurice Vincent Wilkes|doi=10.1016/0066-4138(64)90013-8|pages=1}}
*{{cite journal|last=Wilkes|first=Maurice Vincent|year=1964|title=Lists and Why They are Useful|journal=Proceeds of the ACM National Conference, Philadelphia 1964|publisher=ACM|issue=P–64|pages=F1–1|authorlink=Maurice Vincent Wilkes}}
*{{cite web|url=http://isis.poly.edu/kulesh/stuff/src/klist/|title=Linux Kernel Linked List Explained|last=Shanmugasundaram|first=Kulesh|date=2005-04-04|accessdate=2009-09-21}}

==External links==
{{Commons category|Linked lists}}
*[http://scanftree.com/Data_Structure/Circular Introduction to Circular Linked] from the Scanftree
*[http://nist.gov/dads/HTML/linkedList.html Description] from the [[Dictionary of Algorithms and Data Structures]]
*[http://cslibrary.stanford.edu/103/ Introduction to Linked Lists], Stanford University Computer Science Library
*[http://cslibrary.stanford.edu/105/ Linked List Problems], Stanford University Computer Science Library
*[http://opendatastructures.org/versions/edition-0.1g/ods-python/3_Linked_Lists.html Open Data Structures - Chapter 3 - Linked Lists]
*[http://www.google.com/patents?vid=USPAT7028023 Patent for the idea of having nodes which are in several linked lists simultaneously] (note that this technique was widely used for many decades before the patent was granted)

{{Data structures}}

{{DEFAULTSORT:Linked List}}
[[Category:Linked lists|*]]
[[Category:Articles with example C code]]
[[Category:Data structures]]</text>
      <sha1>ay5x28mfmerw6itntr1a8e3tjl1qwcz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Weak array</title>
    <ns>0</ns>
    <id>43035212</id>
    <revision>
      <id>615247443</id>
      <parentid>614671540</parentid>
      <timestamp>2014-07-02T04:25:56Z</timestamp>
      <contributor>
        <username>DavidCary</username>
        <id>39203</id>
      </contributor>
      <comment>add references, as requested, etc.</comment>
      <text xml:space="preserve" bytes="1956">{{multiple issues|
{{confusing|date=June 2014}}
{{lead missing|date=June 2014}}
{{technical|date=June 2014}}
{{unreferenced|date=June 2014}}
{{Orphan|date=June 2014}}
}}

==Weak Array==

In [[programming languages]], a weak array refers to an [[array]] that contains only [[weak reference]]s or [[weak pointer]]s. As such, the references contained within these arrays do not protect the referenced objects from [[garbage collection]]. This type of array is defined in contrast to a typical (strong) array, which contains references which prevent the referenced objects from garbage collection until the reference itself is garbage collected.

The garbage collector replaces a pointer in the weak array with the [[null pointer]] if it discovers that no strong pointer points to the same object.&lt;ref&gt;
[http://www.lispworks.com/documentation/lw60/LW/html/lw-777.htm &quot;LispWorks User Guide: set-array-weak&quot;].
&lt;/ref&gt;

===Uses===

One of the most common uses for weak arrays is in a data sink model. If a data source is feeding into many sinks, it may be the case that some sinks become redundant over time. If the data source is maintaining a set of strong references to the sinks, none of the redundant sinks can be garbage collected due to the strong nature of the pointers held to them from the source. The problem becomes more severe if the data source is permanent, all sinks referenced by the source in this model may never be removed by the garbage collector due to the permanent strong references in the data source.

If the same model is used with the data source maintaining a weak array of data sink references, any redundant sinks could be collected as and when they are no longer in use, even if the weak pointers in the source are held indefinitely. This allows systems to be much more efficient over their lifetime without changing their structure or adding explicit management code.


{{reflist}}

[[Category:Arrays]]
[[Category:Data structures]]</text>
      <sha1>ds6n42euy73gk1apqsjjal1p3c3kkk1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>System Service Descriptor Table</title>
    <ns>0</ns>
    <id>28395211</id>
    <revision>
      <id>616740837</id>
      <parentid>616740649</parentid>
      <timestamp>2014-07-13T04:13:55Z</timestamp>
      <contributor>
        <username>Zakblade2000</username>
        <id>13917760</id>
      </contributor>
      <text xml:space="preserve" bytes="1462">The '''System Service Descriptor Table''' ('''SSDT''') is an internal [[dispatch table]] within [[Microsoft Windows]].

[[Hooking]] SSDT calls is often used as a technique in both Windows [[rootkit]]s and [[antivirus software]].&lt;ref&gt;{{Cite web|url=http://www.symantec.com/connect/articles/windows-rootkits-2005-part-one|title= Windows rootkits of 2005, part one|work=Symantec|year=2005}}&lt;/ref&gt;&lt;ref name=&quot;ZDNET2010&quot;&gt;{{Cite web|url=http://www.zdnet.co.uk/news/security-threats/2010/05/11/attack-defeats-most-antivirus-software-40088896/ |year=2010|title=Attack defeats 'most' antivirus software|work=ZD Net UK}}&lt;/ref&gt;

In 2010, many computer security products which relied on hooking SSDT calls were shown to be vulnerable to [[Exploit (computer security)|exploits]] using [[race condition]]s to attack the products' security checks.&lt;ref name=&quot;ZDNET2010&quot;/&gt;
==Structure of the SSDT==
&lt;source lang=&quot;cpp&quot;&gt;
typedef struct _KSERVICE_DESCRIPTOR_TABLE
{
    PULONG ServiceTableBase; 
    PULONG ServiceCounterTableBase; 
    ULONG NumberOfServices; 
    PUCHAR ParamTableBase; 
}KSERVICE_DESCRIPTOR_TABLE,*PKSERVICE_DESCRIPTOR_TABLE;
&lt;/source&gt;
The pointer to this structure is '''KeServiceDescriptorTable''', exported by [[ntoskrnl.exe]].
== References ==
{{Reflist}}

[[Category:Windows technology]]
[[Category:Computer security]]
[[Category:Rootkits]]
[[Category:Windows NT kernel]]
[[Category:Data structures]]
[[Category:Windows rootkit techniques]]

{{Windows-stub}}</text>
      <sha1>ns43tvrc4flqilugkyjjtiah7is0rqn</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Implicit data structure</title>
    <ns>0</ns>
    <id>3669635</id>
    <revision>
      <id>626825275</id>
      <parentid>626825271</parentid>
      <timestamp>2014-09-23T22:41:41Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor/>
      <comment>Reverting possible vandalism by [[Special:Contributions/SwagDBoss|SwagDBoss]] to version by Primergrey. False positive? [[User:ClueBot NG/FalsePositives|Report it]]. Thanks, [[User:ClueBot NG|ClueBot NG]]. (1964517) (Bot)</comment>
      <text xml:space="preserve" bytes="2738">{{copy edit|date=June 2014}}

In [[computer science]], an '''implicit [[data structure]]''' stores very little information other than the main data.  These storage schemes retain no pointers, represent the file of n k-key records as a simple n by k array n, and thus retrieve much faster. In implicit data structures, the only structural information to be given is to allow the array to grow and shrink as n. No extra information is required. It is called &quot;implicit&quot; because most of the structure of the elements is expressed implicitly by their order. Another term used interchangeably is '''space efficient'''. Definitions of “very little” are vague and can mean from O(1) to O(log ''n'') extra space. Everything is accessed in-place, by reading bits at various positions in the data. To achieve optimal coding, we use bits instead of bytes. Implicit data structures are frequently also [[succinct data structure]]s.

Although it may be argued that disk space is no longer a problem and improving space utilization should not be a priority, the issue that implicit data structures are designed to improve is [[main memory]] utilization. Hard disks, or any other means of large data capacity, I/O devices, are orders of magnitudes slower than main memory. Hence, the higher percentage of a task can fit in buffers in main memory, with less dependence on slow I/O devices. Also, if a larger chunk of an implicit data structure fits in main memory, the operations performed on it can be faster, even if the asymptotic running time is not as good as its space-oblivious counterpart. Furthermore, since the CPU-cache is usually much smaller than main memory, implicit data structures can improve cache-efficiency and thus running speed, especially if the method used improves locality.

== Implicit data structure for weighted element==
For presentation of elements with different weights, several data structures are required. The structure uses one more location besides those required for values of elements. The first structure supports [[worst case]] search time in terms of rank of weight of elements with respect to set of weights. If the elements are drawn from [[uniform distribution (continuous)|uniform distribution]], then variation of this structure takes average time. The same result obtains for the data structures in which the intervals between consecutive values have access [[probabilities]].

== Examples ==
Examples of implicit data structures include 
*[[Binary heap]]
*[[ Beap ]]

==Further reading ==
* See publications of [http://photon.poly.edu/~hbr/  Hervé Brönnimann], [http://www.cs.uwaterloo.ca/~imunro/ J. Ian Munro], [http://www.cs.purdue.edu/people/gnf Greg Frederickson ]

[[Category:Data structures]]</text>
      <sha1>de6dnmgvn022dzvks0wtbcswt218ujj</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Inversion list</title>
    <ns>0</ns>
    <id>43628014</id>
    <revision>
      <id>623650079</id>
      <parentid>623649496</parentid>
      <timestamp>2014-09-01T01:28:19Z</timestamp>
      <contributor>
        <username>BarbarianAtTheGates</username>
        <id>22293171</id>
      </contributor>
      <comment>Add category Search Algorithms</comment>
      <text xml:space="preserve" bytes="1264">(Inversion list is '''not''' [[inverted index]].)

Inversion list in computer science is a data structure that describes a set of non-overlapping numeric ranges, stored in increasing order.

The set is stored in an array.  Every other element is the first element of a range, and every other element is the first element '''after''' that range (a half-open range).

For example, for ranges 10-14, 25-37, the inversion list would be:

10 15 25 38

To search whether an item belongs to any of the ranges, a [[binary search]] is made.  If the search ends in a &quot;first&quot; element, the searched item is in the set.  If the search ends in an &quot;after&quot; element, or outside the array, the searched item is not in the set.

This data structure is used in many [[Unicode]] implementations for storing Unicode character ranges (like &quot;Greek characters&quot;).

== External links ==
* [http://books.google.com/books?id=wn5sXG8bEAcC&amp;pg=PA504&amp;lpg=PA504&amp;dq=inversion+list+unicode Unicode Demystified (Google Books snippet, pages 504-)]
* [http://macchiato.com/slides/Bits_of_Unicode.ppt Bits of Unicode (Powerpoint presentation, pages 17-22)]
* [http://perldoc.perl.org/Unicode/UCD.html#*prop_invlist()* Unicode::UCD Perl module]

[[Category:Data structures]]
[[Category:Search algorithms]]</text>
      <sha1>fvc1yonuiirmls53368cl066xri11wz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Node (computer science)</title>
    <ns>0</ns>
    <id>998074</id>
    <revision>
      <id>624903381</id>
      <parentid>623311168</parentid>
      <timestamp>2014-09-10T05:51:24Z</timestamp>
      <contributor>
        <username>Alvin Seville</username>
        <id>8629244</id>
      </contributor>
      <comment>removing and categorizing, removing spaces, adding a space</comment>
      <text xml:space="preserve" bytes="7458">{{Refimprove|date=August 2014}}
A '''node''' is a basic unit used in computer science. Nodes are devices or data points on a larger network. Devices such as a personal computer, cell phone, or printer are nodes. When defining nodes on the internet, a node is anything that has an [[IP Address]]. Nodes are individual parts of a larger [[data structure]], such as [[linked lists]] and tree data structures. Nodes contain [[data]] and also may link to other nodes. Links between nodes are often implemented by [[Pointer (computer programming)|pointers]].

[[Image:6n-graf.svg|thumb|250px|&lt;ref&gt;[[Graph theory]]&lt;/ref&gt; The image provides a simplified view of a network, where each of the numbers represents a different node.]]

==Nodes and Trees==
[[Image:binary tree.svg|right|192px|thumb|A simple binary tree of size 9 and height 3, with a root node whose value is 2. The above tree is unbalanced and not sorted.&lt;ref&gt;[[Binary tree]]&lt;/ref&gt;]]
Nodes are often arranged into tree structures. These structures are [[binary trees]].

A node represents the information contained in a single structure. These nodes may contain a value or condition, or possibly serve as another independent data structure. Nodes are represented by a single parent node. The highest point on a tree structure is called a root node which does not have a parent node, but serves as the parent or 'grandparent' of all of the nodes below it in the tree. The height of a node is determined by the longest path from root node to the furthest leaf node, and the height of the tree is equal to the height of the root node. Node depth is determined by the distance between that particular node and the root node. The root node is said to have a depth of zero.&lt;ref&gt;{{cite book | last=Teukolsky | first=Roselyn | title=Barron's AP Computer Science A|publisher=[[Barron's]]|isbn = 978-1-4380-0152-4|year=2013 }}&lt;/ref&gt; Data can be discovered along these network paths. An IP address uses this kind of system of nodes to define its location in a network. &lt;ref&gt;http://www.eecs.berkeley.edu/~bh/ssch18/trees.html&lt;/ref&gt;

===Definitions===
*'''Child''': A child node is a node extending from another node. For example, a computer with internet access could be considered a child node of a node representing the internet. The inverse relationship is that of a '''parent node'''. If node ''C'' is a child of node ''A'', then ''A'' is the parent node of ''C''.
*'''Degree''': the degree of a node is the number of children of the node.
*'''Depth''': the depth of node ''A'' is the length of the path from ''A'' to the root node. The root node is said to have depth 0.
*'''Edge''': the connection between nodes.
*'''Forest''': a set of trees.
*'''Height''': the height of node ''A'' is the length of the longest path through children to a leaf node.
*'''Internal node''': a node with at least one child.
*'''Leaf node''': a node with no children.
*'''Root node''': a node distinguished from the rest of the tree nodes. Usually, it is is depicted as the highest node of the tree.
*'''Sibling nodes''': these are nodes connected to the same parent node.

===XML, HTML, XHTML and DOM [[Extensible Markup Language]], [[HyperText Markup Language]], [[Extensible HyperText Markup Language]], and [[Document Object Model]]===
Another common use of node trees is done through web site programming. In programming, XML is used to communicate information between computer programmers and computers alike. For this reason XML is used to create common [[communication protocols]] used in office-productivity software and it serves as the base for the development of modern web languages like [[XHTML]]. Though similar in how it is approached by a programer, HTML is typically the language used to develop website text and design. While XML, HTML and XHTML provide the language and expression, DOM serves as a translator. DOM&lt;ref&gt;http://www.w3schools.com/dom/dom_intro.asp&lt;/ref&gt;

==== Node Type ====
Different types of nodes in a tree are represented by a specific interfaces. In other words, the node type is defined by how it communicates with other nodes. Each node has a node type property, which specifies the type of node, such as sibling or leaf. 
For example, if the node type property is the constant properties for a node, this property specifies the type of the node. So if a node type property is the constant node ELEMENT_NODE, one can know that this node object is an object Element. This object uses Element interface to define all the methods and properties of that particular node.
Node Types

Different W3C [[World Wide Web Consortium]] node types and descriptions:
*'''Document''' represents the entire document (the root-node of the DOM tree) 	
*'''DocumentFragment''' represents a &quot;lightweight&quot; Document object, which can hold a portion of a document 
*'''DocumentType''' provides an interface to the entities defined for the document 
*'''ProcessingInstruction''' represents a processing instruction 
*'''EntityReference''' represents an entity reference  
*'''Element''' 	represents an element 
*'''Attr''' represents an attribute 
*'''Text''' represents textual content in an element or attribute
*'''CDATASection''' represents a CDATA section in a document (text that will NOT be parsed by a parser)
*'''Comment''' represents a comment 
*'''Entity''' represents an entity 
*'''Notation''' represents a notation declared in the DTD

NodeType 	Named Constant
1 	ELEMENT_NODE
2 	ATTRIBUTE_NODE
3 	TEXT_NODE
4 	CDATA_SECTION_NODE
5 	ENTITY_REFERENCE_NODE
6 	ENTITY_NODE
7 	PROCESSING_INSTRUCTION_NODE
8 	COMMENT_NODE
9 	DOCUMENT_NODE
10 	DOCUMENT_TYPE_NODE
11 	DOCUMENT_FRAGMENT_NODE
12 	NOTATION_NODE

====Node Object====
A node object is represented by a single node in a tree.
Node can be an element node, attribute node, text node, or any type that is described in section &quot;node type&quot;.
Please note that all objects can inherit properties and methods for dealing with parent and child nodes, but not all of the objects have parent or child nodes. For example, text nodes that cannot have child nodes, similar nodes to add child nodes results in a [[DOM]] error.

Objects in the DOM tree may be addressed and manipulated by using methods on the objects. The public interface of a DOM is specified in its application programming interface (API). The history of the Document Object Model is intertwined with the history of the &quot;browser wars&quot; of the late 1990s between Netscape Navigator and Microsoft Internet Explorer, as well as with that of JavaScript and JScript, the first scripting languages to be widely implemented in the layout engines of web browsers.&lt;ref&gt;[[Document Object Model]]&lt;/ref&gt;

==References==
{{Reflist}}
{{refbegin}}

{{refend}}

==External links==
* [http://www.community-of-knowledge.de/beitrag/data-trees-as-a-means-of-presenting-complex-data-analysis/ Data Trees as a Means of Presenting Complex Data Analysis] by Sally Knipe
* [http://www.nist.gov/dads/HTML/tree.html Description] from the [[Dictionary of Algorithms and Data Structures]]
* [http://tree.phi-sci.com STL-like C++ tree class]
* [http://ideainfo.8m.com Description of tree data structures from ideainfo.8m.com]
* [http://wormweb.org/celllineage WormWeb.org: Interactive Visualization of the ''C. elegans'' Cell Tree] - Visualize the entire cell lineage tree of the nematode ''C. elegans'' (javascript)
* [ref : http://www.allisons.org/ll/AlgDS/Tree/]

[[Category:Computer science]]
[[Category:Data structures]]</text>
      <sha1>0mbxx92lt13bfofzi0jwe9ar4zmq3u6</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Irregular matrix</title>
    <ns>0</ns>
    <id>3366265</id>
    <revision>
      <id>555529303</id>
      <parentid>538258358</parentid>
      <timestamp>2013-05-17T16:02:39Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q12797410]]</comment>
      <text xml:space="preserve" bytes="977">An '''irregular matrix''', or '''ragged matrix''', can be described as a [[matrix (mathematics)|matrix]] that has a different number of elements in each row. Ragged matrices are not used in [[linear algebra]], since standard matrix transformations cannot be performed on them, but they are useful as [[array data structure|array]]s in [[computing]]. Irregular matrices are typically stored using [[Iliffe vector]]s.

For example, the following is an irregular matrix:

:&lt;math&gt;
  \begin{bmatrix}
    1 &amp; 31 &amp; 12&amp; -3 \\
    7 &amp; 2 \\
    1 &amp; 2 &amp; 2
  \end{bmatrix}
&lt;/math&gt;

== See also ==

* [[Matrix (mathematics)]]
* [[Regular matrix (disambiguation)]]
* [[Sparse matrix]]
* [[Array data structure]]

==References==

* Paul E. Black, [http://www.nist.gov/dads/HTML/raggedmatrix.html Ragged matrix], from [[Dictionary of Algorithms and Data Structures]], Paul E. Black, ed., [[National Institute of Standards and Technology|NIST]], 2004.

[[Category:Arrays]]
[[Category:Matrices]]</text>
      <sha1>0svto1aqsjefkfqk7jyah8jvqzvxk24</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Bit array</title>
    <ns>0</ns>
    <id>1189937</id>
    <revision>
      <id>625503910</id>
      <parentid>625486522</parentid>
      <timestamp>2014-09-14T10:13:33Z</timestamp>
      <contributor>
        <username>Intgr</username>
        <id>246230</id>
      </contributor>
      <comment>Revert, n is already an integer and ⌈n⌉ can be confused wiht array subscription. Undid revision 625486522 by [[Special:Contributions/Chadha.varun|Chadha.varun]] ([[User talk:Chadha.varun|talk]])</comment>
      <text xml:space="preserve" bytes="17948">{{refimprove|date=December 2010}}

A '''bit array''' (also known as '''bitmap''', '''bitset''', '''bit string''', or '''bit vector''') is an [[array data structure]] that compactly stores [[bit]]s. It can be used to implement a simple [[set data structure]]. A bit array is effective at exploiting bit-level parallelism in hardware to perform operations quickly. A typical bit array stores ''kw'' bits, where ''w'' is the number of bits in the unit of storage, such as a [[byte]] or [[Word (computer architecture)|word]], and ''k'' is some nonnegative integer. If ''w'' does not divide the number of bits to be stored, some space is wasted due to [[Fragmentation (computing)|internal fragmentation]].

== Definition ==
A bit array is a mapping from some domain (almost always a range of integers) to values in the set {0, 1}. The values can be interpreted as dark/light, absent/present, locked/unlocked, valid/invalid, et cetera. The point is that there are only two possible values, so they can be stored in one bit. The array can be viewed as a subset of the domain (e.g. {0, 1, 2, ..., ''n''&amp;minus;1}), where a 1 bit indicates a number in the set and a 0 bit a number not in the set. This set data structure uses about ''n''/''w'' words of space, where ''w'' is the number of bits in each [[Word (computer architecture)|machine word]]. Whether the least significant bit or the most significant bit indicates the smallest-index number is largely irrelevant, but the former tends to be preferred.

== Basic operations ==
Although most machines are not able to address individual bits in memory, nor have instructions to manipulate single bits, each bit in a word can be singled out and manipulated using [[bitwise operation]]s. In particular:
* OR can be used to set a bit to one: 11101010 OR 00000100 = 11101110
* AND can be used to set a bit to zero: 11101010 AND 11111101 = 11101000
* AND together with zero-testing can be used to determine if a bit is set:
::11101010 AND 00000001 = 00000000 = 0
::11101010 AND 00000010 = 00000010 ≠ 0
* XOR can be used to invert or toggle a bit:
::11101010 XOR 00000100 = 11101110
::11101110 XOR 00000100 = 11101010
* NOT can be used to invert all bits.
::NOT 10110010 = 01001101

To obtain the bit mask needed for these operations, we can use a [[Bitwise operation#Bit shifts|bit shift]] operator to shift the number 1 to the left by the appropriate number of places, as well as [[bitwise negation]] if necessary.

Given two bit arrays of the same size representing sets, we can compute their [[union (set theory)|union]], [[intersection (set theory)|intersection]], and [[complement (set theory)|set-theoretic difference]] using ''n''/''w'' simple bit operations each (2''n''/''w'' for difference), as well as the [[Signed number representations#Ones' complement|complement]] of either:
&lt;syntaxhighlight lang=&quot;text&quot;&gt;
for i from 0 to n/w-1
    complement_a[i] := not a[i]
    union[i]        := a[i] or b[i]
    intersection[i] := a[i] and b[i]
    difference[i]   := a[i] and (not b[i])
&lt;/syntaxhighlight&gt;

If we wish to iterate through the bits of a bit array, we can do this efficiently using a doubly nested loop that loops through each word, one at a time. Only ''n''/''w'' memory accesses are required:
&lt;syntaxhighlight lang=&quot;text&quot;&gt;
for i from 0 to n/w-1
    index := 0    // if needed
    word := a[i]
    for b from 0 to w-1
        value := word and 1 ≠ 0
        word := word shift right 1
        // do something with value
        index := index + 1    // if needed
&lt;/syntaxhighlight&gt;
Both of these code samples exhibit ideal [[locality of reference]], which will subsequently receive large performance boost from a data cache. If a cache line is ''k'' words, only about ''n''/''wk'' cache misses will occur.

== More complex operations ==
=== Population / Hamming weight ===
If we wish to find the number of 1 bits in a bit array, sometimes called the population count or Hamming weight, there are efficient branch-free algorithms that can compute the number of bits in a word using a series of simple bit operations. We simply run such an algorithm on each word and keep a running total. Counting zeros is similar. See the [[Hamming weight]] article for examples of an efficient implementation.

=== Sorting ===
Similarly, sorting a bit array is trivial to do in O(''n'') time using [[counting sort]] &amp;mdash; we count the number of ones ''k'', fill the last ''k''/''w'' words with ones, set only the low ''k'' mod ''w'' bits of the next word, and set the rest to zero.

=== Inversion ===

Vertical flipping of a one-bit-per-pixel image, or some FFT algorithms, require to flip
the bits of individual words (so &lt;code&gt;b31 b30 ... b0&lt;/code&gt; becomes &lt;code&gt;b0 ... b30 b31&lt;/code&gt;).
When this operation is not available on the processor, it's still possible to proceed by successive passes, in this example on 32 bits:

&lt;syntaxhighlight lang=&quot;c&quot;&gt;
exchange two 16bit halfwords
exchange bytes by pairs (0xddccbbaa -&gt; 0xccddaabb)
...
swap bits by pairs
swap bits (b31 b30 ... b1 b0 -&gt; b30 b31 ... b0 b1)

The last operation can be written ((x&amp;0x55555555)&lt;&lt;1) | (x&amp;0xaaaaaaaa)&gt;&gt;1)).
&lt;/syntaxhighlight&gt;

=== Find first one ===
The [[find first set]] or ''find first one'' operation identifies the index or position of the least significant ''one'' bit in a word, and has widespread hardware support and efficient algorithms for its computation. When a [[priority queue]] is stored in a bit array, find first one can be used to identify the highest priority element in the queue. To expand a word-size ''find first one'' to longer arrays, one can find the first nonzero word and then run ''find first one'' on that word. The related operations ''find first zero'', ''count leading zeros'', ''count leading ones'', ''count trailing zeros'', ''count trailing ones'', and ''log base 2'' (see [[find first set]]) can also be extended to a bit array in a straightforward manner.

== Compression ==

A bit array is the densest storage for &quot;random&quot; bits, that is, where each bit is equally likely to be 0 or 1, and each one is independent. But most data is not random, so it may be possible to store it more compactly. For example, the data of a typical fax image is not random and can be compressed. [[Run-length encoding]] is commonly used to compress these long streams. However, by compressing bit arrays too aggressively we run the risk of losing the benefits due to bit-level parallelism ([[Vectorization (parallel computing)|vectorization]]). Thus, instead of compressing bit arrays as streams of bits, we might compress them as streams bytes or words (see [[Bitmap_index#Compression|Bitmap index (compression)]]).

The specific compression technique and implementation details can affect performance. Thus, it might be helpful in practice to [https://github.com/lemire/simplebitmapbenchmark benchmark the various implementations].

Examples:
* [http://code.google.com/p/compressedbitset/ compressedbitset]: WAH Compressed BitSet for Java
* [http://code.google.com/p/javaewah/ javaewah]: A compressed alternative to the Java BitSet class (using Enhanced WAH)
* [http://ricerca.mat.uniroma3.it/users/colanton/docs/concise.pdf CONCISE]: COmpressed 'N' Composable Integer Set, another bitmap compression scheme
* [https://github.com/lemire/EWAHBoolArray  EWAHBoolArray]: A compressed bitmap/bitset class in C++
* [http://code.google.com/p/csharpewah/ CSharpEWAH]: compressed bitset class in C#
* [http://code.google.com/p/sparsebitmap/ SparseBitmap]: a simple sparse bitmap implementation in Java

== Advantages and disadvantages ==
Bit arrays, despite their simplicity, have a number of marked advantages over other data structures for the same problems:
* They are extremely compact; few other data structures can store ''n'' independent pieces of data in ''n''/''w'' words.
* They allow small arrays of bits to be stored and manipulated in the register set for long periods of time with no memory accesses.
* Because of their ability to exploit bit-level parallelism, limit memory access, and maximally use the [[data cache]], they often outperform many other data structures on practical data sets, even those that are more asymptotically efficient.
However, bit arrays aren't the solution to everything. In particular:
* Without compression, they are wasteful set data structures for sparse sets (those with few elements compared to their range) in both time and space. For such applications, compressed bit arrays, [[Judy array]]s, [[trie]]s, or even [[Bloom filter]]s should be considered instead.
* Accessing individual elements can be expensive and difficult to express in some languages. If random access is more common than sequential and the array is relatively small, a byte array may be preferable on a machine with byte addressing. A word array, however, is probably not justified due to the huge space overhead and additional cache misses it causes, unless the machine only has word addressing.

== Applications ==
Because of their compactness, bit arrays have a number of applications in areas where space or efficiency is at a premium. Most commonly, they are used to represent a simple group of boolean flags or an ordered sequence of boolean values.

Bit arrays are used for [[priority queue]]s, where the bit at index ''k'' is set if and only if ''k'' is in the queue; this data structure is used, for example, by the [[Linux kernel]], and benefits strongly from a find-first-zero operation in hardware.

Bit arrays can be used for the allocation of [[page (computing)|memory pages]], [[inode]]s, disk sectors, etc. In such cases, the term ''bitmap'' may be used. However, this term is frequently used to refer to [[raster graphics|raster images]], which may use multiple [[color depth|bits per pixel]].

Another application of bit arrays is the [[Bloom filter]], a probabilistic [[set data structure]] that can store large sets in a small space in exchange for a small probability of error. It is also possible to build probabilistic [[hash table]]s based on bit arrays that accept either false positives or false negatives.

Bit arrays and the operations on them are also important for constructing [[succinct data structure]]s, which use close to the minimum possible space. In this context, operations like finding the ''n''th 1 bit or counting the number of 1 bits up to a certain position become important.

Bit arrays are also a useful abstraction for examining streams of [[data compression|compressed]] data, which often contain elements that occupy portions of bytes or are not byte-aligned. For example, the compressed [[Huffman coding]] representation of a single 8-bit character can be anywhere from 1 to 255 bits long.

In [[information retrieval]], bit arrays are a good representation for the [[posting list]]s of very frequent terms. If we compute the gaps between adjacent values in a list of strictly increasing integers and encode them using [[unary coding]], the result is a bit array with a 1 bit in the ''n''th position if and only if ''n'' is in the list. The implied probability of a gap of ''n'' is 1/2&lt;sup&gt;''n''&lt;/sup&gt;. This is also the special case of [[Golomb coding]] where the parameter M is 1; this parameter is only normally selected when -log(2-''p'')/log(1-''p'') ≤ 1, or roughly the term occurs in at least 38% of documents.

== Language support ==
The [[C (programming language)|C programming language]]'s ''[[bitfield]]s'', pseudo-objects found in structs with size equal to some number of bits, are in fact small bit arrays; they are limited in that they cannot span words. Although they give a convenient syntax, the bits are still accessed using bitwise operators on most machines, and they can only be defined statically (like C's static arrays, their sizes are fixed at compile-time). It is also a common idiom for C programmers to use words as small bit arrays and access bits of them using bit operators.  A widely available header file included in the [[X11]] system, xtrapbits.h, is “a portable way for systems to define bit field manipulation of arrays of bits.”  A more explanatory description of aforementioned approach can be found in the [http://c-faq.com/misc/bitsets.html comp.lang.c faq].

In [[C++]], although individual &lt;code&gt;bool&lt;/code&gt;s typically occupy the same space as a byte or an integer, the [[Standard Template Library|STL]] type &lt;code&gt;vector&lt;bool&gt;&lt;/code&gt; is a [[partial template specialization]] in which bits are packed as a space efficiency optimization. Since bytes (and not bits) are the smallest addressable unit in C++, the [] operator does ''not'' return a reference to an element, but instead returns a [[Proxy pattern|proxy reference]]. This might seem a minor point, but it means that &lt;code&gt;vector&lt;bool&gt;&lt;/code&gt; is ''not'' a standard STL container, which is why the use of &lt;code&gt;vector&lt;bool&gt;&lt;/code&gt; is generally discouraged. Another unique STL class, &lt;code&gt;bitset&lt;/code&gt;,&lt;ref name=&quot;c++&quot; /&gt; creates a vector of bits fixed at a particular size at compile-time, and in its interface and syntax more resembles the idiomatic use of words as bit sets by C programmers. It also has some additional power, such as the ability to efficiently count the number of bits that are set. The [[Boost C++ Libraries]] provide a &lt;code&gt;dynamic_bitset&lt;/code&gt; class&lt;ref name=&quot;boost&quot; /&gt; whose size is specified at run-time.

The [[D programming language]] provides bit arrays in both of its competing standard libraries.  In Phobos, they are provided in &lt;code&gt; std.bitmanip&lt;/code&gt;, and in Tango, they are provided in &lt;code&gt;tango.core.BitArray&lt;/code&gt;.  As in C++, the [] operator does not return a reference, since individual bits are not directly addressable on most hardware, but instead returns a &lt;code&gt;bool&lt;/code&gt;.

In [[Java (programming language)|Java]], the class {{Javadoc:SE|java/util|BitSet}} creates a bit array that is then manipulated with functions named after bitwise operators familiar to C programmers. Unlike the &lt;code&gt;bitset&lt;/code&gt; in C++, the Java &lt;code&gt;BitSet&lt;/code&gt; does not have a &quot;size&quot; state (it has an effectively infinite size, initialized with 0 bits); a bit can be set or tested at any index. In addition, there is a class {{Javadoc:SE|java/util|EnumSet}}, which represents a Set of values of an [[enumerated type]] internally as a bit vector, as a safer alternative to bitfields.

The [[.NET Framework]] supplies a &lt;code&gt;BitArray&lt;/code&gt; collection class. It stores boolean values, supports random access and bitwise operators, can be iterated over, and its &lt;code&gt;Length&lt;/code&gt; property can be changed to grow or truncate it.

Although [[Standard ML]] has no support for bit arrays, Standard ML of New Jersey has an extension, the &lt;code&gt;BitArray&lt;/code&gt; structure, in its SML/NJ Library. It is not fixed in size and supports set operations and bit operations, including, unusually, shift operations.

[[Haskell (programming language)|Haskell]] likewise currently lacks standard support for bitwise operations, but both GHC and Hugs provide a &lt;code&gt;Data.Bits&lt;/code&gt; module with assorted bitwise functions and operators, including shift and rotate operations and an &quot;unboxed&quot; array over boolean values may be used to model a Bit array, although this lacks support from the former module.

In [[Perl]], strings can be used as expandable bit arrays. They can be manipulated using the usual bitwise operators (&lt;code&gt;~ | &amp; ^&lt;/code&gt;),&lt;ref&gt;http://perldoc.perl.org/perlop.html#Bitwise-String-Operators&lt;/ref&gt; and individual bits can be tested and set using the ''vec'' function.&lt;ref&gt;http://perldoc.perl.org/functions/vec.html&lt;/ref&gt;

In [[Ruby (programming language)|Ruby]], you can access (but not set) a bit of an integer (&lt;code&gt;Fixnum&lt;/code&gt; or &lt;code&gt;Bignum&lt;/code&gt;) using the bracket operator (&lt;code&gt;[]&lt;/code&gt;), as if it were an array of bits.

Apple's [[Core Foundation]] library contains [http://developer.apple.com/library/mac/#documentation/CoreFoundation/Reference/CFBitVectorRef/Reference/reference.html CFBitVector] and [http://developer.apple.com/library/mac/#documentation/CoreFoundation/Reference/CFMutableBitVectorRef/Reference/reference.html#//apple_ref/doc/uid/20001500 CFMutableBitVector] structures.

[[PL/I]] supports arrays of ''bit strings'' of arbitrary length, which may be either fixed-length or varying.  The array elements may be ''aligned''&amp;mdash; each element begins on a byte or word boundary&amp;mdash; or ''unaligned''&amp;mdash; elements immediately follow each other with no padding.

Hardware description languages such as [[VHDL]], [[Verilog]], and [[SystemVerilog]] natively support bit vectors as these are used to model storage elements like [[Flip-flop (electronics)|flip-flops]], hardware busses and hardware signals in general. In hardware verification languages such as [[OpenVera]], [[e (verification language)|''e'']] and [[SystemVerilog]], bit vectors are used to sample values from the hardware models, and to represent data that is transferred to hardware during simulations.

==See also==
* [[Bit field]]
* [[Bitboard]] Chess and similar games.
* [[Bitmap index]]
* [[Binary numeral system]]
* [[Bitstream]]
* [[Judy array]]

==References==
{{Reflist | refs =
&lt;ref name=&quot;c++&quot;&gt;[http://www.sgi.com/tech/stl/bitset.html std::bitset]&lt;/ref&gt;
&lt;ref name=&quot;boost&quot;&gt;[http://www.boost.org/libs/dynamic_bitset/dynamic_bitset.html boost::dynamic_bitset]&lt;/ref&gt;
}}

==External links==
* [http://bmagic.sourceforge.net BitMagic] C++ library for efficient platform independent bitsets.
* [http://www-cs-faculty.stanford.edu/~knuth/fasc1a.ps.gz mathematical bases] by Pr. D.E.Knuth
* [https://pypi.python.org/pypi/bitarray bitarray module] for Python
* [http://www.gotw.ca/publications/N1185.pdf vector&lt;bool&gt; Is Nonconforming, and Forces Optimization Choice]
* [http://www.gotw.ca/publications/N1211.pdf vector&lt;bool&gt;: More Problems, Better Solutions]
* [https://github.com/noporpoise/BitArray BitArray library] for C

{{Data structures}}

[[Category:Arrays]]
[[Category:Bit data structures]]</text>
      <sha1>lyr5rip295dd4axoeeassjv9s6v6vaw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Sparse array</title>
    <ns>0</ns>
    <id>697257</id>
    <revision>
      <id>625937556</id>
      <parentid>618171587</parentid>
      <timestamp>2014-09-17T12:24:16Z</timestamp>
      <contributor>
        <ip>2.123.221.11</ip>
      </contributor>
      <comment>/* Sparse Array as Linked List */ Doubles don't have a fixed size.</comment>
      <text xml:space="preserve" bytes="3858">{{multiple issues|
{{technical|date=February 2012}}
{{unreferenced|date=October 2011}}
}}

In [[computer science]], a '''sparse array''' is an [[Array data structure|array]] in which most of the elements have the same value (known as the default value—usually 0 or [[nullable type|null]]). The occurrence of zero elements in a large array is inefficient for both computation and [[Computer storage|storage]]. An array in which there is a large number of zero elements is referred to as being sparse. 

In the case of sparse arrays, one can ask for a value from an &quot;empty&quot; array position. If one does this, then for an array of numbers, a value of zero should be returned, and for an array of objects, 
a value of null should be returned.

A [[Naive algorithm|naive]] implementation of an array may allocate space for the entire array, but in the case where there are few non-default values, this implementation is inefficient. Typically the algorithm used instead of an ordinary array is determined by other known features (or statistical features) of the array. For instance, if the sparsity is known in advance or if the elements are arranged according to some function (e.g., the elements occur in blocks).&lt;br /&gt;
A [[Heap_(programming)|heap]] [[malloc|memory allocator]] in a program might choose to store regions of blank space in a linked list rather than storing all of the allocated regions in, say a [[bit array]].&lt;!-- so what? blank space is not an indexed array (sparse or not), indeed --&gt;

== Representation ==
Sparse Array can be represented as

Sparse_Array[{pos1 -&gt; val1, pos2 -&gt; val2,...}] or&lt;br /&gt;
Sparse_Array[{pos1, pos2,...} -&gt; {val1, val2,...}]
  
which yields a sparse array in which values &lt;math&gt;val_i&lt;/math&gt; appear at positions &lt;math&gt;pos_i&lt;/math&gt;.

== Sparse Array as Linked List ==
An obvious question that might be asked is why we need a linked list to represent a sparse array if we can represent it easily using a normal array. The answer to this question lies in the fact that while representing a sparse array as a normal array, a lot of space is allocated for zero or null elements. For example, consider the following array declaration:&lt;br /&gt;
:double arr[1000][1000];
When we define this array, enough space for 1,000,000 doubles is allocated. If each double requires 8 bytes of memory, this array will require 8 million [[byte]]s of memory. Because this is a sparse array, most of its elements will have a value of zero (or null). Hence, defining this array will soak up all this space which will result in wastage of memory (compared to an array in which memory has been allocated only for the nonzero elements). An effective way to overcome this problem is to represent the array using a linked list which requires less memory as only elements having non-zero value are stored. Also, when a linked list is used, the array elements can be accessed through fewer iterations than in a normal array.

A sparse array as a linked list contains nodes linked to each other. In a one-dimensional sparse array, each node consists of an &quot;index&quot; (position) of the non-zero element and the &quot;value&quot; at that position and a node pointer &quot;next&quot; (for linking to the next node), nodes are linked in order as per the index. In the case of a two-dimensional sparse array, each node contains a row index, a column index (which together give its position), a value at that position and a pointer to the next node.

== See also ==
* [[Sparse matrix]]

==External links==
* [http://boost.org/libs/numeric/ublas/doc/vector_sparse.htm Boost sparse vector class]
* [http://portal.acm.org/citation.cfm?id=1363086&amp;jmp=cit&amp;coll=GUIDE&amp;dl=GUIDE  Rodolphe Buda, &quot;Two Dimensional Aggregation Procedure: An Alternative to the Matrix Algebraic Algorithm&quot;, ''Computational Economics'', 31(4), May, pp.397–408, 2008.]

{{Data structures}}

[[Category:Arrays]]</text>
      <sha1>r757s8m5wm4xpzyrgyjdc2qlebr5v21</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Suffix array</title>
    <ns>0</ns>
    <id>1303494</id>
    <revision>
      <id>621528613</id>
      <parentid>621528058</parentid>
      <timestamp>2014-08-16T20:05:45Z</timestamp>
      <contributor>
        <username>Matěj Grabovský</username>
        <id>7208440</id>
      </contributor>
      <minor/>
      <comment>meh, better wording</comment>
      <text xml:space="preserve" bytes="19210">{| class=&quot;infobox&quot; style=&quot;width: 22em&quot;
! colspan=&quot;3&quot; style=&quot;font-size: 125%; text-align: center;&quot; | Suffix array
|-
! [[List of data structures|Type]]
| colspan=&quot;2&quot; | [[Array data structure|Array]]
|-
! Invented by
{{!}} colspan=&quot;2&quot; {{!}}  {{harvtxt|Manber|Myers|1990}}
|-
! colspan=&quot;3&quot; class=&quot;navbox-abovebelow&quot; | [[Time complexity]]&lt;br /&gt;in [[big O notation]]
|-
|
| Average
| Worst case
|-
! Space
| &lt;math&gt;\mathcal{O}(n)&lt;/math&gt;
| &lt;math&gt;\mathcal{O}(n)&lt;/math&gt;
|-
! Construction
| &lt;math&gt;\mathcal{O}(n)&lt;/math&gt;
| &lt;math&gt;\mathcal{O}(n)&lt;/math&gt;
|}

In [[computer science]], a '''suffix array''' is a sorted [[Array data structure|array]] of all [[Suffix (computer science)|suffixes]] of a [[String (computer science)|string]]. It is a data structure used, among others, in full text indices, data compression algorithms and within the field of [[bioinformatics]].{{sfn|Abouelhoda|Kurtz|Ohlebusch|2002}}

Suffix arrays were introduced by {{harvtxt|Manber|Myers|1990}} as a simple, space efficient alternative to [[suffix tree]]s. They have independently been discovered by {{harvtxt|Gonnet|Baeza-Yates|Snider|1992}} under the name ''PAT array''.

== Definition ==
Let &lt;math&gt;S=S[1]S[2]...S[n]&lt;/math&gt; be a string and let &lt;math&gt;S[i,j]&lt;/math&gt; denote the substring of &lt;math&gt;S&lt;/math&gt; ranging from &lt;math&gt;i&lt;/math&gt; to &lt;math&gt;j&lt;/math&gt;.

The suffix array &lt;math&gt;A&lt;/math&gt; of &lt;math&gt;S&lt;/math&gt; is now defined to be an array of integers providing the starting positions of [[Suffix (computer science)|suffixes]] of &lt;math&gt;S&lt;/math&gt; in [[lexicographical order]]. This means, an entry &lt;math&gt;A[i]&lt;/math&gt; contains the starting position of the &lt;math&gt;i&lt;/math&gt;-th smallest suffix in &lt;math&gt;S&lt;/math&gt; and thus for all &lt;math&gt;1 &lt; i \leq n&lt;/math&gt;: &lt;math&gt;S[A[i-1],n] &lt; S[A[i],n]&lt;/math&gt;.

== Example ==

Consider the text &lt;math&gt;S&lt;/math&gt;=&lt;code&gt;banana$&lt;/code&gt; to be indexed:
{| class=&quot;wikitable&quot;
|-
! {{left header}} | i
| 1 || 2 || 3 || 4 || 5 || 6 || 7
|-
! {{left header}} | &lt;math&gt;S[i]&lt;/math&gt;
| b || a || n || a || n || a || $
|}

The text ends with the special sentinel letter &lt;code&gt;$&lt;/code&gt; that is unique and lexicographically smaller than any other character. The text has the following suffixes:

{|   class=&quot;wikitable&quot;
!  align=&quot;left&quot; | Suffix !!  align=&quot;left&quot; | i
|-  class=&quot;odd&quot;
|  align=&quot;left&quot; | banana$ ||  align=&quot;left&quot; | 1
|-  class=&quot;even&quot;
|  align=&quot;left&quot; | anana$ ||  align=&quot;left&quot; | 2
|-  class=&quot;odd&quot;
|  align=&quot;left&quot; | nana$ ||  align=&quot;left&quot; | 3
|-  class=&quot;even&quot;
|  align=&quot;left&quot; | ana$ ||  align=&quot;left&quot; | 4
|-  class=&quot;odd&quot;
|  align=&quot;left&quot; | na$ ||  align=&quot;left&quot; | 5
|-  class=&quot;even&quot;
|  align=&quot;left&quot; | a$ ||  align=&quot;left&quot; | 6
|-  class=&quot;odd&quot;
|  align=&quot;left&quot; | $ ||  align=&quot;left&quot; | 7
|}

These suffixes can be sorted:

{|   class=&quot;wikitable&quot;
!  align=&quot;left&quot; | Suffix !!  align=&quot;left&quot; | i
|-  class=&quot;odd&quot;
|  align=&quot;left&quot; | $ ||  align=&quot;left&quot; | 7
|-  class=&quot;even&quot;
|  align=&quot;left&quot; | a$ ||  align=&quot;left&quot; | 6
|-  class=&quot;odd&quot;
|  align=&quot;left&quot; | ana$ ||  align=&quot;left&quot; | 4
|-  class=&quot;even&quot;
|  align=&quot;left&quot; | anana$ ||  align=&quot;left&quot; | 2
|-  class=&quot;odd&quot;
|  align=&quot;left&quot; | banana$ ||  align=&quot;left&quot; | 1
|-  class=&quot;even&quot;
|  align=&quot;left&quot; | na$ ||  align=&quot;left&quot; | 5
|-  class=&quot;odd&quot;
|  align=&quot;left&quot; | nana$ ||  align=&quot;left&quot; | 3
|}

The suffix array &lt;math&gt;A&lt;/math&gt; contains the starting positions of these sorted suffixes:

{| class=&quot;wikitable&quot;
! {{left header}} | i
| 1 || 2 || 3 || 4 || 5 || 6 || 7
|-
! {{left header}} | &lt;math&gt;A[i]&lt;/math&gt;
| 7 || 6 || 4 || 2 || 1 || 5 || 3
|}

Complete array with suffixes itself :

{| class=&quot;wikitable&quot;
! {{left header}} | i
| 1 || 2 || 3 || 4 || 5 || 6 || 7
|-
! {{left header}} | &lt;math&gt;A[i]&lt;/math&gt;
| 7 || 6 || 4 || 2 || 1 || 5 || 3
|-
! {{left header}} | 1
| $ || a || a || a || b || n || n
|-
! {{left header}} | 2
|   || $ || n || n || a || a || a
|-
! {{left header}} | 3
|   ||   || a || a || n || $ || n
|-
! {{left header}} | 4
|   ||   || $ || n || a ||  || a
|-
! {{left header}} | 5
|   ||   ||  || a || n ||  || $
|-
! {{left header}} | 6
|   ||   ||  || $ || a ||  || 
|-
! {{left header}} | 7
|   ||   ||  ||  || $ ||  || 

|}

So for example, &lt;math&gt;A[3]&lt;/math&gt; contains the value 4, and therefore refers to the suffix starting at position 4 within &lt;math&gt;S&lt;/math&gt;, which is the suffix &lt;code&gt;ana$&lt;/code&gt;.

== Correspondence to suffix trees ==

Suffix arrays are closely related to [[suffix tree]]s:

* Suffix arrays can be constructed by performing a [[depth-first traversal]] of a suffix tree. The suffix array corresponds to the leaf-labels given in the order in which these are visited during the traversal, if edges are visited in the lexicographical order of their first character.
* A suffix tree can be constructed in linear time by using a combination of suffix and [[LCP array]]. For a description of the algorithm, see the [[LCP array#Suffix Tree Construction|corresponding section]] in the [[LCP array]] article.

It has been shown that every suffix tree algorithm can be systematically replaced with an algorithm that uses a suffix array enhanced with additional information (such as the [[LCP array]]) and solves the same problem in the same time complexity.{{sfn|Abouelhoda|Kurtz|Ohlebusch|2004}}
Advantages of suffix arrays over suffix trees include improved space requirements, simpler linear time construction algorithms (e.g., compared to [[Ukkonen's algorithm]]) and improved cache locality.{{sfn|Abouelhoda|Kurtz|Ohlebusch|2002}}

== Space Efficiency ==

Suffix arrays were introduced by {{harvtxt|Manber|Myers|1990}} in order to improve over the space requirements of [[suffix tree]]s: Suffix arrays store &lt;math&gt;n&lt;/math&gt; integers. Assuming an integer requires &lt;math&gt;4&lt;/math&gt; bytes, a suffix array requires &lt;math&gt;4n&lt;/math&gt; bytes in total. This is significantly less than the &lt;math&gt;20n&lt;/math&gt; bytes which are required by a careful suffix tree implementation.{{sfn|Kurtz|1999}}

However, in certain applications, the space requirements of suffix arrays may still be prohibitive. Analyzed in bits, a suffix array requires &lt;math&gt;\mathcal{O}(n \log n)&lt;/math&gt; space, whereas the original text over an alphabet of size &lt;math&gt;\sigma&lt;/math&gt; does only require &lt;math&gt;\mathcal{O}(n \log \sigma)&lt;/math&gt; bits.
For a human genome with &lt;math&gt;\sigma = 4&lt;/math&gt; and &lt;math&gt;n = 3.4 \times 10^9&lt;/math&gt; the suffix array would therefore occupy about 16 times more memory than the genome itself.

Such discrepancies motivated a trend towards [[compressed suffix array]]s and [[BWT]]-based compressed full-text indices such as the [[FM-index]]. These data structures require only space within the size of the text or even less.

== Construction Algorithms ==

A suffix tree can be built in &lt;math&gt;\mathcal{O}(n)&lt;/math&gt; and can be converted into a suffix array by traversing the tree depth-first also in &lt;math&gt;\mathcal{O}(n)&lt;/math&gt;, so there exist algorithms that can build a suffix array in &lt;math&gt;\mathcal{O}(n)&lt;/math&gt;.  

A naive approach to construct a suffix array is to use a [[Comparison sort|comparison-based sorting algorithm]]. These algorithms require &lt;math&gt;\mathcal{O}(n \log n)&lt;/math&gt; suffix comparisons, but a suffix comparison runs in &lt;math&gt;\mathcal{O}(n)&lt;/math&gt; time, so the overall runtime of this approach is &lt;math&gt;\mathcal{O}(n^2 \log n)&lt;/math&gt;.

More advanced algorithms take advantage of the fact that the suffixes to be sorted are not arbitrary strings but related to each other. These algorithms strive to achieve the following goals:{{sfn|Puglisi|Smyth|Turpin|2007}}
* minimal asymptotic complexity &lt;math&gt;\Theta(n)&lt;/math&gt;
* lightweight in space, meaning little or no working memory beside the text and the suffix array itself is needed
* fast in practice

One of the first algorithms to achieve all goals is the SA-IS algorithm of {{harvtxt|Nong|Zhang|Chan|2009}}. The algorithm is also rather simple (&amp;lt; 100 [[Source lines of code|LOC]]) and can be enhanced to simultaneously construct the [[LCP array]].{{sfn|Fischer|2011}} The SA-IS algorithm is one of the fastest known suffix array construction algorithms. A careful [https://sites.google.com/site/yuta256/sais implementation by Yuta Mori] outperforms most other linear or super-linear construction approaches.

Beside time and space requirements, suffix array construction algorithms are also differentiated by their supported [[Alphabet (computer science)|alphabet]]:  ''constant alphabets'' where the alphabet size is bound by a constant, ''integer alphabets'' where characters are integers in a range depending on &lt;math&gt;n&lt;/math&gt; and ''general alphabets'' where only character comparisons are allowed.{{sfn|Burkhardt|Kärkkäinen|2003}}

Most suffix array construction algorithms are based on one of the following approaches:{{sfn|Puglisi|Smyth|Turpin|2007}}
* ''Prefix doubling'' algorithms are based on a strategy of {{harvtxt|Karp|Miller|Rosenberg|1972}}. The idea is to find prefixes that honor the lexicographic ordering of suffixes. The assessed prefix length doubles in each iteration of the algorithm until a prefix is unique and provides the rank of the associated suffix.
* ''Recursive'' algorithms follow the approach of the suffix tree construction algorithm by {{harvtxt|Farach|1997}} to recursively sort a subset of suffixes. This subset is then used to infer a suffix array of the remaining suffixes. Both of these suffix arrays are then merged to compute the final suffix array.
* ''Induced copying'' algorithms are similar to recursive algorithms in the sense that they use an already sorted subset to induce a fast sort of the remaining suffixes. The difference is that these algorithms favor iteration over recursion to sort the selected suffix subset. A survey of this diverse group of algorithms has been put together by {{harvtxt|Puglisi|Smyth|Turpin|2007}}.

A well-known recursive algorithm for integer alphabets is the ''DC3 / skew'' algorithm of {{harvtxt|Kärkkäinen|Sanders|2003}}. It runs in linear time and has successfully been used as the basis for parallel{{sfn|Kulla|Sanders|2007}} and [[External memory algorithm|external memory]]{{sfn|Dementiev|Kärkkäinen|Mehnert|Sanders|2008}} suffix array construction algorithms.

Recent work by {{harvtxt|Salson|Lecroq|Léonard|Mouchard|2009}} proposes an algorithm for updating the suffix array of a text that has been edited instead of rebuilding a new suffix array from scratch. Even if the theoretical worst-case time complexity is &lt;math&gt;\mathcal{O}(n \log n)&lt;/math&gt;, it appears to perform well in practice: experimental results from the authors showed that their implementation of dynamic suffix arrays is generally more efficient than rebuilding when considering the insertion of a reasonable number of letters in the original text.

== Applications ==

The suffix array of a string can be used as an [[Index (search engine)|index]] to quickly locate every occurrence of a substring pattern &lt;math&gt;P&lt;/math&gt; within the string &lt;math&gt;S&lt;/math&gt;. Finding every occurrence of the pattern is equivalent to finding every suffix that begins with the substring. Thanks to the lexicographical ordering, these suffixes will be grouped together in the suffix array and can be found efficiently with two [[binary search]]es. The first search locates the starting position of the interval, and the second one determines the end position:

&lt;source lang=&quot;python&quot;&gt;
    def search(P):
        l = 0; r = n
        while l &lt; r:
            mid = (l+r) / 2
            if P &gt; suffixAt(A[mid]):
                l = mid + 1
            else:
                r = mid
        s = l; r = n
        while l &lt; r:
            mid = (l+r) / 2
            if P &lt; suffixAt(A[mid]):
                r = mid
            else:
                l = mid + 1
        return (s, r)&lt;/source&gt;
Finding the substring pattern &lt;math&gt;P&lt;/math&gt; of length &lt;math&gt;m&lt;/math&gt; in the string &lt;math&gt;S&lt;/math&gt; of length &lt;math&gt;n&lt;/math&gt; takes &lt;math&gt;\mathcal{O}(m \log n)&lt;/math&gt; time, given that a single suffix comparison needs to compare &lt;math&gt;m&lt;/math&gt; characters. {{harvtxt|Manber|Myers|1990}} describe how this bound can be improved to &lt;math&gt;\mathcal{O}(m + \log n)&lt;/math&gt; time using [[LCP array|LCP]] information. The idea is that a pattern comparison does not need to re-compare certain characters, when it is already known that these are part of the longest common prefix of the pattern and the current search interval.  {{harvtxt|Abouelhoda|Kurtz|Ohlebusch|2004}} improve the bound even further and achieve a search time of &lt;math&gt;\mathcal{O}(m)&lt;/math&gt; as known from [[suffix tree]]s.

Suffix sorting algorithms can be used to compute the [[Burrows–Wheeler transform|Burrows–Wheeler transform (BWT)]]. The [[Burrows–Wheeler transform|BWT]] requires sorting of all cyclic permutations of a string. If this string ends in a special end-of-string character that is lexicographically smaller than all other character (i.e., $), then the order of the sorted rotated [[Burrows–Wheeler transform|BWT]] matrix corresponds to the order of suffixes in a suffix array. The [[Burrows–Wheeler transform|BWT]] can therefore be computed in linear time by first constructing a suffix array of the text and then deducing the [[Burrows–Wheeler transform|BWT]] string: &lt;math&gt;BWT[i] = S[A[i]-1]&lt;/math&gt;.

Suffix arrays can also be used to look up substrings in [[Example-Based Machine Translation]], demanding much less storage than a full [[phrase table]] as used in [[Statistical machine translation]].

Many additional applications of the suffix array require the [[LCP array]]. Some of these are detailed in the [[LCP array#Applications|application section]] of the latter.

== Notes ==
{{Reflist}}

== References ==
* {{cite journal|ref=harv
 | doi=10.1016/S1570-8667(03)00065-0
 | title=Replacing suffix trees with enhanced suffix arrays
 | year=2004
 | last1=Abouelhoda | first1=Mohamed Ibrahim
 | last2=Kurtz | first2=Stefan
 | last3=Ohlebusch | first3=Enno
 | journal=Journal of Discrete Algorithms
 | volume=2
 | pages=53}}
* {{cite conference|ref=harv
 | title = Suffix arrays: a new method for on-line string searches
 | year = 1990
 | conference = First Annual ACM-SIAM Symposium on Discrete Algorithms
 | pages = 319–327
 | url = http://dl.acm.org/citation.cfm?id=320176.320218
 | last1 = Manber	 | first1 =  Udi | author1-link = Udi_Manber
 | last2 = Myers	 | first2 =  Gene | author2-link = Gene_Myers
}}
* {{cite journal|ref=harv
 | title = Suffix arrays: a new method for on-line string searches
 | year = 1993
 | journal = SIAM Journal on Computing
 | volume = 22
 | pages = 935-948
 | doi = 10.1137/0222058
 | url = http://dl.acm.org/citation.cfm?id=320176.320218
 | last1 = Manber	 | first1 =  Udi | author1-link = Udi_Manber
 | last2 = Myers	 | first2 =  Gene | author2-link = Gene_Myers
}}
* {{cite journal|ref=harv
 | title = New indices for text: PAT trees and PAT arrays
 | year = 1992
 | journal = Information retrieval: data structures and algorithms
 | last1 = Gonnet	 | first1 =  G.H
 | last2 =  Baeza-Yates	 | first2 =  R.A
 | last3 = Snider        | first3 = T
}}
* {{cite journal|ref=harv
 | title = Reducing the space requirement of suffix trees
 | year = 1999
 | journal = Software-Practice and Experience
 | volume = 29
 | issue = 13
 | last1 = Kurtz | first1 =  S
 | doi=10.1002/(SICI)1097-024X(199911)29:13&lt;1149::AID-SPE274&gt;3.0.CO;2-O
 | pages=1149
}}
* {{cite journal|ref=harv
 | doi = 10.1007/3-540-45784-4_35|chapter=The Enhanced Suffix Array and Its Applications to Genome Analysis|title=Algorithms in Bioinformatics|series=Lecture Notes in Computer Science|year=2002|last1=Abouelhoda|first1=Mohamed Ibrahim|last2=Kurtz|first2=Stefan|last3=Ohlebusch|first3=Enno|isbn=978-3-540-44211-0|volume=2452|pages=449
}}
* {{cite journal|ref=harv
 | doi = 10.1145/1242471.1242472|title=A taxonomy of suffix array construction algorithms|year=2007|last1=Puglisi|first1=Simon J.|last2=Smyth|first2=W. F.|last3=Turpin|first3=Andrew H.|journal=ACM Computing Surveys|volume=39|issue=2|pages=4
}}
* {{cite journal|ref=harv
 | doi = 10.1109/DCC.2009.42|chapter=Linear Suffix Array Construction by Almost Pure Induced-Sorting|title=2009 Data Compression Conference|year=2009|last1=Nong|first1=Ge|last2=Zhang|first2=Sen|last3=Chan|first3=Wai Hong|isbn=978-0-7695-3592-0|pages=193
}}
* {{cite journal|ref=harv
 | doi = 10.1007/978-3-642-22300-6_32|chapter=Inducing the LCP-Array|title=Algorithms and Data Structures|series=Lecture Notes in Computer Science|year=2011|last1=Fischer|first1=Johannes|isbn=978-3-642-22299-3|volume=6844|pages=374
}}
* {{cite journal|ref=harv
 | doi = 10.1016/j.jda.2009.02.007|title=Dynamic extended suffix arrays|year=2010|last1=Salson|first1=M.|last2=Lecroq|first2=T.|last3=Léonard|first3=M.|last4=Mouchard|first4=L.|journal=Journal of Discrete Algorithms|volume=8|issue=2|pages=241
}}
* {{cite journal|ref=harv
  | doi = 10.1007/3-540-44888-8_5|chapter=Fast Lightweight Suffix Array Construction and Checking|title=Combinatorial Pattern Matching|series=Lecture Notes in Computer Science|year=2003|last1=Burkhardt|first1=Stefan|last2=Kärkkäinen|first2=Juha|isbn=978-3-540-40311-1|volume=2676|pages=55
}}
* {{cite journal|ref=harv
  | doi = 10.1145/800152.804905|chapter=Rapid identification of repeated patterns in strings, trees and arrays|title=Proceedings of the fourth annual ACM symposium on Theory of computing  - STOC '72|year=1972|last1=Karp|first1=Richard M.|last2=Miller|first2=Raymond E.|last3=Rosenberg|first3=Arnold L.|pages=125
}}
* {{cite journal|ref=harv
  | doi = 10.1109/SFCS.1997.646102|chapter=Optimal suffix tree construction with large alphabets|title=Proceedings 38th Annual Symposium on Foundations of Computer Science|year=1997|last1=Farach|first1=M.|isbn=0-8186-8197-7|pages=137
}}
* {{cite journal|ref=harv
  | doi = 10.1007/3-540-45061-0_73|chapter=Simple Linear Work Suffix Array Construction|title=Automata, Languages and Programming|series=Lecture Notes in Computer Science|year=2003|last1=Kärkkäinen|first1=Juha|last2=Sanders|first2=Peter|isbn=978-3-540-40493-4|volume=2719|pages=943
}}
* {{cite journal|ref=harv
  | doi = 10.1145/1227161.1402296|title=Better external memory suffix array construction|year=2008|last1=Dementiev|first1=Roman|last2=Kärkkäinen|first2=Juha|last3=Mehnert|first3=Jens|last4=Sanders|first4=Peter|journal=Journal of Experimental Algorithmics|volume=12|pages=1
}}
* {{cite journal|ref=harv
  | doi = 10.1016/j.parco.2007.06.004|title=Scalable parallel suffix array construction|year=2007|last1=Kulla|first1=Fabian|last2=Sanders|first2=Peter|journal=Parallel Computing|volume=33|issue=9|pages=605
}}

== External links ==
* [http://algs4.cs.princeton.edu/63suffix/SuffixArray.java.html Suffix Array in Java]
* [http://code.google.com/p/compression-code/downloads/list Suffix sorting module for BWT in C code]
* [http://www.codeodor.com/index.cfm/2007/12/24/The-Suffix-Array/1845 Suffix Array Implementation in Ruby]
* [http://sary.sourceforge.net/index.html.en Suffix array library and tools]
* [http://pizzachili.dcc.uchile.cl/ Project containing various Suffix Array c/c++ Implementations with a unified interface]
* [http://code.google.com/p/libdivsufsort/ A fast, lightweight, and robust C API library to construct the suffix array]
* [http://code.google.com/p/pysuffix/ Suffix Array implementation in Python]

[[Category:Arrays]]
[[Category:Substring indices]]
[[Category:String data structures]]</text>
      <sha1>3ov0zoatrvdvyzoh0qbs413js08ehxd</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>C-list (computer security)</title>
    <ns>0</ns>
    <id>5952938</id>
    <revision>
      <id>578474703</id>
      <parentid>486964123</parentid>
      <timestamp>2013-10-23T23:37:09Z</timestamp>
      <contributor>
        <username>ChrisGualtieri</username>
        <id>16333418</id>
      </contributor>
      <minor/>
      <comment>General Fixes using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="983">In [[Object-capability model|capability-based computer security]], a '''C-list''' is an [[array data structure|array]] of capabilities, usually associated with a [[process (computing)|process]] and maintained by the [[kernel (computer science)|kernel]].  The program running in the process does not manipulate capabilities directly, but refers to them via '''C-list indexes'''—integers indexing into the C-list.

The [[file descriptor|file descriptor table]] in [[Unix]] is an example of a C-list.  Unix processes do not manipulate [[file descriptor]]s directly, but refer to them via file descriptor numbers, which are C-list indexes.

In the [[KeyKOS]] and [[Extremely Reliable Operating System|EROS]] operating systems, a process's capability registers constitute a C-list.&lt;ref&gt;[http://www.cap-lore.com/CapTheory/Glossary.html Glossary, cap-lore.com]&lt;/ref&gt;

== References ==
{{Reflist}}

{{Object-capability security}}

[[Category:Arrays]]
[[Category:Operating system security]]</text>
      <sha1>5uin7q1byetb701xt64xxtv7frxjdci</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>VList</title>
    <ns>0</ns>
    <id>558740</id>
    <revision>
      <id>569987139</id>
      <parentid>542148647</parentid>
      <timestamp>2013-08-24T12:21:06Z</timestamp>
      <contributor>
        <username>Siskus</username>
        <id>4718163</id>
      </contributor>
      <text xml:space="preserve" bytes="6408">{{distinguish2|[[Vlist]], a municipality in the Netherlands}}
In [[computer science]], the '''VList''' is a [[persistent data structure|persistent]] [[data structure]] designed by [[Phil Bagwell]] in 2002 that combines the fast indexing of [[Array data structure|arrays]] with the easy extension of [[cons]]-based (or singly linked) [[linked list]]s.&lt;ref&gt; {{Citation | title=Fast Functional Lists, Hash-Lists, Deques and Variable Length Arrays | first1=Phil | last1=Bagwell | year=2002 | publisher=EPFL | url=http://infoscience.epfl.ch/record/64410/files/techlists.pdf}} &lt;/ref&gt;

Like arrays, VLists have constant-time lookup on average and are highly compact, requiring only [[Big-O notation|O]](log ''n'') storage for pointers, allowing them to take advantage of [[locality of reference]].  Like singly linked or cons-based lists, they are [[persistent data structure|persistent]], and elements can be added to or removed from the front in constant time. Length can also be found in O(log ''n'') time.

The primary operations of a VList are:
* Locate the ''k''th element (O(1) average, O(log ''n'') worst-case)
* Add an element to the front of the VList (O(1) average, with an occasional allocation)
* Obtain a new array beginning at the second element of an old array (O(1))
* Compute the length of the list (O(log ''n''))

The primary advantage VLists have over arrays is that different updated versions of the VList automatically share structure. Because VLists are immutable, they are most useful in [[functional programming language]]s, where their efficiency allows a purely functional implementation of data structures traditionally thought to require mutable arrays, such as [[hash table]]s. 

However, VLists also have a number of disadvantages over their competitors:
* While immutability is a benefit, it is also a drawback, making it inefficient to modify elements in the middle of the array.
* Access near the end of the list can be as expensive as O(log ''n''); it is only constant on average over all elements. This is still, however, much better than performing the same operation on cons-based lists.
* Wasted space in the first block is proportional to ''n''.  This is similar to linked lists, but there are data structures with less overhead.  When used as a [[Persistent data structure|fully persistent data structure]], the overhead may be considerably higher and this data structure may not be appropriate.

== Structure ==

The underlying structure of a VList can be seen as a singly linked list of arrays whose sizes decrease geometrically; in its simplest form, the first contains the first half of the elements in the list, the next the first half of the remainder, and so on. Each of these blocks stores some information such as its size and a pointer to the next.

&lt;center&gt;[[File:VList example diagram.png|A diagram of a simple VList]]&lt;br&gt;
&lt;small&gt;''An array-list. The reference shown refers to the VList (2,3,4,5,6).''&lt;/small&gt;
&lt;/center&gt;

The average constant-time indexing operation comes directly from this structure; given a random valid index, we simply observe the size of the blocks and follow pointers until we reach the one it should be in. The chance is 1/2 that it falls in the first block and we need not follow any pointers; the chance is 1/4 we have to follow only one, and so on, so that the expected number of pointers we have to follow is:

&lt;math&gt;\sum_{i=1}^{\lceil log_2 n \rceil} \frac{i-1}{2^i} &lt; \sum_{i=1}^{\infty} \frac{i-1}{2^i} = 1.&lt;/math&gt;

Any particular reference to a VList is actually a &lt;''base'', ''offset''&gt; pair indicating the position of its first element in the data structure described above. The ''base'' part indicates which of the arrays its first element falls in, while the ''offset'' part indicates its index in that array. This makes it easy to &quot;remove&quot; an element from the front of the list; we simply increase the offset, or increase the base and set the offset to zero if the offset goes out of range. If a particular reference is the last to leave a block, the block will be [[garbage collection (computer science)|garbage-collected]] if such facilities are available, or otherwise must be freed explicitly.

Because the lists are constructed incrementally, the first array in the array list may not contain twice as many values as the next one, although the rest do; this does not significantly impact indexing performance. We nevertheless allocate this much space for the first array, so that if we add more elements to the front of the list in the future we can simply add them to this list and update the size. If the array fills up, we create a new array, twice as large again as this one, and link it to the old first array.

The trickier case, however, is adding a new item to the front of a list, call it A, which starts somewhere in the middle of the array-list data structure. This is the operation that allows VLists to be persistent. To accomplish this, we create a new array, and we link it to the array containing the first element of A. The new array must also store the offset of the first element of A in that array. Then, we can proceed to add any number of items we like to our new array, and any references into this new array will point to VLists which share a tail of values with the old array. Note that with this operation it is possible to create VLists which degenerate into simple linked lists, thus obliterating the performance claims made at the beginning of this article.

==Variants==
VList may be modified to support the implementation of a [[growable array]].  In the application of a [[growable array]], [[immutable object|immutability]] is no longer required.  Instead of growing at the beginning of the list, the ordering interpretation is reversed to allow growing at the end of the array.

==See also==
* [[Purely functional]]

== References ==
{{reflist}}

==External links==
* [http://www.ootl.org/doc/vlist.html C++ implementation of VLists]
* [http://www.codeproject.com/KB/collections/vlist.aspx C# implementation of VLists]
* [http://git.savannah.gnu.org/cgit/guile.git/tree/module/ice-9/vlist.scm Scheme implementation of VLists and VList-based hash lists] for [[GNU Guile]]
* [http://planet.plt-scheme.org/package-source/krhari/pfds.plt/1/3/vlist.ss Scheme (Typed Racket) implementation of VLists] for [[Racket (programming language)|Racket]]

[[Category:Arrays]]
[[Category:Linked lists]]</text>
      <sha1>p4xoveixsmx1odb0k7328uu4x2hiqar</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hashed array tree</title>
    <ns>0</ns>
    <id>12673184</id>
    <revision>
      <id>594714949</id>
      <parentid>586823328</parentid>
      <timestamp>2014-02-09T19:58:04Z</timestamp>
      <contributor>
        <ip>24.126.215.38</ip>
      </contributor>
      <comment>Replace ^1/2 with sqrt symbol</comment>
      <text xml:space="preserve" bytes="4821">In [[computer science]], a '''hashed array tree''' (HAT) is a [[dynamic array]] data-structure published by Edward Sitarski in 1996,&lt;ref name=&quot;sitarski96&quot;&gt;{{Citation | title=HATs: Hashed array trees | contribution=Algorithm Alley | journal=Dr. Dobb's Journal | date=September 1996 | first1=Edward | last1=Sitarski | volume=21 | issue=11 | url=http://www.ddj.com/architect/184409965?pgno=5}}&lt;/ref&gt; maintaining an array of separate memory fragments (or &quot;leaves&quot;) to store the data elements, unlike simple dynamic arrays which maintain their data in one contiguous memory area. Its primary objective is to reduce the amount of element copying due to automatic array resizing operations, and to improve memory usage patterns.

Whereas simple dynamic arrays based on [[Dynamic array#Geometric expansion and amortized cost|geometric expansion]] waste linear (Ω(''n'')) space, where ''n'' is the number of elements in the [[Array data structure|array]], hashed array trees waste only order ''O''({{sqrt|''n''}}) storage space. An optimization of the algorithm allows to eliminate data copying completely, at a cost of increasing the wasted space.

It can perform [[Random access|access]] in constant ([[Big O notation|O]](1)) time, though slightly slower than simple dynamic arrays. The algorithm has O(1) amortized performance when appending a series of objects to the end of a hashed array tree. Contrary to its name, it does not use [[hash function]]s.

[[Image:HashedArrayTree16.svg|thumb|219px|right|A full Hashed Array Tree with 16 elements]]

==Definitions==
As defined by Sitarski, a hashed array tree has a top-level directory containing a [[power of two]] number of leaf arrays.  All leaf arrays are the same size as the top-level directory.  This structure superficially resembles a [[hash table]] with array-based collision chains, which is the basis for the name ''hashed array tree''. A full hashed array tree can hold ''m''&lt;sup&gt;2&lt;/sup&gt; elements, where ''m'' is the size of the top-level directory.&lt;ref name=&quot;sitarski96&quot; /&gt; The use of powers of two enables faster physical addressing through bit operations instead of arithmetic operations of [[quotient]] and [[remainder]]&lt;ref name=&quot;sitarski96&quot; /&gt; and ensures the O(1) amortized performance of append operation in the presence of occasional global array copy while expanding.

==Expansions and size reductions==
In a usual dynamic array [[Dynamic array#Geometric expansion and amortized cost|geometric expansion]] scheme, the array is reallocated as a whole sequential chunk of memory with the new size a double of its current size (and the whole data is then moved to the new location). This ensures O(1) amortized operations at a cost of O(n) wasted space, as the enlarged array is filled to the half of its new capacity.

When a hashed array tree is full, its directory and leaves must be restructured to twice their prior size to accommodate additional append operations. The data held in old structure is then moved into the new locations. Only one new leaf is then allocated and added into the top array which thus becomes filled only to a quarter of its new capacity. All the extra leaves are not allocated yet, and will only be allocated when needed, thus wasting only ''O''({{sqrt|''n''}}) of storage.

There are multiple alternatives for reducing size: when a Hashed Array Tree is one eighth full, it can be restructured to a smaller, half-full hashed array tree; another option is only freeing unused leaf arrays, without resizing the leaves. Further optimizations include adding new leaves without resizing, growing the directory array as needed, possibly through geometric expansion. This would eliminate the need for data copying completely, at the cost of making the wasted space ''O''(''n''), with a small coefficient, and only performing restructuring when a set threshold overhead is reached.&lt;ref name=&quot;sitarski96&quot; /&gt;

==Related data structures==
Brodnik et al.&lt;ref&gt;{{Citation | title=Resizable Arrays in Optimal Time and Space | date=Technical Report CS-99-09 | url=http://www.cs.uwaterloo.ca/research/tr/1999/09/CS-99-09.pdf | year=1999 | first1=Andrej | last1=Brodnik | first2=Svante | last2=Carlsson | first5=ED | last5=Demaine | first4=JI | last4=Munro | first3=Robert | last3=Sedgewick | author3-link=Robert Sedgewick (computer scientist) | publisher=Department of Computer Science, University of Waterloo}}&lt;/ref&gt; presented a [[dynamic array]] algorithm with a similar space wastage profile to hashed array trees.  Brodnik's implementation retains previously allocated leaf arrays, with a more complicated address calculation function as compared to hashed array trees.

===See also===
* [[Dynamic array]]
* [[Unrolled linked list]]
* [[VList]]
* [[B-tree]]

==References==
{{reflist}}

&lt;!--  --&gt;

{{Data structures}}

[[Category:Arrays]]</text>
      <sha1>rsjvkmp551ta1dhkz79ebawybgk09af</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Row-major order</title>
    <ns>0</ns>
    <id>1620786</id>
    <revision>
      <id>623272858</id>
      <parentid>623271059</parentid>
      <timestamp>2014-08-29T05:36:02Z</timestamp>
      <contributor>
        <username>Bgwhite</username>
        <id>264323</id>
      </contributor>
      <comment>Do [[Wikipedia:GENFIXES|general fixes]] and cleanup. -, [[WP:AWB/T|typo(s) fixed]]: ie, → i.e.,, eg  → e.g.  (2) using [[Project:AWB|AWB]] (10396)</comment>
      <text xml:space="preserve" bytes="5665">In computing, '''row-major order''' and '''column-major order''' describe methods for arranging multidimensional [[array (computing)|arrays]] in linear storage such as [[RAM|memory]].

The difference is simply this: in row-major order, rows of the array are contiguous in memory; in column-major order, the columns are contiguous.

Array layout is critical for correctly passing arrays between programs written in different languages. It is also important for performance when traversing an array because accessing array elements that are contiguous in memory is usually faster than accessing elements which are not, due to [[Cache (computing)|caching]].&lt;ref&gt;https://software.intel.com/sites/products/documentation/hpc/composerxe/en-us/2011Update/fortran/lin/optaps/fortran/optaps_prg_arrs_f.htm&lt;/ref&gt;  In some media such as tape or [[flash memory]], accessing sequentially is orders of magnitude faster than nonsequential access.

==Explanation and example==

Following conventional [[matrix (mathematics)#Notation|matrix notation]], rows are numbered by the first index of a two-dimensional array and columns by the second index, i.e., ''a''&lt;sub&gt;''1,2''&lt;/sub&gt; is the second element of the first row, counting downwards and rightwards.  (Note this is the opposite of [[Cartesian coordinate system#Notation and conventions|Cartesian conventions]].)

The difference between row-major and column-major order is simply that the order of the dimensions is reversed.  Equivalently, in row-major order the rightmost indices vary faster as one steps through consecutive memory locations, while in column-major order the leftmost indices vary faster.

This array
:&lt;math&gt; \begin{bmatrix}
11 &amp; 12 &amp; 13 \\
21 &amp; 22 &amp; 23 \end{bmatrix}&lt;/math&gt;

Would be stored as follows in the two orders:

{{col-start|width=50%}}
{{col-break}}
{| class=&quot;wikitable&quot; style=&quot;text-align:center; width:200px&quot;
|+ Column-Major Order&lt;br&gt;e.g. Fortran
|-
! Address
! Value
|-
! 0
| 11
|-
! 1
| 21
|-
! 2
| 12
|-
! 3
| 22
|-
! 4
| 13
|-
! 5
| 23
|}

{{col-break}}
{| class=&quot;wikitable&quot; style=&quot;text-align:center; width:200px&quot;
|+ Row-Major Order&lt;br&gt;e.g. C
|-
! Address
! Value
|-
! 0
| 11
|-
! 1
| 12
|-
! 2
| 13
|-
! 3
| 21
|-
! 4
| 22
|-
! 5
| 23
|}
{{col-end}}

==Programming Languages==

Programming languages which support multi-dimensional arrays have a native storage order for these arrays.

'''Row-major order''' is used in [[C (programming language)|C]]/[[C++]], [[Mathematica]], [[PL/I]], [[Pascal (programming language)|Pascal]], [[Python (programming language)|Python]], [[Speakeasy (computational environment)|Speakeasy]], [[SAS language|SAS]] and others.

'''Column-major order''' is used in [[Fortran]], [[OpenGL]] and [[OpenGL ES]], [[MATLAB]],&lt;ref&gt;MATLAB documentation, [http://www.mathworks.co.uk/help/matlab/matlab_external/matlab-data.html#f22019 MATLAB Data Storage] (retrieved from Mathworks.co.uk, January 2014).&lt;/ref&gt; [[GNU Octave]], [[S-Plus]],&lt;ref name=&quot;WinBUGS&quot; &gt;{{harvtxt|Spiegelhalter|Thomas|Best|Lunn|2003|p=17}}: {{citation|title=WinBUGS User&amp;nbsp;Manual|edition=Version 1.4|date=January 2003|first=David|last=Spiegelhalter|authorlink=David Spiegelhalter|first2=Andrew|last2=Thomas|first3=Nicky|last3=Best|first4=Dave|last4=Lunn|publisher=MRC Biostatistics Unit, Institute of Public Health|location=Robinson Way, Cambridge CB2 2SR, UK|url=http://www.mrc-bsu.cam.ac.uk/bugs|ref=harv|chapter=Formatting of data: S-Plus format|id=[http://www.mrc-bsu.cam.ac.uk/bugs/winbugs/manual14.pdf  PDF document]}}&lt;/ref&gt; [[R (programming language)|R]],&lt;ref&gt;''An Introduction to R'', [http://cran.r-project.org/doc/manuals/R-intro.html#Arrays Section 5.1: Arrays] (retrieved March 2010).&lt;/ref&gt; [[Julia (programming language)|Julia]], [[Rasdaman]], and [[Scilab]].

==Transposition==

As exchanging the indices of an array is the essence of [[transpose|array transposition]], an array stored as row-major but read as column-major (or vice versa) will appear transposed.  As actually performing this [[in-place matrix transposition|rearrangement in memory]] is typically an expensive operation, some systems provide options to specify individual matrices as being stored transposed.

For example, the [[Basic Linear Algebra Subprograms]] functions are passed flags indicating which arrays are transposed.&lt;ref&gt;http://www.netlib.org/blas/&lt;/ref&gt;

==Address calculation in general==

The concept trivially generalizes to arrays with more than two dimensions.

For a ''d''-dimensional &lt;math&gt;N_1 \times N_2 \times \cdots \times N_d&lt;/math&gt; array with dimensions ''N''&lt;sub&gt;''k''&lt;/sub&gt; (''k''=1...''d'').  A given element of this array is specified by a [[tuple]] &lt;math&gt;(n_1, n_2, \ldots, n_d)&lt;/math&gt; of ''d'' (zero-based) indices &lt;math&gt;n_k \in [0,N_k - 1]&lt;/math&gt;.

In '''row-major order''', the ''last'' dimension is contiguous, so that the memory-offset of this element is given by:

:&lt;math&gt;n_d + N_d \cdot (n_{d-1} + N_{d-1} \cdot (n_{d-2} + N_{d-2} \cdot (\cdots + N_2 n_1)\cdots)))
= \sum_{k=1}^d \left( \prod_{\ell=k+1}^d N_\ell \right) n_k
&lt;/math&gt;

In '''column-major order''', the ''first'' dimension is contiguous, so that the memory-offset of this element is given by:

:&lt;math&gt;n_1 + N_1 \cdot (n_2 + N_2 \cdot (n_3 + N_3 \cdot (\cdots + N_{d-1} n_d)\cdots)))
= \sum_{k=1}^d \left( \prod_{\ell=1}^{k-1} N_\ell \right) n_k
&lt;/math&gt;

== See also ==
* [[Matrix representation]]
* [[Vectorization (mathematics)]], the equivalent of turning a matrix into the corresponding column-major vector.

==References==
&lt;references/&gt;
* Donald E. Knuth, ''[[The Art of Computer Programming]] Volume 1: Fundamental Algorithms'', third edition, section 2.2.6 (Addison-Wesley: New York, 1997).

[[Category:Arrays]]</text>
      <sha1>rnvys12dsoeyjj0bxj09z41d72rh1o4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Judy array</title>
    <ns>0</ns>
    <id>484569</id>
    <revision>
      <id>619862227</id>
      <parentid>619861290</parentid>
      <timestamp>2014-08-04T19:55:53Z</timestamp>
      <contributor>
        <username>Mattbierner</username>
        <id>11285742</id>
      </contributor>
      <comment>/* Memory allocation */</comment>
      <text xml:space="preserve" bytes="3583">In [[computer science]] and [[software engineering]], a '''Judy array''' is a [[data structure]] that has high performance, low memory usage and implements an [[associative array]]. Unlike normal arrays, Judy arrays may be sparse, that is, they may have large ranges of unassigned indices. They can be used for storing and looking up values using integer or string keys. The key benefits of using a Judy array is its scalability, high performance, memory efficiency and ease of use.&lt;ref&gt;http://packages.debian.org/wheezy/libjudy-dev&lt;/ref&gt;

Judy arrays are both speed- and memory-efficient {{clarify|date=October 2013}}, with no tuning or configuration required and therefore they can sometime replace common in-memory dictionary implementations (like [[red-black tree]]s or [[hash table]]s) and work better with very large data sets{{dubious|Large data sets efficiency|reason=Comparison with advanced hash tables needed|date=October 2013}}{{citation needed|date=October 2013}}.

Roughly speaking, Judy arrays are highly optimised 256-ary [[radix tree]]s.&lt;ref&gt;Alan Silverstein, &quot;[http://judy.sourceforge.net/application/shop_interm.pdf Judy IV Shop Manual]&quot;, 2002&lt;/ref&gt; Judy arrays use over 20 different compression techniques on [[trie]] nodes to reduce memory usage.

The Judy array was invented by Douglas Baskins and named after his sister.&lt;ref&gt;http://judy.sourceforge.net/&lt;/ref&gt;

==Terminology==
Expanse, population and density are commonly used when it comes to Judy arrays. As they are not commonly used in tree search literature, it is important to define them:

# ''Expanse'' is a range of possible keys, e.g. 200...300, etc.
# ''Population'' is the count of keys contained in an expanse, e.g. a population of 5 could be the keys 200, 360, 400, 512, and 720
# ''Density'' is used to describe the sparseness of an expanse of keys: Density = Population/Expanse

==Benefits==

===Memory allocation===
Judy arrays are [[Dynamic_array|dynamic]] and can grow or shrink as elements are added to, or removed from, the array. The maximum size of a Judy array is bounded by machine memory.&lt;ref&gt;Advances in databases: concepts, systems and applications : By Kotagiri Ramamohanarao&lt;/ref&gt; The memory used by Judy arrays is nearly proportional to the number of elements (population) in the Judy array.

===Speed===
Judy arrays are designed to keep the number of processor [[cache-line]] fills as low as possible, and the algorithm is internally complex in an attempt to satisfy this goal as often as possible. Due to these [[CPU cache|cache]] optimizations, Judy arrays are fast, sometimes even faster than a [[hash table]], especially for very big datasets. Despite Judy arrays being a type of [[trie]], they consume much less memory than hash tables. Also because a Judy array is a trie, it is possible to do an ordered sequential traversal of keys, which is not possible in hash tables.

==Drawbacks==
The HP (SourceForge) implementation of Judy arrays appears to be the subject of US patent 6735595.&lt;ref&gt;http://www.google.com/patents/US6735595&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*[http://judy.sourceforge.net/ Main Judy arrays site]
*[http://judy.sourceforge.net/downloads/10minutes.htm How Judy arrays work and why they are so fast]
*[http://judy.sourceforge.net/application/shop_interm.pdf A complete technical description of Judy arrays]
*[http://www.nothings.org/computer/judy/ An independent performance comparison of Judy to Hash Tables]
*[http://code.google.com/p/judyarray A compact implementation of Judy arrays in 1250 lines of C code]

[[Category:Arrays]]</text>
      <sha1>0pg9qk5y6nwt81zne3zh49fb0bfk1r2</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Gap buffer</title>
    <ns>0</ns>
    <id>378974</id>
    <revision>
      <id>624192391</id>
      <parentid>616774897</parentid>
      <timestamp>2014-09-04T18:54:00Z</timestamp>
      <contributor>
        <ip>86.157.182.147</ip>
      </contributor>
      <text xml:space="preserve" bytes="4079">{{refimprove|date=April 2012}}
A '''gap buffer''' in [[computer science]] is a [[dynamic array]] that allows efficient insertion and deletion operations clustered near the same location. Gap buffers are especially common in [[text editor]]s, where most changes to the text occur at or near the current location of the [[cursor (computers)|cursor]]. The text is stored in a large buffer in two contiguous segments, with a gap between them for inserting new text. Moving the cursor involves copying text from one side of the gap to the other (sometimes copying is delayed until the next operation that changes the text).  Insertion adds new text at the end of the first segment; deletion deletes it.

Text in a gap buffer is represented as two [[string (computer science)|strings]], which take very little extra space and which can be searched and displayed very quickly, compared to more sophisticated [[data structure]]s such as [[linked list]]s. However, operations at different locations in the text and ones that fill the gap (requiring a new gap to be created) may require copying most of the text, which is especially inefficient for large files.  The use of gap buffers is based on the assumption that such recopying occurs rarely enough that its cost can be [[amortized analysis|amortized]] over the more common cheap operations. This makes the gap buffer a simpler alternative to the [[rope (data structure)|rope]] for use in text editors&lt;ref name=&quot;chucarroll&quot;&gt;Mark C. Chu-Carroll. &quot;[http://scienceblogs.com/goodmath/2009/02/18/gap-buffers-or-why-bother-with-1/ Gap Buffers, or, Don’t Get Tied Up With Ropes?]&quot; ''ScienceBlogs'', 2009-02-18. Accessed 2013-01-30.&lt;/ref&gt; such as [[Emacs]].&lt;ref name=&quot;elisp&quot;&gt;[http://www.gnu.org/software/emacs/elisp/html_node/Buffer-Gap.html emacs gap buffer info] Accessed 2013-01-30.&lt;/ref&gt;

==Example==

Below are some examples of operations with buffer gaps.  The gap is represented by the empty space between the square brackets.  This representation is a bit misleading: in a typical implementation, the endpoints of the gap are tracked using [[pointer (computer programming)|pointer]]s or array indices, and the contents of the gap are ignored; this allows, for example, deletions to be done by adjusting a pointer without changing the text in the buffer.  It is a common programming practice to use a semi-open interval for the gap pointers, i.e. the start-of-gap points to the invalid character following the last character in the first buffer, and the end-of-gap points to the first valid character in the second buffer (or equivalently, the pointers are considered to point &quot;between&quot; characters).

Initial state:

 This is the way [                     ]out.

User inserts some new text:

 This is the way the world started [   ]out.

User moves the cursor before &quot;started&quot;; system moves &quot;started &quot; from the first buffer to the second buffer.

 This is the way the world [   ]started out.

User adds text filling the gap; system creates new gap:

 This is the way the world as we know it [                   ]started out.

==See also==

* [[Dynamic array]], the special case of a gap buffer where the gap is always at the end
* [[Zipper (data structure)]], conceptually a generalization of the gap buffer.
* [[Linked list]]
* [[Circular buffer]]
* [[Rope (computer science)]]

== References ==
{{reflist}}

==External links==
* [http://www.codeproject.com/KB/recipes/GenericGapBuffer.aspx Overview and implementation in .NET/C#]
* [http://www.lazyhacker.com/gapbuffer/gapbuffer.htm Brief overview and sample C++ code]
* [http://www.codeproject.com/KB/recipes/SplitArrayDictionary.aspx Implementation of a cyclic sorted gap buffer in .NET/C#]
* [http://history.dcs.ed.ac.uk/archive/apps/ecce/hmd/e915.imp.html Use of gap buffer in early editor.] (First written somewhere between 1969 and 1971)
*[http://www.gnu.org/software/emacs/elisp/html_node/Buffer-Gap.html emacs gap buffer info](Emacs gap buffer reference)
*[http://www.common-lisp.net/project/flexichain/download/StrandhVilleneuveMoore.pdf Text Editing]

[[Category:Arrays]]</text>
      <sha1>rghyn6fvyt9pw8uag6iywlr5h2rbw07</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Parallel array</title>
    <ns>0</ns>
    <id>991858</id>
    <revision>
      <id>622238856</id>
      <parentid>616067387</parentid>
      <timestamp>2014-08-21T19:39:42Z</timestamp>
      <contributor>
        <username>Dalton Quinn</username>
        <id>21675110</id>
      </contributor>
      <comment>Cache utilization is only worse when accessing a large parallel array non-sequentially, resulting in two cache misses instead of one.</comment>
      <text xml:space="preserve" bytes="5587">In [[computing]], a '''parallel array''' is a [[data structure]] for representing [[array data structure|array]]s of [[Record (computer science)|records]]. It keeps a separate, homogeneous array for each field of the record, each having the same number of elements. Then, objects located at the same index in each array are implicitly the fields of a single record. Pointers from one object to another are replaced by array indices. This contrasts with the normal approach of storing all fields of each record together in memory. For example, one might declare an array of 100 names, each a string, and 100 ages, each an integer, associating each name with the age that has the same index.

An example in [[C (programming language)|C]] using parallel arrays:

&lt;source lang=&quot;c&quot;&gt;
int  ages[]   = {0,          17,        2,          52,         25};
char *names[] = {&quot;None&quot;,     &quot;Mike&quot;,    &quot;Billy&quot;,    &quot;Tom&quot;,      &quot;Stan&quot;};
int  parent[] = {0 /*None*/, 3 /*Tom*/, 1 /*Mike*/, 0 /*None*/, 3 /*Tom*/};

for(i = 1; i &lt;= 4; i++) {
    printf(&quot;Name: %s, Age: %d, Parent: %s \n&quot;,
           names[i], ages[i], names[parent[i]]);
}
&lt;/source&gt;
in [[Perl]] (using a hash of arrays to hold references to each array):
&lt;source lang=&quot;perl&quot;&gt;
my %data = (
    first_name   =&gt; ['Joe',  'Bob',  'Frank',  'Hans'    ],
    last_name    =&gt; ['Smith','Seger','Sinatra','Schultze'],
    height_in_cm =&gt; [169,     158,    201,      199      ]);

for $i (0..$#{$data{first_name}}) {
    printf &quot;Name: %s %s\n&quot;, $data{first_name}[$i], $data{last_name}[$i];
    printf &quot;Height in CM: %i\n&quot;, $data{height_in_cm}[$i];
}
&lt;/source&gt;
Or, in [[Python Programming Language|Python]]:
&lt;source lang=&quot;python&quot;&gt;
firstName  = ['Joe',  'Bob',  'Frank',  'Hans'    ]
lastName   = ['Smith','Seger','Sinatra','Schultze']
heightInCM = [169,     158,    201,      199      ]

for i in xrange(len(firstName)):
    print &quot;Name: %s %s&quot; % (firstName[i], lastName[i])
    print &quot;Height in CM: %s&quot; % heightInCM[i]
&lt;/source&gt;

==Pros and cons==

Parallel arrays have a number of practical advantages over the normal approach:
* They can be used in languages which support only arrays of primitive types and not of records (or perhaps don't support records at all).
* Parallel arrays are simple to understand and use, and are often used where declaring a record is more trouble than it's worth.
* They can save a substantial amount of space in some cases by avoiding alignment issues. For example, one of the fields of the record can be a single bit, and its array would only need to reserve one bit for each record, whereas in the normal approach many more bits would &quot;pad&quot; the field so that it consumes an entire byte or a word.
* If the number of items is small, array indices can occupy significantly less space than full pointers, particularly on architectures with large words.
* Sequentially examining a single field of each record in the array is very fast on modern machines, since this amounts to a linear traversal of a single array, exhibiting ideal [[locality of reference]] and cache behavior.

However, parallel arrays also have several strong disadvantages, which serves to explain why they are not generally preferred:

* They have significantly worse locality of reference when visiting the records non-sequentially and examining multiple fields of each record.
* They obscure the relationship between fields of a single record.
* They have little direct language support (the language and its syntax typically express no relationship between the arrays in the parallel array).
* They are expensive to grow or shrink, since each of several arrays must be reallocated.  Multi-level arrays can ameliorate this problem, but impacts performance due to the additional indirection needed to find the desired elements.

The bad locality of reference can be alleviated in some cases: if a structure can be divided into groups of fields that are generally accessed together, an array can be constructed for each group, and its elements are records containing only these subsets of the larger structure's fields. This is a valuable way of speeding up access to very large structures with many members, while keeping the portions of the structure tied together. An alternative to tying them together using array indexes is to use [[Reference (computer science)|reference]]s to tie the portions together, but this can be less efficient in time and space. Another alternative is to mock up a record structure in a single-dimensional array by declaring an array of n*m size and referring to the r-th field in record i as element as array(m*i+r). Some [[compiler optimization]]s, particularly for [[vector processor]]s, are able to perform this transformation automatically when arrays of structures are created in the program.{{Citation needed|date=April 2012}}

== See also ==
* [[Linked list#Linked lists using arrays of nodes|An example in the linked list article]]
* [[Column-oriented DBMS]]

== References ==

* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Page 209 of section 10.3: Implementing pointers and objects.
* {{cite web|url=http://msmvps.com/blogs/jon_skeet/archive/2014/06/03/anti-pattern-parallel-collections.aspx|title=Anti-pattern: parallel collections|last=Skeet|first=Jon|date=3 June 2014|accessdate=8 July 2014}}

[[Category:Arrays]]
[[Category:Articles with example C code]]
[[Category:Articles with example Perl code]]
[[Category:Articles with example Python code]]</text>
      <sha1>jz5yxa2j1m9q7hckzoosfgcr455bcv6</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Matrix representation</title>
    <ns>0</ns>
    <id>1523927</id>
    <revision>
      <id>611350409</id>
      <parentid>611189549</parentid>
      <timestamp>2014-06-03T08:00:02Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor/>
      <comment>/* Basic mathematical operations */[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="4607">{{about|the layout of matrices in the memory of computers|the representation of [[group (mathematics)|groups]] and [[algebra over a field|algebras]] by matrices in linear algebra|representation theory}}
'''Matrix representation''' is a method used by a [[computer language]] to store [[matrix (mathematics)|matrices]] of more than one dimension in [[computer storage|memory]].
[[Fortran]] and [[C (programming language)|C]] use different schemes. [[Fortran]] uses &quot;Column Major&quot;, in which all the elements for a given column are stored contiguously in memory. [[C (programming language)|C]] uses &quot;Row Major&quot;, which stores all the elements for a given row contiguously in memory.
[[LAPACK]] defines various matrix representations in memory. There is also [[Sparse matrix representation]]  and [[Morton-order matrix representation]].
According to the documentation, in [[LAPACK]] the [[unitary matrix]] representation is optimized.&lt;ref name=utexas&gt;{{cite web|title=Representation of Orthogonal or Unitary Matrices|url=http://www.ma.utexas.edu/documentation/lapack/node117.html|publisher=University of Texas at Austin|accessdate=14 September 2011}}&lt;/ref&gt;  Some languages such as [[Java programming language|Java]] store matrices using [[Iliffe vector]]s. These are particularly useful for storing [[irregular matrix|irregular matrices]]. Matrices are of primary importance in [[linear algebra]].

== Basic mathematical operations ==
{{main|Matrix (mathematics)#Basic operations}}
An m&amp;nbsp;×&amp;nbsp;n (read as m by n) order [[Matrix (mathematics)|matrix]] is a set of numbers arranged in m rows and n columns. Matrices of the same order can be added by adding the corresponding elements. Two matrices can be multiplied, the condition being that the number of columns of the first matrix is equal to the number of rows of the second matrix. Hence, if an m&amp;nbsp;×&amp;nbsp;n matrix is multiplied with an n&amp;nbsp;×&amp;nbsp;r matrix, then the resultant matrix will be of the order m&amp;nbsp;×&amp;nbsp;r.&lt;ref name=ramana&gt;{{cite book|last=Ramana|first=B.V|title=Higher Engineering Mathematics|year=2008|publisher=Tata Mcgraw-Hill|location=New Delhi|isbn=978-0-07-063419-0}}&lt;/ref&gt;

Operations like row operations or column operations can be performed on a matrix, using which we can obtain the inverse of a matrix. The inverse may be obtained by determining the adjoint as well.&lt;ref name=ramana /&gt; rows and columns are the different classes of matrices

== Basics of 2D array ==
The mathematical definition of a matrix finds applications in computing and database management, a basic starting point being the concept of [[Standard array|arrays]]. A two-dimensional array can function exactly like a matrix.
Two-dimensional arrays can be visualized as a table consisting of rows and columns.
* int a[3][4], declares an integer array of 3 rows and 4 columns. Index of row will start from 0 and will go up to 2.
* Similarly, index of column will start from 0 and will go up to 3.&lt;ref name=balagurusamy&gt;{{cite book|last=Balagurusamy|first=E|title=Programming in ANSI C|year=2006|publisher=Tata McGraw-Hill|location=New Delhi}}&lt;/ref&gt;

{| class=&quot;wikitable&quot;
|-
| ||Column 0||Column 1||Column 2||Column 3
|-
| row 0||a[0][0]||a[0][1]||a[0][2]||a[0][3]
|-
| row 1||a[1][0]||a[1][1]||a[1][2]||a[1][3]
|-
| row 2||a[2][0]||a[2][1]||a[2][2]||a[2][3]
|}
This table shows arrangement of elements with their indices.

Initializing Two-Dimensional arrays:
Two-Dimensional arrays may be initialized by providing a list of initial values.

int a[2][3] = {1,2,3,4,5,6,} or int a[2][3] = &lt;nowiki&gt;{{2,3,4}},{{4,4,5}}&lt;/nowiki&gt;;

Calculation of Address :
An m x n matrix (a[1...m][1...n]) where the row index varies from 1 to m and column index from 1 to n,a&lt;sub&gt;ij&lt;/sub&gt; denotes the number in the i&lt;sup&gt;th&lt;/sup&gt; row and the j&lt;sup&gt;th&lt;/sup&gt; column. In the computer memory, all elements are stored linearly using contiguous addresses. Therefore,in order to store a two-dimensional matrix a, two dimensional address space must be mapped to one-dimensional address space.In  the computer's memory matrices are stored in either [[Row-major order]] or [[Column-major order]] form.

== See also ==
* [[Row-major order|Row- and column-major order]]
* [[Sparse matrix]]
* [[Skyline matrix]]

==References==
&lt;references /&gt;

==External links==
*[http://developer.r-project.org/Sparse.html a description of sparse matrices] in R.
{{note|1|1}} R. LEHOUCQ, The computation of elementary unitary matrices, Computer Science Dept. Technical Report CS-94-233, University of Tennessee, Knoxville, 1994. (LAPACK Working Note 72).

[[Category:Matrices]]
[[Category:Arrays]]</text>
      <sha1>ckuhihio7fvoaimv4r600x5iv7p2l2p</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Circular buffer</title>
    <ns>0</ns>
    <id>11891734</id>
    <revision>
      <id>625572150</id>
      <parentid>625293894</parentid>
      <timestamp>2014-09-14T21:27:01Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>Tagging using [[Project:AWB|AWB]] (10458)</comment>
      <text xml:space="preserve" bytes="19417">{{Multiple issues|
{{Tone|date=April 2011}}
{{Refimprove|date=September 2014}}
{{Contradict|about=the end pointer: it is confused with the next pointer, resulting in a contradiction|date=April 2013}}
}}

[[Image:Circular buffer.svg|thumb|200px|A ring showing, conceptually, a circular buffer. This visually shows that the buffer has no real end and it can loop around the buffer. However, since memory is never physically created as a ring, a linear representation is generally used as is done below.]]

A '''circular buffer''', '''cyclic buffer''' or '''ring buffer''' is a [[data structure]] that uses a single, fixed-size [[buffer (computer science)|buffer]] as if it were connected end-to-end.
This structure lends itself easily to buffering [[data stream]]s.

==Uses==
The useful property of a circular buffer is that it does not need to have its elements shuffled around when one is consumed.
(If a non-circular buffer were used then it would be necessary to shift all elements when one is consumed.)
In other words, the circular buffer is well-suited as a [[FIFO ()|FIFO]] buffer while a standard, non-circular buffer is well suited as a [[LIFO (computing)|LIFO]] buffer.

Circular buffering makes a good implementation strategy for a [[Queue (data structure)|queue]] that has fixed maximum size. Should a maximum size be adopted for a queue, then a circular buffer is a completely ideal implementation; all queue operations are constant time. However, expanding a circular buffer requires shifting memory, which is comparatively costly. For arbitrarily expanding queues, a [[linked list]] approach may be preferred instead.

In some situations, overwriting circular buffer can be used, e.g. in multimedia. If the buffer is used as the bounded buffer in the [[producer-consumer problem]] then it is probably desired for the producer (e.g., an audio generator) to overwrite old data if the consumer (e.g., the [[sound card]]) is unable to momentarily keep up. Also, the [[LZ77]] family of lossless data compression algorithms operates on the assumption that strings seen more recently in a data stream are more likely to occur soon in the stream. Implementations store the most recent data in a circular buffer.

==How it works==
A circular buffer first starts empty and of some predefined length.
For example, this is a 7-element buffer:

:[[Image:Circular buffer - empty.svg|250px]]

Assume that a 1 is written into the middle of the buffer (exact starting location does not matter in a circular buffer):

:[[Image:Circular buffer - XX1XXXX.svg|250px]]

Then assume that two more elements are added &amp;mdash; 2 &amp; 3 &amp;mdash; which get appended after the 1:

:[[Image:Circular buffer - XX123XX.svg|250px]]

If two elements are then removed from the buffer, the oldest values inside the buffer are removed.
The two elements removed, in this case, are 1 &amp; 2, leaving the buffer with just a 3:

:[[Image:Circular buffer - XXXX3XX.svg|250px]]

If the buffer has 7 elements then it is completely full:

:[[Image:Circular buffer - 6789345.svg|250px]]

A consequence of the circular buffer is that when it is full and a subsequent write is performed, then it starts overwriting the oldest data.
In this case, two more elements &amp;mdash; A &amp; B &amp;mdash; are added and they ''overwrite'' the 3 &amp; 4:

:[[Image:Circular buffer - 6789AB5.svg|250px]]

Alternatively, the routines that manage the buffer could prevent overwriting the data and return an error or raise an [[exception handling|exception]].
Whether or not data is overwritten is up to the semantics of the buffer routines or the application using the circular buffer.

Finally, if two elements are now removed then what would be returned is '''not''' 3 &amp; 4 but 5 &amp; 6 because A &amp; B overwrote the 3 &amp; the 4 yielding the buffer with:

:[[Image:Circular buffer - X789ABX.svg|250px]]

==Circular buffer mechanics==
What is not shown in the example above is the mechanics of how the circular buffer is managed.

=== Start/end pointers (head/tail)===
Generally, a circular buffer requires four [[pointer (computer programming)|pointers]]:
* one to the actual buffer in memory
* one to the buffer end in memory (or alternately: the size of the buffer)
* one to point to the start of valid data (or alternately: amount of data written to the buffer)
* one to point to the end of valid data (or alternately: amount of data read from the buffer)
Alternatively, a fixed-length buffer with two integers to keep track of indices can be used in languages that do not have pointers.

Taking a couple of examples from above.
(While there are numerous ways to label the pointers and exact semantics can vary, this is one way to do it.)

This image shows a partially full buffer:

:[[Image:Circular buffer - XX123XX with pointers.svg|250px]]

This image shows a full buffer with two elements having been overwritten:

:[[Image:Circular buffer - 6789AB5 with pointers.svg|250px]]

What to note about the second one is that after each element is overwritten then the start pointer is incremented as well.

== Difficulties ==

=== Full / Empty Buffer Distinction ===

A small disadvantage of relying on pointers or relative indices of the start and end of data is, that in the case the buffer is entirely full, both pointers point to the same element:{{contradiction-inline|reason=this section treats the end pointer as a next pointer, resulting in a contradiction|date=April 2013}}

:[[Image:Circular buffer - 6789AB5 full.svg|250px]]

This is exactly the same situation as when the buffer is empty:

:[[Image:Circular buffer - 6789AB5 empty.svg|250px]]

To solve this confusion there are a number of solutions:

* [[Circular buffer#Always Keep One Slot Open|Always keep one slot open.]]
* [[Circular buffer#Use a Fill Count|Use a fill count to distinguish the two cases.]]
* [[Circular buffer#Mirroring|Use an extra mirroring bit to distinguish the two cases.]]
* [[Circular buffer#Read / Write Counts|Use read and write counts to get the fill count from.]]
* [[Circular buffer#Absolute indices|Use absolute indices.]]
* [[Circular buffer#Record last operation|Record last operation.]]

==== Always Keep One Slot Open ====

This design always keeps one slot unallocated. A full buffer has at most &lt;math&gt;(\text{size}-1)&lt;/math&gt; slots.
If both pointers refer to the same slot, the buffer is empty. If the end (write) pointer refers to the slot preceding the one referred to by the start (read) pointer, the buffer is full.

The advantage is:
* The solution is simple and robust.

The disadvantages are:
* One slot is lost, so it is a bad compromise when the buffer size is small or the slot is big or is implemented in hardware.
* The full test requires a modulo operation

'''Example implementation, [[C (programming language)|'C' language]]'''
&lt;source lang=&quot;c&quot;&gt;
/* Circular buffer example, keeps one slot open */

#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

/* Opaque buffer element type.  This would be defined by the application. */
typedef struct { int value; } ElemType;

/* Circular buffer object */
typedef struct {
    int         size;   /* maximum number of elements           */
    int         start;  /* index of oldest element              */
    int         end;    /* index at which to write new element  */
    ElemType   *elems;  /* vector of elements                   */
} CircularBuffer;

void cbInit(CircularBuffer *cb, int size) {
    cb-&gt;size  = size + 1; /* include empty elem */
    cb-&gt;start = 0;
    cb-&gt;end   = 0;
    cb-&gt;elems = calloc(cb-&gt;size, sizeof(ElemType));
}

void cbFree(CircularBuffer *cb) {
    free(cb-&gt;elems); /* OK if null */
}

int cbIsFull(CircularBuffer *cb) {
    return (cb-&gt;end + 1) % cb-&gt;size == cb-&gt;start;
}

int cbIsEmpty(CircularBuffer *cb) {
    return cb-&gt;end == cb-&gt;start;
}

/* Write an element, overwriting oldest element if buffer is full. App can
   choose to avoid the overwrite by checking cbIsFull(). */
void cbWrite(CircularBuffer *cb, ElemType *elem) {
    cb-&gt;elems[cb-&gt;end] = *elem;
    cb-&gt;end = (cb-&gt;end + 1) % cb-&gt;size;
    if (cb-&gt;end == cb-&gt;start)
        cb-&gt;start = (cb-&gt;start + 1) % cb-&gt;size; /* full, overwrite */
}

/* Read oldest element. App must ensure !cbIsEmpty() first. */
void cbRead(CircularBuffer *cb, ElemType *elem) {
    *elem = cb-&gt;elems[cb-&gt;start];
    cb-&gt;start = (cb-&gt;start + 1) % cb-&gt;size;
}

int main(int argc, char **argv) {
    CircularBuffer cb;
    ElemType elem = {0};

    int testBufferSize = 10; /* arbitrary size */
    cbInit(&amp;cb, testBufferSize);

    /* Fill buffer with test elements 3 times */
    for (elem.value = 0; elem.value &lt; 3 * testBufferSize; ++ elem.value)
        cbWrite(&amp;cb, &amp;elem);

    /* Remove and print all elements */
    while (!cbIsEmpty(&amp;cb)) {
        cbRead(&amp;cb, &amp;elem);
        printf(&quot;%d\n&quot;, elem.value);
    }

    cbFree(&amp;cb);
    return 0;
}
&lt;/source&gt;

==== Use a Fill Count ====
This approach replaces the end pointer with a counter that tracks the number of readable items in the buffer.  This unambiguously indicates when the buffer is empty or full and allows use of all buffer slots.

The performance impact should be negligible, since this approach adds the costs of maintaining the counter and computing the tail slot on writes but eliminates the need to maintain the end pointer and simplifies the fullness test.

The advantage is:
* The test for full/empty is simple

The disadvantages are:
* You need modulo for read and write
* Read and write operation must share the counter field, so it requires synchronization in multi-threaded situation.

Note: When using [[Semaphore (programming)|semaphores]] in a [[Producer-consumer problem|Producer-consumer]] model, the semaphores act as a fill count.

'''Differences from previous example'''
&lt;source lang=&quot;c&quot;&gt;
/* This approach replaces the CircularBuffer 'end' field with the
   'count' field and changes these functions: */

void cbInit(CircularBuffer *cb, int size) {
    cb-&gt;size  = size;
    cb-&gt;start = 0;
    cb-&gt;count = 0;
    cb-&gt;elems = (ElemType *)calloc(cb-&gt;size, sizeof(ElemType));
}

int cbIsFull(CircularBuffer *cb) {
    return cb-&gt;count == cb-&gt;size; }

int cbIsEmpty(CircularBuffer *cb) {
    return cb-&gt;count == 0; }

void cbWrite(CircularBuffer *cb, ElemType *elem) {
    int end = (cb-&gt;start + cb-&gt;count) % cb-&gt;size;
    cb-&gt;elems[end] = *elem;
    if (cb-&gt;count == cb-&gt;size)
        cb-&gt;start = (cb-&gt;start + 1) % cb-&gt;size; /* full, overwrite */
    else
        ++ cb-&gt;count;
}

void cbRead(CircularBuffer *cb, ElemType *elem) {
    *elem = cb-&gt;elems[cb-&gt;start];
    cb-&gt;start = (cb-&gt;start + 1) % cb-&gt;size;
    -- cb-&gt;count;
}
&lt;/source&gt;

==== Mirroring ====

Another solution is to remember the number of times each read and write pointers have wrapped and compare this to distinguish empty and full situations. In fact only the parity of the number of wraps is necessary, so you only need to keep an extra bit. You can see this as if the buffer adds a virtual mirror and the pointers point either to the normal or to the mirrored buffer.

:[[Image:Circular buffer - mirror solution full and empty.svg|500px]]

It is easy to see above that when the pointers (including the extra msb bit) are equal, the buffer is empty, while if the pointers differ only by the extra msb bit, the buffer is full.

The advantages are:
* The test for full/empty is simple
* No modulo operation is needed
* The source and sink of data can implement independent policies for dealing with a full buffer and overrun while adhering to the rule that only the source of data modifies the write count and only the sink of data modifies the read count.  This can result in elegant and robust circular buffer implementations even in multi-threaded environments.{{Citation needed|date=April 2014}}
 
The disadvantage is:
* You need one more bit for read and write pointer

'''Differences from Always Keep One Slot Open example'''
&lt;source lang=&quot;c&quot;&gt;
/* This approach adds one bit to end and start pointers */

/* Circular buffer object */
typedef struct {
    int         size;   /* maximum number of elements           */
    int         start;  /* index of oldest element              */
    int         end;    /* index at which to write new element  */
    int         s_msb;
    int         e_msb;
    ElemType   *elems;  /* vector of elements                   */
} CircularBuffer;
 
void cbInit(CircularBuffer *cb, int size) {
    cb-&gt;size  = size;
    cb-&gt;start = 0;
    cb-&gt;end   = 0;
    cb-&gt;s_msb = 0;
    cb-&gt;e_msb = 0;
    cb-&gt;elems = (ElemType *)calloc(cb-&gt;size, sizeof(ElemType));
}

int cbIsFull(CircularBuffer *cb) {
    return cb-&gt;end == cb-&gt;start &amp;&amp; cb-&gt;e_msb != cb-&gt;s_msb; }

int cbIsEmpty(CircularBuffer *cb) {
    return cb-&gt;end == cb-&gt;start &amp;&amp; cb-&gt;e_msb == cb-&gt;s_msb; }

void cbIncr(CircularBuffer *cb, int *p, int *msb) {
    *p = *p + 1;
    if (*p == cb-&gt;size) {
        *msb ^= 1;
        *p = 0;
    }
}

void cbWrite(CircularBuffer *cb, ElemType *elem) {
    cb-&gt;elems[cb-&gt;end] = *elem;
    if (cbIsFull(cb)) /* full, overwrite moves start pointer */
        cbIncr(cb, &amp;cb-&gt;start, &amp;cb-&gt;s_msb);
    cbIncr(cb, &amp;cb-&gt;end, &amp;cb-&gt;e_msb);
}

void cbRead(CircularBuffer *cb, ElemType *elem) {
    *elem = cb-&gt;elems[cb-&gt;start];
    cbIncr(cb, &amp;cb-&gt;start, &amp;cb-&gt;s_msb);
}
&lt;/source&gt;

If the size is a power of two, the implementation is simpler and the separate msb variables are no longer necessary, removing the disadvantage:

'''Differences from Always Keep One Slot Open example'''
&lt;source lang=&quot;c&quot;&gt;
/* This approach adds one bit to end and start pointers */

/* Circular buffer object */
typedef struct {
    int         size;   /* maximum number of elements           */
    int         start;  /* index of oldest element              */
    int         end;    /* index at which to write new element  */
    ElemType   *elems;  /* vector of elements                   */
} CircularBuffer;
 
void cbInit(CircularBuffer *cb, int size) {
    cb-&gt;size  = size;
    cb-&gt;start = 0;
    cb-&gt;end   = 0;
    cb-&gt;elems = (ElemType *)calloc(cb-&gt;size, sizeof(ElemType));
}

void cbPrint(CircularBuffer *cb) {
    printf(&quot;size=0x%x, start=%d, end=%d\n&quot;, cb-&gt;size, cb-&gt;start, cb-&gt;end);
}

int cbIsFull(CircularBuffer *cb) {
    return cb-&gt;end == (cb-&gt;start ^ cb-&gt;size); /* This inverts the most significant bit of start before comparison */ }

int cbIsEmpty(CircularBuffer *cb) {
    return cb-&gt;end == cb-&gt;start; }

int cbIncr(CircularBuffer *cb, int p) {
    return (p + 1)&amp;(2*cb-&gt;size-1); /* start and end pointers incrementation is done modulo 2*size */
}

void cbWrite(CircularBuffer *cb, ElemType *elem) {
    cb-&gt;elems[cb-&gt;end&amp;(cb-&gt;size-1)] = *elem;
    if (cbIsFull(cb)) /* full, overwrite moves start pointer */
        cb-&gt;start = cbIncr(cb, cb-&gt;start);
    cb-&gt;end = cbIncr(cb, cb-&gt;end);
}

void cbRead(CircularBuffer *cb, ElemType *elem) {
    *elem = cb-&gt;elems[cb-&gt;start&amp;(cb-&gt;size-1)];
    cb-&gt;start = cbIncr(cb, cb-&gt;start);
}
&lt;/source&gt;

==== Read / Write Counts ====

Another solution is to keep counts of the number of items written to and read from the circular buffer. Both counts are stored in signed integer variables with numerical limits larger than the number of items that can be stored and are allowed to wrap freely.

The unsigned difference (write_count - read_count) always yields the number of items placed in the buffer and not yet retrieved.  This can indicate that the buffer is empty, partially full, completely full (without waste of a storage location) or in a state of overrun.

The advantage is:
* The source and sink of data can implement independent policies for dealing with a full buffer and overrun while adhering to the rule that only the source of data modifies the write count and only the sink of data modifies the read count.  This can result in elegant and robust circular buffer implementations even in multi-threaded environments.

The disadvantage is:
* You need two additional variables.

==== Absolute indices ====

It is possible to optimize the previous solution by using indices instead of pointers: indices can store read/write counts instead of the offset from start of the buffer, the separate variables in the above solution are removed and relative indices are obtained on the fly by division [[modulo operation|modulo]] the buffer's length.

The advantage is:
* No extra variables are needed.

The disadvantages are:
* Every access needs an additional ''modulo'' operation.
* If counter wrap is possible, complex logic can be needed if the buffer's length is not a divisor of the counter's capacity.
On binary computers, both of these disadvantages disappear if the buffer's length is a power of two—at the cost of a constraint on possible buffers lengths.

==== Record last operation ====

Another solution is to keep a flag indicating whether the most recent operation was a read or a write. If the two pointers are equal, then the flag will show whether the buffer is full or empty: if the most recent operation was a write, the buffer must be full, and conversely if it was a read, it must be empty.

The advantages are:
* Only a single bit needs to be stored (which may be particularly useful if the algorithm is implemented in hardware)
* The test for full/empty is simple

The disadvantage is:
* You need an extra variable
* Read and write operations must share the flag, so it will probably require synchronization in multi-threaded situation.

=== Multiple Read Pointers ===

A little bit more complex are multiple read pointers on the same circular buffer. This is useful if you have ''n'' threads, which are reading from the same buffer, but ''one'' thread writing to the buffer.

=== Chunked Buffer ===

Much more complex are different chunks of data in the same circular buffer. The writer is not only writing elements to the buffer, it also assigns these elements to chunks {{Citation needed|date=April 2009}}.

The reader should not only be able to read from the buffer, it should also get informed about the chunk borders.

Example: The writer is reading data from small files, writing them into the same circular buffer. The reader is reading the data, but needs to know when and which file is starting at a given position.

== Variants ==

Perhaps the most common version of the circular buffer uses 8-bit bytes as elements.

Some implementations of the circular buffer use fixed-length elements that are bigger than 8-bit bytes—16-bit integers for audio buffers,
53-byte ATM cells for telecom buffers, etc.
Each item is contiguous and has the correct [[data alignment]],
so software reading and writing these values can be faster than software that handles non-contiguous and non-aligned values.

[[Ping-pong buffer]]ing can be considered a very specialized circular buffer with exactly two large fixed-length elements.

The [[Simon Cooke#Inventions|Bip Buffer]] is very similar to a circular buffer,
except it always returns contiguous blocks (which can be variable length).&lt;ref name=&quot;cooke&quot;&gt;
[[Simon Cooke]].
[http://www.codeproject.com/Articles/3479/The-Bip-Buffer-The-Circular-Buffer-with-a-Twist &quot;The Bip Buffer - The Circular Buffer with a Twist&quot;].
2003.
&lt;/ref&gt;

== External links ==
{{reflist}}
* [[c2:CircularBuffer|CircularBuffer]] at the [[Portland Pattern Repository]]
* [http://www.boost.org/doc/libs/1_39_0/libs/circular_buffer/doc/circular_buffer.html Boost: Templated Circular Buffer Container]
* http://www.dspguide.com/ch28/2.htm

{{Data structures}}

[[Category:Computer memory]]
[[Category:Arrays]]</text>
      <sha1>t1onwhn5obp3d2qvwi0fyhfw1y5o54w</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Grid file</title>
    <ns>0</ns>
    <id>6505771</id>
    <revision>
      <id>608223994</id>
      <parentid>544508899</parentid>
      <timestamp>2014-05-12T13:44:40Z</timestamp>
      <contributor>
        <ip>139.30.4.28</ip>
      </contributor>
      <comment>/* Uses */</comment>
      <text xml:space="preserve" bytes="4949">In [[computer science]], a '''grid file''' or '''bucket grid''' is a [[point access method]] which splits a space into a non-periodic [[wikt:grid|grid]] where one or more cells of the grid refer to a small set of points. Grid files (a [[symmetric]] [[data structure]]) provide an efficient method of storing these indexes on disk to perform complex data lookups.

It provides a grid of ''n''-dimensions where ''n'' represents how many keys can be used to reference a single point.

Grid files do not contain any data themselves but instead contain references to the correct [[bucket (computing)|bucket]].

==Uses==
A '''grid file''' is usually used in cases where a single value can be referenced by multiple keys.

A grid file began being used because &quot;traditional file structures that provide multikey access to records, for example, inverted files, are extensions of file structures originally designed for single-key access. They manifest various deficiencies in particular for multikey access to highly dynamic files.&quot; &lt;ref name=&quot;tgf&quot;&gt;J. Nievergelt, H. Hinterberger ''The Grid File: An Adaptable, Symmetric Multikey File Structure''. Institut fur Informatik, ETH and K. C. Sevcik, 1984. Abstract, pp.1.&lt;/ref&gt;

In a traditional single dimensional data structure (e.g. [[Associative array|hash]]), a search on a single criterion is usually very simple but searching for a second criterion can be much more complex.

Grid files represent a special kind of hashing, where the traditional hash is replaced by a grid directory.

==Examples==
===Census Database&lt;ref&gt;[[Donald Knuth]]. ''The Art of Computer Programming'', Volume 3: ''Sorting and Searching'', Second Edition. Addison-Wesley, 1998. ISBN 0-201-89685-0. Section 6.5: Searching, pp.564&amp;ndash;566.&lt;/ref&gt;&lt;ref&gt;Elmasri &amp; Navathe ''Fundamentals of Database Systems'', Third Edition. Addison-Wesley, 2000. ISBN 0-201-54263-3. Section 6.4.3: Grid Files, pp.185.&lt;/ref&gt;===
Consider a database containing data from a census. A single [[record (database)|record]] represents a single household, and all records are grouped into buckets. All records in a bucket can be indexed by either their city (which is the same for all records in the bucket), and the streets in that city whose names begin with the same letter. 

A grid file can be used to provide an efficient index for this structure, where records come in groupings of 26, each of them relating to street names in a city starting with one of the letters of the alphabet. This structure can be thought of as an [[array data structure|array]], [[table (database)|table]], or [[grid (spatial index)|grid]] with two dimensions which we will call the x and y axes.

One may consider the x-axis to be the city and the y-axis to be each of the letters in the alphabet, or alternatively, the first letter of each street.

Each record in this structure is known as a cell. Each cell will contain a [[Pointer (computer programming)|pointer]] to the appropriate bucket in the database where the actual data is stored. An extra cell, or record header, may be required to store the name of the city. Other cells grouped with it will only need to contain the pointer to their respective bucket, since the first cell corresponds to street names beginning with &quot;A&quot;, the second to &quot;B&quot;, and so on.

The database can be further extended to contain a continent field to expand the census to other continents. This would cause records in the same bucket to correspond to households on a street beginning with the same letter, in the same city, in the same continent.

The cells in the grid file would then consist of a city header, and six (one for each continent, not including [[Antarctica]]) groupings of 26 cells relating to the streets with the same starting letter, in the same city, on the same continent and could now be thought of as a three-dimensional array.

==Advantages==
Since a single entry in the grid file contains pointers to all records indexed by the specified keys:&lt;ref name=&quot;adv&quot;&gt;http://www.cs.sfu.ca/CC/354/zaiane/material/notes/Chapter11/node24.html&lt;/ref&gt;
* No special computations are required
* Only the right records are retrieved
* Can also be used for single search key queries
* Easy to extend to queries on ''n'' search keys
* Significant improvement in processing time for multiple-key queries
* Has a two-disk-access upper bound for accessing data. &lt;ref name=&quot;tgf&quot; /&gt;

==Disadvantages==
However, because of the nature of the grid file, which gives it its advantages, there are also some disadvantages:&lt;ref name=&quot;adv&quot; /&gt;
* Imposes space overhead
* Performance overhead on insertion and deletion

==Related Data Structures==
* [[multilayer grid file]]
* [[twin grid files]]
* [[BANG file]]

==See also==
* [[Lattice graph]]
* [[Grid (spatial index)]]
* [[Index (database)]], [[Quadtree]], [[Kd-tree]], [[UB-tree]], [[R-tree]], [[range tree]] as alternatives.

==References==
{{reflist}}
* 
*

[[Category:Files]]
[[Category:Arrays]]</text>
      <sha1>lsnrj12hi1a1ofvjwqipsm6fndlavhe</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>BANG file</title>
    <ns>0</ns>
    <id>9969569</id>
    <revision>
      <id>617746017</id>
      <parentid>617743815</parentid>
      <timestamp>2014-07-20T19:48:16Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{Unreferenced}}</comment>
      <text xml:space="preserve" bytes="794">{{unreferenced|date=July 2014}}
A '''BANG file''' (balanced and nested [[grid file]]) is a [[point access method]] which divides space into a nonperiodic grid. Each spatial dimension is divided by a linear hash. Cells may intersect, and points may be distributed between them.

== Another meaning ==
In some early [[International Computers Limited|ICL]] [[Mainframe computer|mainframe]] computers, a '''bang file''' was a temporary data storage file whose name began with a '''[[!]]''' character (which is sometimes nicknamed &quot;bang&quot;) and automatically deleted when the run or user session ended.

== See also ==
* [[twin grid file]]. 

==External links==
*http://ce.sharif.edu/~dic-ads/d.php?ref=1&amp;origin=Euler+tour&amp;r=bangfile.1.7

[[Category:Files]]
[[Category:Arrays]]

{{datastructure-stub}}</text>
      <sha1>g6ng9ipktgwdmjr5fvrtr2ojp4qua0h</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Comparison of programming languages (array)</title>
    <ns>0</ns>
    <id>13941999</id>
    <revision>
      <id>617401423</id>
      <parentid>617081548</parentid>
      <timestamp>2014-07-18T02:17:49Z</timestamp>
      <contributor>
        <ip>203.206.132.27</ip>
      </contributor>
      <comment>/* Array system cross-reference list */ Fixed colours</comment>
      <text xml:space="preserve" bytes="35405">{{one source|date=June 2013}}
{{ProgLangCompare}}

This '''comparison of programming languages (array)''' compares the features of [[array data structure]]s or [[matrix (mathematics)|matrix]] processing for over 48 various computer [[programming language]]s.

== Syntax ==

=== Array dimensions ===

The following list contains [[programming language syntax|syntax]] examples on how to determine the dimensions (index of the first element, the last element and/or the size in elements):

{| class=&quot;wikitable&quot; style=&quot;font-size: 90%;&quot;
|- 
! Size !! First !! Last !! Languages
|-
| name&lt;tt&gt;&lt;nowiki&gt;'&lt;/nowiki&gt;Length&lt;/tt&gt;|| name&lt;tt&gt;&lt;nowiki&gt;'&lt;/nowiki&gt;First&lt;/tt&gt; || name&lt;tt&gt;&lt;nowiki&gt;'&lt;/nowiki&gt;Last&lt;/tt&gt;
| [[Ada (programming language)|Ada]]
|-
| &lt;tt&gt;UPB&lt;/tt&gt; name - &lt;tt&gt;LWB&lt;/tt&gt; name&lt;tt&gt;+1&lt;/tt&gt; or &lt;br&gt; &lt;tt&gt;2 UPB&lt;/tt&gt; name - &lt;tt&gt;2 LWB&lt;/tt&gt; name&lt;tt&gt;+1&lt;/tt&gt; etc.|| &lt;tt&gt;LWB&lt;/tt&gt; name or &lt;tt&gt;2 LWB&lt;/tt&gt; name etc.|| &lt;tt&gt;UPB&lt;/tt&gt; name or &lt;tt&gt;2 UPB&lt;/tt&gt; name etc.
|[[ALGOL 68]]
|-
| name&lt;tt&gt;.Length&lt;/tt&gt; || name&lt;tt&gt;.GetLowerBound(&lt;/tt&gt;dimension&lt;tt&gt;)&lt;/tt&gt; || name&lt;tt&gt;.GetUpperBound(&lt;/tt&gt;dimension&lt;tt&gt;)&lt;/tt&gt;
| [[C Sharp (programming language)|C#]], [[Visual Basic .NET]], [[Windows PowerShell]], [[F Sharp (programming language)|F#]]
|-
|&lt;tt&gt;arrayLen(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; or name.&lt;tt&gt;len()&lt;/tt&gt; || name&lt;tt&gt;[1]&lt;/tt&gt; || name&lt;tt&gt;[&lt;/tt&gt;name.&lt;tt&gt;len()]&lt;/tt&gt;
| [[CFML]]
|-
| &lt;tt&gt;max(shape(name))&lt;/tt&gt; || 0 || &lt;tt&gt;max(shape(name))-1&lt;/tt&gt;
| [[Ch (computer programming)|Ch]]
|-
| &lt;tt&gt;(length &lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt;|| &lt;tt&gt;0&lt;/tt&gt; || &lt;tt&gt;(1- (length &lt;/tt&gt;name&lt;tt&gt;))&lt;/tt&gt;
| [[Common Lisp]]
|-
| &lt;tt&gt;SIZE(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;LBOUND(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;UBOUND(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt;
| [[Fortran]]
|-
| &lt;tt&gt;len(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;0&lt;/tt&gt; || &lt;tt&gt;len(&lt;/tt&gt;name&lt;tt&gt;) - 1&lt;/tt&gt; 
| [[Go (programming language)|Go]]
|-
| &lt;tt&gt;rangeSize (bounds &lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;fst (bounds &lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;snd (bounds &lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt;
| [[Haskell (programming language)|Haskell]]
|-
| &lt;tt&gt;(length &lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt;|| &lt;tt&gt;0&lt;/tt&gt; || &lt;tt&gt;(1- (length &lt;/tt&gt;name&lt;tt&gt;))&lt;/tt&gt;
| [[ISLISP]]
|-
| name&lt;tt&gt;.length&lt;/tt&gt; || &lt;tt&gt;0&lt;/tt&gt; || name&lt;tt&gt;.length - 1&lt;/tt&gt;
| [[Java (programming language)|Java]], [[JavaScript]], [[D (programming language)|D]], [[Scala (programming language)|Scala]]
|-
| &lt;tt&gt;#&lt;/tt&gt;name || &lt;tt&gt;0&lt;/tt&gt; || &lt;tt&gt;&lt;:@#&lt;/tt&gt;name
| [[J (programming language)|J]]
|-
| &lt;tt&gt;#&lt;/tt&gt;name || &lt;tt&gt;1&lt;/tt&gt; || &lt;tt&gt;#&lt;/tt&gt;name
| [[Lua (programming language)|Lua]]
|-
|&lt;tt&gt;Length[&lt;/tt&gt;name&lt;tt&gt;]&lt;/tt&gt; || &lt;tt&gt;1&lt;/tt&gt; or &lt;tt&gt;First[&lt;/tt&gt;name&lt;tt&gt;]&lt;/tt&gt; || &lt;tt&gt;-1&lt;/tt&gt; or &lt;tt&gt;Last[&lt;/tt&gt;name&lt;tt&gt;]&lt;/tt&gt;
| [[Mathematica]]
|-
|&lt;tt&gt;length(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;1&lt;/tt&gt; || &lt;tt&gt;end&lt;/tt&gt;
| [[MATLAB]], [[GNU Octave]], [[Julia (programming language)|Julia]]
|-
|&lt;tt&gt;Length(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; || name&lt;tt&gt;[0]&lt;/tt&gt; || name&lt;tt&gt;[Length(&lt;/tt&gt;name&lt;tt&gt;)-1]&lt;/tt&gt;
| [[Object Pascal]]
|-
| &lt;tt&gt;[&lt;/tt&gt;name&lt;tt&gt; count]&lt;/tt&gt; || &lt;tt&gt;0&lt;/tt&gt; || &lt;tt&gt;[&lt;/tt&gt;name&lt;tt&gt; count] - 1&lt;/tt&gt;
| [[Objective-C]] (&lt;code&gt;NSArray *&lt;/code&gt; only)
|-
| &lt;tt&gt;Array.length &lt;/tt&gt;name|| &lt;tt&gt;0&lt;/tt&gt; || &lt;tt&gt;Array.length &lt;/tt&gt;name&lt;tt&gt; - 1&lt;/tt&gt; 
| [[OCaml]]
|-
| &lt;tt&gt;scalar(@&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;$[&lt;/tt&gt; || &lt;tt&gt;$#&lt;/tt&gt;name
| [[Perl]]
|-
| @name&lt;tt&gt;.elems&lt;/tt&gt; || &lt;tt&gt;0&lt;/tt&gt; || @name&lt;tt&gt;.end&lt;/tt&gt;
| [[Perl 6]]
|-
| &lt;tt&gt;count(&lt;/tt&gt;$name&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;0&lt;/tt&gt; || &lt;tt&gt;count(&lt;/tt&gt;$name&lt;tt&gt;) - 1&lt;/tt&gt;
| [[PHP]]
|-
| &lt;tt&gt;len(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;0&lt;/tt&gt; || &lt;tt&gt;-1&lt;/tt&gt; or &lt;tt&gt;len(&lt;/tt&gt;name&lt;tt&gt;) - 1&lt;/tt&gt; 
| [[Python (programming language)|Python]]
|-
| name&lt;tt&gt;.size&lt;/tt&gt; || &lt;tt&gt;0&lt;/tt&gt; (name&lt;tt&gt;.first&lt;/tt&gt; will also refer to this element) || &lt;tt&gt;-1&lt;/tt&gt; or name&lt;tt&gt;.size - 1&lt;/tt&gt; (name&lt;tt&gt;.last&lt;/tt&gt; will also refer to this element)
| [[Ruby (programming language)|Ruby]]
|-
| &lt;tt&gt;length(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;0&lt;/tt&gt; || &lt;tt&gt;-1&lt;/tt&gt; or &lt;tt&gt;length(&lt;/tt&gt;name&lt;tt&gt;)-1&lt;/tt&gt;
| [[S-Lang (programming library)|S-Lang]]
|-
| &lt;tt&gt;(vector-length &lt;/tt&gt;vector&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;0&lt;/tt&gt; || &lt;tt&gt;(- (vector-length &lt;/tt&gt;vector&lt;tt&gt;) 1)&lt;/tt&gt;
| [[Scheme (programming language)|Scheme]]
|-
| name &lt;tt&gt;size&lt;/tt&gt; || &lt;tt&gt;1&lt;/tt&gt; (name &lt;tt&gt;first&lt;/tt&gt; will also refer to this element) || name &lt;tt&gt;size&lt;/tt&gt; (name &lt;tt&gt;last&lt;/tt&gt; will also refer to this element)
| [[Smalltalk]]
|-
| &lt;tt&gt;UBound(&lt;/tt&gt;name&lt;tt&gt;)-LBound(&lt;/tt&gt;name&lt;tt&gt;)+1&lt;/tt&gt; || &lt;tt&gt;LBound(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;UBound(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt;
|[[Visual Basic]]
|-
| &lt;tt&gt;UBound(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt; || &lt;tt&gt;0&lt;/tt&gt; || &lt;tt&gt;UBound(&lt;/tt&gt;name&lt;tt&gt;)&lt;/tt&gt;
|[[Xojo]]
|}

=== Indexing ===

The following list contains Syntax examples on how a single element of an array can be accessed.

{| class=&quot;wikitable&quot; 
|- 
! Format !! Languages 
|-
| &lt;tt&gt;name'''['''index''']''' or name'''['''index&lt;sub&gt;1&lt;/sub&gt;''',''' index&lt;sub&gt;2&lt;/sub&gt;''']''' etc.&lt;/tt&gt;
| [[ALGOL 68]], [[Pascal (programming language)|Pascal]], [[Object Pascal]], [[C Sharp (programming language)|C#]], [[S-Lang (programming library)|S-Lang]]&lt;ref name=&quot;indexing&quot;/&gt;
|-
| &lt;tt&gt;name'''['''index''']'''&lt;/tt&gt;
| [[C (programming language)|C]], [[CFML]], [[Ch (computer programming)|Ch]], [[C++]], [[D (programming language)|D]], [[Go (programming language)|Go]], [[Java (programming language)|Java]], [[ActionScript]] 3.0, [[JavaScript]], [[Julia (programming language)|Julia]], [[Lua (programming language)|Lua]], [[Objective-C]] (&lt;code&gt;NSArray *&lt;/code&gt;), [[Perl]],&lt;ref name=&quot;indexing&quot;/&gt; [[Python (programming language)|Python]],&lt;ref name=&quot;indexing&quot;/&gt; [[Ruby (programming language)|Ruby]],&lt;ref name=&quot;indexing&quot;/&gt; [[Swift (Apple programming language)|Swift]]
|-
| &lt;tt&gt;'''$'''name'''['''index''']'''&lt;/tt&gt;
| [[Perl]],&lt;ref name=&quot;indexing&quot;/&gt; [[PHP]], [[Windows PowerShell]]&lt;ref name=&quot;indexing&quot;/&gt;
|-
| &lt;tt&gt;'''@'''name'''['''index''']'''&lt;/tt&gt;
| [[Perl 6]]
|-
| &lt;tt&gt;name'''('''index''')'''&lt;/tt&gt;
| [[Ada (programming language)|Ada]], [[BASIC]], [[COBOL]], [[Fortran]], [[IBM RPG|RPG]], [[MATLAB]], [[Scala (programming language)|Scala]], [[Visual Basic]], [[Visual Basic .NET]], [[Xojo]]
|-
| &lt;tt&gt;name'''.('''index''')'''&lt;/tt&gt;
| [[OCaml]]
|-
| &lt;tt&gt;name'''.['''index''']'''&lt;/tt&gt;
| [[F Sharp (programming language)|F#]]
|-
| &lt;tt&gt;name''' ! '''index&lt;/tt&gt;
| [[Haskell (programming language)|Haskell]]
|-
| &lt;tt&gt;'''(vector-ref''' name index''')'''&lt;/tt&gt;
| [[Scheme (programming language)|Scheme]]
|-
| &lt;tt&gt;'''(aref''' name index''')'''&lt;/tt&gt;
| [[Common Lisp]]
|-
| &lt;tt&gt;'''(elt''' name index''')'''&lt;/tt&gt;
| [[ISLISP]]
|-
| &lt;tt&gt;name '''[&lt;span/&gt;['''index''']]'''&lt;/tt&gt;
| [[Mathematica]]&lt;ref name=&quot;indexing&quot;/&gt;
|-
| &lt;tt&gt;name''' at:'''index&lt;/tt&gt;
| [[Smalltalk]] 
|-
| &lt;tt&gt;'''['''name''' objectAtIndex:'''index''']'''&lt;/tt&gt;
| [[Objective-C]] (&lt;code&gt;NSArray *&lt;/code&gt; only)
|-
| &lt;tt&gt;index'''{'''name&lt;/tt&gt;
| [[J (programming language)|J]]
|}

===Slicing===

The following list contains syntax examples on how a range of element of an array can be accessed.

In the following table:
*&lt;tt&gt;first&lt;/tt&gt; - the index of the first element in the slice
*&lt;tt&gt;last&lt;/tt&gt; - the index of the last element in the slice
*&lt;tt&gt;end&lt;/tt&gt; - one more than the index of last element in the slice
*&lt;tt&gt;len&lt;/tt&gt; - the length of the slice (= end - first)
*&lt;tt&gt;step&lt;/tt&gt; - the number of array elements in each (default 1)

{| class=&quot;wikitable&quot;
|- 
! Format !! Languages 
|-
| &lt;tt&gt;name'''['''first''':'''last''']'''&lt;/tt&gt;
| [[ALGOL 68]],&lt;ref name=&quot;s1&quot;/&gt; [[Julia (programming language)|Julia]]
|-
| &lt;tt&gt;name'''['''first''':'''end''':'''step''']'''&lt;/tt&gt;
| [[Python (programming language)|Python]],&lt;ref name=&quot;s2&quot;/&gt;&lt;ref name=&quot;s4&quot;/&gt; [[Go (programming language)|Go]]
|-
| &lt;tt&gt;name'''['''first'''..'''last''']'''&lt;/tt&gt;
| [[Pascal (programming language)|Pascal]], [[Object Pascal]], [[Delphi]], [[D (programming language)|D]]
|-
| &lt;tt&gt;'''$'''name'''['''first'''..'''last''']'''&lt;/tt&gt;
| [[Windows PowerShell]]
|-
| &lt;tt&gt;'''@'''name'''['''first'''..'''last''']'''&lt;/tt&gt;
| [[Perl]]&lt;ref name=&quot;s3&quot;/&gt;
|-
| &lt;tt&gt;name'''['''first'''..'''last''']''' &lt;br /&gt; name'''['''first'''...'''end''']''' &lt;br /&gt; name'''['''first''', '''len''']'''&lt;/tt&gt;
| [[Ruby (programming language)|Ruby]]&lt;ref name=&quot;s4&quot;/&gt;
|-
| &lt;tt&gt;name'''('''first'''..'''last''')'''&lt;/tt&gt;
| [[Ada (programming language)|Ada]]&lt;ref name=&quot;s1&quot;/&gt;
|-
| &lt;tt&gt;name'''('''first''':'''last''')'''&lt;/tt&gt;
| [[Fortran]],&lt;ref name=&quot;s1&quot;/&gt;&lt;ref name=&quot;s2&quot;/&gt; [[MATLAB]]&lt;ref name=&quot;s1&quot;/&gt;&lt;ref name=&quot;s3&quot;/&gt;
|-
| &lt;tt&gt;name'''[&lt;span/&gt;['''first''';;'''last''']]'''&lt;/tt&gt;
| [[Mathematica]]&lt;ref name=&quot;s1&quot;/&gt;&lt;ref name=&quot;s2&quot;/&gt;&lt;ref name=&quot;s4&quot;/&gt;
|-
| &lt;tt&gt;name'''[&lt;span/&gt;['''first''':'''last''']]'''&lt;/tt&gt;
| [[S-Lang (programming library)|S-Lang]]&lt;ref name=&quot;s1&quot;/&gt;&lt;ref name=&quot;s2&quot;/&gt;&lt;ref name=&quot;s3&quot;/&gt;
|-
| &lt;tt&gt;name'''.['''first'''..'''last''']'''&lt;/tt&gt;
| [[F Sharp (programming language)|F#]]
|-
| &lt;tt&gt;name'''.slice('''first''', '''last''')'''&lt;/tt&gt;
| [[JavaScript]], [[Scala (programming language)|Scala]]
|-
| &lt;tt&gt;name'''.slice('''first''', '''len''')'''&lt;/tt&gt;
| [[CFML]]
|-
| &lt;tt&gt;'''array_slice('''name''', '''first''', '''len''')'''&lt;/tt&gt;
| [[PHP]]&lt;ref name=&quot;s4&quot;/&gt;
|-
| &lt;tt&gt;'''(subseq''' name first end''')'''&lt;/tt&gt;
| [[Common Lisp]]
|-
| &lt;tt&gt;'''(subseq''' name first end''')'''&lt;/tt&gt;
| [[ISLISP]]
|-
| &lt;tt&gt;'''Array.sub''' name first len&lt;/tt&gt;
| [[OCaml]]
|-
| &lt;tt&gt;'''['''name''' subarrayWithRange:NSMakeRange('''first''', '''len''')]'''{{ns}}&lt;/tt&gt;
| [[Objective-C]] (&lt;code&gt;NSArray *&lt;/code&gt; only)
|-
| &lt;tt&gt;'''('''first'''([+i.@(-~)'''end'''){'''name&lt;/tt&gt;
| [[J (programming language)|J]]
|-
| &lt;tt&gt;name'''['''first'''..&lt;'''end''']''' &lt;br /&gt; name'''['''first'''...'''last''']'''&lt;/tt&gt;
| [[Swift (Apple programming language)|Swift]]
|}

==Array system cross-reference list==
&lt;!-- This section is linked from [[array data type|Array]] --&gt;
{| class=&quot;sortable wikitable&quot; style=&quot;text-align: center; font-size: smaller;&quot;
|+ 
! [[Programming language]]
! [[Array data type#Index origin|Default base index]]
! Specifiable index type&lt;ref name=&quot;cr16&quot;/&gt;
! Specifiable base index
! [[bounds checking|Bound check]]
! [[array data type#Multi-dimensional arrays|Multidimensional]]
! Dynamically-sized
! [[array programming|Vectorized operations]]
|-
| [[Ada (programming language)|Ada]]
| style=&quot;background:honeydew;&quot;  | index type&lt;ref name=&quot;cr17&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:azure;&quot;     | init&lt;ref name=&quot;cr1&quot;/&gt;
| style=&quot;background:cornsilk;&quot;  | some, others definable&lt;ref name=&quot;cr5&quot;/&gt;
|-
| [[ALGOL 68]]
| style=&quot;background:azure;&quot;     | 1
| style=&quot;background:grey80;&quot;    | no&lt;ref name=&quot;cr25&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:cornsilk;&quot;  | varies
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:azure;&quot;     | user definable
|-
| [[APL (programming language)|APL]]
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:cornsilk;&quot;  | 0 or 1&lt;ref name=&quot;cr7&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:azure;&quot;     | init&lt;ref name=&quot;cr1&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
|-
| [[BASIC]]
| style=&quot;background:cornsilk;&quot;     | 0
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:azure;&quot;     | init&lt;ref name=&quot;cr1&quot;/&gt;
| style=&quot;background:grey80;&quot;    | ?
|-
| [[C (programming language)|C]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no&lt;ref name=&quot;cr26&quot;/&gt;
| style=&quot;background:seashell;&quot;  | unchecked
| style=&quot;background:honeydew;&quot;  | yes, also array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:lavender;&quot;  | init,&lt;ref name=&quot;cr1&quot;/&gt;&lt;ref name=&quot;cr4&quot;/&gt; heap&lt;ref name=&quot;cr3&quot;/&gt;
| style=&quot;background:seashell;&quot;  | no
|-
| [[Ch (computer programming)|Ch]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes, also array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:lavender;&quot;  | init,&lt;ref name=&quot;cr1&quot;/&gt;&lt;ref name=&quot;cr4&quot;/&gt; heap&lt;ref name=&quot;cr3&quot;/&gt;
| style=&quot;background:seashell;&quot;  | yes
|-
| [[C++]]&lt;ref name=&quot;cr5&quot;/&gt;
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no&lt;ref name=&quot;cr26&quot;/&gt;
| style=&quot;background:seashell;&quot;  | unchecked
| style=&quot;background:honeydew;&quot;  | yes, also array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:AliceBlue;&quot; | heap&lt;ref name=&quot;cr3&quot;/&gt;
| style=&quot;background:seashell;&quot;  | no
|-
| [[C Sharp (programming language)|C#]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:AliceBlue;&quot; | heap&lt;ref name=&quot;cr3&quot;/&gt;&lt;ref name=&quot;cr9&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes ([[LINQ]] select)
|-
| [[CFML]]
| style=&quot;background:azure;&quot;     | 1
| style=&quot;background:grey80;&quot;    | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes, also array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:seashell;&quot;  | yes
| style=&quot;background:seashell;&quot;    | no
|-
| [[COBOL]]
| style=&quot;background:azure;&quot;     | 1
| style=&quot;background:grey80;&quot;    | no&lt;ref name=&quot;cr27&quot;/&gt;
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;&lt;ref name=&quot;cr28&quot;/&gt;
| style=&quot;background:seashell;&quot;  | no&lt;ref name=&quot;cr14&quot;/&gt;
| style=&quot;background:grey80;&quot;    | some intrinsics
|-
| [[Common Lisp]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked&lt;ref name=&quot;cr15&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes (map or map-into)
|-
| [[D (programming language)|D]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:honeydew;&quot;  | yes&lt;ref&gt;[http://dlang.org/hash-map.html Associative Arrays - D Programming Language]&lt;/ref&gt;
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:cornsilk;&quot;  | varies&lt;ref name=&quot;cr11&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:grey80;&quot;    | ?
|-
| [[F Sharp (programming language)|F#]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:AliceBlue;&quot; | heap&lt;ref name=&quot;cr3&quot;/&gt;&lt;ref name=&quot;cr9&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes (map)
|-
| [[FreeBASIC]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:azure;&quot;     | init,&lt;ref name=&quot;cr1&quot;/&gt; init&lt;ref name=&quot;cr21&quot;/&gt;
| style=&quot;background:grey80;&quot;    | ?
|-
| [[Fortran]]
| style=&quot;background:azure;&quot;     | 1
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:cornsilk;&quot;  | varies&lt;ref name=&quot;cr12&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
|-
| [[FoxPro]]
| style=&quot;background:azure;&quot;     | 1
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:grey80;&quot;    | ?
|-
| [[Go (programming language)|Go]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
|-
| [[Haskell (programming language)|Haskell]]
| style=&quot;background:grey80;&quot;    | none (specified on init)
| style=&quot;background:honeydew;&quot;  | yes&lt;ref name=&quot;cr24&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes, also array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:azure;&quot;     | init&lt;ref name=&quot;cr1&quot;/&gt;
| style=&quot;background:grey80;&quot;    | ?
|-
| [[IDL (programming language)|IDL]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
|-
| [[ISLISP]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | init&lt;ref name=&quot;cr1&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes (map or map-into)
|-
| [[J (programming language)|J]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:azure;&quot;     | init&lt;ref name=&quot;cr1&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
|-
| [[Java (programming language)|Java]]&lt;ref name=&quot;cr5&quot;/&gt;
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:azure;&quot;     | init&lt;ref name=&quot;cr1&quot;/&gt;
| style=&quot;background:grey80;&quot;    | ?
|-
| [[JavaScript]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:grey80;&quot;    | no
| style=&quot;background:grey80;&quot;    | no
| style=&quot;background:honeydew;&quot;  | checked&lt;ref name=&quot;cr22&quot;/&gt;
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
|-
| [[Julia (programming language)|Julia]]
| style=&quot;background:azure;&quot;  | 1
| style=&quot;background:grey80;&quot;    | yes
| style=&quot;background:grey80;&quot;    | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:cornsilk;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
|-
| [[Lua (programming language)|Lua]]
| style=&quot;background:azure;&quot;     | 1
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:cornsilk;&quot;  | partial&lt;ref name=&quot;cr20&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:grey80;&quot;    | ?
|-
| [[Mathematica]]
| style=&quot;background:azure;&quot;     | 1
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
|-
| [[MATLAB]]
| style=&quot;background:azure;&quot;     | 1
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes&lt;ref name=&quot;cr8&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
|-
| [[Oberon (programming language)|Oberon]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:grey80;&quot;    | ?
|-
| [[Oberon-2]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:grey80;&quot;    | ?
|-
| [[Objective-C]]&lt;ref name=&quot;cr5&quot;/&gt;
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:seashell;&quot;  | no
|-
| [[OCaml]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked by default
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:azure;&quot;     | init&lt;ref name=&quot;cr1&quot;/&gt;
| style=&quot;background:grey80;&quot;    | ?
|-
| [[Pascal (programming language)|Pascal]], [[Object Pascal]]
| style=&quot;background:honeydew;&quot;  | index type&lt;ref name=&quot;cr17&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:cornsilk;&quot;  | varies&lt;ref name=&quot;cr13&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:cornsilk;&quot;  | varies&lt;ref name=&quot;cr10&quot;/&gt;
| style=&quot;background:cornsilk;&quot;  | some
|-
| [[Perl]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | yes (&lt;code&gt;$[&lt;/code&gt;)
| style=&quot;background:honeydew;&quot;  | checked&lt;ref name=&quot;cr22&quot;/&gt;
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:grey80;&quot;    | no&lt;ref name=&quot;cr18&quot;/&gt;
|-
| [[Perl 6]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked&lt;ref name=&quot;cr22&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
|-
| [[PHP]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:honeydew;&quot;  | yes&lt;ref name=&quot;cr23&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes&lt;ref name=&quot;cr23&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | checked&lt;ref name=&quot;cr23&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
|-
| [[PL/I]]
| style=&quot;background:azure;&quot;     | 1
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:grey80;&quot;    | yes
| style=&quot;background:grey80;&quot;    | no
| style=&quot;background:grey80;&quot;    | ?
|-
| [[Python (programming language)|Python]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | no&lt;ref name=&quot;cr19&quot;/&gt;
|-
| [[IBM RPG|RPG]]
| style=&quot;background:azure;&quot;     | 1
| style=&quot;background:grey80;&quot;    | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:grey80;&quot;    | ?
|-
| [[Ruby (programming language)|Ruby]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:grey80;&quot;    | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked&lt;ref name=&quot;cr22&quot;/&gt;
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:grey80;&quot;    | ?
|-
| [[S-Lang (programming library)|S-Lang]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
|-
| [[Scala (programming language)|Scala]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:azure;&quot;     | init&lt;ref name=&quot;cr1&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes (map)
|-
| [[Scheme (programming language)|Scheme]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:azure;&quot;     | init&lt;ref name=&quot;cr1&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes (map)
|-
| [[Smalltalk]]&lt;ref name=&quot;cr5&quot;/&gt;
| style=&quot;background:azure;&quot;     | 1
| style=&quot;background:grey80;&quot;    | ?
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes&lt;ref name=&quot;cr6&quot;/&gt;
| style=&quot;background:grey80;&quot;    | ?
|-
| [[Swift (Apple programming language)|Swift]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:cornsilk;&quot;  | array of array&lt;ref name=&quot;cr2&quot;/&gt;
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:grey80;&quot;    | ?
|-
| [[Visual Basic]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:grey80;&quot;    | ?
|-
| [[Visual Basic .NET]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes ([[LINQ]] select)
|-
| [[Windows PowerShell]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | heap
| style=&quot;background:grey80;&quot;    | ?
|-
| [[Xojo]]
| style=&quot;background:cornsilk;&quot;  | 0
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | no
|-
| [[XPath]]
| style=&quot;background:azure;&quot;     | 1
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | checked
| style=&quot;background:seashell;&quot;  | no
| style=&quot;background:honeydew;&quot;  | yes
| style=&quot;background:honeydew;&quot;  | yes

|-class=&quot;sortbottom&quot;
! Programming language
! Default base index
! Specifiable index type&lt;ref name=&quot;cr16&quot;/&gt;
! Specifiable base index
! Bound check
! Multidimensional
! Dynamically-sized
! Vectorized operations

|}

== Vectorized array operations ==

Some compiled languages such as [[Ada (programming language)|Ada]] and [[Fortran]], and some scripting languages such as [[IDL (programming language)|IDL]], [[MATLAB]], and [[S-Lang (programming library)|S-Lang]], have native support for vectorized operations on arrays.  For example, to perform an element by element sum of two arrays, &lt;tt&gt;a&lt;/tt&gt; and &lt;tt&gt;b&lt;/tt&gt; to produce a third &lt;tt&gt;c&lt;/tt&gt;, it is only necessary to write
&lt;pre&gt;
    c = a + b
&lt;/pre&gt;
In addition to support for vectorized arithmetic and relational operations, these languages also vectorize common mathematical functions such as sine. For example, if &lt;tt&gt;x&lt;/tt&gt; is an array, then
&lt;pre&gt;
    y = sin (x)
&lt;/pre&gt;
will result in an array &lt;tt&gt;y&lt;/tt&gt; whose elements are sine of the corresponding elements of the array &lt;tt&gt;x&lt;/tt&gt;.

Vectorized index operations are also supported.  As an example,
&lt;source lang=&quot;fortran&quot;&gt;
    even = x(2::2);
    odd = x(::2);
&lt;/source&gt;
is how one would use [[Fortran]] to create arrays from the even and odd entries of an array.  Another common use of vectorized indices is a filtering operation.  Consider a clipping operation of a sine wave where amplitudes larger than 0.5 are to be set to 0.5.  Using [[S-Lang (programming library)|S-Lang]], this may accomplished by
&lt;pre&gt;
    y = sin(x);
    y[where(abs(y)&gt;0.5)] = 0.5;
&lt;/pre&gt;

=== Mathematical matrices management ===
{| class=&quot;wikitable&quot;
! Language/&lt;br&gt;Library !! Create !! Determinant !! Transpose !! Element !! Column !! Row !! [[Eigenvalues]] 
|-
| [[Fortran]]
| &lt;tt&gt;m = RESHAPE([x11, x12, ...], SHAPE(m))&lt;/tt&gt;
| 
| &lt;tt&gt;TRANSPOSE&lt;wbr/&gt;(m)&lt;/tt&gt;
| &lt;tt&gt;m(i,j)&lt;/tt&gt;
| &lt;tt&gt;m(:,j)&lt;/tt&gt;
| &lt;tt&gt;m(i,:)&lt;/tt&gt;
|
|-
| [[Ch (computer programming)|Ch]] &lt;ref&gt;[http://www.softintegration.com/products/features/ch_vs_matlab.html Ch numerical features]&lt;/ref&gt;
| &lt;tt&gt;m = {...}&lt;/tt&gt;
| &lt;tt&gt;determinant&lt;wbr/&gt;(m)&lt;/tt&gt;
| &lt;tt&gt;transpose&lt;wbr/&gt;(m)&lt;/tt&gt;
| &lt;tt&gt;{{j|m[i-1][j-1]}}&lt;/tt&gt;
| &lt;tt&gt;shape&lt;wbr/&gt;(m,0)&lt;/tt&gt;
| &lt;tt&gt;shape&lt;wbr/&gt;(m,1)&lt;/tt&gt;
| &lt;tt&gt;eigen&lt;wbr/&gt;(output, m, NULL)&lt;/tt&gt;
|-
| [[Mathematica]]
| &lt;tt&gt;m = &amp;#123;&amp;#123;x11, x12, ...}, ...}&lt;/tt&gt;
| &lt;tt&gt;Det[m]&lt;/tt&gt;
| &lt;tt&gt;Transpose&lt;wbr/&gt;[m]&lt;/tt&gt;
| &lt;tt&gt;&lt;nowiki&gt;m[[i,j]]&lt;/nowiki&gt;&lt;/tt&gt;
| &lt;tt&gt;{{j|&lt;nowiki&gt;m[[;;,j]]&lt;/nowiki&gt;}}&lt;/tt&gt;
| &lt;tt&gt;&lt;nowiki&gt;m[[i]]&lt;/nowiki&gt;&lt;/tt&gt;
| &lt;tt&gt;Eigenvalues&lt;wbr/&gt;[m]&lt;/tt&gt;
|-
| [[MATLAB]] /&lt;br/&gt;[[GNU Octave]]
| &lt;tt&gt;m = [...]&lt;/tt&gt;
| &lt;tt&gt;det(m)&lt;/tt&gt;
| &lt;tt&gt;m'&lt;/tt&gt;
| &lt;tt&gt;m(i,j)&lt;/tt&gt;
| &lt;tt&gt;m(:,j)&lt;/tt&gt;
| &lt;tt&gt;m(i,:)&lt;/tt&gt;
| &lt;tt&gt;eig(m)&lt;/tt&gt;
|-
| [[NumPy]]
| &lt;tt&gt;m = mat(...) &lt;/tt&gt;
| 
| &lt;tt&gt;m.T&lt;/tt&gt;
| &lt;tt&gt;{{j|m[i-1,j-1]}}&lt;/tt&gt;
| &lt;tt&gt;m[:,i-1]&lt;/tt&gt;
| &lt;tt&gt;m[i-1,:]&lt;/tt&gt;
|
|-
| [[S-Lang]]
| &lt;tt&gt;m = reshape&lt;wbr/&gt;([x11, x12, ...], [new-dims])&lt;/tt&gt;
|
| &lt;tt&gt;m = transpose&lt;wbr/&gt;(m)&lt;/tt&gt;
| &lt;tt&gt;m[i,j]&lt;/tt&gt;&lt;wbr/&gt;
| &lt;tt&gt;m[*,j]&lt;/tt&gt;
| &lt;tt&gt;m[j,*]&lt;/tt&gt;
|
|-
| [[SymPy]]
| &lt;tt&gt;m = Matrix(...)&lt;/tt&gt;
| 
| &lt;tt&gt;m.T&lt;/tt&gt;
| &lt;tt&gt;{{j|m[i-1,j-1]}}&lt;/tt&gt;
|
|
|
|-
|}

== References ==

{{Reflist|refs=
&lt;ref name=&quot;indexing&quot;&gt;The index may be a negative number, indicating the corresponding number of places before the end of the array.&lt;/ref&gt;

&lt;ref name=&quot;s1&quot;&gt;Slices for multidimensional arrays are also supported and defined similarly.&lt;/ref&gt;
&lt;ref name=&quot;s2&quot;&gt;Slices of the type &lt;code&gt;''first'':''last'':''step''&lt;/code&gt; are also supported.&lt;/ref&gt;
&lt;ref name=&quot;s3&quot;&gt;More generally, for 1-d arrays [[Perl]] and [[S-Lang (programming library)|S-Lang]] permit slices of the form&lt;code&gt;''array''[''indices'']&lt;/code&gt;, where &lt;code&gt;''indices''&lt;/code&gt; can be a range such mentioned in footnote 2 or an explicit list of indices, e.g., '&lt;code&gt;[0,9,3,4]&lt;/code&gt;', as well as a combination of the two, e.g., &lt;code&gt;A[[[0:3]],7,9,[11:2:-3]]]&lt;/code&gt;.&lt;/ref&gt;
&lt;ref name=&quot;s4&quot;&gt;&lt;tt&gt;''last''&lt;/tt&gt; or &lt;tt&gt;''end''&lt;/tt&gt; may be a negative number, indicating to stop at the corresponding number of places before the end of the array.&lt;/ref&gt;

&lt;ref name=&quot;cr1&quot;&gt;Size can only be chosen on initialization after which it is fixed&lt;/ref&gt;
&lt;ref name=&quot;cr2&quot;&gt;Allows arrays of arrays which can be used to emulate most—but not all—aspects multi-dimensional arrays&lt;/ref&gt;
&lt;ref name=&quot;cr3&quot;&gt;Size can only be chosen on initialization when memory is allocated on the heap, as distinguished from when it is allocated on the stack. This note need not be made for a language that always allocates arrays on the heap&lt;/ref&gt;
&lt;ref name=&quot;cr4&quot;&gt;C99 allows for variable size arrays; however there is almost no compiler available to support this new feature&lt;/ref&gt;
&lt;ref name=&quot;cr5&quot;&gt;This list is strictly comparing language features. In every language (even assembler) it is possible to provide improved array handling via add on libraries. This language has improved array handling as part of its standard library&lt;/ref&gt;
&lt;ref name=&quot;cr6&quot;&gt;The class Array is fixed-size, but OrderedCollection is dynamic&lt;/ref&gt;
&lt;ref name=&quot;cr7&quot;&gt;The indexing base can be 0 or 1, but is set for a whole &quot;workspace&quot;&lt;/ref&gt;
&lt;ref name=&quot;cr8&quot;&gt;At least 2 dimensions (scalar numbers are 1×1 arrays, vectors are 1×n or n×1 arrays)&lt;/ref&gt;
&lt;ref name=&quot;cr9&quot;&gt;Allows creation of fixed-size arrays in &quot;unsafe&quot; code, allowing for enhanced [[interoperability]] with other language&lt;/ref&gt;
&lt;ref name=&quot;cr10&quot;&gt;Varies by implementation.  Newer implementations (FreePascal, Object Pascal (Delphi)) permit heap-based dynamic arrays&lt;/ref&gt;
&lt;ref name=&quot;cr11&quot;&gt;Behaviour can be tuned using compiler switches. As in DMD 1.0 bounds are checked in debug mode and unchecked in release mode for efficiency reasons&lt;/ref&gt;
&lt;ref name=&quot;cr12&quot;&gt;Almost all Fortran implementations offer bounds checking options via compiler switches.  However by default, bounds checking is usually turned off for efficiency reasons&lt;/ref&gt;
&lt;ref name=&quot;cr13&quot;&gt;Many implementations (Turbo Pascal, Object Pascal (Delphi), FreePascal) allow the behaviour to be changed by compiler switches and in-line directives&lt;/ref&gt;
&lt;ref name=&quot;cr14&quot;&gt;COBOL provides a way to specify that the usable size of an array is variable, but this can never be greater than the declared maximum size, which is also the allocated size&lt;/ref&gt;
&lt;ref name=&quot;cr15&quot;&gt;Most Common Lisp implementations allow checking to be selectively disabled&lt;/ref&gt;
&lt;ref name=&quot;cr16&quot;&gt;The index type can be a freely chosen [[integer (computer science)|integer type]], [[enumerated type]], or [[character (computing)|character type]]. For arrays with non-compact index types see: [[Associative array]]&lt;/ref&gt;
&lt;ref name=&quot;cr17&quot;&gt;The default base index is the lowest value of the index type used&lt;/ref&gt;
&lt;ref name=&quot;cr18&quot;&gt;Standard [[Perl]] array data types do not support vectorized operations as defined here.  However, the [[Perl Data Language]]extension adds array objects with this ability&lt;/ref&gt;
&lt;ref name=&quot;cr19&quot;&gt;The standard [[Python (programming language)|Python]] array type, &lt;code&gt;list&lt;/code&gt;, does not support vectorized operations as defined here.  However, the [[numpy]] extension adds array objects with this ability&lt;/ref&gt;
&lt;ref name=&quot;cr20&quot;&gt;By specifying a base index, arrays at an arbitrary base can be created. However, by default, Lua's length operator does not consider the base index of the array when calculating the length. This behavior can be changed via metamethods&lt;/ref&gt;
&lt;ref name=&quot;cr21&quot;&gt;FreeBASIC supports both variable array lengths and fixed length arrays.  Arrays declared with no index range are created as variable-length arrays, while arrays with a declared range are created as fixed-length arrays&lt;/ref&gt;
&lt;ref name=&quot;cr22&quot;&gt;In these languages, one can access or write to an array index greater than or equal to the length of the array, and the array will implicitly grow to that size. This may appear at first as if the bounds are not checked; however, the bounds are checked in order to decide to grow the array, and you do not have unsafe memory access like you do in C&lt;/ref&gt;
&lt;ref name=&quot;cr23&quot;&gt;PHP's &quot;arrays&quot; are associative arrays. You can use integers and strings as the keys (indexes); floats can also be used as the key but are truncated to integers. There is not really any &quot;base index&quot; or &quot;bounds&quot;&lt;/ref&gt;
&lt;ref name=&quot;cr24&quot;&gt;Haskell arrays (Data.Array) allow using any type which is an instance of Ix as index type. So a custom type can be defined and used as an index type as long as it instances Ix. Also, tuples of Ix types are also Ix types; this is commonly used to implement multi-dimensional arrays&lt;/ref&gt;
&lt;ref name=&quot;cr25&quot;&gt;ALGOL 68 arrays must be subscripted (and sliced) by type &lt;tt&gt;INT&lt;/tt&gt;. However a hash function could be used to convert other types to &lt;tt&gt;INT&lt;/tt&gt;.  e.g. &lt;tt&gt;name'''['''hash(&quot;string&quot;)''']'''&lt;/tt&gt;&lt;/ref&gt;
&lt;ref name=&quot;cr26&quot;&gt;Because C does not bound-check indices, a pointer to the interior of any array can be defined that will symbolically act as a pseudo-array that accommodates negative indices or any integer index origin&lt;/ref&gt;
&lt;ref name=&quot;cr27&quot;&gt;COBOL arrays may be indexed with &quot;INDEX&quot; types, distinct from integer types&lt;/ref&gt;
&lt;ref name=&quot;cr28&quot;&gt;While COBOL only has arrays-of-arrays, array elements can be accessed with a multi-dimensional-array-like syntax, where the language automatically matches the indexes to the arrays enclosing the item being referenced&lt;/ref&gt;
}}

[[Category:Arrays]]
[[Category:Programming language comparisons]]</text>
      <sha1>r83n1iey72p51u2etpqgn2gj6luxwzf</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Iliffe vector</title>
    <ns>0</ns>
    <id>1696737</id>
    <revision>
      <id>626858888</id>
      <parentid>615447347</parentid>
      <timestamp>2014-09-24T05:23:18Z</timestamp>
      <contributor>
        <username>Spoon!</username>
        <id>96195</id>
      </contributor>
      <text xml:space="preserve" bytes="2085">In [[computer programming]], an '''Iliffe vector''', also known as a '''display''', is a [[data structure]] used to implement multi-dimensional [[Array data structure|arrays]].  An Iliffe vector for an ''n''-dimensional array (where ''n''&amp;nbsp;≥&amp;nbsp;2) consists of a vector (or 1-dimensional array) of [[Pointer (computer programming)|pointer]]s to an (''n''&amp;nbsp;−&amp;nbsp;1)-dimensional array.  They are often used to avoid the need for expensive multiplication operations when performing address calculation on an array element. They can also be used to implement [[triangular array]]s, or other kinds of irregularly shaped arrays. The data structure is named after [[John K. Iliffe]].

Their disadvantages include the need for multiple chained pointer indirections to access an element, and the extra work required to determine the next row in an ''n''-dimensional array to allow an optimising compiler to prefetch it. Both of these are a source of delays on systems where the CPU is significantly faster than main memory.

The Iliffe vector for a 2-dimensional array is simply a vector of pointers to vectors of data, i.e., the Iliffe vector represents the columns of an array where each column element is a pointer to a row vector.

Multidimensional arrays in languages such as [[Java (programming language)|Java]], [[Python (programming language)|Python]] (multidimensional lists), [[Ruby (programming language)|Ruby]], [[Visual Basic .NET]], [[Perl]], [[PHP]], [[JavaScript]], [[Objective-C]], [[Swift (programming language)|Swift]], and [[Atlas Autocode]] are implemented as Iliffe vectors.

Iliffe vectors are contrasted with [[dope vector]]s in languages such as [[Fortran#Fortran 90|Fortran]], which contain the stride factors and offset values for the subscripts in each dimension.

==Notes==
{{reflist}}

==References==
*{{cite journal| author=John K. Iliffe|title=The Use of The Genie System in Numerical Calculations|journal=Annual Review in Automatic Programming|volume=2|year=1961|page=25|doi=10.1016/S0066-4138(61)80002-5}}

[[Category:Arrays]]


{{comp-sci-stub}}</text>
      <sha1>shvfty0jgms9c3axd0cq4bcl6yuqfiw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Variable-length array</title>
    <ns>0</ns>
    <id>13854884</id>
    <revision>
      <id>612677094</id>
      <parentid>612676938</parentid>
      <timestamp>2014-06-12T20:04:09Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>Removing redundant link</comment>
      <text xml:space="preserve" bytes="4659">In [[computer programming]], a '''variable-length array''' (or '''VLA''') is an [[array data structure]] of [[automatic variable|automatic storage duration]] whose length is determined at run time (instead of at compile time).&lt;ref&gt;http://docs.cray.com/books/004-2179-001/html-004-2179-001/z893434830malz.html&lt;/ref&gt;

Programming languages that support VLAs include [[Ada (programming language)|Ada]], [[Algol 68]] (for non-flexible rows), [[APL (programming language)|APL]], [[C99]] (and subsequently in [[C11 (C standard revision)|C11]]&lt;ref&gt;http://pic.dhe.ibm.com/infocenter/ratdevz/v8r0/topic/com.ibm.xlcpp111.aix.doc/language_ref/variable_length_arrays.html&lt;/ref&gt; relegated to a conditional feature which implementations aren't required to support;&lt;ref&gt;http://gcc.gnu.org/onlinedocs/gcc/Variable-Length.html&lt;/ref&gt;&lt;ref&gt;ISO 9899:2011 Programming Languages - C 6.7.6.2&amp;nbsp;4&lt;/ref&gt; on some platforms, could be implemented previously with &lt;code&gt;alloca()&lt;/code&gt; or similar functions) and [[C Sharp (programming language)|C#]] (as unsafe-mode stack-allocated arrays), [[COBOL]], [[Fortran|Fortran 90]], [[J (programming language)|J]].

== Memory ==
=== Allocation ===
One problem that may be hidden by a language's support for VLAs is that of the underlying memory allocation: in environments where there is a clear distinction between a [[heap memory|heap]] and a [[stack-based memory allocation|stack]], it may not be clear which, if any, of those will store the VLA.&lt;ref&gt;https://archive.stsci.edu/fits/users_guide/node65.html&lt;/ref&gt;

For example, the [[GNU Compiler Collection|GNU C Compiler]] allocates memory for VLAs on the stack.&lt;ref&gt;http://gcc.gnu.org/onlinedocs/gfortran/Code-Gen-Options.html&lt;/ref&gt;
VLAs, like all objects in C, are limited to SIZE_MAX bytes.&lt;ref&gt;§6.5.3.4 and §7.20.3 of the C11 standard (n1570.pdf)&lt;/ref&gt;

=== Variable access ===
In some programming languages VLAs can be accessed via [[Pointer (computer programming)|pointers]], but the size can no longer be obtained when de-referenced as they are considered complete types.&lt;ref&gt;http://msdn.microsoft.com/en-us/library/4s7x1k91.aspx&lt;/ref&gt;

== Examples ==

The following [[C99]] function allocates a variable-length array of a specified size, fills it with floating-point values, then passes it to another function for processing.  Because the array is declared as an automatic variable, its lifetime ends when the &lt;code&gt;read_and_process&lt;/code&gt; function returns.

&lt;source lang=&quot;c&quot;&gt;
float read_and_process(int n)
{
    float vals[n];

    for (int i = 0; i &lt; n; i++)
        vals[i] = read_val();
    return process(vals, n);
}

&lt;/source&gt;

Following is the same example in [[Ada (programming language)|Ada]]. Note that Ada arrays
carry their bounds with them, there is no need to pass the length to the Process function.
&lt;source lang=&quot;Ada&quot;&gt;
type Vals_Type is array (Positive range &lt;&gt;) of Float;

function Read_And_Process (N : Integer) return Float is
   Vals : Vals_Type (1 .. N);
begin
   for I in 1 .. N loop
      Vals (I) := Read_Val;
   end loop;
   return Process (Vals);
end Read_And_Process;
&lt;/source&gt;

The equivalent [[Fortran|Fortran 90]] function is:

&lt;source lang=&quot;fortran&quot;&gt;
function read_and_process(n) result(o)
    integer,intent(in)::n
    real::o

    real,dimension(n)::vals
    real::read_val, process
    integer::i
 
    do i = 1,n
       vals(i) = read_val()
    end do
    o = process(vals, n)
end function read_and_process
&lt;/source&gt;

The following [[COBOL]] fragment declares a variable-length array of records, &lt;code&gt;DEPT-PERSON&lt;/code&gt;, having a length (number of members) specified by the value of &lt;code&gt;PEOPLE-CNT&lt;/code&gt;.

&lt;source lang=&quot;cobol&quot;&gt;
DATA DIVISION.
WORKING-STORAGE SECTION.
01  DEPT-PEOPLE.
    05  PEOPLE-CNT          PIC S9(4) BINARY.
    05  DEPT-PERSON         OCCURS 0 TO 20 TIMES DEPENDING ON PEOPLE-CNT.
        10  PERSON-NAME     PIC X(20).
        10  PERSON-WAGE     PIC S9(7)V99 PACKED-DECIMAL.
&lt;/source&gt;

The following [[C Sharp (programming language)|C#]] fragment declares a variable-length array of integers. The &quot;unsafe&quot; keyword would require an assembly containing this code to be marked as unsafe.

&lt;source lang=&quot;csharp&quot;&gt;
unsafe void declareStackBasedArray(int size)
{
    int *pArray = stackalloc int[size];
    pArray[0] = 123;
}
&lt;/source&gt;

==Dynamic vs. automatic==

Languages such as [[Java (programming language)|Java]] technically do not provide variable-length arrays, because all array objects in those languages are dynamically allocated on the [[dynamic memory allocation|heap]], and therefore do not have [[automatic variable|automatic storage]] duration for arrays.

== References ==
{{reflist}}

[[Category:Arrays]]</text>
      <sha1>c9gu81yutk1thijenvyw8ck3ryvexgg</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Array data type</title>
    <ns>0</ns>
    <id>22817874</id>
    <revision>
      <id>616222631</id>
      <parentid>616222296</parentid>
      <timestamp>2014-07-09T13:40:18Z</timestamp>
      <contributor>
        <username>Pratyya Ghosh</username>
        <id>16166519</id>
      </contributor>
      <comment>Reverted 1 [[WP:AGF|good faith]] edit by [[Special:Contributions/27.97.26.0|27.97.26.0]] using [[WP:STiki|STiki]]</comment>
      <text xml:space="preserve" bytes="19087">{{distinguish|Array data structure}}

In [[computer science]], an '''array type'''    is a [[data type]] that is meant to describe a collection of ''elements'' ([[value (computer science)|values]] or [[variable (computer science)|variables]]), each selected by one or more indices (identifying keys) that can be computed at [[Run time (program lifecycle phase)|run time]] by the program.  Such a collection is usually called an '''array variable''', '''array value''', or simply '''array'''.&lt;ref name=&quot;sebesta&quot;&gt;Robert W. Sebesta (2001) Concepts of Programming Languages. Addison-Wesley. 4th edition (1998), 5th edition (2001), ISBN   9780201385960&lt;/ref&gt;  By analogy with the mathematical concepts of [[vector (mathematics)|vector]] and [[matrix (mathematics)|matrix]], array types with one and two indices are often called '''vector type''' and '''matrix type''', respectively.

Language support for array types may include certain [[built-in type|built-in]] array data types, some syntactic constructions (''array type constructors'') that the [[programmer]] may use to define such types and declare array variables, and special notation for indexing array elements.&lt;ref name=&quot;sebesta&quot;/&gt;  For example, in the [[Pascal programming language]], the declaration &lt;code&gt;type MyTable = array [1..4,1..2] of integer&lt;/code&gt;, defines a new array data type called &lt;code&gt;MyTable&lt;/code&gt;. The declaration &lt;code&gt;var A: MyTable&lt;/code&gt; then defines a variable &lt;code&gt;A&lt;/code&gt; of that type, which is an aggregate of eight elements, each being an integer variable identified by two indices. In the Pascal program, those elements are denoted &lt;code&gt;A[1,1]&lt;/code&gt;, &lt;code&gt;A[1,2]&lt;/code&gt;, &lt;code&gt;A[2,1]&lt;/code&gt;,… &lt;code&gt;A[4,2]&lt;/code&gt;.&lt;ref name=&quot;pascal&quot;&gt;K. Jensen and Niklaus Wirth, PASCAL User Manual and Report. Springer. Paperback edition (2007) 184 pages, ISBN 978-3540069508&lt;/ref&gt;  Special array types are often defined by the language's standard [[library (computer science)|libraries]].

Arrays are distinguished from [[List (abstract data type)|lists]] in that arrays allow [[random access]], while lists only allow sequential access.{{citation needed|date=February 2014}} Dynamic lists are also more common and easier to implement than [[dynamic array]]s. Array types are distinguished from [[record (computer science)|record]] types mainly because they allow the element indices to be computed at [[Run time (program lifecycle phase)|run time]], as in the Pascal [[assignment statement|assignment]] &lt;code&gt;A[I,J] := A[N-I,2*J]&lt;/code&gt;.  Among other things, this feature allows a single iterative [[statement (computer science)|statement]] to process arbitrarily many elements of an array variable.

In more theoretical contexts, especially in [[type theory]] and in the description of abstract [[algorithm]]s, the terms &quot;array&quot; and &quot;array type&quot; sometimes refer to an [[abstract data type]] (ADT) also called ''abstract array'' or may refer to an ''[[associative array]]'', a [[mathematics|mathematical]] model with the basic operations and behavior of a typical array type in most languages — basically, a collection of elements that are selected by indices computed at run-time.

Depending on the language, array types may overlap (or be identified with) other data types that describe aggregates of values, such as [[list (computing)|lists]] and [[string (computer science)|strings]].  Array types are often implemented by [[array data structure]]s, but sometimes by other means, such as [[hash table]]s, [[linked list]]s, or [[search tree]]s.

== History ==

Assembly languages and low-level languages like BCPL&lt;ref&gt;John Mitchell, Concepts of Programming Languages. Cambridge University Press.&lt;/ref&gt; generally have no syntactic support for arrays.

Because of the importance of array structures for efficient computation, the earliest high-level programming languages, including [[FORTRAN]] (1957), [[COBOL]] (1960), and [[Algol 60]] (1960), provided support for multi-dimensional arrays.

==Abstract arrays==
An array data structure can be mathematically modeled as an [[abstract data structure]] (an ''abstract array'') with two operations
:''get''(''A'', ''I''): the data stored in the element of the array ''A'' whose indices are the integer [[tuple]] ''I''.
:''set''(''A'',''I'',''V''): the array that results by setting the value of that element to ''V''.
These  operations are required to satisfy the [[axiom]]s&lt;ref&gt;Lukham, Suzuki (1979), &quot;Verification of array, record, and pointer operations in Pascal&quot;. ''ACM Transactions on Programming Languages and Systems'' '''1(2)''', 226&amp;ndash;244.&lt;/ref&gt;
:''get''(''set''(''A'',''I'', ''V''), ''I'')&amp;nbsp;=&amp;nbsp;''V''
:''get''(''set''(''A'',''I'', ''V''), ''J'')&amp;nbsp;=&amp;nbsp;''get''(''A'', ''J'') if ''I''&amp;nbsp;≠&amp;nbsp;''J''
for any array state ''A'', any value ''V'', and any tuples ''I'', ''J'' for which the operations are defined.

The first axiom means that each element behaves like a variable.  The second axiom means that elements with distinct indices behave as [[aliasing (computing)|disjoint]] variables, so that storing a value in one element does not affect the value of any other element.

These axioms do not place any constraints on the set of valid index tuples ''I'', therefore this abstract model can be used for [[triangular array|triangular matrices]] and other oddly-shaped arrays.

== Implementations ==
{{Unreferenced section|date=May 2009}}

In order to effectively implement variables of such types as [[Array data structure|array structures]] (with indexing done by [[pointer arithmetic]]), many languages restrict the indices to [[integer (computer science)|integer]] data types (or other types that can be interpreted as integers, such as [[byte]]s and [[enumerated type]]s), and require that all elements have the same data type and storage size.  Most of those languages also restrict each index to a finite [[interval (mathematics)|interval]] of integers, that remains fixed throughout the lifetime of the array variable.  In some [[compiler|compiled]] languages, in fact, the index ranges may have to be known at [[compile time]].

On the other hand, some programming languages provide more liberal array types, that allow indexing by arbitrary values, such as [[floating point|floating-point numbers]],  [[string (computer science)|strings]], [[object-oriented programming|objects]], [[reference (computer science)|references]], etc.. Such index values cannot be restricted to an interval, much less a fixed interval.  So, these languages usually allow arbitrary new elements to be created at any time.  This choice precludes the implementation of array types as array data structures.  That is, those languages use array-like syntax to implement a more general [[associative array]] semantics, and must therefore be implemented by a [[hash table]] or some other [[search data structure]].

== Language support ==
{{Unreferenced section|date=May 2009}}

=== Multi-dimensional arrays ===

The number of indices needed to specify an element is called the ''dimension'', ''dimensionality'', or [[rank (computer programming)|rank]] of the array type.  (This nomenclature conflicts with the concept of dimension in linear algebra,&lt;ref&gt;see the [[Matrix (mathematics)#Definition|definition of a matrix]]&lt;/ref&gt; where it is the number of elements.  Thus, an array of numbers with 5 rows and 4 columns, hence 20 elements, is said to have dimension 2 in computing contexts, but represents a matrix with dimension 4-by-5 or 20 in mathematics.  Also, the computer science meaning of &quot;rank&quot; is similar to its [[tensor rank|meaning in tensor algebra]] but not to the linear algebra concept of [[matrix rank|rank of a matrix]].)

[[File:Array of array storage.svg|120px|right|A two-dimensional array stored as a one-dimensional array of one-dimensional arrays (rows).]]
Many languages support only one-dimensional arrays. In those languages, a multi-dimensional array is typically represented by an [[Iliffe vector]], a one-dimensional array of [[reference (computer science)|references]] to arrays of one dimension less. A two-dimensional array, in particular, would be implemented as a vector of pointers to its rows.  Thus an element in row ''i'' and column ''j'' of an array ''A'' would be accessed by double indexing (''A''[''i''][''j''] in typical notation).  This way of emulating multi-dimensional arrays allows the creation of ''ragged'' or ''jagged'' arrays, where each row may have a different size — or, in general, where the valid range of each index depends on the values of all preceding indices.

This representation for multi-dimensional arrays is quite prevalent in C and C++ software.  However, C and C++ will use a linear indexing formula for multi-dimensional arrays that are declared as such, e.g. by &lt;code&gt;int A[10][20]&lt;/code&gt; or &lt;code&gt;int A[m][n]&lt;/code&gt;, instead of the traditional &lt;code&gt;int **A&lt;/code&gt;.&lt;ref&gt;Brian W. Kernighan and Dennis M. Ritchie (1988), ''The C programming Language''. Prentice-Hall, 205 pages.&lt;/ref&gt;{{rp|p.81}}

=== Indexing notation ===

Most programming languages that support arrays support the ''store'' and ''select'' operations, and have special syntax for indexing.  Early languages used parentheses, e.g. &lt;code&gt;A(i,j)&lt;/code&gt;, as in FORTRAN; others choose square brackets, e.g. &lt;code&gt;A[i,j]&lt;/code&gt; or &lt;code&gt;A[i][j]&lt;/code&gt;, as in Algol 60 and Pascal.

=== Index types ===

Array data types are most often implemented as array structures: with the indices restricted to integer (or totally ordered) values, index ranges fixed at array creation time, and multilinear element addressing. This was the case in most [[Third-generation programming language|&quot;third generation&quot;]] languages, and is still the case of most [[systems programming language]]s such as [[Ada (programming language)|Ada]], [[C programming language|C]], and [[C++]]. In some languages, however, array data types have the semantics of associative arrays, with indices of arbitrary type and dynamic element creation. This is the case in some [[scripting languages]] such as [[Awk programming language|Awk]] and [[Lua (programming language)|Lua]], and of some array types provided by standard [[C++]] libraries.

=== Bounds checking ===

Some languages (like Pascal and Modula) perform [[bounds checking]] on every access, raising an [[exception (computer science)|exception]] or aborting the program when any index is out of its valid range. Compilers may allow these checks to be turned off to trade safety for speed. Other languages (like FORTRAN and C) trust the programmer and perform no checks.   Good compilers may also analyze the program to determine the range of possible values that the index may have, and this analysis may lead to [[bounds-checking elimination]].

=== Index origin ===

Some languages, such as C,  provide only [[Zero-based numbering|zero-based]] array types, for which the minimum valid value for any index is 0.  This choice is convenient for array implementation and address computations.  With a language such as C, a pointer to the interior of any array can be defined that will symbolically act as a pseudo-array that accommodates negative indices.  This works only because C does not check an index against bounds when used.

Other languages provide only ''one-based'' array types, where each index starts at 1; this is the traditional convention in mathematics for matrices and mathematical [[sequence]]s.  A few languages, such as Pascal, support ''n-based'' array types,  whose minimum legal indices are chosen by the programmer.  The relative merits of each choice have been the subject of heated debate.  Zero-based indexing has a natural advantage to one-based indexing in avoiding [[Off-by-one error|off-by-one]] or [[fencepost error]]s.&lt;ref&gt;[[Edsger W. Dijkstra]], [http://www.cs.utexas.edu/users/EWD/transcriptions/EWD08xx/EWD831.html Why numbering should start at zero]&lt;/ref&gt;

See [[comparison of programming languages (array)]] for the base indices used by various languages.

=== Highest index ===
The relation between numbers appearing in an array declaration and the index of that array's last element also varies by language. In many languages (such as C), one should specify the number of elements contained in the array; whereas in others (such as Pascal and [[Visual Basic .NET]]) one should specify the numeric value of the index of the last element. Needless to say, this distinction is immaterial in languages where the indices start at 1.

=== Array algebra ===

Some programming languages support [[array programming]], where operations and functions defined for certain data types are implicitly extended to arrays of elements of those types. Thus one can write ''A''+''B'' to add corresponding elements of two arrays ''A'' and ''B''.  Usually these languages provide both the [[Hadamard product (matrices)|element-by-element multiplication]] and the standard [[dot product|matrix product]] of [[linear algebra]], and which of these is represented by the ''*'' operator varies by language.

Languages providing array programming capabilities have proliferated since the innovations in this area of  [[APL programming language|APL]].  These are core capabilities of [[domain-specific language]]s such as
[[GAUSS (programming language)|GAUSS]], [[Interactive Data Language|IDL]], [[Matlab]], and [[Mathematica]]. They are a core facility in newer languages, such as [[Julia (programming language)|Julia]] and recent versions of [[Fortran]]. These capabilities are also provided via standard extension libraries for other general purpose programming languages (such as the widely used [[NumPy]] library for [[Python (programming language)|Python]]).

=== String types and arrays ===

Many languages provide a built-in [[string (computer science)|string]] data type, with specialized notation (&quot;[[string literal]]s&quot;) to build values of that type. In some languages (such as C), a string is just an array of characters, or is handled in much the same way.  Other languages, like [[Pascal programming language|Pascal]], may provide vastly different operations for strings and arrays.

=== Array index range queries ===

Some programming languages provide operations that return the size (number of elements) of a vector, or, more generally, range of each index of an array.  In [[C (programming language)|C]] and [[C++]] arrays do not support the ''size'' function, so programmers often have to declare separate variable to hold the size, and pass it to procedures as a separate parameter.

Elements of a newly created array may have undefined values (as in C), or may be defined to have a specific &quot;default&quot; value such as 0 or a null pointer (as in Java).

In [[C++]] a std::vector object supports the ''store'', ''select'', and ''append'' operations with the performance characteristics discussed above. Vectors can be queried for their size and can be resized. Slower operations like inserting an element in the middle are also supported.

=== Slicing ===

An [[array slicing]] operation takes a subset of the elements of an array-typed entity (value or variable) and then assembles them as another array-typed entity, possibly with other indices. If array types are implemented as array structures, many useful slicing operations (such as selecting a sub-array, swapping indices, or reversing the direction of the indices) can be performed very efficiently by manipulating the [[dope vector]] of the structure.  The possible slicings depend on the implementation details: for example, FORTRAN allows slicing off one column of a matrix variable, but not a row, and treat it as a vector; whereas C allow slicing off a row from a matrix, but not a column.

On the other hand, other slicing operations are possible when array types are implemented in other ways. &lt;!--- Complete: In [[Python (programming language)|Pyhon]], for example, array slices may be deleted or replaced by longer or shorter arrays--&gt;

=== Resizing ===

Some languages allow ''[[dynamic array]]s'' (also called ''resizable'', ''growable'', or ''extensible''): array variables whose index ranges may be expanded at any time after creation,  without changing the values of its current elements.

For one-dimensional arrays, this facility may be provided as an operation &quot;&lt;code&gt;append&lt;/code&gt;(''A'',''x'')&quot; that increases the size of the array ''A'' by one and then sets the value of the last element to ''x''.  Other array types (such as Pascal strings) provide a concatenation operator, which can be used together with slicing to achieve that effect and more. In some languages, assigning a value to an element of an array automatically extends the array, if necessary, to include that element. In other array types, a slice can be replaced by an array of different size&quot; with subsequent elements being renumbered accordingly — as in Python's list assignment &quot;''A''[5:5] = [10,20,30]&quot;, that inserts three new elements (10,20, and 30) before element &quot;''A''[5]&quot;.  Resizable arrays are conceptually similar to [[list (computer science)|lists]], and the two concepts are synonymous in some languages.

An extensible array can be implemented as a fixed-size array, with a counter that records how many elements are actually in use.  The &lt;code&gt;append&lt;/code&gt; operation merely increments the counter; until the whole array is used, when the &lt;code&gt;append&lt;/code&gt; operation may be defined to fail. This is an implementation of a [[dynamic array]] with a fixed capacity, as in the &lt;tt&gt;string&lt;/tt&gt; type of Pascal. Alternatively, the &lt;code&gt;append&lt;/code&gt; operation may re-allocate the underlying array with a larger size, and copy the old elements to the new area.

==See also==
*[[Array access analysis]]
*[[Array programming]]
*[[Array slicing]]
*[[Bounds checking]] and [[index checking]]
*[[Bounds checking elimination]]
*[[Delimiter-separated values]]
*[[Comparison of programming languages (array)]]
*[[Parallel array]]

=== Related types ===
*[[Variable-length array]]
*[[Dynamic array]]
*[[Sparse array]]

==References==
{{reflist|2}}

==External links==
{{Wikibooks|Data Structures/Arrays}}
{{Wiktionary|array}}
{{Commons category|Array data structure}}
* [http://www.nist.gov/dads/HTML/array.html NIST's Dictionary of Algorithms and Data Structures: Array]

{{Data types}}

[[Category:Arrays|*]]
[[Category:Data types]]
[[Category:Composite data types]]

[[bn:অ্যারে]]
[[ca:Vector (programació)]]
[[cs:Pole (datová struktura)]]
[[de:Feld (Datentyp)]]
[[es:Vector (informática)]]
[[eo:Aro (komputiko)]]
[[ko:배열]]
[[id:Larik]]
[[is:Fylki (tölvunarfræði)]]
[[it:Array]]
[[he:מערך (מבנה נתונים)]]
[[hu:Tömb]]
[[nl:Array]]
[[ja:配列]]
[[no:Tabell (datastruktur)]]
[[pl:Tablica]]
[[pt:Array]]
[[ru:Индексный массив]]
[[simple:Array]]
[[sk:Pole (údajová štruktúra)]]
[[sl:Tabela (računalništvo)]]
[[sr:Низ (структура података)]]
[[fi:Taulukko (tietorakenne)]]
[[sv:Array]]
[[ta:அணி (கணினியியல்)]]
[[tr:Dizi (bilgisayar bilimleri)]]
[[uk:Масив (структура даних)]]
[[zh:数组]]</text>
      <sha1>f485jlojozxmdec2fg821uwt4pqnelw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Range (computer programming)</title>
    <ns>0</ns>
    <id>8508082</id>
    <revision>
      <id>619011020</id>
      <parentid>572677468</parentid>
      <timestamp>2014-07-29T18:41:55Z</timestamp>
      <contributor>
        <ip>160.83.73.18</ip>
      </contributor>
      <comment>Changed link to PDF</comment>
      <text xml:space="preserve" bytes="2641">{{Refimprove|date=December 2006}}
In [[computer science]], the term '''range''' may refer to one of three things:
# The possible values that may be stored in a [[variable (programming)|variable]].
# The upper and lower bounds of an [[Array data structure|array]].
# An alternative to [[iterator]].

==Range of a variable==
The range of a variable is given as the set of possible values that that variable can hold. In the case of an integer, the variable definition is restricted to whole numbers only, and the range will cover every number within its range (including the maximum and minimum). For example, the range of a [[signedness|signed]] [[16-bit]] [[Integer (computer science)|integer]] variable is all the integers from &amp;minus;32,768 to +32,767.

==Range of an array==
{{Main|Array data type#Indexing notation}}

When an array is numerically indexed, its range is the upper and lower bound of the array. Depending on the environment, a warning, a [[fatal error]], or unpredictable behavior will occur if the program attempts to access an array element that is outside the range. In some [[programming languages]], such as [[C (programming language)|C]], arrays have a fixed lower bound (zero) and will contain data at each position up to the upper bound (so an array with 5 elements will have a range of 0 to 4). In others, such as [[PHP]], an array may have holes where no element is defined, and therefore an array with a range of 0 to 4 will have ''up to'' 5 elements (and a minimum of 2).

== Range as an alternative to iterator ==
Another meaning of ''range'' in computer science is an alternative to [[iterator]]. When used in this sense, range is defined as &quot;a pair of begin/end iterators packed together&quot;.&lt;ref name=&quot;itersmustgo&quot;&gt;{{Cite web
| author = [[Andrei Alexandrescu]]
| title = Iterators Must Go
| url = http://zao.se/~zao/boostcon/09/2009_presentations/wed/iterators-must-go.pdf
| date = 6 May 2009
| accessdate = 29 July 2014
| publisher = BoostCon 2009
}}&lt;/ref&gt; It is argued &lt;ref name=&quot;itersmustgo&quot; /&gt; that &quot;Ranges are a superior abstraction&quot; (compared to iterators) for several reasons, including better safety. 

In particular, such ranges are supported in [[Boost C++ Libraries]]&lt;ref&gt;[http://www.boost.org/libs/range/index.html Boost.Range documentation]&lt;/ref&gt; and the [[D (programming language)|D]] standard library.&lt;ref&gt;[http://dlang.org/phobos/std_range.html D Phobos Runtime Library std.range module]&lt;/ref&gt;

==See also==
*[[Interval (mathematics)|Interval]]

== References ==
{{reflist}}

{{DEFAULTSORT:Range (Computer Science)}}
[[Category:Programming constructs]]
[[Category:Arrays]]


{{Compu-prog-stub}}</text>
      <sha1>0c2sea8kncyc1smzpp84tsv6xatw4lm</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Index mapping</title>
    <ns>0</ns>
    <id>25165023</id>
    <revision>
      <id>571197881</id>
      <parentid>571197688</parentid>
      <timestamp>2013-09-02T09:31:26Z</timestamp>
      <contributor>
        <ip>41.160.45.74</ip>
      </contributor>
      <comment>/* The array was 0-based but commented as 1-based; this led to a bug in this code. Fixed comment and code. */</comment>
      <text xml:space="preserve" bytes="3185">{{multiple issues |one source=January 2012 |essay-like=January 2012}}
'''Index mapping''' is a [[computer science]] term (also known as a &quot;'''trivial [[hash function]]'''&quot;) that is used to describe the mapping{{clarification needed|date=January 2012}} of [[raw data]], used directly as in [[array index]], for an [[array data structure|array]]. The technique can be most effective for mapping data with a small range{{clarification needed|date=January 2012}}. If the array encompasses all combinations of input, a range check is not required. 

==Applicable arrays== 
In practice there are many examples of data exhibiting a small range of valid values all of which are suitable for processing{{clarification needed|date=January 2012}} using a trivial hash function including:
 
* [[month]] in the year (1–12) – see [[#C example 1|C examples]] below
* [[day]] in the month (1–31)
* [[day of the week]] (1–7)
* human lifespan{{clarification needed|date=January 2012}} (0–130) – e.g. lifecover actuary tables, fixed term mortgage  
* [[ASCII]] characters {{clarification needed|date=January 2012}} (0–127), encompassing common mathematical operator symbols, digits, punctuation marks and English language alphabet 
* [[EBCDIC]] characters {{clarification needed|date=January 2012}} (0–255)

==Examples==
The following two examples demonstrate how a simple non-iterative table lookup, using a trivial hash function, can eliminate conditional testing &amp; branching completely thereby reducing [[instruction path length]] significantly. Although both examples are shown here as functions, the required code would be better [[Inline expansion|inlined]] to avoid function call overhead in view of their obvious simplicity.

===C example 1===
This example&lt;ref&gt;[http://ols.fedoraproject.org/GCC/Reprints-2008/sayle-reprint.pdf &quot;A Superoptimizer Analysis of Multiway Branch Code Generation&quot;] by Roger Anthony Sayle&lt;/ref&gt; of a C function – returning TRUE if a month (x) contains 30 days (otherwise FALSE), illustrates the concept succinctly
&lt;source lang=&quot;c&quot;&gt;
 if (((unsigned)x &gt; 12) || ((unsigned)x &lt;= 0) return 0; /*x&gt;12 or x&lt;=0?*/
 static const int T[12] ={0,0,0,1,0,1,0,0,1,0,1,0};     /* 0-based table 'if 30 days =1,else 0'  */
 return T[x - 1];                                       /* return with boolean 1 = true, 0=false */
&lt;/source&gt;

===C example 2===
Example of another C function – incrementing a month number (x) by 1 and automatically resetting if greater than 12
&lt;source lang=&quot;c&quot;&gt;                                            
 static const int M[12] ={2,3,4,5,6,7,8,9,10,11,12,1}; /* 0-based table to increment x          */
 return M[x - 1];                                      /* return with new month number          */
&lt;/source&gt;

==See also==
*[[Multiway branch]]
*[[lookup table]]
==References==
{{Reflist}}
==External links==
*[http://ols.fedoraproject.org/GCC/Reprints-2008/sayle-reprint.pdf A Superoptimizer Analysis of Multiway Branch Code Generation] by Roger Anthony Sayle

[[Category:Arrays]]
[[Category:Associative arrays]]
[[Category:Articles with example C code]]
[[Category:Hashing|*]]
[[Category:Search algorithms]]

&lt;!-- do interwikis exist? --&gt;</text>
      <sha1>kiwesc9zpfi9tb51ytrxgpf902qgs63</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Rank (computer programming)</title>
    <ns>0</ns>
    <id>366007</id>
    <revision>
      <id>596593111</id>
      <parentid>506155033</parentid>
      <timestamp>2014-02-22T05:03:12Z</timestamp>
      <contributor>
        <username>Wizardman</username>
        <id>713860</id>
      </contributor>
      <minor/>
      <comment>clean up using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="2158">{{Unreferenced|date=December 2009}}
In [[computer programming]], '''rank''' with no further specifications is usually a synonym for (or refers to) &quot;number of dimensions&quot;; thus, a bi-dimensional array has rank ''two'', a three-dimensional array has rank ''three'' and so on.
Strictly, no formal definition can be provided which applies to every [[programming language]], since each of them has its own concepts, [[Formal semantics of programming languages|semantics]] and terminology; the term may not even be applicable or, to the contrary, applied with a very specific meaning in the context of a given language.

In the case of [[APL programming language|APL]] the notion applies to every operand; and [[Binary function|dyad]]s (&quot;binary functions&quot;) have a ''left rank'' and a ''right rank''.

The box below instead shows how ''rank of a type'' and ''rank of an array expression'' could be defined (in a semi-formal style) for C++ and illustrates a simple way to calculate them at compile time.

&lt;source lang=&quot;cpp&quot;&gt;
#include &lt;cstddef&gt;
 
/* Rank of a type
 * -------------
 *
 * Let the rank of a type T be the number of its dimensions if
 * it is an array; zero otherwise (which is the usual convention)
 */
template &lt;typename t&gt; struct rank
{ static const std::size_t value = 0; };

template&lt;typename t, std::size_t n&gt;
struct rank&lt;t[n]&gt;
{ static const std::size_t value = 1 + rank&lt;t&gt;::value; };

/* Rank of an expression
 *
 * Let the rank of an expression be the rank of its type
 */
template &lt;typename t, std::size_t n&gt;
char(&amp;rankof(t(&amp;)[n]))[n];
&lt;/source&gt;
 
Given the code above the rank of a type T can be calculated at compile time by
 
:&lt;source lang=&quot;cpp&quot;&gt;rank&lt;T&gt;::value&lt;/source&gt;
 
and the rank of an array-expression ''expr'' by
 
:&lt;source lang=&quot;cpp&quot;&gt;sizeof(rankof(expr))&lt;/source&gt;

==See also==
*[[Rank (linear algebra)]], for a definition of ''rank'' as applied to [[matrix (mathematics)|matrices]]
*[[Rank (J programming language)]], a concept of the same name in the [[J (programming language)|J programming language]]

{{DEFAULTSORT:Rank (Computer Programming)}}
[[Category:Arrays]]
[[Category:Programming language topics]]


{{Compu-lang-stub}}</text>
      <sha1>rnb2h5xde83zmdulhlf8qgzh4df991a</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Stride of an array</title>
    <ns>0</ns>
    <id>366038</id>
    <revision>
      <id>602265397</id>
      <parentid>601226716</parentid>
      <timestamp>2014-04-01T13:48:42Z</timestamp>
      <contributor>
        <ip>220.255.2.98</ip>
      </contributor>
      <comment>Undid revision 601226716 by [[Special:Contributions/67.187.2.203|67.187.2.203]] ([[User talk:67.187.2.203|talk]]) (Changed second to Sex. How did anyone miss this?)</comment>
      <text xml:space="preserve" bytes="5061">{{Unreferenced|date=December 2009}}
In [[computer programming]], the '''stride of an array''' (also referred to as '''increment''', '''pitch''' or '''step size''') refers to the number of locations in [[Computer data storage#Primary memory|memory]] between beginnings of successive [[Array data structure|array]] elements, measured in [[byte]]s or in units of the size of the array's elements. In other words: stride is the number of bytes one has to move to get from one element to the next (or previous) one.
Stride cannot be smaller than element size (it would mean that elements are overlapping) but can be larger (indicating extra space between elements).

An array with stride of exactly the same size as the size of each of its elements is contiguous in memory. Such arrays are sometimes said to have '''unit stride'''. Unit stride arrays are sometimes more efficient than non-unit stride arrays, while non-unit stride arrays are sometimes more efficient than unit stride arrays, particularly for 2D or multi-dimensional arrays, depending on the effects of [[CPU cache|caching]] and the access patterns used. This can be attributed to the [[Locality of reference|Principle of Locality]], specifically spatial locality.

==Reasons for non-unit stride==
Arrays may have a stride larger than their elements' width in bytes in at least three cases: 

=== Padding===
Many languages (including [[C (programming language)|C]] and [[C++]]) allow structures to be [[Data structure alignment|padded]] to better take advantage either of the [[word length]] and/or cache line size of the machine. For example:

&lt;source lang=&quot;c&quot;&gt;
struct ThreeBytesWide {
    char a[3];
};

struct ThreeBytesWide myArray[100];
&lt;/source&gt;

In the above code snippet, &lt;code&gt;myArray&lt;/code&gt; might well turn out to have a stride of four bytes, rather than three, if the C code were compiled for a [[32-bit]] architecture, and the compiler had optimized (as is usually the case) for minimum processing time rather than minimum memory usage.

===Overlapping parallel arrays===
Some languages allow arrays of structures to be treated as overlapping [[parallel array]]s with non-unit stride:

&lt;source lang=&quot;c&quot;&gt;
#include &lt;stdio.h&gt;

struct MyRecord {
    int value;
    char *text;
};

/* Print the contents of an array of ints with the given stride */
void print_some_ints(const int *arr, int length, size_t stride)
{
    int i;
    printf(&quot;Address\t\tValue\n&quot;);
    for (i=0; i &lt; length; ++i) {
        printf(&quot;%p\t%d\n&quot;, arr, arr[0]);
        arr = (int *)((unsigned char *)arr + stride);
    }
}

int main(void)
{
    int ints[100] = {0};
    struct MyRecord records[100] = {0};

    print_some_ints(&amp;ints[0], 100, sizeof ints[0]);
    print_some_ints(&amp;records[0].value, 100, sizeof records[0]);
    return 0;
}
&lt;/source&gt;

This idiom is a form of [[type punning]].

===Array cross-section ===
Some languages like [[PL/I]] allow what is known as an ''array cross-section'', which select certain columns  or rows from a larger array&lt;ref&gt;{{cite book|last=Hughes|first=Joan K|title=PL/I Structured Programming (second ed.)|year=1979|publisher=John Wiley and Sons|location=New York|isbn=0-471-01908-9}}&lt;/ref&gt;{{rp|p.262}}.  For example, if a two-dimensional array is declared as
&lt;source lang=PLI&gt;
  declare some_array (12,2)fixed;
&lt;/source&gt;
an array consisting only of the second column may be referenced as
&lt;source lang=PLI&gt;
  some_array(*,2)
&lt;/source&gt;

===Example of multidimensional array with non-unit stride===
Non-unit stride is particularly useful for images. It allows for creating subimages without copying the pixel data. Java example:

&lt;source lang=java&gt;
  public class GrayscaleImage {
    private final int width, height, widthStride;
    /** Pixel data. Pixel in single row are always considered contiguous in this example. */
    private final byte[] pixels;
    /** Offset of the first pixel within pixels */
    private final int offset;

    /** Constructor for contiguous data */
    public Image(int width, int height, byte[] pixels) {
      this.width = width;
      this.height = height;
      this.pixels = pixels;
      this.offset = 0;
      this.widthStride = width;
    }

    /** Subsection constructor */
    public Image(int width, int height, byte[] pixels, int offset, int widthStride) {
      this.width = width;
      this.height = height;
      this.pixels = pixels;
      this.offset = offset;
      this.widthStride = widthStride;
    }

    /** Returns a subregion of this Image as a new Image. This and the new image share
        the pixels, so changes to the returned image will be reflected in this image. */
    public Image crop(int x1, int y1, int x2, int y2) {
      return new Image(x2 - x1, y2 - y1, data, offset + y1*widthStride + x1, widthStride);
    }

    /** Returns pixel value at specified coordinate */
    public byte getPixelAt(int x, int y) {
      return pixels[offset + y * widthStride + x];
    }
  }
&lt;/source&gt;

==References==
{{Reflist}}

{{DEFAULTSORT:Stride Of An Array}}
[[Category:Arrays]]
[[Category:Articles with example C code]]</text>
      <sha1>bk02pkp0qj4ww46n68gey51khvhxhec</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Packed storage matrix</title>
    <ns>0</ns>
    <id>1451556</id>
    <revision>
      <id>549101766</id>
      <parentid>532098965</parentid>
      <timestamp>2013-04-07T04:27:09Z</timestamp>
      <contributor>
        <username>Steel1943</username>
        <id>2952402</id>
      </contributor>
      <comment>Disambiguated: [[programming]] → [[Mathematical programming]]</comment>
      <text xml:space="preserve" bytes="1446">{{Unreferenced|date=December 2009}}

A '''packed storage matrix''', also known as '''packed matrix''', is a term used in [[Mathematical programming|programming]] for representing an &lt;math&gt;m\times n&lt;/math&gt; [[Matrix (mathematics)|matrix]]. It is a more compact way than an m-by-n rectangular array by exploiting a special structure of the matrix.

Typical examples of matrices that can take advantage of packed storage include:
* [[Symmetric matrix|symmetric]] or [[hermitian matrix]]
* [[Triangular matrix]]
* [[Banded matrix]].

==Code examples (Fortran)==
Both of the following storage schemes are used extensively in BLAS and LAPACK.

An example of packed storage for hermitian matrix:
&lt;pre&gt;
complex:: A(n,n) ! a hermitian matrix
complex:: AP(n*(n+1)/2) ! packed storage for A
! the lower triangle of A is stored column-by-column in AP.
! unpacking the matrix AP to A
do j=1,n
  k = j*(j-1)/2
  A(1:j,j) = AP(1+k:j+k)
  A(j,1:j-1) = conjg(AP(1+k:j-1+k))
end do
&lt;/pre&gt;

An example of packed storage for banded matrix:

&lt;pre&gt;
real:: A(m,n) ! a banded matrix with kl subdiagonals and ku superdiagonals
real:: AP(-kl:ku,n) ! packed storage for A
! the band of A is stored column-by-column in AP. Some elements of AP are unused.
! unpacking the matrix AP to A
do j=1,n
  forall(i=max(1,j-kl):min(m,j+ku)) A(i,j) = AP(i-j,j)
end do
print *,AP(0,:) ! the diagonal
&lt;/pre&gt;

{{DEFAULTSORT:Packed Storage Matrix}}
[[Category:Arrays]]
[[Category:Matrices]]</text>
      <sha1>3rcaxwie8miscxvdlmn7iup0k8bfjxa</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Lookup table</title>
    <ns>0</ns>
    <id>356457</id>
    <revision>
      <id>626661634</id>
      <parentid>624482879</parentid>
      <timestamp>2014-09-22T18:58:43Z</timestamp>
      <contributor>
        <ip>128.244.38.5</ip>
      </contributor>
      <comment>updated dead link</comment>
      <text xml:space="preserve" bytes="18143">In [[computer science]], a '''lookup table''' is an [[Array data structure|array]] that replaces runtime computation with a simpler array indexing operation. The savings in terms of processing time can be significant, since retrieving a value from memory is often faster than undergoing an 'expensive' computation or [[input/output]] operation.&lt;ref&gt;http://pmcnamee.net/c++-memoization.html&lt;/ref&gt; The tables may be precalculated  and stored in [[Static memory allocation|static]] program storage, calculated (or [[Prefetcher|&quot;pre-fetched&quot;]]) as part of a program's initialization phase ([[memoization]]&lt;!--note: not the same as 'memorization'--&gt;), or even stored in hardware in application-specific platforms. Lookup tables are also used extensively to validate input values by matching against a list of valid (or invalid) items in an array and, in some programming languages, may include pointer functions (or offsets to labels) to process the matching input.

== History ==
[[Image:Abramowitz&amp;Stegun.page97.agr.jpg|thumb|Part of a 20th-century table of [[common logarithm]]s in the reference book [[Abramowitz and Stegun]].]]
Before the advent of computers, lookup tables of values were used to speed up hand calculations of complex functions, such as in [[trigonometry]], [[Common logarithm|logarithms]], and statistical density functions&lt;ref&gt;{{cite book
|editor1-last= Campbell-Kelly
|editor1-first= Martin
|editor2-last= Croarken
|editor2-first= Mary
|editor3-last= Robson
|editor3-first= Eleanor
|title=  The History of Mathematical Tables From Sumer to Spreadsheets
|url=
|edition= 1st
|date= October 2, 2003
|origyear= 2003
|location= New York, USA
|isbn= 978-0-19-850841-0
|oclc=
|doi=
|bibcode=
}}
&lt;/ref&gt;

In ancient (499 CE) India, [[Aryabhata]] created one of the first [[Aryabhata's sine table|sine tables]], which he encoded in a Sanskrit-letter-based number system. In 493 A.D., [[Victorius of Aquitaine]] wrote a 98-column multiplication table which gave (in [[Roman numerals]]) the product of every number from 2 to 50 times and the rows were &quot;a list of numbers starting with one thousand, descending by hundreds to one hundred, then descending by tens to ten, then by ones to one, and then the fractions down to 1/144&quot; &lt;ref&gt;Maher, David. W. J. and John F. Makowski. &quot;Literary Evidence for Roman Arithmetic With Fractions&quot;,   'Classical Philology'  (2001) Vol. 96  No. 4 (2001)  pp. 376–399.  (See page p.383.)&lt;/ref&gt; Modern school children are often taught to memorize &quot;[[times table]]s&quot; to avoid calculations of the most commonly used numbers (up to 9 x 9 or 12 x 12).

Early in the history of computers, [[input/output]] operations were particularly slow – even in comparison to processor speeds of the time. It made sense to reduce expensive read operations by a form of manual [[cache (computing)|caching]] by creating either static lookup tables (embedded in the program) or dynamic prefetched arrays to contain only the most commonly occurring data items. Despite the introduction of systemwide caching that now automates this process, application level lookup tables can still improve performance for data items that rarely, if ever, change.

== Examples ==

=== Simple lookup in an array, an associative array or a linked list (unsorted list) ===
This is known as a [[linear search]] or [[brute-force search]], each element being checked for equality in turn and the associated value, if any, used as a result of the search. This is often the slowest search method unless frequently occurring values occur early in the list. For a one-dimensional array or [[linked list]], the lookup is usually to determine whether or not there is a match with an 'input' data value.

=== Binary search in an array or an associative array (sorted list) ===
An example of a &quot;[[divide and conquer algorithm]]&quot;, [[binary search]] involves each element being found by determining which half of the table a match may be found in and repeating until either success or failure. Only possible if the list is sorted but gives good performance even if the list is lengthy.

=== Trivial hash function ===
For a [[trivial hash function]] lookup, the unsigned [[raw data]] value is used ''directly'' as an index to a one-dimensional table to extract a result. For small ranges, this can be amongst the fastest lookup, even exceeding binary search speed with zero branches and executing in [[constant time]].

====Counting '' bits in a series of bytes====
One discrete problem that is expensive to solve on many computers, is that of counting the number of bits which are set to 1 in a (binary) number, sometimes called the ''[[Hamming weight|population function]]''. For example, the decimal number &quot;37&quot; is &quot;00100101&quot; in binary, so it contains three bits that are set to binary &quot;1&quot;.

A simple example of [[C (programming language)|C]] code, designed to count the 1 bits in a ''int'', might look like this:

&lt;source lang=&quot;c&quot;&gt;
int count_ones(unsigned int x) {
    int result = 0;
    while (x != 0)
        result++, x = x &amp; (x-1);
    return result;
}
&lt;/source&gt;

This apparently simple algorithm can take potentially hundreds of cycles even on a modern architecture, because it makes many branches in the loop - and branching is slow. This can be ameliorated using [[loop unrolling]] and some other compiler optimizations. There is however a simple and much faster algorithmic solution - using a [[trivial hash function]] table lookup.

Simply construct a static table, ''bits_set'', with 256 entries giving the number of one bits set in each possible byte value (e.g. 0x00 = 0, 0x01 = 1, 0x02 = 1, and so on). Then use this table to find the number of ones in each byte of the integer using a [[trivial hash function]] lookup on each byte in turn, and sum them. This requires no branches, and just four indexed memory accesses, considerably faster than the earlier code.
 
&lt;source lang=&quot;c&quot;&gt;
 /* (this code assumes that 'int' is 32-bits wide) */
 int count_ones(unsigned int x) {
    return bits_set[ x        &amp; 255] + bits_set[(x &gt;&gt;  8) &amp; 255]
         + bits_set[(x &gt;&gt; 16) &amp; 255] + bits_set[(x &gt;&gt; 24) &amp; 255];
}
&lt;/source&gt;
The above source can be improved easily, (avoiding AND'ing, and shifting) by 'recasting' 'x' as a 4 byte unsigned char array and, preferably, coded in-line as a single statement instead of being a function.
Note that even this simple algorithm can be too slow now, because the original code might run faster from the cache of modern processors, and (large) lookup tables do not fit well in caches and can cause a slower access to memory (in addition, in the above example, it requires computing addresses within a table, to perform the four lookups needed).

=== Lookup tables in image processing ===
In data analysis applications, such as [[Image Processing|image processing]], a lookup table (LUT) is used to transform the input data into a more desirable output format.  For example, a grayscale picture of the planet Saturn will be transformed into a color image to emphasize the differences in its rings.

A classic example of reducing run-time computations using lookup tables is to obtain the result of a [[trigonometry]] calculation, such as the [[sine]] of a value. Calculating trigonometric functions can substantially slow a computing application.  The same application can finish much sooner when it first precalculates the sine of a number of values, for example for each whole number of degrees (The table can be defined as static variables at compile time, reducing repeated run time costs). 
When the program requires the sine of a value, it can use the lookup table to retrieve the closest sine value from a memory address, and may also take the step of interpolating to the sine of the desired value, instead of calculating by mathematical formula. Lookup tables are thus used by mathematics co-processors in computer systems. An error in a lookup table was responsible for Intel's infamous [[Pentium FDIV bug|floating-point divide bug]].

Functions of a single variable (such as sine and cosine) may be implemented by a simple array.  Functions involving two or more variables require multidimensional array indexing techniques.  The latter case may thus employ a two-dimensional array of '''power[x][y]''' to replace a function to calculate '''x&lt;sup&gt;y&lt;/sup&gt;''' for a limited range of x and y values. Functions that have more than one result may be implemented with lookup tables that are arrays of structures.

As mentioned, there are intermediate solutions that use tables in combination with a small amount of computation, often using [[interpolation]]. Pre-calculation combined with interpolation can produce higher accuracy for values that fall between two precomputed values. This technique requires slightly more time to be performed but can greatly enhance accuracy in applications that require the higher accuracy. Depending on the values being precomputed, pre-computation with interpolation can also be used to shrink the lookup table size while maintaining accuracy.

In [[image processing]], lookup tables are often called '''[[3D LUT|LUT]]'''s and give an output value for each of a range of index values. One common LUT, called the ''colormap'' or ''[[Palette (computing)|palette]]'', is used to determine the colors and intensity values with which a particular image will be displayed. In [[computed tomography]],  &quot;windowing&quot; refers to a related concept for determining how to display the intensity of measured radiation..

While often effective, employing a lookup table may nevertheless result in a severe penalty if the computation that the LUT replaces is relatively simple. Memory retrieval time and the complexity of memory requirements can increase application operation time and system complexity relative to what would be required by straight formula computation. The possibility of [[Cache pollution|polluting the cache]] may also become a problem. Table accesses for large tables will almost certainly cause a [[cache miss]]. This phenomenon is increasingly becoming an issue as processors outpace memory. A similar issue appears in [[rematerialization]], a [[compiler optimization]]. In some environments, such as the [[Java (programming language)|Java programming language]], table lookups can be even more expensive due to mandatory bounds-checking involving an additional comparison and branch for each lookup.

There are two fundamental limitations on when it is possible to construct a lookup table for a required operation. One is the amount of memory that is available: one cannot construct a lookup table larger than the space available for the table, although it is possible to construct disk-based lookup tables at the expense of lookup time. The other is the time required to compute the table values in the first instance; although this usually needs to be done only once, if it takes a prohibitively long time, it may make the use of a lookup table an inappropriate solution. As previously stated however, tables can be statically defined in many cases.

=== Computing sines ===

Most computers, which only perform basic arithmetic operations, cannot directly calculate the [[sine]] of a given value. Instead, they use the [[CORDIC]] algorithm or a complex formula such as the following [[Taylor series]] to compute the value of sine to a high degree of precision:

:&lt;math&gt;\operatorname{sin}(x) \approx x - \frac{x^3}{6} + \frac{x^5}{120} - \frac{x^7}{5040}&lt;/math&gt; (for ''x'' close to 0)

However, this can be expensive to compute, especially on slow processors, and there are many applications, particularly in traditional [[computer graphics]], that need to compute many thousands of sine values every second. A common solution is to initially compute the sine of many evenly distributed values, and then to find the sine of ''x'' we choose the sine of the value closest to ''x''. This will be close to the correct value because sine is a [[continuous function]] with a bounded rate of change. For example:

  ''real array'' sine_table[-1000..1000]
  '''for''' x '''from''' -1000 '''to''' 1000
      sine_table[x] := sine(pi * x / 1000)

  '''function''' lookup_sine(x)
      '''return''' sine_table[round(1000 * x / pi)]

[[Image:Interpolation example linear.svg|thumb|Linear interpolation on a portion of the sine function|right]]

Unfortunately, the table requires quite a bit of space: if IEEE double-precision floating-point numbers are used, over 16,000 bytes would be required. We can use fewer samples, but then our precision will significantly worsen. One good solution is [[linear interpolation]], which draws a line between the two points in the table on either side of the value and locates the answer on that line. This is still quick to compute, and much more accurate for [[smooth function]]s such as the sine function. Here is our example using linear interpolation:

  '''function''' lookup_sine(x)
      x1 := floor(x*1000/pi)
      y1 := sine_table[x1]
      y2 := sine_table[x1+1]
      '''return''' y1 + (y2-y1)*(x*1000/pi-x1)

Another solution that uses a quarter of the space but takes a bit longer to compute would be to take into account the relationships between sine and cosine along with their symmetry rules. In this case, the lookup table is calculated by using the sine function for the first quadrant (i.e. sin(0..pi/2)). When we need a value, we assign a variable to be the angle wrapped to the first quadrant. We then wrap the angle to the four quadrants (not needed if values are always between 0 and 2*pi) and return the correct value (i.e. first quadrant is a straight return, second quadrant is read from pi/2-x, third and fourth are negatives of the first and second respectively). For cosine, we only have to return the angle shifted by pi/2 (i.e. x+pi/2). For tangent, we divide the sine by the cosine (divide-by-zero handling may be needed depending on implementation):

  '''function''' init_sine()
      '''for''' x '''from''' 0 '''to''' (360/4)+1
          sine_table[x] := sine(2*pi * x / 360)
  
  '''function''' lookup_sine(x)
      x  = '''wrap''' x '''from''' 0 '''to''' 360
      y := '''mod''' (x, 90)
  
      '''if''' (x &lt;  90) '''return'''  sine_table[   y]
      '''if''' (x &lt; 180) '''return'''  sine_table[90-y]
      '''if''' (x &lt; 270) '''return''' -sine_table[   y]
                   '''return''' -sine_table[90-y]
  
  '''function''' lookup_cosine(x)
      '''return''' lookup_sine(x + 90)
  
  '''function''' lookup_tan(x)
      '''return''' (lookup_sine(x) / lookup_cosine(x))

When using interpolation, the size of the lookup table can be reduced by using ''[[nonuniform sampling]]'', which means that where the function is close to straight, we use few sample points, while where it changes value quickly we use more sample points to keep the approximation close to the real curve. For more information, see [[interpolation]].

== Other usage of lookup tables ==

=== Caches ===
{{main|Cache (computing)}}
Storage caches (including disk caches for files, or processor caches for either code or data) work also like a lookup table. The table is built with very fast memory instead of being stored on slower external memory, and maintains two pieces of data for a subrange of bits composing an external memory (or disk) address (notably the lowest bits of any possible external address):
* one piece (the tag) contains the value of the remaining bits of the address; if these bits match with those from the memory address to read or write, then the other piece contains the cached value for this address.
* the other piece maintains the data associated to that address.
A single (fast) lookup is performed to read the tag in the lookup table at the index specified by the lowest bits of the desired external storage address, and to determine if the memory address is hit by the cache. When a hit is found, no access to external memory is needed (except for write operations, where the cached value may need to be updated asynchronously to the slower memory after some time, or if the position in the cache must be replaced to cache another address).

=== Hardware LUTs ===
{{expand section|date=April 2014|reason=too abstract}}
In [[digital logic]], an ''n''-bit lookup table can be implemented with a [[multiplexer]] whose select lines are the inputs of the LUT and whose inputs are constants. An ''n''-bit LUT can encode any ''n''-input [[Boolean function]] by modeling such functions as [[truth table]]s. This is an efficient way of encoding [[Boolean logic]] functions, and LUTs with 4-6 bits of input are in fact the key component of modern [[field-programmable gate array]]s (FPGAs).

== See also ==
* [http://www.avidil.net/theegoworks-3d-luts/ TheEgoWorks LUTs] A LUT emulating the Hollywood epic teal-orange look.
*[[Branch table]]
*[[Gal's accurate tables]]
*[[Memoization]]
*[[Memory bound function]]
*[[Shift register lookup table]]
*[[Palette (computing)|Palette]] and [[CLUT|Colour Look-Up Table]] – for the usage in computer graphics
*[[3D LUT]] – usage in film

==References==
{{reflist}}

==External links==
* [http://en.wikibooks.org/wiki/360_Assembly/Branch_Instructions Fast table lookup using input character as index for branch table]
* [http://webster.cs.ucr.edu/AoA/Windows/HTML/TableLookups.html Art of Assembly: Calculation via Table Lookups]{{dead link|date=May 2014}}
* [http://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetTable &quot;Bit Twiddling Hacks&quot; (includes lookup tables)] By Sean Eron Anderson of [[Stanford university]]
* [https://web.archive.org/web/20120831094028/http://apl.jhu.edu/~paulmac/c++-memoization.html Memoization in C++] by Paul McNamee, [[Johns Hopkins University]] showing savings
* [http://books.google.co.uk/books?id=gJrmszNHQV4C&amp;lpg=PT169&amp;dq=beautiful%20code%20%22population%20count%22&amp;pg=PT169#v=onepage&amp;q=beautiful%20code%20%22population%20count%22&amp;f=false &quot;The Quest for an Accelerated Population Count&quot; ] by  Henry S. Warren, Jr.

{{DEFAULTSORT:Lookup Table}}
[[Category:Arrays]]
[[Category:Associative arrays]]
[[Category:Computer performance]]
[[Category:Software optimization]]
[[Category:Articles with example C code]]</text>
      <sha1>ckxh5xsmo99cttkmcsb14t2d0nkgcm4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Bounds checking</title>
    <ns>0</ns>
    <id>486833</id>
    <revision>
      <id>618515575</id>
      <parentid>611900762</parentid>
      <timestamp>2014-07-26T07:46:06Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>Removing invisible Unicode character/ Replacing [[hard space]]s  See [[WP:CHECKWIKI]] error #16 fix + other fixes, replaced: →   (16) using [[Project:AWB|AWB]] (10331)</comment>
      <text xml:space="preserve" bytes="6697">{{no footnotes|date=March 2012}}
In [[computer programming]], '''bounds checking''' is any method of detecting whether a [[variable (programming)|variable]] is within some [[upper and lower bounds|bounds]] before it is used. It is usually used to ensure that a number fits into a given type (range checking), or that a variable being used as an [[Array data structure|array]] index is within the bounds of the array (index checking). A failed bounds check usually results in the generation of some sort of [[Exception handling|exception]] signal.

Because performing bounds checking during every usage is time-consuming, it is not always done. [[Bounds-checking elimination]] is a [[compiler optimization]] technique that eliminates unneeded bounds checking.

==Range checking==
A range check is a check to make sure a number is within a certain range; for example, to ensure that a value about to be assigned to a sixteen-bit integer is within the capacity of a sixteen-bit integer (i.e. checking against [[Arithmetic overflow|wrap-around]]). This is not quite the same as [[type checking]]. Other range checks may be more restrictive; for example, a variable to hold the number of a calendar month may be declared to accept only the range 1 to 12.

==Index checking==
Index checking means that, in all [[Expression (programming)|expressions]] indexing an array, the index value is checked against the bounds of the array (which were established when the array was defined), and if the index is out-of-bounds, further execution is suspended via some sort of error. Because using a number outside of the upper range in an array may cause the program to crash, or may introduce security vulnerabilities (see [[buffer overflow]]), index checking is a part of many [[High-level programming language|high-level languages]].

Pascal, Fortran, Java have index checking ability. The [[VAX]] computer has an INDEX assembly instruction for array index checking which takes six operands, all of which can use any VAX addressing mode. The B6500 and similar [[Burroughs Corporation|Burroughs]] computers performed bound checking via hardware, irrespective of which computer language had been compiled to produce the machine code.  A limited number of later [[CPU]]s have specialised instructions for checking bounds, e.g. The CHK2 instruction on the [[Motorola 68000#Interrupts|Motorola 68000]] series.

Many [[programming language]]s, such as [[C (programming language)|C]], never perform automatic bounds checking to raise speed. However, this leaves many [[off-by-one error]]s and [[buffer overflow]]s uncaught. Many programmers believe these languages sacrifice too much for rapid execution.{{who|date=July 2013}} In his 1980 [[Turing Award]] lecture, [[C. A. R. Hoare]] described his experience in the design of [[ALGOL 60]], a language that included bounds checking, saying:

&lt;blockquote&gt;A consequence of this principle is that every occurrence of every subscript of every subscripted variable was on every occasion checked at run time against both the upper and the lower declared bounds of the array. Many years later we asked our customers whether they wished us to provide an option to switch off these checks in the interest of efficiency on production runs. Unanimously, they urged us not to—they already knew how frequently subscript errors occur on production runs where failure to detect them could be disastrous. I note with fear and horror that even in 1980, language designers and users have not learned this lesson. In any respectable branch of engineering, failure to observe such elementary precautions would have long been against the law.&lt;/blockquote&gt;

Mainstream languages that enforce run time checking include [[Ada (programming language)|Ada]], [[C Sharp (programming language)|C#]], [[Haskell (programming language)|Haskell]], [[Java (programming language)|Java]], [[JavaScript]], [[Lisp (programming language)|Lisp]], [[PHP]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]], and [[Visual Basic]]. The [[D (programming language)|D]] and [[OCaml]] languages have run time bounds checking that is enabled or disabled with a compiler switch. C# also supports ''unsafe regions'': sections of code that (among other things) temporarily suspend bounds checking to raise efficiency. These are useful for speeding up small time-critical bottlenecks without sacrificing the safety of a whole program.

== Data validation ==

In the context of data collection and data quality, bounds checking refers to checking that the data is not trivially invalid. For example, a percentage measurement must be in the range 0 to 100; the height of an adult person must be in the range 0 to 3 meters.

== See also ==
* [[Dynamic code analysis]]

==References==
* “[http://www.feustel.us/Feustel%20&amp;%20Associates/Advantages.pdf On The Advantages Of Tagged Architecture]”, IEEE Transactions On Computers, Volume C-22, Number 7, July, 1973.

* “[http://zoo.cs.yale.edu/classes/cs422/2011/bib/hoare81emperor.pdf The Emperor’s Old Clothes]”, The 1980 ACM Turing Award Lecture, CACM volume 24 number 2, February 1981, pp 75–83.

* “[http://www.doc.ic.ac.uk/~phjk/BoundsChecking.html Bounds Checking for C]”, Richard Jones and Paul Kelly, Imperial College, July 1995.

* “[http://public.support.unisys.com/aseries/docs/clearpath-mcp-11.0/pdf/38347639-000.pdf ClearPath Enterprise Servers MCP Security Overview]”, Unisys, April 2006.

* “[http://llvm.org/pubs/2007-SOSP-SVA.pdf Secure Virtual Architecture: A Safe Execution Environment for Commodity Operating Systems]”, John Criswell, Andrew Lenharth, Dinakar Dhurjati, Vikram Adve, SOSP'07 21st ACM Symposium on Operating Systems Principles, 2007.

* “[http://staff.aist.go.jp/y.oiwa/FailSafeC/index-en.html Fail-Safe C]”, Yutaka Oiwa. Implementation of the Memory-safe Full ANSI-C Compiler. ACM SIGPLAN Conference on Programing Language Design and Implementations (PLDI2009), June 2009.

* “[http://code.google.com/p/address-sanitizer/ address-sanitizer]”, Timur Iskhodzhanov, Alexander Potapenko, Alexey Samsonov, Kostya Serebryany, Evgeniy Stepanov, Dmitriy Vyukov, LLVM Dev Meeting, November 18, 2011.
* [http://sourceforge.net/projects/safeclib/ Safe C Library of Bounded APIs]

* {{cite journal|last=|first=|date=February 20, 2009|title=The Safe C Library|journal=[[Dr. Dobb's Journal]]|url=http://www.drdobbs.com/cpp/the-safe-c-library/214502214}}
* [http://www.owasp.org.cn/OWASP_Conference/2011/17_.pdf Safe C API—Concise solution of buffer overflow, The OWASP Foundation, OWASP AppSec, Beijing 2011 ]

{{DEFAULTSORT:Bounds Checking}}
[[Category:Computer errors]]
[[Category:Arrays]]</text>
      <sha1>428rq6i8tbkh2iz5e6ipsbbzrr6u72f</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Dope vector</title>
    <ns>0</ns>
    <id>366016</id>
    <revision>
      <id>571585342</id>
      <parentid>571577566</parentid>
      <timestamp>2013-09-05T00:57:38Z</timestamp>
      <contributor>
        <username>Peter Flass</username>
        <id>7557079</id>
      </contributor>
      <minor/>
      <comment>Fixing typo raised by [[User:BracketBot|BracketBot]]</comment>
      <text xml:space="preserve" bytes="1831">In [[computer programming]], a '''dope vector''' is a [[data structure]] used to hold information about a [[data object]],&lt;ref&gt;Pratt T. and M. Zelkowitz, Programming Languages: Design and Implementation (Third Edition), Prentice Hall, Upper Saddle River, NJ, (1996) pp 114&lt;/ref&gt; e.g. an [[Array data structure|array]], especially its [[Computer Storage|memory layout]]. 

A dope vector typically contains information about the type of array element, [[Rank (computer programming)|rank of an array]], the [[extents of an array]], and the [[stride of an array]] as well as a pointer to the block in memory containing the array elements.

It is often used in [[compilers]] to pass entire arrays between [[Subroutine|procedures]] in a [[high level language]] like [[Fortran]]. 
 
The dope vector includes an identifier, a length, a parent address, and a next child address.  The identifier was an assigned name and was mostly useless, but the length was the amount of allocated storage to this vector from the end of the dope vector that contained data of use to the internal processes of the computer. This length by many was called the offset, span of vector length. The parent and child references were absolute core references, or register and offset settings to the parent or child depending on the type of computer.  

Dope vectors were managed internally by the operating system and allowed the processor to allocate and de-allocate storage in specific segments as needed.

Later dope vectors had a status bit that told the system if they were active; if it was not active it would be reallocated when needed. Using this technology the computer could perform a more granular memory management.

==See also==
[[Data descriptor]]

==References==
&lt;references/&gt;

{{DEFAULTSORT:Dope Vector}}
[[Category:Arrays]]


{{compu-prog-stub}}</text>
      <sha1>1hubrkea68alip97g02p520e5oi9m7f</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Dynamic array</title>
    <ns>0</ns>
    <id>1456434</id>
    <revision>
      <id>608766020</id>
      <parentid>608677078</parentid>
      <timestamp>2014-05-16T00:55:43Z</timestamp>
      <contributor>
        <ip>75.79.3.84</ip>
      </contributor>
      <comment>/* Performance */  consistency</comment>
      <text xml:space="preserve" bytes="12601">[[File:Dynamic array.svg|thumb|Several values are inserted at the end of a dynamic array using geometric expansion. Grey cells indicate space reserved for expansion. Most insertions are fast (constant time), while some are slow due to the need for reallocation (&amp;Theta;(''n'') time, labelled with turtles). The ''logical size'' and ''capacity'' of the final array are shown.]]
In [[computer science]], a '''dynamic array''', '''growable array''', '''resizable array''', '''dynamic table''', '''mutable array''', or '''array list''' is a [[random access]], variable-size list [[data structure]] that allows elements to be added or removed. It is supplied with standard libraries in many modern mainstream programming languages.

A dynamic array is not the same thing as a [[dynamic memory allocation|dynamically allocated array]], which is a fixed-size [[Array data structure|array]] whose size is fixed when the array is allocated, although a dynamic array may use such a fixed-size array as a back end.&lt;ref name=&quot;java_util_ArrayList&quot;&gt;See, for example, the [http://hg.openjdk.java.net/jdk6/jdk6/jdk/file/e0e25ac28560/src/share/classes/java/util/ArrayList.java source code of java.util.ArrayList class from OpenJDK 6].&lt;/ref&gt;

== Bounded-size dynamic arrays and capacity ==

The simplest dynamic array is constructed by allocating a fixed-size array and then dividing it into two parts: the first stores the elements of the dynamic array and the second is reserved, or unused. We can then add or remove elements at the end of the dynamic array in constant time by using the reserved space, until this space is completely consumed. The number of elements used by the dynamic array contents is its ''logical size'' or ''size'', while the size of the underlying array is called the dynamic array's ''capacity'' or ''physical size'', which is the maximum possible size without relocating data.&lt;ref&gt;{{citation|author=Lambert, Kenneth Alfred|title=Physical size and logical size|work=Fundamentals of Python: From First Programs Through Data Structures|page=510|url=http://books.google.com/books?id=VtfM3YGW5jYC&amp;pg=PA518&amp;lpg=PA518&amp;dq=%22logical+size%22+%22dynamic+array%22&amp;source=bl&amp;ots=9rXJ9tGomJ&amp;sig=D5dRs802ax43NmEpKa1BUWFk1qs&amp;hl=en&amp;sa=X&amp;ei=CC1JUcLqGufLigKTjYGwCA&amp;ved=0CGkQ6AEwBw#v=onepage&amp;q=%22logical%20size%22%20%22dynamic%20array%22&amp;f=false|publisher=Cengage Learning|year=2009|isbn=1423902181}}&lt;/ref&gt;

In applications where the logical size is bounded, the fixed-size data structure suffices. This may be short-sighted, as more space may be needed later. A [[List of software development philosophies|philosophical programmer]] may prefer to write the code to make every array capable of resizing from the outset, then return to using fixed-size arrays during [[program optimization]]. Resizing the underlying array is an expensive task, typically involving copying the entire contents of the array.

== Geometric expansion and amortized cost ==

To avoid incurring the cost of resizing many times, dynamic arrays resize by a large amount, such as doubling in size, and use the reserved space for future expansion. The operation of adding an element to the end might work as follows:
&lt;syntaxhighlight lang=&quot;c&quot;&gt;
function insertEnd(dynarray a, element e)
    if (a.size = a.capacity)
        // resize a to twice its current capacity:
        a.capacity ← a.capacity * 2 
        // (copy the contents to the new memory location here)
    a[a.size] ← e
    a.size ← a.size + 1
&lt;/syntaxhighlight&gt;
As ''n'' elements are inserted, the capacities form a [[geometric progression]]. Expanding the array by any constant proportion ensures that inserting ''n'' elements takes [[Big O notation|''O''(''n'')]] time overall, meaning that each insertion takes [[Amortized analysis|amortized]] constant time. The value of this proportion ''a'' leads to a time-space tradeoff: the average time per insertion operation is about ''a''/(''a''−1), while the number of wasted cells is bounded above by (''a''−1)''n''. The choice of ''a'' depends on the library or application: some textbooks use ''a''&amp;nbsp;=&amp;nbsp;2,&lt;ref name=&quot;gt-ad&quot;&gt;{{citation|first1=Michael T.|last1=Goodrich|author1-link=Michael T. Goodrich|first2=Roberto|last2=Tamassia|author2-link=Roberto Tamassia|title=Algorithm Design: Foundations, Analysis and Internet Examples|publisher=Wiley|year=2002|contribution=1.5.2 Analyzing an Extendable Array Implementation|pages=39–41}}.&lt;/ref&gt;&lt;ref name=&quot;clrs&quot;&gt;{{Introduction to Algorithms|chapter=17.4 Dynamic tables|edition=2|pages=416–424}}&lt;/ref&gt; but Java's ArrayList implementation uses ''a''&amp;nbsp;=&amp;nbsp;3/2&lt;ref name=&quot;java_util_ArrayList&quot; /&gt; and the C implementation of [[Python (programming language)|Python]]'s list data structure uses ''a''&amp;nbsp;=&amp;nbsp;9/8.&lt;ref&gt;[http://svn.python.org/projects/python/trunk/Objects/listobject.c List object implementation] from python.org, retrieved 2011-09-27.&lt;/ref&gt;

Many dynamic arrays also deallocate some of the underlying storage if its size drops below a certain threshold, such as 30% of the capacity. This threshold must be strictly smaller than 1/''a'' in order to support mixed sequences of insertions and removals with amortized constant cost.

Dynamic arrays are a common example when teaching [[amortized analysis]].&lt;ref name=&quot;gt-ad&quot;/&gt;&lt;ref name=&quot;clrs&quot;/&gt;

== Performance ==

{{List data structure comparison}}
The dynamic array has performance similar to an array, with the addition of new operations to add and remove elements:

* Getting or setting the value at a particular index (constant time)
* Iterating over the elements in order (linear time, good cache performance)
* Inserting or deleting an element in the middle of the array (linear time)
* Inserting or deleting an element at the end of the array (constant amortized time)

Dynamic arrays benefit from many of the advantages of arrays, including good [[locality of reference]] and [[data cache]] utilization, compactness (low memory use), and [[random access]]. They usually have only a small fixed additional overhead for storing information about the size and capacity. This makes dynamic arrays an attractive tool for building cache-friendly data structures. However, in languages like Python or Java that enforce reference semantics, the dynamic array generally will not store the actual data, but rather it will store references to the data that resides in other areas of memory. In this case, accessing items in the array sequentially will actually involve accessing multiple non-contiguous areas of memory, so the many advantages of the cache-friendliness of this data structure are lost.

Compared to [[linked list]]s, dynamic arrays have faster indexing (constant time versus linear time) and typically faster iteration due to improved locality of reference; however, dynamic arrays require linear time to insert or delete at an arbitrary location, since all following elements must be moved, while linked lists can do this in constant time. This disadvantage is mitigated by the [[gap buffer]] and ''tiered vector'' variants discussed under ''Variants'' below.  Also, in a highly [[Fragmentation (computer)|fragmented]] memory region, it may be expensive or impossible to find contiguous space for a large dynamic array, whereas linked lists do not require the whole data structure to be stored contiguously.

A [[Self-balancing binary search tree|balanced tree]] can store a list while providing all operations of both dynamic arrays and linked lists reasonably efficiently, but both insertion at the end and iteration over the list are slower than for a dynamic array, in theory and in practice, due to non-contiguous storage and tree traversal/manipulation overhead.

== Variants ==
[[Gap buffer]]s are similar to dynamic arrays but allow efficient insertion and deletion operations clustered near the same arbitrary location. Some [[deque]] implementations use [[Deque#Implementations|array deques]], which allow amortized constant time insertion/removal at both ends, instead of just one end.

Goodrich&lt;ref&gt;{{Citation | title=Tiered Vectors: Efficient Dynamic Arrays for Rank-Based Sequences | first1=Michael T. | last1=Goodrich | author1-link = Michael T. Goodrich | first2=John G. | last2=Kloss II | year=1999 | url=http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7503 | journal=[[Workshop on Algorithms and Data Structures]] | pages=205–216 | doi=10.1007/3-540-48447-7_21 | volume=1663 | series=Lecture Notes in Computer Science | isbn=978-3-540-66279-2}}&lt;/ref&gt; presented a dynamic array algorithm called ''Tiered Vectors'' that provided O(n&lt;sup&gt;1/2&lt;/sup&gt;) performance for order preserving insertions or deletions from the middle of the array.

[[Hashed array tree|Hashed Array Tree]] (HAT) is a dynamic array algorithm published by Sitarski in 1996.&lt;ref name=&quot;sitarski96&quot;&gt;{{Citation | title=HATs: Hashed array trees | contribution=Algorithm Alley | journal=Dr. Dobb's Journal | date=September 1996 | first1=Edward | last1=Sitarski | volume=21 | issue=11 | url=http://www.ddj.com/architect/184409965?pgno=5}}&lt;/ref&gt;  Hashed Array Tree wastes order n&lt;sup&gt;1/2&lt;/sup&gt; amount of storage space, where n is the number of elements in the array. The algorithm has O(1) amortized performance when appending a series of objects to the end of a Hashed Array Tree.

In a 1999 paper,&lt;ref name=&quot;brodnik&quot;&gt;{{Citation | title=Resizable Arrays in Optimal Time and Space | date=Technical Report CS-99-09 | url=http://www.cs.uwaterloo.ca/research/tr/1999/09/CS-99-09.pdf | year=1999 | first1=Andrej | last1=Brodnik | first2=Svante | last2=Carlsson | first5=ED | last5=Demaine | first4=JI | last4=Munro | first3=Robert | last3=Sedgewick | author3-link=Robert Sedgewick (computer scientist) | publisher=Department of Computer Science, University of Waterloo}}&lt;/ref&gt; Brodnik et al. describe a tiered dynamic array data structure, which wastes only n&lt;sup&gt;1/2&lt;/sup&gt; space for ''n'' elements at any point in time, and they prove a lower bound showing that any dynamic array must waste this much space if the operations are to remain amortized constant time. Additionally, they present a variant where growing and shrinking the buffer has not only amortized but worst-case constant time.

Bagwell (2002)&lt;ref&gt;{{Citation | title=Fast Functional Lists, Hash-Lists, Deques and Variable Length Arrays | first1=Phil | last1=Bagwell | year=2002 | publisher=EPFL | url=http://citeseer.ist.psu.edu/bagwell02fast.html}}&lt;/ref&gt; presented the [[VList]] algorithm, which can be adapted to implement a dynamic array.

== Language support ==

[[C++]]'s [[Vector (C++)|&lt;code&gt;std::vector&lt;/code&gt;]] is an implementation of dynamic arrays, as are the &lt;code&gt;ArrayList&lt;/code&gt;&lt;ref&gt;Javadoc on {{Javadoc:SE|java/util|ArrayList}}&lt;/ref&gt; classes supplied with the [[Java (programming language)|Java]] API and the [[.NET Framework]]. The generic &lt;code&gt;List&lt;&gt;&lt;/code&gt; class supplied with version 2.0 of the .NET Framework is also implemented with dynamic arrays. [[Smalltalk]]'s &lt;code&gt;OrderedCollection&lt;/code&gt; is a dynamic array with dynamic start and end-index, making the removal of the first element also O(1). [[Python (Programming Language)|Python]]'s &lt;code&gt;list&lt;/code&gt; datatype implementation is a dynamic array. [[Delphi (programming language)|Delphi]] and [[D (programming language)|D]] implement dynamic arrays at the language's core. [[Ada (programming language)|Ada]]'s [[wikibooks:Ada Programming/Libraries/Ada.Containers.Vectors|&lt;code&gt;Ada.Containers.Vectors&lt;/code&gt;]] generic package provides dynamic array implementation for a given subtype. Many scripting languages such as [[Perl]] and [[Ruby (programming language)|Ruby]] offer dynamic arrays as a built-in [[primitive data type]]. Several cross-platform frameworks provide dynamic array implementations for [[C (programming language)|C]]: &lt;code&gt;CFArray&lt;/code&gt; and &lt;code&gt;CFMutableArray&lt;/code&gt; in [[Core Foundation]]; &lt;code&gt;GArray&lt;/code&gt; and &lt;code&gt;GPtrArray&lt;/code&gt; in [[GLib]].

== References ==
&lt;references /&gt;

== External links ==
* [http://www.nist.gov/dads/HTML/dynamicarray.html NIST Dictionary of Algorithms and Data Structures: Dynamic array]
* [http://www.bsdua.org/libbsdua.html#vpool VPOOL] - C language implementation of dynamic array.
* [http://www.collectionspy.com CollectionSpy] &amp;mdash; A Java profiler with explicit support for debugging ArrayList- and Vector-related issues.
* [http://opendatastructures.org/versions/edition-0.1e/ods-java/2_Array_Based_Lists.html Open Data Structures - Chapter 2 - Array-Based Lists]

{{Data structures}}

{{DEFAULTSORT:Dynamic Array}}
[[Category:Arrays]]
[[Category:Articles with example pseudocode]]</text>
      <sha1>jy7urdgonthgj31fv4vabif0w3egik4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Array slicing</title>
    <ns>0</ns>
    <id>683334</id>
    <revision>
      <id>603889161</id>
      <parentid>578999035</parentid>
      <timestamp>2014-04-12T15:39:18Z</timestamp>
      <contributor>
        <username>Wavelength</username>
        <id>271168</id>
      </contributor>
      <comment>inserting 1 [[hyphen]]: —&gt; &quot;higher-dimensional&quot;—[[User talk:Wavelength#Hyphenation]] [to Archive 6]</comment>
      <text xml:space="preserve" bytes="20456">{{About|the data structure operation|other uses of slicing|Slicing (disambiguation)}}
In [[computer programming]], '''array slicing''' is an operation that extracts certain elements from an array and packages them as another array, possibly with different number of indices (or [[dimensions]]) and different index ranges. Two common examples are extracting a substring from a string of characters (e.g. &quot;''ell''&quot; from &quot;h''ell''o&quot;), and extracting a row (or a column) of a rectangular [[matrix (mathematics)|matrix]] to be used as a [[vector (mathematics)|vector]].

Depending on the [[programming language]] and context, the elements of the new array may be [[aliasing (computing)|aliased to]] (i.e., share memory with) those of the original array.

==Details==
For &quot;one-dimensional&quot; (single-indexed) arrays &amp;mdash; vectors, sequence, strings etc. &amp;mdash; the most common slicing operation is extraction of zero or more consecutive elements. Thus, if we have a vector containing elements (2, 5, 7, 3, 8, 6, 4, 1), and we want to create an array slice from the 3rd to the 6th items, we get (7, 3, 8, 6). In [[programming language]]s that use a 0-based indexing scheme, the slice would be from index ''2'' to ''5''.

Reducing the range of any index to a single value effectively eliminates that index. This feature can be used, for example, to extract one-dimensional slices (vectors) or two-dimensional slices (rectangular matrices) from a  three-dimensional array. However, since the range can be specified at run-time, type-checked languages may require an explicit (compile-time) notation to actually eliminate the trivial indices.

General array slicing can be implemented (whether or not built into the language) by referencing every array through a [[dope vector]] or ''descriptor'' &amp;mdash; a record that contains the address of the first array element, and then the range of each index and the corresponding coefficient in the indexing formula. This technique also allows immediate array [[transpose|transposition]], index reversal, subsampling, etc. For languages like [[C (programming language)|C]], where the indices always start at zero, the dope vector of an array with ''d'' indices has at least 1 + 2''d'' parameters. For languages that allow arbitrary lower bounds for indices, like [[Pascal programming language|Pascal]], the dope vector needs 1 + 3''d'' entries.

If the array abstraction does not support true negative indices (as for example the arrays of [[Ada (programming language)|Ada]] and [[Pascal (programming language)|Pascal]] do), then negative indices for the bounds of the slice for a given dimension are sometimes used to specify an offset from the end of the array in that dimension. In 1-based schemes, -1 generally would indicate the second-to-last item, while in a 0-based system, it would mean the very last item.

==History==
The concept of slicing was surely known even before the invention of [[compiler]]s. Slicing as a language feature probably started with [[FORTRAN]] (1957), more as a consequence of non-existent type and range checking than by design. The concept was also alluded to in the preliminary report for the [[ALGOL 58|IAL]] (ALGOL 58) in that the syntax allowed one or more indices of an array element (or, for that matter, of a procedure call) to be omitted when used as an actual parameter.

[[Kenneth E. Iverson|Kenneth Iverson]]'s [[APL programming language|APL]] (1957) had very flexible multi-dimensional array slicing, which contributed much to the language's expressive power and popularity.

[[ALGOL 68]] (1968) introduced comprehensive multi-dimension array slicing and trimming features.

Array slicing facilities have been incorporated in several modern languages, such as [[Ada (programming language)|Ada 2005]], [[Boo programming language|Boo]], [[Cobra (programming language)|Cobra]], [[D programming language|D]], [[Fortran|Fortran 90]], [[Go (programming language)|Go]], [[Matlab programming language|Matlab]], [[Perl]], [[Python (programming language)|Python]], [[S-Lang (programming language)|S-Lang]], [[Windows PowerShell]] and the mathematical/statistical languages [[Octave programming language|GNU Octave]], [[S programming language|S]] and [[R programming language|R]].

==Timeline of slicing in various programming languages==

===1966: [[Fortran#FORTRAN 66|Fortran 66]]===
The Fortran 66 programmers were only able to take advantage of slicing matrices by row, and then only when passing that row to a [[subroutine]]:
&lt;syntaxhighlight lang=&quot;fortran&quot;&gt;
       SUBROUTINE PRINT V(VEC, LEN)
         REAL VEC(*)
         PRINT *, (VEC(I), I = 1, LEN)
       END

       PROGRAM MAIN
         PARAMETER(LEN = 3)
         REAL MATRIX(LEN, LEN)
         DATA MATRIX/1, 1, 1, 2, 4, 8, 3, 9, 27/
         CALL PRINT V(MATRIX(1, 2), LEN)
       END
&lt;/syntaxhighlight&gt;
&lt;!-- BTW: I suspect that PARAMETER and PRINT *, were not part of F66 too. --&gt;

Result:
  2. 4. 8.
Note that there is no [[dope vector]] in FORTRAN 66 hence the length of the slice must also be passed as an argument - or some other means - to the &lt;code&gt;SUBROUTINE&lt;/code&gt;. 1970s [[Pascal (programming language)|Pascal]] and [[C (programming language)|C]] had similar restrictions.

===1968: [[Algol 68]]===
Algol68 final report contains an early example of slicing, slices are specified in the form:
&lt;syntaxhighlight lang=&quot;algol68&quot;&gt;
 [lower bound:upper bound] ¢ for computers with extended character sets ¢
&lt;/syntaxhighlight&gt;
or:
&lt;syntaxhighlight lang=&quot;algol68&quot;&gt;
 (LOWER BOUND..UPPER BOUND) # FOR COMPUTERS WITH ONLY 6 BIT CHARACTERS. #
&lt;/syntaxhighlight&gt;
Both bounds are inclusive and can be omitted, in which case they default to the declared array bounds. Neither the stride facility, nor diagonal slice aliases are part of the revised report.

Examples:
&lt;syntaxhighlight lang=&quot;algol68&quot;&gt;
 [3, 3]real a := ((1, 1, 1), (2, 4, 8), (3, 9, 27)); # declaration of a variable matrix #
 [,]  real c = ((1, 1, 1), (2, 4, 8), (3, 9, 27));   # constant matrix, the size is implied #

 ref[]real row := a[2,];                    # alias/&lt;u&gt;ref&lt;/u&gt; to a row slice #
 ref[]real col2 = a[, 2];                   # permanent alias/&lt;u&gt;ref&lt;/u&gt; to second column #

 print ((a[:, 2], newline));                # second column slice #
 print ((a[1⌈a, :], newline));              # last row slice #
 print ((a[:, 2⌈a], newline));              # last column slice #
 print ((a[:2, :2], newline));              # leading 2-by-2 submatrix &quot;slice&quot; #
&lt;/syntaxhighlight&gt;

 +1.0000&lt;sub&gt;10&lt;/sub&gt;+0 +4.0000&lt;sub&gt;10&lt;/sub&gt;+0 +9.0000&lt;sub&gt;10&lt;/sub&gt;+0
 +3.0000&lt;sub&gt;10&lt;/sub&gt;+0 +9.0000&lt;sub&gt;10&lt;/sub&gt;+0 +2.7000&lt;sub&gt;10&lt;/sub&gt;+1
 +1.0000&lt;sub&gt;10&lt;/sub&gt;+0 +8.0000&lt;sub&gt;10&lt;/sub&gt;+0 +2.7000&lt;sub&gt;10&lt;/sub&gt;+1
 +1.0000&lt;sub&gt;10&lt;/sub&gt;+0 +1.0000&lt;sub&gt;10&lt;/sub&gt;+0 +2.0000&lt;sub&gt;10&lt;/sub&gt;+0 +4.0000&lt;sub&gt;10&lt;/sub&gt;+0

===1970s: [[MATLAB]]/[[GNU Octave]]/[[Scilab]]===
 &gt; A = round(rand(3, 4, 5)*10) # 3x4x5 three-dimensional or cubic array
 &gt; A(:, :, 3) # 3x4 two-dimensional array along first and second dimensions
 ans =
 
   8  3  5  7
   8  9  1  4
   4  4  2  5
 
 &gt; A(:, 2:3, 3) # 3x2 two-dimensional array along first and second dimensions
 ans =
 
   3 5
   9 1
   4 2
&lt;source lang=&quot;octave&quot;&gt;
 &gt; A(2:end, :, 3) # 2x4 two-dimensional array using the 'end' keyword; works with GNU Octave 3.2.4
 ans =

    6    1    4    6
   10    1    3    1
&lt;/source&gt;
 &gt; A(1, :, 3) # single-dimension array along second dimension
 ans =
 
   8  3  5  7
 
 &gt; A(1, 2, 3) # single value
 ans = 3

===1976&lt;!--197?--&gt;: [[S programming language|S]]/[[R programming language|R]]===
Arrays in [[S programming language|S]] and [[R programming language|GNU R]] are always one-based, thus the indices of a new slice will begin with ''one'' for each dimension, regardless of the previous indices. Dimensions with length of ''one'' will be dropped (unless drop = FALSE). Dimension names (where present) will be preserved.

 &gt; A &lt;- array(1:60, dim = c(3, 4, 5)) # 3x4x5 three-dimensional or cubic array
 &gt; A[{{Not a typo|, ,}} 3] # 3x4 two-dimensional array along first and second dimensions
      [, 1] [, 2] [, 3] [, 4]
 [1,]   25   28   31   34
 [2,]   26   29   32   35
 [3,]   27   30   33   36
 &gt; A[, 2:3, 3, drop = FALSE] # 3x2x1 cubic array subset (preserved dimensions)
 {{Not a typo|, ,}} 1
 
      [, 1] [, 2]
 [1,]   28   31
 [2,]   29   32
 [3,]   30   33
 &gt; A[, 2, 3]  # single-dimension array along first dimension
 [1] 28 29 30
 &gt; A[1, 2, 3] # single value
 [1] 28

===1977: [[Fortran#FORTRAN 77|Fortran 77]]===
The Fortran 77 standard introduced the ability to slice and [[concatenation|concatenate]] strings:

&lt;source lang=FORTRAN&gt;
PROGRAM MAIN
  PRINT *, 'ABCDE'(2:4)
END
&lt;/source&gt;

Produces:
 BCD

Such strings could be passed by &lt;u&gt;reference&lt;/u&gt; to another subroutine, the length would also be passed transparently to the subroutine as a kind of '''short''' dope vector.

&lt;source lang=FORTRAN&gt;
SUBROUTINE PRINT S(STR)
  CHARACTER *(*)STR
  PRINT *, STR
END

PROGRAM MAIN
  CALL PRINT S('ABCDE'(2:4))
END
&lt;/source&gt;

Again produces:
 BCD

===1979: [[Sinclair_BASIC#Sinclair_BASIC|Sinclair_BASIC]] ZX80/81/Spectrum===
The standard ROM of the ZX80/81/Spectrum offers BASIC with the ability to slice and [[concatenation|concatenate]] strings:

in the command part (x TO y) which points out the needed array slice the x and y value can be omitted giving the meaning to use all chained array cells (FROM x TO end ) or (begin TO y). With multidimensional array's the slicing is only possible with the last level dimension.

&lt;source lang=&quot;zxbasic&quot;&gt;
10 LET a$=&quot;ABCDE&quot;(2 to 4)
20 PRINT a$
&lt;/source&gt;

Produces:
 BCD

&lt;source lang=&quot;zxbasic&quot;&gt;
10 LET a$=&quot;ABCDE&quot;
20 LET b$=a$(4 TO)+a$(2 TO 3)+a$(1)
30 PRINT b$
&lt;/source&gt;

Produces:
 DEBCA

===1983: [[Ada (programming language)|Ada 83]] and above===
{{Wikibooks|Ada_Programming|Types/array}}
Ada 83 supports slices for all array types. Like [[Fortran#FORTRAN 77|Fortran 77]] such arrays could be passed by &lt;u&gt;reference&lt;/u&gt; to another subroutine, the length would also be passed transparently to the subroutine as a kind of '''short''' dope vector.

&lt;source lang=ADA&gt;
with Text_IO;
 
procedure Main is
   Text : String := &quot;ABCDE&quot;;
begin
   Text_IO.Put_Line (Text (2 .. 4));
end Main;
&lt;/source&gt;

Produces:
 BCD

'''Note:''' Since in Ada indices are n-based  the term &lt;code&gt;Text (2 .. 4)&lt;/code&gt; will result in an Array with the base index of 2.

The definition for &lt;code&gt;Text_IO.Put_Line&lt;/code&gt; is:

&lt;source lang=ADA&gt;
package Ada.Text_IO is
   
   procedure Put_Line(Item : in  String);
&lt;/source&gt;

The definition for &lt;code&gt;String&lt;/code&gt; is:

&lt;source lang=ADA&gt;
package Standard is

   subtype Positive is Integer range 1 .. Integer'Last;

   type String is array(Positive range &lt;&gt;) of Character;
   pragma Pack(String);
&lt;/source&gt;

As Ada supports true negative indices as in &lt;code&gt;type History_Data_Array is array (-6000 .. 2010) of History_Data;&lt;/code&gt; it places no special meaning on negative indices. In the example above the term &lt;code&gt; Some_History_Data (-30 .. 30)&lt;/code&gt; would slice the &lt;code&gt;History_Data&lt;/code&gt; from 30 [[Before Christ|BC]] to 30 [[Anno Domini|AD]].

===1987: [[Perl]]===
If we have
&lt;source lang=Perl&gt;@a = (2, 5, 7, 3, 8, 6, 4);&lt;/source&gt;
as above, then the first 3 elements, middle 3 elements and last 3 elements would be:

&lt;source lang=Perl&gt;
@a[0..2];   # (2, 5, 7)
@a[2..4];   # (7, 3, 8)
@a[-3..-1]; # (8, 6, 4)
&lt;/source&gt;

Perl supports negative list indices. The -1 index is the last element, -2 the penultimate element, etc.
In addition, Perl supports slicing based on expressions, for example:

&lt;source lang=Perl&gt;
@a[ 3.. $#a ];   # 4th element until the end (3, 8, 6, 4)
@a[ grep { !($_ % 3) } (0...$#a) ];    # 1st, 4th and 7th element (2,3,4)
@a[ grep { !(($_+1) % 3) } (0..$#a) ]; # every 3rd element (7,6)
&lt;/source&gt;

===1991: [[Python (programming language)|Python]]===
If you have a list
&lt;source lang=Python&gt;
nums = [1, 3, 5, 7, 8, 13, 20]
&lt;/source&gt;
, then it is possible to slice by using a notation similar to element retrieval:
&lt;source lang=Python&gt;
nums[3]   #equals 7, no slicing
nums[:3]  #equals [1, 3, 5], from index 0 (inclusive) until index 3 (exclusive)
nums[1:5] #equals [3, 5, 7, 8]
nums[-3:] #equals [8, 13, 20]
&lt;/source&gt;
Note that Python allows negative list indices. The index -1 represents the last element, -2 the penultimate element, etc.
Python also allows a step property by appending an extra colon and a value. For example:
&lt;source lang=Python&gt;
nums[3::]  #equals [7, 8, 13, 20], same as nums[3:]
nums[::3]  #equals [1, 7, 20] (starting at index 0 and getting every third element)
nums[1:5:2] #equals [3, 7] (from index 1 until index 5 and getting every second element)
&lt;/source&gt;

===1992: [[Fortran#Fortran_90|Fortran 90]] and above===
In Fortran 90, slices are specified in the form

&lt;source lang=FORTRAN&gt;
lower_bound:upper_bound[:stride]
&lt;/source&gt;

Both bounds are inclusive and can be omitted, in which case they default to the declared
array bounds. Stride defaults to 1. Example:

&lt;source lang=FORTRAN&gt;
real, dimension(m, n):: a  ! declaration of a matrix
  
print *, a(:, 2) ! second column
print *, a(m, :) ! last row
print *, a(:10, :10) ! leading 10-by-10 submatrix
&lt;/source&gt;

===1998: [[S-Lang (programming language)|S-Lang]]===
Array slicing was introduced in version 1.0. Earlier versions did not
support this feature.

Suppose that A is a 1-d array such as
&lt;pre&gt;
    A = [1:50];           % A = [1, 2, 3, ...49, 50]
&lt;/pre&gt;
Then an array B of first 5 elements of A may be created using
&lt;pre&gt;
    B = A[[:4]];
&lt;/pre&gt;
Similarly, B may be assigned to an array of the last 5 elements of A via:
&lt;pre&gt;
    B = A[[-5:]];
&lt;/pre&gt;
Other examples of 1-d slicing include:
&lt;pre&gt;
    A[-1]                 % The last element of A
    A[*]                  % All elements of A
    A[[::2]]              % All even elements of A
    A[[1::2]]             % All odd elements of A
    A[[-1::-2]]           % All even elements in the reversed order
    A[[[0:3], [10:14]]]   % Elements 0-3 and 10-14
&lt;/pre&gt;

Slicing of higher-dimensional arrays works similarly:
&lt;pre&gt;
    A[-1, *]              % The last row of A
    A[[1:5], [2:7]]       % 2d array using rows 1-5 and columns 2-7
    A[[5:1:-1], [2:7]]    % Same as above except the rows are reversed
&lt;/pre&gt;

Array indices can also be arrays of integers. For example, suppose
that &lt;code&gt;I = [0:9]&lt;/code&gt; is an array of 10 integers. Then
&lt;code&gt;A[I]&lt;/code&gt; is equivalent to an array of the first 10 elements
of &lt;code&gt;A&lt;/code&gt;. A practical example of this is a sorting
operation such as:
&lt;pre&gt;
    I = array_sort(A);    % Obtain a list of sort indices
    B = A[I];             % B is the sorted version of A
    C = A[array_sort(A)]; % Same as above but more concise.
&lt;/pre&gt;

===1999: [[D programming language|D]]===
Consider the array:

&lt;source lang=D&gt;
int[] a = [2, 5, 7, 3, 8, 6, 4, 1];
&lt;/source&gt;

Take a slice out of it:

&lt;source lang=D&gt;
int[] b = a[2 .. 5];
&lt;/source&gt;

and the contents of &lt;code&gt;b&lt;/code&gt; will be &lt;code&gt;[7, 3, 8]&lt;/code&gt;. The first index of the slice is inclusive, the second is exclusive.

&lt;source lang=D&gt;
auto c = a[$ - 4 .. $ - 2];
&lt;/source&gt;

means that the dynamic array &lt;code&gt;c&lt;/code&gt; now contains &lt;code&gt;[8, 6]&lt;/code&gt; because inside the [] the &lt;code&gt;$&lt;/code&gt; symbol refers to the length of the array.

D array slices are aliased to the original array, so:

&lt;source lang=D&gt;
b[2] = 10;
&lt;/source&gt;

means that &lt;code&gt;a&lt;/code&gt; now has the contents &lt;code&gt;[2, 5, 7, 3, 10, 6, 4, 1]&lt;/code&gt;. To create a copy of the array data, instead of only an alias, do:

&lt;source lang=D&gt;
auto b = a[2 .. 5].dup;
&lt;/source&gt;

Unlike Python, D slice bounds don't saturate, so code equivalent to this Python code is an error in D:

&lt;source lang=Python&gt;
&gt;&gt;&gt; d = [10, 20, 30]
&gt;&gt;&gt; d[1 : 5]
[20, 30]
&lt;/source&gt;

===2004: [[SuperCollider]]===
The programming language [[SuperCollider]] implements some concepts from [[J (programming language)|J]]/[[APL (programming language)|APL]]. Slicing looks as follows:
&lt;pre&gt;
a = [3, 1, 5, 7]           // assign an array to the variable a
a[0..1]                    // return the first two elements of a
a[..1]                     // return the first two elements of a: the zero can be omitted
a[2..]                     // return the element 3 till last one
a[[0, 3]]                  // return the first and the fourth element of a

a[[0, 3]] = [100, 200]     // replace the first and the fourth element of a
a[2..] = [100, 200]        // replace the two last elements of a

// assign a multidimensional array to the variable a
a = [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]; 
a.slice(2, 3);             // take a slice with coordinates 2 and 3 (returns 13)
a.slice(nil, 3);           // take an orthogonal slice (returns [3, 8, 13, 18])
&lt;/pre&gt;

===2005: [[Friendly interactive shell|fish]]===
Arrays in [[Friendly interactive shell|fish]] are always one-based, thus the indices of a new slice will begin with ''one'', regardless of the previous indices.
&lt;source lang=&quot;bash&quot;&gt;
 &gt; set A (seq 3 2 11)       # $A is an array with the values 3, 5, 7, 9, 11
 
 &gt; echo $A[(seq 2)]         # Print the first two elements of $A 
 3 5
 
 &gt; set B $A[1 2]            # $B contains the first and second element of $A, i.e. 3, 5
 
 &gt; set -e A[$B]; echo $A    # Erase the third and fifth elements of $A, print $A
 3 5 9
&lt;/source&gt;

===2006: [[Cobra (programming language)|Cobra]]===

Cobra supports Python-style slicing. If you have a list
 &lt;source lang=Python&gt;
nums = [1, 3, 5, 7, 8, 13, 20]
&lt;/source&gt;

, then the first 3 elements, middle 3 elements, and last 3 elements would be:

&lt;source lang=Python&gt;
nums[:3]  # equals [1, 3, 5]
nums[2:5] # equals [5, 7, 8]
nums[-3:] # equals [8, 13, 20]
&lt;/source&gt;

Cobra also supports slicing-style syntax for 'numeric for loops':
&lt;source lang=Python&gt;
for i in 2 : 5
    print i
# prints 2, 3, 4

for j in 3
    print j
# prints 0, 1, 2
&lt;/source&gt;

===2006: [[Windows PowerShell]]===
Arrays are zero-based in PowerShell and can be defined using the comma operator:
&lt;source lang=&quot;powershell&quot;&gt; $a = 2, 5, 7, 3, 8, 6, 4, 1&lt;/source&gt;

Print the first two elements of $a:
&lt;source lang=&quot;powershell&quot;&gt; $a[0, 1]          	    # Print 2 and 5&lt;/source&gt;

Take a slice out of it using the range operator:
&lt;source lang=&quot;powershell&quot;&gt; $b = $a[2..5]          # Content of $b will be: 7, 3, 8, 6.&lt;/source&gt;

Get the last 3 elements:
&lt;source lang=&quot;powershell&quot;&gt; $a[-3..-1]             # Print 6, 4, 1&lt;/source&gt;

Return the content of the array in reverse order:
&lt;source lang=&quot;powershell&quot;&gt; $a[($a{{Not a typo|.}}Length - 1)..0]   # Length is a property of System.Object[]&lt;/source&gt;

===2009: [[Go (programming language)|Go]]===

Go supports Python-style syntax for slicing (except negative indices are not supported). Arrays and slices can be sliced. If you have a slice
&lt;syntaxhighlight lang=&quot;go&quot;&gt;
nums := []int{1, 3, 5, 7, 8, 13, 20}
&lt;/syntaxhighlight&gt;
then the first 3 elements, middle 3 elements, last 3 elements, and a copy of the entire slice would be:
&lt;syntaxhighlight lang=&quot;go&quot;&gt;
nums[:3]  // equals []int{1, 3, 5}
nums[2:5] // equals []int{5, 7, 8}
nums[4:]  // equals []int{8, 13, 20}
nums[:]   // equals []int{1, 3, 5, 7, 8, 13, 20}
&lt;/syntaxhighlight&gt;

Slices in Go are reference types, which means that different slices may refer to the same underlying array.

===2010: [[Cilk Plus]]===

Cilk Plus supports syntax for array slicing as an extension to C and C++.
&lt;pre&gt;
array_base [lower_bound:length[:stride]]*
&lt;/pre&gt;
Cilk Plus slicing looks as follows:
&lt;source lang=&quot;c&quot;&gt;
A[:]     // All of vector A
B[2:6]   // Elements 2 to 7 of vector B
C[:][5]  // Column 5 of matrix C
D[0:3:2] // Elements 0, 2, 4 of vector D
&lt;/source&gt;

Differs from Fortran array slicing syntax by using length as the second parameter instead of the upper bound, in order to be consistent with standard C libraries.

Differs from Fortran array assignment semantics in that assignments are required to be either non-overlapping or perfectly overlapping, otherwise the result is undefined. This means temporaries are never required by the semantics.

==See also==
* [[Comparison of programming languages (array)#Slicing]]

{{DEFAULTSORT:Array Slicing}}
&lt;!--Categories--&gt;
[[Category:Arrays]]
[[Category:Programming constructs]]
[[Category:Articles with example Ada code]]
[[Category:Articles with example ALGOL 68 code]]
[[Category:Articles with example D code]]
[[Category:Articles with example Fortran code]]
[[Category:Articles with example Perl code]]
[[Category:Articles with example Python code]]</text>
      <sha1>e4v36717afe9xh6z2ee9wjk7eajvwpb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Range query (data structures)</title>
    <ns>0</ns>
    <id>35266324</id>
    <revision>
      <id>618781731</id>
      <parentid>618770262</parentid>
      <timestamp>2014-07-28T07:30:57Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>/* Examples */ min was actually already covered by &quot;Semigroup operators&quot; + copyedit</comment>
      <text xml:space="preserve" bytes="11165">{{for|finding items that fall within a range|range query (database)}}
In [[data structure]]s, a '''range query''' consists of preprocessing some input data into a data structure to efficiently answer any number of queries on any subset of the input. Particularly, there is a group of problems that have been extensively studied where the input is an [[Array data structure|array]] of unsorted numbers  and a query consists in computing some function on a specific range of the array. In this article we describe some of these problems together with their solutions.

==Statement Of The Problem==

We may state the problem of range queries in the following way:
a range query &lt;math&gt;q_f(A,i,j)&lt;/math&gt; on an array &lt;math&gt;A=[a_1,a_2,..,a_n]&lt;/math&gt; of ''n'' elements of some set &lt;math&gt;S&lt;/math&gt;, denoted &lt;math&gt;A[1,n]&lt;/math&gt;, takes two indices &lt;math&gt;1\leq i\leq j\leq n&lt;/math&gt;, a function &lt;math&gt;f&lt;/math&gt;  defined over arrays of elements of &lt;math&gt;S&lt;/math&gt; and outputs &lt;math&gt;f(A[i,j])= f(a_i,\ldots,a_j)&lt;/math&gt;. This should be done space and time efficient.

consider for instance &lt;math&gt;f = sum&lt;/math&gt; and &lt;math&gt;A[1,n]&lt;/math&gt; and array of numbers, the range query &lt;math&gt;sum(A,i,j)&lt;/math&gt; computes &lt;math&gt;sum(A[i,j]) = (a_i+\ldots + a_j)&lt;/math&gt;, for any &lt;math&gt;1 \leq i  \leq j  \leq n&lt;/math&gt;. These queries may be answered in constant time and using &lt;math&gt;O(n)&lt;/math&gt; extra space by calculating  the sums of the first &lt;math&gt;i&lt;/math&gt; elements of &lt;math&gt;A&lt;/math&gt; and storing them into an auxiliar array &lt;math&gt;B&lt;/math&gt;, such that &lt;math&gt;B[i]&lt;/math&gt; contains the sum of the first &lt;math&gt;i&lt;/math&gt; elements of &lt;math&gt;A&lt;/math&gt; for every  &lt;math&gt;0\leq i\leq n&lt;/math&gt;.Therefore any query might be answered by doing &lt;math&gt;sum(A[i,j]) = B[j] - B[i-1]&lt;/math&gt;.

This strategy may be extended for every [[Group theory|group]] operator &lt;math&gt;f&lt;/math&gt; where the notion of &lt;math&gt;f^{-1}&lt;/math&gt; is well defined and easily computable.&lt;ref name=&quot;morin&quot;&gt;{{cite journal|first=Danny|last=Krizanc|first2=Pat|last2=Morin|first3=Michiel H. M.|last3=Smid|title=Range Mode and Range Median Queries on Lists and Trees|journal=ISAAC|year=2003|pages=517–526|url=http://cg.scs.carleton.ca/~morin/publications/}}&lt;/ref&gt; Finally notice this solution might be extended for  arrays of dimension two with a similar preprocessing.&lt;ref name=menhe&gt;{{cite journal|last=Meng|first=He|first2=J. Ian|last2=Munro|first3=Patrick K.|last3=Nicholson|title=Dynamic Range Selection in Linear Space|journal=ISAAC|year=2011|pages=160–169}}&lt;/ref&gt;

==Examples==

===Semigroup operators===
[[File:LowesCommon.png|thumb|right|300x200px|alt=A Constructing the corresponding cartesian tree to solve a range minimum query.|[[Range minimum query]] reduced to the [[lowest common ancestor]] problem.]]
{{main|Range minimum query}}

When the function of interest in a range query is a [[semigroup]] operator the notion of &lt;math&gt;f^{-1}&lt;/math&gt; is not always defined, therefore we can not use an analogous strategy to the previous section. Yao  showed&lt;ref name=&quot;yao&quot;&gt;{{cite journal|last=Yao, A. C|title=Space-Time Tradeoﬀ for Answering Range Queries|journal=e 14th Annual ACM Symposium on the Theory of Computing|year=1982|pages=128–136}}&lt;/ref&gt; that there exists an efficient solution for range queries that involve semigroup operators. He proved that for any constant &lt;math&gt;c&lt;/math&gt;, a preprocessing of time and space &lt;math&gt;\theta(c\cdot n)&lt;/math&gt; allows to answer range queries on lists where &lt;math&gt;f&lt;/math&gt; is a semigroup operator in &lt;math&gt;\theta(\alpha_c(n))&lt;/math&gt; time, where &lt;math&gt;\alpha_k&lt;/math&gt; is a certain functional inverse of the [[Ackermann function]].

There are some semigroup operators that admit slightly better solutions. For instance when &lt;math&gt;f\in \{\max,\min\}&lt;/math&gt;. Assume &lt;math&gt; f = \min&lt;/math&gt; then &lt;math&gt;\min(A[1..n])&lt;/math&gt; returns the index of the [[minimum]] element of &lt;math&gt;A[1..n]&lt;/math&gt;. Then &lt;math&gt;\min(A, i,j)&lt;/math&gt; denotes the corresponding minimum range query. There are several data structures that allow to answer a range minimum query in &lt;math&gt;O(1)&lt;/math&gt; time using a preprocessing of time and space &lt;math&gt;O(n)&lt;/math&gt;. Probably the simplest solution to sketch here is based on the equivalence between this problem and the [[Lowest common ancestor]] problem. We briefly describe this solution.

The [[cartesian tree]]  &lt;math&gt;T_A&lt;/math&gt; of an array &lt;math&gt;A[1,n]&lt;/math&gt; has as root &lt;math&gt;a_i = min\{a_1,a_2,\ldots,a_n\}&lt;/math&gt; and it has as left and right subtrees the cartesian tree of &lt;math&gt;A[1,i-1]&lt;/math&gt; and the cartesian tree of &lt;math&gt;A[i+1,n]&lt;/math&gt; respectively. It is easy to see that a range minimum query &lt;math&gt;min(A,i,j)&lt;/math&gt; is the [[lowest common ancestor]] in &lt;math&gt;T_A&lt;/math&gt; of &lt;math&gt;a_i&lt;/math&gt; and &lt;math&gt;a_j&lt;/math&gt;. Since the lowest common ancestor is solvable in constant time using a preprocessing of time and space &lt;math&gt;O(n)&lt;/math&gt;  thus so does the range minimum query problem. The solution when ''f = max'' is analogous. Cartesian trees can be constructed in linear time.

===Mode===
{{Main|Range mode query}}

The ''[[Mode (statistics)|mode]]'' of an array ''A'' is the element that appears the most in ''A''. For instance the mode of &lt;math&gt;A=[4,5,6,7,4,]&lt;/math&gt; is ''4''. In case of ties any of the most frequent elements might be picked as mode. A range mode query consists in preprocessing &lt;math&gt;A[1,n]&lt;/math&gt; such that we can find the mode in any range of &lt;math&gt;A[1,n]&lt;/math&gt;. Several data structures have been devised to solve this problem, we summarize some of the results in the following table.&lt;ref name=morin /&gt;

{| class=&quot;wikitable&quot;
|-
! Range Mode Queries || ||
|-
| Space || Query Time || Restrictions
|-
| &lt;math&gt;O(n^{2-2\epsilon})&lt;/math&gt; ||&lt;math&gt; O(n^\epsilon \log n)&lt;/math&gt; || &lt;math&gt;0\leq \epsilon\leq 1/2&lt;/math&gt;
|-
| &lt;math&gt;O(n^2\log\log n/ \log n)&lt;/math&gt; || &lt;math&gt;O(1)&lt;/math&gt; || 
|-
|}

Recently Jørgensen et al. proved a lower bound on the cell probe model of &lt;math&gt;\Omega(\frac{\log n}{\log (S w/n)})&lt;/math&gt; for any data structure that uses &lt;math&gt;S &lt;/math&gt; cells.&lt;ref name=jorgensen&gt;{{cite journal|last=Greve|first=M|last2=J{\o}rgensen|first2= A.|last3=Larsen|first3= K.|last4=Truelsen|first4= J.|title=Cell probe lower bounds and approximations for range mode|journal=Automata, Languages and Programming|year=2010|pages=605–616}}&lt;/ref&gt;

===Median===

This particular case is of special interest since finding the [[median]] has several applications, for further reference see.&lt;ref name=heriel&gt;{{cite journal|first=Sariel|last=Har-Peled|first2=S.|last2=Muthukrishnan|title=Range Medians|journal=ESA|year=2008|pages=503–514}}&lt;/ref&gt; On the other hand, the median problem, a special case of the [[selection problem]], is solvable in O(''n''), by the [[median of medians]] algorithm.&lt;ref name=tarjanmedian&gt;{{cite doi|10.1016/S0022-0000(73)80033-9}}&lt;/ref&gt; However its generalization through range median queries is recent.&lt;ref name=ethpaper /&gt; A range median query &lt;math&gt;median(A,i,j)&lt;/math&gt; where ''A,i'' and ''j'' have the usual meanings returns the median element of &lt;math&gt;A[i,j]&lt;/math&gt;. Equivalently, &lt;math&gt;median(A,i,j)&lt;/math&gt; should return the element of &lt;math&gt;A[i,j]&lt;/math&gt; of rank &lt;math&gt;\frac{j-i}{2}&lt;/math&gt;. Note that range median queries can not be solved by following any of the previous methods discussed above including Yao's approach for semigroup operators.&lt;ref name=&quot;morin kranakis&quot; /&gt;

There have been studied two variants of this problem, the ''offline'' version, where all the ''k'' queries of interest are given in a batch and we are interested in reduce the total cost and a version where all the preprocessing is done ''up front'' and we are interested in optimize the cost of any subsequent single query. Concerning the first variant of the problem recently was proven that can be solved in time &lt;math&gt;O(n\log k + k \log n)&lt;/math&gt; and space &lt;math&gt;O(n\log k)&lt;/math&gt;. We describe such a solution.&lt;ref name=&quot;ethpaper&quot;&gt;{{cite journal|last=Beat|first=Gfeller|author2=Peter Sanders|title=Towards Optimal Range Medians|journal=ICALP (1)|year=2009|pages=475–486}}&lt;/ref&gt;

The following pseudo code shows how to find the element of rank &lt;math&gt;r&lt;/math&gt; in &lt;math&gt;A[i,j]&lt;/math&gt; an unsorted array of distinct elements, to find the range medians we set &lt;math&gt;r=\frac{j-i}{2}&lt;/math&gt;.

&lt;syntaxhighlight lang=&quot;cpp&quot;&gt;

rangeMedian(A,i,j,r){

  if A.length() == 1 return A[1]

  if A.low is undefined then
    m = median(A)
    A.low  = [e in A | e &lt;= m]
    A.high = [e in A | e &gt; m ]

 calculate t  the number of elements of A[i,j] that belong to A.low

 if r &lt;= t return rangeMedian(A.low, i,j,r)
  else return rangeMedian(A.high, i,j, r-t)
}
&lt;/syntaxhighlight&gt;

Procedure ''rangeMedian'' partitions A, using A's median, into two arrays ''A.low'' and ''A.high'', where the former contains
the elements of ''A'' that are less than or equal to the median ''m'' and the latter the rest of the elements of ''A''.  If we know that the number of elements of &lt;math&gt;A[i,j]&lt;/math&gt; that
end up in ''A.low'' is ''t'' and this number is bigger than ''r'' then we should keep looking for the element of rank ''r'' in ''A.low'' else we should look for the element of rank &lt;math&gt;(r-t)&lt;/math&gt; in A.high. To find &lt;math&gt;t&lt;/math&gt;, it is enough to find the maximum index &lt;math&gt;m\leq i-1&lt;/math&gt; such that &lt;math&gt;a_m&lt;/math&gt; is in ''A.low'' and the maximum index &lt;math&gt;l\leq j&lt;/math&gt; such that &lt;math&gt;a_l&lt;/math&gt;
is in A.high. Then &lt;math&gt;t=l-m&lt;/math&gt;. The total cost for any query, without considering the partitioning part, is &lt;math&gt;\log n&lt;/math&gt; since at most &lt;math&gt;\log n&lt;/math&gt; recursion calls are done
and only a constant number of operations are performed in each of them (to get the value of &lt;math&gt;t&lt;/math&gt; [[fractional cascading]] should be used).
If a linear algorithm to find the medians is used, the total cost of preprocessing for &lt;math&gt;k&lt;/math&gt; range median queries is &lt;math&gt; n\log k&lt;/math&gt;. Clearly this algorithm can be easily modified to solve the up front version of the problem.&lt;ref name=ethpaper /&gt;

==Related Problems==
All the problems described above have been studied for higher dimensions as well as their dynamic versions. On the other hand, range queries might be extended to other data structures like [[Tree (data structure)|trees]],&lt;ref name=&quot;morin kranakis&quot;&gt;{{cite journal|first=P|last=Bose|first2=E.|last2=Kranakis|first3=P.|last3=Morin|first5=Y.|last5=Tang|title=Approximate range mode and range median queries|journal=In Proceedings of the 22nd Symposium on Theoretical Aspects of Computer Science (STACS 2005), volume 3404 of Lecture Notes in ComputerScience|year=2005|pages=377–388|url=http://cg.scs.carleton.ca/~morin/publications/}}&lt;/ref&gt; such as the [[level ancestor problem]]. A similar family of problems are [[Range searching|orthogonal range]] queries also known as counting queries.

==References==
{{Reflist}}

==External links==
*[http://opendatastructures.org/versions/edition-0.1c/ods-java/node64.html Open Data Structure - Chapter 13 - Data Structures for Integers]
*[http://www.cs.au.dk/~gerth/papers/isaac09median.pdf Data Structures for Range Median Queries - Gerth Stolting Brodal and Allan Gronlund Jorgensen]

&lt;!--- Categories ---&gt;
{{CS-Trees}}

[[Category:Arrays]]
[[Category:Articles created via the Article Wizard]]</text>
      <sha1>44358k4utv4wdm7oy20ohdyxjnjxb0t</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>LCP array</title>
    <ns>0</ns>
    <id>36849795</id>
    <revision>
      <id>610587691</id>
      <parentid>609667294</parentid>
      <timestamp>2014-05-29T03:55:30Z</timestamp>
      <contributor>
        <ip>69.120.106.146</ip>
      </contributor>
      <comment>/* History */</comment>
      <text xml:space="preserve" bytes="26958">{{more footnotes|date=September 2012}}

{| class=&quot;infobox&quot; style=&quot;width: 22em&quot;
! colspan=&quot;3&quot; style=&quot;font-size: 125%; text-align: center;&quot; | LCP array
|-
! [[List of data structures|Type]]
| colspan=&quot;2&quot; | [[Array data structure|Array]]
|-
! Invented by
{{!}} colspan=&quot;2&quot; {{!}}  {{harvtxt|Manber|Myers|1990}}
|-
! colspan=&quot;3&quot; class=&quot;navbox-abovebelow&quot; | [[Time complexity]] and [[space complexity]]&lt;br /&gt;in [[big O notation]]
|-
|
| Average
| Worst case
|-
! Space
| &lt;math&gt;\mathcal{O}(n)&lt;/math&gt;
| &lt;math&gt;\mathcal{O}(n)&lt;/math&gt;
|-
! Construction
| &lt;math&gt;\mathcal{O}(n)&lt;/math&gt;
| &lt;math&gt;\mathcal{O}(n)&lt;/math&gt;
|}

In [[computer science]], the '''longest common prefix array''' (LCP [[Array data structure|array]]) is an auxiliary [[data structure]] to the [[suffix array]]. It stores the lengths of the longest common prefixes between pairs of consecutive [[Suffix (computer science)|suffixes]] in the sorted suffix array. In other words, it is the length of prefix that is common between the two consecutive suffixes in a sorted suffix array.

Example:

LCP of '''a''' and '''a'''abba is 1.

LCP of '''ab'''aabba and '''ab'''ba is 2.

Augmenting the suffix array with the LCP array allows to efficiently simulate top-down and bottom-up [[Tree traversal|traversals]] of the [[suffix tree]],{{sfn|Kasai|Lee|Arimura|Arikawa|2001}}{{sfn|Abouelhoda|Kurtz|Ohlebusch|2004}} speeds up pattern matching on the suffix array{{sfn|Manber|Myers|1993}} and is a prerequisite for compressed suffix trees.{{sfn|Ohlebusch|Fischer|Gog|2010}}

== History ==

The LCP array was introduced by [[Udi Manber]] and [[Gene Myers]] alongside the suffix array in order to improve the running time of their string search algorithm.{{sfn|Manber|Myers|1993}}

[[Gene Myers]]: Former Vice president of Informatics Research at Celera Genomics

[[Udi Manber]]: Vice president of  engineering at Google.

== Definition ==

Let &lt;math&gt;A&lt;/math&gt; be the suffix array of the string &lt;math&gt;S=s_1,s_2,...s_n$&lt;/math&gt; and let &lt;math&gt;\operatorname{lcp}(v,w)&lt;/math&gt; denote the length of the longest common prefix between two strings &lt;math&gt;v&lt;/math&gt; and &lt;math&gt;w&lt;/math&gt;. Let further denote &lt;math&gt;S[i,j]&lt;/math&gt; the substring of &lt;math&gt;S&lt;/math&gt; ranging from &lt;math&gt;i&lt;/math&gt; to &lt;math&gt;j&lt;/math&gt;.

Then the LCP array &lt;math&gt;H[1,n]&lt;/math&gt; is an integer array of size &lt;math&gt;n&lt;/math&gt; such that &lt;math&gt;H[1]&lt;/math&gt; is undefined and &lt;math&gt;H[i]=\operatorname{lcp}(S[A[i-1],n],S[A[i],n])&lt;/math&gt; for every &lt;math&gt;1&lt;i\leq n&lt;/math&gt;. Thus &lt;math&gt;H[i]&lt;/math&gt; stores the length of longest common prefix of the [[Lexicographical order|lexicographically]] &lt;math&gt;i&lt;/math&gt;'th smallest suffix and its predecessor in the suffix array.

== Example ==
Consider the string &lt;math&gt;S=banana$&lt;/math&gt;:
{| class=&quot;wikitable&quot;
|-
! {{left header}} | i
| 1 || 2 || 3 || 4 || 5 || 6 || 7
|-
! {{left header}} | S[i]
| b || a || n || a || n || a || $
|}
and its corresponding suffix array &lt;math&gt;A&lt;/math&gt; :
{| class=&quot;wikitable&quot;
! {{left header}} | i
| 1 || 2 || 3 || 4 || 5 || 6 || 7
|-
! {{left header}} | A[i]
| 7 || 6 || 4 || 2 || 1 || 5 || 3
|}

Complete suffix array with suffixes itself :

{| class=&quot;wikitable&quot;
! {{left header}} | i
| 1 || 2 || 3 || 4 || 5 || 6 || 7
|-
! {{left header}} | A[i]
| 7 || 6 || 4 || 2 || 1 || 5 || 3
|-
! {{left header}} | 1
| $ || a || a || a || b || n || n
|-
! {{left header}} | 2
|   || $ || n || n || a || a || a
|-
! {{left header}} | 3
|   ||   || a || a || n || $ || n
|-
! {{left header}} | 4
|   ||   || $ || n || a ||  || a
|-
! {{left header}} | 5
|   ||   ||  || a || n ||  || $
|-
! {{left header}} | 6
|   ||   ||  || $ || a ||  || 
|-
! {{left header}} | 7
|   ||   ||  ||  || $ ||  ||

|}

Then the LCP array &lt;math&gt;H&lt;/math&gt; is constructed by comparing lexicographically consecutive suffixes to determine their longest common prefix:
{| class=&quot;wikitable&quot;
|-
! {{left header}} | i
| 1 || 2 || 3 || 4 || 5 || 6 || 7
|-
! {{left header}} | H[i]
| &lt;math&gt;\bot&lt;/math&gt; || 0 || 1 || 3 || 0 || 0 || 2
|}

So, for example, &lt;math&gt;H[4]=3&lt;/math&gt; is the length of the longest common prefix &lt;math&gt;ana&lt;/math&gt; shared by the suffixes &lt;math&gt; A[3]=S[4,7]=ana$&lt;/math&gt; and &lt;math&gt;A[4]=S[2,7]=anana$&lt;/math&gt;. Note that &lt;math&gt;H[1]=\bot&lt;/math&gt;, since there is no lexicographically smaller suffix.

== Difference between Suffix Array and LCP Array ? ==

Suffix array : Represents the lexicographic rank of each suffix of an array.

LCP array : Contains the maximum length prefix match between two consecutive suffixes, after they are sorted lexicographically.

== LCP Array usage in finding the number of occurrences of a pattern ==

In order to find the number of occurrences of a given string P (length m) in a text T (length N),

* You must use binary search against the suffix array of T.
* You should speed up the LCP  array usage as an auxiliary data structure. More specifically, you generate a special version of the LCP array (LCP-LR below) and use that.

The issue with using standard binary search (without the LCP information) is that in each of the O(log N) comparisons you need to make, you compare P to the current entry of the suffix array, which means a full string comparison of up to m characters. So the complexity is O(m*log N).

The LCP-LR array helps improve this to O(m+log N), in the following way:

At any point during the binary search algorithm, you consider, as usual, a range (L,...,R) of the suffix array and its central point M, and decide whether you continue your search in the left sub-range (L,...,M) or in the right sub-range (M,...,R).
In order to make the decision, you compare P to the string at M. If P is identical to M, you are done, but if not, you will have compared the first k characters of P and then decided whether P is lexicographically smaller or larger than M. Let's assume the outcome is that P is larger than M.
So, in the next step, you consider (M,...,R) and a new central point M' in the middle:

              M ...... M' ...... R
              |
       we know:
          lcp(P,M)==k
The trick now is that LCP-LR is precomputed such that an O(1)-lookup tells you the longest common prefix of M and M', lcp(M,M').

You know already (from the previous step) that M itself has a prefix of k characters in common with P: lcp(P,M)=k. Now there are three possibilities:

* Case 1: k &lt; lcp(M,M'), i.e. P has fewer prefix characters in common with M than M has in common with M'. This means the (k+1)-th character of M' is the same as that of M, and since P is lexicographically larger than M, it must be lexicographically larger than M', too. So we continue in the right half (M',...,R).

* Case 2: k &gt; lcp(M,M'), i.e. P has more prefix characters in common with M than M has in common with M'. Consequently, if we were to compare P to M', the common prefix would be smaller than k, and M' would be lexicographically larger than P, so, without actually making the comparison, we continue in the left half (M,...,M').

* Case 3: k == lcp(M,M'). So M and M' are both identical with P in the first k characters. To decide whether we continue in the left or right half, it suffices to compare P to M' starting from the (k+1)-th character.

* We continue recursively.

The overall effect is that no character of P is compared to any character of the text more than once. The total number of character comparisons is bounded by m, so the total complexity is indeed O(m+log N).

Obviously, the key remaining question is how did we precompute LCP-LR so it is able to tell us in O(1) time the lcp between any two entries of the suffix array? As you said, the standard LCP array tells you the lcp of consecutive entries only, i.e. lcp(x-1,x) for any x. But M and M' in the description above are not necessarily consecutive entries, so how is that done?

The key to this is to realize that only certain ranges (L,...,R) will ever occur during the binary search: It always starts with (0,...,N) and divides that at the center, and then continues either left or right and divide that half again and so forth. If you think of it: Every entry of the suffix array occurs as central point of exactly one possible range during binary search. So there are exactly N distinct ranges (L...M...R) that can possibly play a role during binary search, and it suffices to precompute lcp(L,M) and lcp(M,R) for those N possible ranges. So that is 2*N distinct precomputed values, hence LCP-LR is O(N) in size.

Moreover, there is a straightforward recursive algorithm to compute the 2*N values of LCP-LR in O(N) time from the standard LCP array – I'd suggest posting a separate question if you need a detailed description of that.

To sum up:

* It is possible to compute LCP-LR in O(N) time and O(2*N)=O(N) space from LCP.

* Using LCP-LR during binary search helps accelerate the search procedure from O(M*log N) to O(M+log N).

* You can use two binary searches to determine the left and right end of the match range for P, and the length of the match range corresponds with the number of occurrences for P.

== Efficient Construction Algorithms ==
LCP array construction algorithms can be divided into two different
categories: algorithms that compute the LCP array as a byproduct to the suffix
array and algorithms that use an already constructed suffix array in order to
compute the LCP values.

{{harvtxt|Manber|Myers|1993}} provide an algorithm to compute the LCP array alongside the suffix array
in &lt;math&gt;O(n \log n)&lt;/math&gt; time. {{harvtxt|Kärkkäinen|Sanders|2003}} show that it is also possible to modify their
&lt;math&gt;O(n)&lt;/math&gt; time algorithm such that it computes the LCP array as well.
{{harvtxt|Kasai|Lee|Arimura|Arikawa|2001}} present the first &lt;math&gt;O(n)&lt;/math&gt; time algorithm (FLAAP) that computes the LCP
array given the text and the suffix array.

Assuming that each text symbol takes
one byte and each entry of the suffix or LCP array takes 4 bytes, the major drawback of their
algorithm is a large space occupancy of &lt;math&gt;13n&lt;/math&gt; bytes, while the
original output (text, suffix array, LCP array) only occupies &lt;math&gt;9n&lt;/math&gt;
bytes. Therefore {{harvtxt|Manzini|2004}} created a refined version of the algorithm of {{harvtxt|Kasai|Lee|Arimura|Arikawa|2001}} (lcp9) and reduced the
space occupancy to &lt;math&gt;9n&lt;/math&gt; bytes. {{harvtxt|Kärkkäinen|Manzini|Puglisi|2009}} provide another refinement of
Kasai's algorithm (&lt;math&gt;\Phi&lt;/math&gt;-algorithm) that improves the running
time. Rather than the actual LCP array, this algorithm builds the ''permuted''
LCP (PLCP) array, in which the values appear in text order rather than lexicographical order.

{{harvtxt|Gog|Ohlebusch|2011}} provide two algorithms that although being theoretically slow
(&lt;math&gt;O(n^2)&lt;/math&gt;) were faster than the above mentioned algorithms in
practice.

As of 2012, the currently fastest linear-time LCP array construction algorithm is due to {{harvtxt|Fischer|2011}},
which in turn is based on one of the fastest suffix array construction algorithms by {{harvtxt|Nong|Zhang|Chan|2009}}.

== Applications ==
As noted by {{harvtxt|Abouelhoda|Kurtz|Ohlebusch|2004}} several string processing problems can be solved by the following kinds of [[tree traversal]]s:
* bottom-up traversal of the complete suffix tree
* top-down traversal of a subtree of the suffix tree
* suffix tree traversal using the suffix links.

{{harvtxt|Kasai|Lee|Arimura|Arikawa|2001}} show how to simulate a bottom-up traversal of the [[suffix tree]] using only the [[suffix array]] and LCP array. {{harvtxt|Abouelhoda|Kurtz|Ohlebusch|2004}} enhance the suffix array with the LCP array and additional data structures and describe how this ''enhanced suffix array'' can be used to simulate ''all three kinds'' of suffix tree traversals. {{harvtxt|Fischer|Heun|2007}} reduce the space requirements of the enhanced suffix array by preprocessing the LCP array for [[Range Minimum Query|range minimum queries]]. Thus,  ''every'' problem that can be solved by suffix tree algorithms can also be solved using the ''enhanced suffix array''.{{sfn|Abouelhoda|Kurtz|Ohlebusch|2004}}

Deciding if a pattern &lt;math&gt;P&lt;/math&gt; of length &lt;math&gt;m&lt;/math&gt; is a substring of a string &lt;math&gt;S&lt;/math&gt; of length &lt;math&gt;n&lt;/math&gt; takes &lt;math&gt; O(m \log n)&lt;/math&gt; time if only the suffix array is used. By additionally using the LCP information, this bound can be improved to &lt;math&gt;O(m + \log n)&lt;/math&gt; time.{{sfn|Manber|Myers|1993}} {{harvtxt|Abouelhoda|Kurtz|Ohlebusch|2004}} show how to improve this running time even further to achieve optimal &lt;math&gt;O(m)&lt;/math&gt; time. Thus, using suffix array and LCP array information, the decision query can be answered as fast as using the [[suffix tree]].

The LCP array is also an essential part of compressed suffix trees which provide full suffix tree functionality like suffix links and [[lowest common ancestor]] queries.{{sfn|Sadakane|2007}}{{sfn|Fischer|Mäkinen|Navarro|2009}} Furthermore it can be used together with the suffix array to compute the Lempel-Ziv [[LZ77 and LZ78|LZ77]] factorization in &lt;math&gt;O(n)&lt;/math&gt; time. {{sfn|Abouelhoda|Kurtz|Ohlebusch|2004}}{{sfn|Crochemore|Ilie|2008}}{{sfn|Crochemore|Ilie|Smyth|2008}}{{sfn|Chen|Puglisi|Smyth|2008}}

The [[longest repeated substring problem]] for a string &lt;math&gt;S&lt;/math&gt; of length &lt;math&gt;n&lt;/math&gt; can be solved in &lt;math&gt;\Theta(n)&lt;/math&gt; time using both the suffix array &lt;math&gt;A&lt;/math&gt; and the LCP array. It is sufficient to perform a linear scan through the LCP array in order to find its maximum value &lt;math&gt;v_{max}&lt;/math&gt; and the corresponding index &lt;math&gt;i&lt;/math&gt; where &lt;math&gt;v_{max}&lt;/math&gt; is stored. The longest substring that occurs at least twice is then given by &lt;math&gt;S[A[i],A[i]+v_{max}-1]&lt;/math&gt;.

The remainder of this section explains two applications of the LCP array in more detail: How the suffix array and the LCP array of a string can be used to construct the corresponding suffix tree and how it is possible to answer LCP queries for arbitrary suffixes using range minimum queries on the LCP array.

=== Suffix Tree Construction ===
Given the suffix array &lt;math&gt;A&lt;/math&gt; and the LCP array &lt;math&gt;H&lt;/math&gt; of a string &lt;math&gt;S=s_1,s_2,...s_n$&lt;/math&gt; of length &lt;math&gt;n+1&lt;/math&gt;, its suffix tree &lt;math&gt;ST&lt;/math&gt; can be constructed in &lt;math&gt;O(n)&lt;/math&gt; time based on the following idea: Start with the partial suffix tree for the lexicographically smallest suffix and repeatedly insert the other suffixes in the order given by the suffix array.

Let &lt;math&gt;ST_{i}&lt;/math&gt; be the partial suffix tree for &lt;math&gt;0\leq i  \leq n&lt;/math&gt;. Further let &lt;math&gt;d(v)&lt;/math&gt; be the length of the concatenation of all path labels from the root of &lt;math&gt;ST_i&lt;/math&gt; to node &lt;math&gt;v&lt;/math&gt;.
 [[File:Constructing the suffix tree of banana based on the suffix array and the LCP array - Case 1.pdf|thumb|400px|Case 1 (&lt;math&gt;d(v)=H[i+1]&lt;/math&gt;): Suppose the suffixes &lt;math&gt;a$&lt;/math&gt;, &lt;math&gt;ana$&lt;/math&gt;, &lt;math&gt;anana$&lt;/math&gt; and &lt;math&gt;banana$&lt;/math&gt; of the string &lt;math&gt;S=banana$&lt;/math&gt; are already added to the suffix tree. Then the suffix &lt;math&gt;na$&lt;/math&gt; is added to the tree as shown in the picture. The ''rightmost'' path is highlighted in red.]]
Start with &lt;math&gt;S_0&lt;/math&gt;, the tree consisting only of the root. To insert &lt;math&gt;A[i+1]&lt;/math&gt; into &lt;math&gt;ST_i&lt;/math&gt;, walk up the ''rightmost'' path beginning at the recently inserted leaf &lt;math&gt;A[i]&lt;/math&gt; to the root, until the deepest node &lt;math&gt;v&lt;/math&gt; with &lt;math&gt;d(v) \leq H[i+1]&lt;/math&gt; is reached.

We need to distinguish two cases:
* &lt;math&gt;d(v)=H[i+1]&lt;/math&gt;: This means that the concatenation of the labels on the root-to-&lt;math&gt;v&lt;/math&gt; path equals the longest common prefix of suffixes &lt;math&gt;A[i]&lt;/math&gt; and &lt;math&gt;A[i+1]&lt;/math&gt;. &lt;br /&gt; In this case, insert &lt;math&gt;A[i+1]&lt;/math&gt; as a new leaf &lt;math&gt;x&lt;/math&gt; of node &lt;math&gt;v&lt;/math&gt; and label the edge &lt;math&gt;(v,x)&lt;/math&gt; with &lt;math&gt;S[A[i+1]+H[i+1],n]&lt;/math&gt;. Thus the edge label consists of the remaining characters of suffix &lt;math&gt;A[i+1]&lt;/math&gt; that are not already represented by the concatenation of the labels of the root-to-&lt;math&gt;v&lt;/math&gt; path. &lt;br /&gt;This creates the partial suffix tree &lt;math&gt;ST_{i+1}&lt;/math&gt;. [[File:Constructing the suffix tree of banana based on the suffix array and the LCP array - Case 2.pdf|thumb|400px|Case 2 (&lt;math&gt;d(v) &lt; H[i+1]&lt;/math&gt;): In order to add suffix &lt;math&gt;nana$&lt;/math&gt;, the edge to the previously inserted suffix &lt;math&gt;na$&lt;/math&gt; has to be split up. The new edge to the new internal node is labeled with the longest common prefix of the suffixes &lt;math&gt;na$&lt;/math&gt; and &lt;math&gt;nana$&lt;/math&gt;. The edges connecting the two leafs are labeled with the ''remaining'' suffix characters that are not part of the prefix.]]

*&lt;math&gt;d(v) &lt; H[i+1]&lt;/math&gt;: This means that the concatenation of the labels on the root-to-&lt;math&gt;v&lt;/math&gt; path displays less characters than the longest common prefix of suffixes &lt;math&gt;A[i]&lt;/math&gt; and &lt;math&gt;A[i+1]&lt;/math&gt; and the ''missing'' characters are contained in the edge label of &lt;math&gt;v&lt;/math&gt;'s ''rightmost'' edge. Therefore we have to  ''split up'' that edge as follows: &lt;br /&gt;Let &lt;math&gt;w&lt;/math&gt; be the child of &lt;math&gt;v&lt;/math&gt; on &lt;math&gt;ST_i&lt;/math&gt;'s rightmost path.
# Delete the edge &lt;math&gt;(v,w)&lt;/math&gt;.
# Add a new internal node &lt;math&gt;y&lt;/math&gt; and a new edge &lt;math&gt;(v,y)&lt;/math&gt; with label &lt;math&gt;S[A[i]+d(v),A[i]+H[i+1]-1]&lt;/math&gt;. The new label consists of the ''missing'' characters of the longest common prefix of &lt;math&gt;A[i]&lt;/math&gt; and &lt;math&gt;A[i+1]&lt;/math&gt;. Thus, the concatenation of the labels of the root-to-&lt;math&gt;y&lt;/math&gt; path now displays the longest common prefix of &lt;math&gt;A[i]&lt;/math&gt; and &lt;math&gt;A[i+1]&lt;/math&gt;.
# Connect &lt;math&gt;w&lt;/math&gt; to the newly created internal node &lt;math&gt;y&lt;/math&gt; by an edge &lt;math&gt;(y,w)&lt;/math&gt; that is labeled &lt;math&gt;S[A[i]+H[i+1],A[i]+d(w)-1]&lt;/math&gt;. The new label consists of the ''remaining'' characters of the deleted edge &lt;math&gt;(v,w)&lt;/math&gt; that were not used as the label of edge &lt;math&gt;(v,y)&lt;/math&gt;.
# Add &lt;math&gt;A[i+1]&lt;/math&gt; as a new leaf &lt;math&gt;x&lt;/math&gt; and connect it to the new internal node &lt;math&gt;y&lt;/math&gt; by an edge &lt;math&gt;(y,x)&lt;/math&gt; that is labeled &lt;math&gt;S[A[i+1]+H[i+1],n]&lt;/math&gt;. Thus the edge label consists of the remaining characters of suffix &lt;math&gt;A[i+1]&lt;/math&gt; that are not already represented by the concatenation of the labels of the root-to-&lt;math&gt;v&lt;/math&gt; path.
# This creates the partial suffix tree &lt;math&gt;ST_{i+1}&lt;/math&gt;.

A simple amortization argument shows that the running time of this algorithm is bounded by &lt;math&gt;O(n)&lt;/math&gt;:

The nodes that are traversed in step &lt;math&gt;i&lt;/math&gt; by walking up the ''rightmost'' path of &lt;math&gt;ST_i&lt;/math&gt; (apart from the last node &lt;math&gt;v&lt;/math&gt;) are removed from the ''rightmost'' path, when &lt;math&gt;A[i+1]&lt;/math&gt; is added to the tree as a new leaf. These nodes will never be traversed again for all subsequent steps &lt;math&gt;j&gt;i&lt;/math&gt;. Therefore, at most &lt;math&gt;2n&lt;/math&gt; nodes will be traversed in total.

=== LCP queries for arbitrary suffixes ===
The LCP array &lt;math&gt;H&lt;/math&gt; only contains the length of the longest common prefix of every pair of consecutive suffixes in the suffix array &lt;math&gt;A&lt;/math&gt;. However, with the help of the inverse suffix array &lt;math&gt;A^{-1}&lt;/math&gt; (&lt;math&gt; A[i]= j \Leftrightarrow A^{-1}[j]= i &lt;/math&gt;, i.e. the suffix &lt;math&gt;S[j,n]&lt;/math&gt; that starts at position &lt;math&gt;j&lt;/math&gt; in &lt;math&gt;S&lt;/math&gt; is stored in position &lt;math&gt;A^{-1}[j]&lt;/math&gt; in &lt;math&gt;A&lt;/math&gt;) and constant-time [[Range Minimum Query|range minimum queries]] on &lt;math&gt;H&lt;/math&gt;, it is possible to determine the length of the longest common prefix of arbitrary suffixes in &lt;math&gt;O(1)&lt;/math&gt; time.

Because of the lexicographic order of the suffix array, every common prefix of the suffixes &lt;math&gt;S[i,n]&lt;/math&gt; and &lt;math&gt;S[j,n]&lt;/math&gt; has to be a common prefix of all suffixes between &lt;math&gt;i&lt;/math&gt;'s position in the suffix array &lt;math&gt; A^{-1}[i]&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt;'s position in the suffix array &lt;math&gt; A^{-1}[j] &lt;/math&gt;. Therefore the length of the longest prefix that is shared by ''all'' of these suffixes is the minimum value in the interval &lt;math&gt;H[A^{-1}[i]+1,A^{-1}[j]]&lt;/math&gt;. This value can be found in constant time if &lt;math&gt;H&lt;/math&gt; is preprocessed for range minimum queries.

Thus given a string &lt;math&gt;S&lt;/math&gt; of length &lt;math&gt;n&lt;/math&gt;  and two arbitrary positions  &lt;math&gt;i,j&lt;/math&gt; in the string &lt;math&gt;S&lt;/math&gt;  with &lt;math&gt; A^{-1}[i] &lt;  A^{-1}[j] &lt;/math&gt;, the length of the longest common prefix of the suffixes &lt;math&gt;S[i,n]&lt;/math&gt; and &lt;math&gt;S[j,n]&lt;/math&gt; can be computed as follows: &lt;math&gt;\operatorname{LCP}(i,j)=H[\operatorname{RMQ}_H(A^{-1}[i]+1,A^{-1}[j])]&lt;/math&gt;.

==Notes==
{{Reflist}}

==References==
*{{cite journal| doi=10.1016/S1570-8667(03)00065-0 |ref=harv| title=Replacing suffix trees with enhanced suffix arrays| year=2004| last1=Abouelhoda| first1=Mohamed Ibrahim| last2=Kurtz| first2=Stefan| last3=Ohlebusch| first3=Enno| journal=Journal of Discrete Algorithms| volume=2| pages=53 }}
*{{cite journal| doi=10.1137/0222058 | ref=harv| title=Suffix Arrays: A New Method for On-Line String Searches| year=1993| last1=Manber| first1=Udi| last2=Myers| first2=Gene| journal=SIAM Journal on Computing| volume=22| issue=5| pages=935}}
*{{cite journal |doi = 10.1007/3-540-48194-X_17 |ref=harv| title = Proceedings of the 12th Annual Symposium on Combinatorial Pattern Matching | year = 2001 | last1 = Kasai | first1 = T. | last2 = Lee | first2 = G. | last3 = Arimura | first3 = H. | last4 = Arikawa | first4 = S. | last5 = Park | first5 = K. | chapter = Linear-Time Longest-Common-Prefix Computation in Suffix Arrays and Its Applications  | series = Lecture Notes in Computer Science | volume = 2089 | pages = 181–192 | isbn = 978-3-540-42271-6
}}
*{{cite journal| doi=10.1007/978-3-642-16321-0_34 |ref=harv| chapter=CST++| title=String Processing and Information Retrieval| series=Lecture Notes in Computer Science| year=2010| last1=Ohlebusch| first1=Enno| last2=Fischer| first2=Johannes| last3=Gog| first3=Simon| isbn=978-3-642-16320-3| volume=6393| pages=322}}
*{{cite journal|ref=harv
 | title =  Simple linear work suffix array construction
 | url = http://dl.acm.org/citation.cfm?id=1759210.1759301
 | year = 2003
 | journal = Proceedings of the 30th international conference on Automata, languages and programming
 | pages = 943–955
 | last1 = Kärkkäinen	 | first1 =  Juha
 | last2 =  Sanders	 | first2 =  Peter
 | accessdate = 2012-08-28	}}
* {{cite journal| doi = 10.1007/978-3-642-22300-6_32 |ref=harv| last1 = Fischer | first1 = Johannes | chapter = Inducing the LCP-Array | title = Algorithms and Data Structures | series = Lecture Notes in Computer Science | volume = 6844 | pages = 374–385 | year = 2011 | isbn = 978-3-642-22299-3 }}
*{{cite journal| doi=10.1007/978-3-540-27810-8_32 |ref=harv| chapter=Two Space Saving Tricks for Linear Time LCP Array Computation| title=Algorithm Theory - SWAT 2004| series=Lecture Notes in Computer Science| year=2004| last1=Manzini| first1=Giovanni| isbn=978-3-540-22339-9| volume=3111| pages=372}}
*{{cite journal| doi=10.1007/978-3-642-02441-2_17 |ref=harv| chapter=Permuted Longest-Common-Prefix Array| title=Combinatorial Pattern Matching| series=Lecture Notes in Computer Science| year=2009| last1=Kärkkäinen| first1=Juha| last2=Manzini| first2=Giovanni| last3=Puglisi| first3=Simon J.| isbn=978-3-642-02440-5| volume=5577| pages=181}}
*{{cite journal| doi=10.1007/978-3-540-92182-0_14 |ref=harv| chapter=Space-Time Tradeoffs for Longest-Common-Prefix Array Computation| title=Algorithms and Computation| series=Lecture Notes in Computer Science| year=2008| last1=Puglisi| first1=Simon J.| last2=Turpin| first2=Andrew| isbn=978-3-540-92181-3| volume=5369| pages=124}}
*{{cite journal|ref=harv
 | title = Fast and Lightweight LCP-Array Construction Algorithms
 | url = http://www.siam.org/proceedings/alenex/2011/alx11_03_gogs.pdf
 | year = 2011
 | journal = Proceedings of the Workshop on Algorithm Engineering and Experiments, ALENEX 2011
 | pages = 25–34
 | last1 = Gog	 | first1 =  Simon
 | last2 =  Ohlebusch	 | first2 =  Enno
 | accessdate = 2012-08-28	}}
*{{cite journal| doi=10.1109/DCC.2009.42 | ref=harv| chapter=Linear Suffix Array Construction by Almost Pure Induced-Sorting| title=2009 Data Compression Conference| year=2009| last1=Nong| first1=Ge| last2=Zhang| first2=Sen| last3=Chan| first3=Wai Hong| isbn=978-0-7695-3592-0| pages=193}}
*{{cite journal | doi = 10.1007/978-3-540-74450-4_41 | ref=harv| last1 = Fischer | first1 = Johannes
| last2 = Heun | first2 = Volker
| chapter = A New Succinct Representation of RMQ-Information and Improvements in the Enhanced Suffix Array
| title = Combinatorics, Algorithms, Probabilistic and Experimental Methodologies
| series = Lecture Notes in Computer Science
| volume = 4614
| pages = 459
| year = 2007
| isbn = 978-3-540-74449-8
}}
*{{Cite journal | doi = 10.1007/s11786-007-0024-4 | ref= harv | last1 = Chen | first1 = G. | last2 = Puglisi | first2 = S. J. | last3 = Smyth | first3 = W. F. | title = Lempel–Ziv Factorization Using Less Time &amp; Space | journal = Mathematics in Computer Science | volume = 1 | issue = 4 | pages = 605 | year = 2008 }}
*{{Cite journal | doi = 10.1016/j.ipl.2007.10.006 | ref=harv | last1 = Crochemore | first1 = M. | last2 = Ilie | first2 = L. | title = Computing Longest Previous Factor in linear time and applications | journal = Information Processing Letters | volume = 106 | issue = 2 | pages = 75 | year = 2008 }}
*{{Cite journal | doi = 10.1109/DCC.2008.36 | ref=harv | last1 = Crochemore | first1 = M. | last2 = Ilie | first2 = L. | last3 = Smyth | first3 = W. F. | chapter = A Simple Algorithm for Computing the Lempel Ziv Factorization | title = Data Compression Conference (dcc 2008) | pages = 482 | year = 2008 | isbn = 978-0-7695-3121-2 }}
*{{Cite journal| doi = 10.1007/s00224-006-1198-x  |ref=harv| last1 = Sadakane | first1 = K. | title = Compressed Suffix Trees with Full Functionality |  journal = Theory of Computing Systems | volume = 41 | issue = 4 | pages = 589–607 | year = 2007 }}
*{{Cite journal | doi = 10.1016/j.tcs.2009.09.012 | ref=harv | last1 = Fischer | first1 = Johannes | last2 = Mäkinen | first2 = Veli | last3 = Navarro | first3 = Gonzalo | title = Faster entropy-bounded compressed suffix trees | journal = Theoretical Computer Science | volume = 410 | issue = 51 | pages = 5354 | year = 2009 }}

== External links ==
*[https://github.com/elventear/sais-lite-lcp Mirror of the ad-hoc-implementation of the code described in {{harvtxt|Fischer|2011}}]
*[https://github.com/simongog/sdsl/ SDSL: Succinct Data Structure Library - Provides various LCP array implementations, Range Minimum Query (RMQ) support structures and many more succinct data structures ]
*[https://github.com/carrotsearch/jsuffixarrays/blob/master/src/main/java/org/jsuffixarrays/Traversals.java Bottom-up suffix tree traversal emulated using suffix array and LCP array (Java)]
*[http://code.google.com/p/text-indexing/ Text-Indexing project] (linear-time construction of suffix trees, suffix arrays, LCP array and Burrows-Wheeler Transform)

[[Category:Arrays]]
[[Category:Substring indices]]
[[Category:String data structures]]</text>
      <sha1>s8odjsuedyc7ibv9iew9mtffmpsvecd</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Reverse lookup</title>
    <ns>0</ns>
    <id>654006</id>
    <revision>
      <id>606736094</id>
      <parentid>586642015</parentid>
      <timestamp>2014-05-02T07:07:55Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>source and clean up</comment>
      <text xml:space="preserve" bytes="1240">'''Reverse lookup''' is the act of using a [[value (computer science)|value]] to retrieve a [[unique key]] in an [[associative array]].&lt;ref&gt;{{citation|title=Think Python|first=Allen|last=Downey|publisher=O'Reilly Media|year=2012|isbn=9781449330729|page=125|url=http://books.google.com/books?id=1mZtP9H6OMQC&amp;pg=PA125}}.&lt;/ref&gt;

Applications of reverse lookup include [[reverse DNS lookup]], which provides the domain name associated with a particular IP address,&lt;ref&gt;{{citation|title=Understanding Directory Services|first1=Beth|last1=Sheresh|first2=Doug|last2=Sheresh|publisher=Sams Publishing|year=2001|isbn=9780672323058|page=237|url=http://books.google.com/books?id=CY98qQNHOl4C&amp;pg=PA237}}.&lt;/ref&gt; and a [[reverse telephone directory]], which provides the name of the entity associated with a particular telephone number.&lt;ref&gt;{{citation|title=Naked in Cyberspace: How to Find Personal Information Online|first=Carole A.|last=Lane|publisher=Information Today, Inc.|year=2002|isbn=9780910965507|page=144|url=http://books.google.com/books?id=G62_mWAGdgMC&amp;pg=PA144}}.&lt;/ref&gt;

== See also ==
* [[Inverse function]]
* [[Reverse dictionary]]

==References==
{{reflist}}


{{computer-science-stub}}
{{information-science-stub}}

[[Category:Arrays]]</text>
      <sha1>s26n9aten2pnxiq79y6rn6vcwvaq5uo</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Range mode query</title>
    <ns>0</ns>
    <id>42412822</id>
    <revision>
      <id>618770040</id>
      <parentid>609250062</parentid>
      <timestamp>2014-07-28T05:10:18Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <minor/>
      <comment>Qwertyus moved page [[Range mode queries]] to [[Range mode query]]: singular per naming conventions</comment>
      <text xml:space="preserve" bytes="14390">{{technical|date=April 2014}}
In [[data structure]]s, the range mode query problem asks to build a data structure on some input data to efficiently answer queries asking for the [[mode]]{{Disambiguation needed|date=May 2014}} of any consecutive subset of the input.

==Problem Statement==

Given an array &lt;math&gt;A[1:n] = [a_1,a_2,...,a_n]&lt;/math&gt;, we wish to answer queries of the form &lt;math&gt;mode(A, i:j)&lt;/math&gt;, where &lt;math&gt;1 \leq i \leq j \leq n&lt;/math&gt;. The mode of an array &lt;math&gt;S = [s_1,s_2,...,s_n]&lt;/math&gt;, &lt;math&gt;mode(S)&lt;/math&gt;, is an element &lt;math&gt;s_i&lt;/math&gt; such that the frequency of &lt;math&gt;s_i&lt;/math&gt; is greater than the frequency of &lt;math&gt;s_j \; \forall j \in \{1,...,n\}&lt;/math&gt;. For example, if &lt;math&gt;S = [1,2,4,2,3,4,2]&lt;/math&gt;, &lt;math&gt;mode(s) = 2&lt;/math&gt;. This definition can be extended to the mode of any subset of the array &lt;math&gt;A[i:j] = [a_i,a_i+1,...,a_j]&lt;/math&gt;.

===Theorem 1===

Let &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; be any [[multiset]]s. If &lt;math&gt;c&lt;/math&gt; is a mode of &lt;math&gt;A \cup B&lt;/math&gt; and &lt;math&gt;c \notin A&lt;/math&gt;, then &lt;math&gt;c&lt;/math&gt; is a mode of &lt;math&gt;B&lt;/math&gt;.

===Proof===

Let &lt;math&gt;c \notin A&lt;/math&gt; be a mode of &lt;math&gt;C = A \cup B&lt;/math&gt; and &lt;math&gt;f_c&lt;/math&gt; be its frequency in &lt;math&gt;C&lt;/math&gt;. Suppose that &lt;math&gt;c&lt;/math&gt; is not a mode of &lt;math&gt;B&lt;/math&gt;. Thus, there exists an element &lt;math&gt;b&lt;/math&gt; with frequency &lt;math&gt;f_b&lt;/math&gt; that is the mode of &lt;math&gt;B&lt;/math&gt;. Since &lt;math&gt;b&lt;/math&gt; is the mode of &lt;math&gt;B&lt;/math&gt; and that &lt;math&gt;c \notin A&lt;/math&gt;, then &lt;math&gt;f_b &gt; f_c&lt;/math&gt;. Thus, &lt;math&gt;b&lt;/math&gt; should be the mode of &lt;math&gt;C&lt;/math&gt; which is a contradiction.

==Results==

{| class=&quot;wikitable&quot;
|-
! Space || Query Time || Restrictions || Source
|-
| &lt;math&gt;O(n)&lt;/math&gt; || &lt;math&gt;O(\sqrt{n})&lt;/math&gt; || || &lt;ref name=chan2013&gt;{{cite journal|last=Chan|first=Timothy M.|first2=Stephane|last2=Durocher|first3=Kasper Green|last3=Larsen|first4=Jason|last4=Morrison|first5=Bryan T.|last5=Wilkinson|title=Linear-Space Data Structures for Range Mode Query in Arrays|journal=Theory of Computing Systems|year=2013|publisher=Springer|pages=1–23|url=http://cs.au.dk/~larsen/papers/linear_mode.pdf}}&lt;/ref&gt;
|-
| &lt;math&gt;O(n)&lt;/math&gt; || &lt;math&gt;O(\sqrt{n/w})&lt;/math&gt; || &lt;math&gt;w&lt;/math&gt; is the word size || &lt;ref name=chan2013 /&gt;
|-
| &lt;math&gt;O(n^2\log\log n/ \log n)&lt;/math&gt; || &lt;math&gt;O(1)&lt;/math&gt; || || &lt;ref name=&quot;morin&quot; /&gt;
|-
| &lt;math&gt;O(n^{2-2\epsilon})&lt;/math&gt; ||&lt;math&gt; O(n^\epsilon \log n)&lt;/math&gt; || &lt;math&gt;0\leq \epsilon\leq 1/2&lt;/math&gt; || &lt;ref name=&quot;morin&quot;&gt;{{cite journal|first=Danny|last=Krizanc|first2=Pat|last2=Morin|first3=Michiel H. M.|last3=Smid|title=Range Mode and Range Median Queries on Lists and Trees|journal=ISAAC|year=2003|pages=517–526|url=http://cg.scs.carleton.ca/~morin/publications/ds/rmq-njc.pdf}}&lt;/ref&gt;
|-
|}

==Lower bound==

Any data structure using &lt;math&gt;S&lt;/math&gt; cells of &lt;math&gt;w&lt;/math&gt; bits each needs &lt;math&gt;\Omega\left(\frac{\log n}{\log (S w/n)}\right)&lt;/math&gt; time to answer a range mode query.&lt;ref name=jorgensen&gt;{{cite journal|last=Greve|first=M|last2=Jørgensen|first2= A.|last3=Larsen|first3= K.|last4=Truelsen|first4= J.|title=Cell probe lower bounds and approximations for range mode|journal=Automata, Languages and Programming|year=2010|pages=605–616}}&lt;/ref&gt;

This contrasts with other range mode problems, such as the range minimum query which have solutions offering constant time query time and linear space. This is due to the hardness of the mode problem, since if we know the mode of &lt;math&gt;A[i:j]&lt;/math&gt; and the mode of &lt;math&gt;A[j+i:k]&lt;/math&gt;, there is no simple way of computing the mode of &lt;math&gt;A[i:k]&lt;/math&gt;. Any element of &lt;math&gt;A[j+i:k]&lt;/math&gt; or &lt;math&gt;A[j+i:k]&lt;/math&gt; could be the mode. For example, if &lt;math&gt;mode(A[i:j]) = a&lt;/math&gt; and its frequency is &lt;math&gt;f_a&lt;/math&gt;, and &lt;math&gt;mode(A[j+1:k]) = b&lt;/math&gt; and its frequency is also &lt;math&gt;f_a&lt;/math&gt;, there could be an element &lt;math&gt;c&lt;/math&gt; with frequency &lt;math&gt;f_a-1&lt;/math&gt; in &lt;math&gt;A[i:j]&lt;/math&gt; and frequency &lt;math&gt;f_a-1&lt;/math&gt; in &lt;math&gt;A[j+1:k]&lt;/math&gt;. &lt;math&gt;a \not= c \not= b&lt;/math&gt;, but its frequency in &lt;math&gt;A[i:k]&lt;/math&gt; is greater than the frequency of &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt;, which makes &lt;math&gt;c&lt;/math&gt; a better candidate for &lt;math&gt;mode(A[i:k])&lt;/math&gt; than &lt;math&gt;a&lt;/math&gt; or &lt;math&gt;b&lt;/math&gt;.

==Linear space data structure with square root query time==

This method by Chan et al.&lt;ref name=chan2013 /&gt; uses &lt;math&gt;O(n + s^2)&lt;/math&gt; space and &lt;math&gt;O(n/s)&lt;/math&gt; query time. By setting &lt;math&gt;s=\sqrt{n}&lt;/math&gt;, we get &lt;math&gt;O(n)&lt;/math&gt; and &lt;math&gt;O(\sqrt{n})&lt;/math&gt; bounds for space and query time.

===Preprocessing===

Let &lt;math&gt;A[1:n]&lt;/math&gt; be an array, and &lt;math&gt;D[1:\Delta]&lt;/math&gt; be an array that contains the distinct values of A, where &lt;math&gt;\Delta&lt;/math&gt; is the number of distinct elements. We define &lt;math&gt;B[1:n]&lt;/math&gt; to be an array such that, for each &lt;math&gt;i&lt;/math&gt;, &lt;math&gt;B[i]&lt;/math&gt; contains the rank (position) of &lt;math&gt;A[i]&lt;/math&gt; in &lt;math&gt;D&lt;/math&gt;. Arrays &lt;math&gt;B,D&lt;/math&gt; can be created by a linear scan of &lt;math&gt;A&lt;/math&gt;.

Arrays &lt;math&gt;Q_1, Q_2, ..., Q_\Delta&lt;/math&gt; are also created, such that, for each &lt;math&gt;a \in \{1,...,\Delta\}&lt;/math&gt;, &lt;math&gt;Q_a = \{b\; |\; B[b] = a\}&lt;/math&gt;. We then create an array &lt;math&gt;B'[1:n]&lt;/math&gt;, such that, for all &lt;math&gt;b \in \{1,...,n\}&lt;/math&gt;, &lt;math&gt;B'[b]&lt;/math&gt; contains the rank of &lt;math&gt;b&lt;/math&gt; in &lt;math&gt;Q_B[b]&lt;/math&gt;. Again, a linear scan of &lt;math&gt;B&lt;/math&gt; suffices to create arrays &lt;math&gt;Q_1,Q_2,...,Q_\Delta&lt;/math&gt; and &lt;math&gt;B'&lt;/math&gt;.

It is now possible to answer queries of the form &quot;is the frequency of &lt;math&gt;B[i]&lt;/math&gt; in &lt;math&gt;B[i:j]&lt;/math&gt; at least &lt;math&gt;q&lt;/math&gt;&quot; in constant time, by checking whether &lt;math&gt;Q_{B[i]}[B'[i]+q-1] \leq j&lt;/math&gt;.

The array is split B into &lt;math&gt;s&lt;/math&gt; blocks &lt;math&gt;b_1,b_2,...,b_s&lt;/math&gt;, each of size &lt;math&gt;t=\lceil n/s\rceil&lt;/math&gt;. Thus, a block &lt;math&gt;b_i&lt;/math&gt; spans over &lt;math&gt;B[i\cdot t+1 : (i+1)t]&lt;/math&gt;. The mode and the frequency of each block or set of consecutive blocks will be pre-computed in two tables &lt;math&gt;S&lt;/math&gt; and &lt;math&gt;S'&lt;/math&gt;. &lt;math&gt;S[b_i,b_j]&lt;/math&gt; is the mode of &lt;math&gt;b_i \cup b_{i+1} \cup ... \cup b_j&lt;/math&gt;, or equivalently, the mode of &lt;math&gt;B[b_it+1 : (b_j+1) t]&lt;/math&gt;, and &lt;math&gt;S'&lt;/math&gt; stores the corresponding frequency. These two tables can be stored in &lt;math&gt;O(s^2)&lt;/math&gt; space, and can be populated in &lt;math&gt;O(s\cdot n)&lt;/math&gt; by scanning &lt;math&gt;B&lt;/math&gt; &lt;math&gt;s&lt;/math&gt; times, computing a row of &lt;math&gt;S,S'&lt;/math&gt; each time with the following algorithm:

 '''algorithm''' computeS_Sprime '''is'''
    '''input:''' Array ''B''=[0:n-1], 
           Array ''D''=[0:Delta-1], 
           Integer ''s''
    '''output:''' Tables ''S'' and ''Sprime''
    let ''S'' &amp;larr; Table(0:s-1,0:s-1)
    let ''Sprime'' &amp;larr; Table(0:s-1,0:s-1)
    let ''firstOccurence'' &amp;larr; Array(0:Delta-1)
    '''for all''' i ''in'' {0,...,Delta-1} '''do'''
        firstOccurence[i] &amp;larr; -1 
    '''end for'''
    '''for''' i &amp;larr; 0:s-1 '''do'''    
        let ''j'' &amp;larr; i*t
        let ''c'' &amp;larr; 0
        let ''fc'' &amp;larr; 0
        let ''noBlock'' &amp;larr; i
        let ''block_start'' &amp;larr; j
        let ''block_end'' &amp;larr; min{(j+1)*t-1, n-1}
        '''while''' j &lt; n '''do'''    
            '''if''' firstOccurence[B[i]] = -1 '''then'''
                firstOccurence[B[i]] &amp;larr; j
            '''end if'''		
            '''if''' atLeastQInstances(firstOccurence[B[j]], block_end, fc+1) '''then'''
                c &amp;larr; B[i]
                fc &amp;larr; fc+1
            '''end if'''		
            '''if''' (j+1)%t = 0 '''or''' j = size-1 '''then'''
                S[i*s+noBlock] &amp;larr; c
                Sprime[i*s+noBlock] &amp;larr; fc			
                noBlock &amp;larr; noBlock+1
                block_end &amp;larr; min{block_end+t, size-1}
            '''end if'''
        '''end while'''
        '''for all''' i '''in''' {0,...,Delta-1} '''do'''
            firstOccurence[i] &amp;larr; -1 
        '''end for'''
    '''end for'''

===Query===

We will define the query algorithm over array &lt;math&gt;B&lt;/math&gt;. This can be translated to an answer over &lt;math&gt;A&lt;/math&gt;, since for any &lt;math&gt;a,i,j&lt;/math&gt;, &lt;math&gt;B[a]&lt;/math&gt; is a mode for &lt;math&gt;B[i:j]&lt;/math&gt; if and only if &lt;math&gt;A[a]&lt;/math&gt; is a mode for &lt;math&gt;A[i:j]&lt;/math&gt;. We can convert an answer for &lt;math&gt;B&lt;/math&gt; to an answer for &lt;math&gt;A&lt;/math&gt; in constant time by looking in &lt;math&gt;A&lt;/math&gt; or &lt;math&gt;B&lt;/math&gt; at the corresponding index.

Given a query &lt;math&gt;mode(B,i,j)&lt;/math&gt;, the query is split in three parts: the prefix, the span and the suffix. Let &lt;math&gt;b_i=\lceil (i-1)/t\rceil&lt;/math&gt; and &lt;math&gt;b_j = \lfloor j/t \rfloor -1&lt;/math&gt;. These denote the indices of the first and last block that are completely contained in &lt;math&gt;B&lt;/math&gt;. The range of these blocks is called the span. The prefix is then &lt;math&gt;B[i:min\{b_i t,j\}]&lt;/math&gt; (the set of indices before the span), and the suffix is &lt;math&gt;B[max\{(b_j+1)t+1,i\}:j]&lt;/math&gt; (the set of indices after the span).  The prefix, suffix or span can be empty, the latter is if &lt;math&gt;b_j &lt; b_i&lt;/math&gt;.

For the span, the mode &lt;math&gt;c&lt;/math&gt; is already stored in &lt;math&gt;S[b_i,b_j]&lt;/math&gt;. Let &lt;math&gt;f_c&lt;/math&gt; be the frequency of the mode, which is stored in &lt;math&gt;S'[b_i,b_j]&lt;/math&gt;. If the span is empty, let &lt;math&gt;f_c=0&lt;/math&gt;. Recall that, by Theorem 1, the mode of &lt;math&gt;B[i:j]&lt;/math&gt; is either an element of the prefix, span or suffix&lt;/span&gt;. A linear scan is performed over each element in the prefix and in the suffix to check if its frequency is greater than the current candidate &lt;math&gt;c&lt;/math&gt;, in which case &lt;math&gt;c&lt;/math&gt; and &lt;math&gt;f_c&lt;/math&gt; are updated to the new value. At the end of the scan, &lt;math&gt;c&lt;/math&gt; contains the mode of &lt;math&gt;B[i:j]&lt;/math&gt; and &lt;math&gt;f_c&lt;/math&gt; its frequency.

====Scanning procedure====

The procedure is similar for both prefix and suffix, so it suffice to run this procedure for both:

Let &lt;math&gt;x&lt;/math&gt; be the index of the current element. There are three cases:
#If &lt;math&gt;Q_{B[x]}[B'[x]-1] \geq i&lt;/math&gt;, then it was present in &lt;math&gt;B[i:x-1]&lt;/math&gt; and its frequency has already been counted. Pass to the next element.
#Otherwise, check if the frequency of &lt;math&gt;B[x]&lt;/math&gt; in &lt;math&gt;B[i:j]&lt;/math&gt; is at least &lt;math&gt;f_c&lt;/math&gt; (this can be done in constant time since it is the equivalent of checking it for &lt;math&gt;B[x:j]&lt;/math&gt;).
##If it is not, then pass to the next element.
##If it is, then compute the actual frequency &lt;math&gt;f_x&lt;/math&gt; of &lt;math&gt;B[x]&lt;/math&gt; in &lt;math&gt;B[i:j]&lt;/math&gt; by a linear scan (starting at index &lt;math&gt;B'[x]+f_c-1&lt;/math&gt;) or a binary search in &lt;math&gt;Q_{B[x]}&lt;/math&gt;. Set &lt;math&gt;c:= B[x]&lt;/math&gt; and &lt;math&gt;f_c := f_x&lt;/math&gt;.

This linear scan (excluding the frequency computations) is bounded by the block size &lt;math&gt;t&lt;/math&gt;, since neither the prefix or the suffix can be greater than &lt;math&gt;t&lt;/math&gt;. A further analysis of the linear scans done for frequency computations shows that it is also bounded by the block size.&lt;ref name=chan2013 /&gt; Thus, the query time is &lt;math&gt;O(t) = O(n/s)&lt;/math&gt;.

==Subquadratic space data structure with constant query time==

This method by &lt;ref name=morin /&gt; uses &lt;math&gt;O\left(\frac{n^2 \log{\log{n}}}{\log{n}}\right)&lt;/math&gt; space for a constant time query. We can observe that, if a constant query time is desired, this is a better solution than the one proposed by Chan et al.,&lt;ref name=chan2013 /&gt; as the latter gives a space of &lt;math&gt;O(n^2)&lt;/math&gt; for constant query time if &lt;math&gt;s=n&lt;/math&gt;.

===Preprocessing===

Let &lt;math&gt;A[1:n]&lt;/math&gt; be an array. The preprocessing is done in three steps:
#Split the array &lt;math&gt;A&lt;/math&gt; in &lt;math&gt;s&lt;/math&gt; blocks &lt;math&gt;b_1,b_2,...,b_s&lt;/math&gt;, where the size of each block is &lt;math&gt;t=\lceil n/s \rceil&lt;/math&gt;. Build a table &lt;math&gt;S&lt;/math&gt; of size &lt;math&gt;s \times s&lt;/math&gt; where &lt;math&gt;S[i,j]&lt;/math&gt; is the mode of &lt;math&gt;b_i \cup b_{i+1} \cup ... \cup b_j&lt;/math&gt;. The total space for this step is &lt;math&gt;O(s^2)&lt;/math&gt;
#For any query &lt;math&gt;mode(A,i,j)&lt;/math&gt;, let &lt;math&gt;b_{i'}&lt;/math&gt; be the block that contains &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;b_{j'}&lt;/math&gt; be the block that contains &lt;math&gt;j&lt;/math&gt;. Let the span be the set of blocks completely contained in &lt;math&gt;A[i:j]&lt;/math&gt;. The mode &lt;math&gt;c&lt;/math&gt; of the block can be retrieved from &lt;math&gt;S&lt;/math&gt;. By Theorem 1, the mode can be either the mode of the prefix (indices of &lt;math&gt;A[i:j]&lt;/math&gt; before the start of the span), the mode of the suffix (indices of &lt;math&gt;A[i:j]&lt;/math&gt; after the end of the span), or &lt;math&gt;c&lt;/math&gt;. The size of the prefix plus the size of the suffix is bounded by &lt;math&gt;2t&lt;/math&gt;, thus the position of the mode isstored as an integer ranging from &lt;math&gt;0&lt;/math&gt; to &lt;math&gt;2t&lt;/math&gt;, where &lt;math&gt;[0:2t-1]&lt;/math&gt;indicates a position in the prefix/suffix and &lt;math&gt;2t&lt;/math&gt; indicates that the mode is the mode of the span. There are &lt;math&gt;\binom{t}{2}&lt;/math&gt; possible queries involving blocks &lt;math&gt;b_{i'}&lt;/math&gt; and &lt;math&gt;b_{j'}&lt;/math&gt;, so these values are stored in a table of size &lt;math&gt;t^2&lt;/math&gt;. Furthermore, there are &lt;math&gt;(2t+1)^{t^2}&lt;/math&gt; such tables, so the total space required for this step is &lt;math&gt;O(t^2 (2t+1)^{t^2})&lt;/math&gt;. To access those tables, a pointer is added in addition to the mode in the table &lt;math&gt;S&lt;/math&gt; for each pair of blocks. 
#To handle queries &lt;math&gt;mode(A,i,j)&lt;/math&gt; where &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt; are in the same block, all such solutions are precomputed. There are &lt;math&gt;O(st^2)&lt;/math&gt; of them, they are stored in a three dimensional table &lt;math&gt;T&lt;/math&gt; of this size.

The total space used by this data structure is &lt;math&gt;O(s^2 + t^2(2t+1)^{t^2} + st^2)&lt;/math&gt;, which reduces to &lt;math&gt;O\left(\frac{n^2 \log{\log{n}}}{\log{n}}\right)&lt;/math&gt; if we take &lt;math&gt;t=\sqrt{\log{n}/\log{\log{n}}}&lt;/math&gt;.

===Query===

Given a query &lt;math&gt;mode(A,i,j)&lt;/math&gt;, check if it is completely contained inside a block, in which case the answer is stored in table &lt;math&gt;T&lt;/math&gt;. If the query spans exactly one or more blocks, then the answer is found in table &lt;math&gt;S&lt;/math&gt;. Otherwise, use the pointer stored in table &lt;math&gt;S&lt;/math&gt; at position &lt;math&gt;S[b_{i'},b_{j'}]&lt;/math&gt;, where &lt;math&gt;b_{i'},b_{j'}&lt;/math&gt; are the indices of the blocks that contain respectively &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt;, to find the table &lt;math&gt;U_{b_{i'},b_{j'}}&lt;/math&gt; that contains the positions of the mode for these blocks and use the position to find the mode in &lt;math&gt;A&lt;/math&gt;. This can be done in constant time.

==References==
{{reflist}}

&lt;!--- Categories ---&gt;

[[Category:Arrays]]</text>
      <sha1>l88h2r7000qc6lu6zbs3qopt3632s08</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Adjacency list</title>
    <ns>0</ns>
    <id>392431</id>
    <revision>
      <id>621644718</id>
      <parentid>618781177</parentid>
      <timestamp>2014-08-17T16:32:37Z</timestamp>
      <contributor>
        <username>Chmarkine</username>
        <id>15398482</id>
      </contributor>
      <minor/>
      <comment>/* Implementation details */change to https if the server sends [[HTTP Strict Transport Security|HSTS]] header, replaced: http://www.python.org → https://www.python.org using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="8311">[[Image:Simple cycle graph.svg|thumb|120px|This undirected cyclic graph can be described by the list {a,b}, {a,c}, {b,c}.]]

In [[graph theory]] and [[computer science]], an '''adjacency list''' representation of a [[graph (mathematics)|graph]] is a collection of unordered lists, one for each vertex in the graph. Each list describes the set of neighbors of its vertex.  See [[Sparse_matrix#Storing_a_sparse_matrix|Storing a sparse matrix]] for alternatives.

==Implementation details==
{|class=&quot;wikitable&quot; align=&quot;right&quot; style=&quot;width:18em;&quot;
|colspan=&quot;3&quot;|The graph pictured above has this adjacency list representation:
|-
|a|| adjacent to || b,c
|-
|b|| adjacent to || a,c
|-
|c|| adjacent to || a,b
|}

An adjacency list representation for a graph associates each vertex in the graph with the collection of its neighboring vertices or edges. There are many variations of this basic idea, differing in the details of how they implement the association between vertices and collections, in how they implement the collections, in whether they include both vertices and edges or only vertices as first class objects, and in what kinds of objects are used to represent the vertices and edges.
*An implementation suggested by [[Guido van Rossum]] uses a [[hash table]] to associate each vertex in a graph with an [[array data structure|array]] of adjacent vertices. In this representation, a vertex may be represented by any hashable object. There is no explicit representation of edges as objects.&lt;ref&gt;{{cite web
  | author = [[Guido van Rossum]]
  | year = 1998
  | title = Python Patterns — Implementing Graphs
  | url = https://www.python.org/doc/essays/graphs/}}&lt;/ref&gt;
*Cormen et al. suggest an implementation in which the vertices are represented by index numbers.&lt;ref&gt;{{cite book
  | title = [[Introduction to Algorithms]], Second Edition
  | publisher = MIT Press and McGraw-Hill
  | year = 2001
  | isbn = 0-262-03293-7
  | pages = 527–529 of section 22.1: Representations of graphs| author-separator = ,
  |author1 = Thomas H. Cormen|author2 = Charles E. Leiserson| author3 = Ronald L. Rivest
  | author4 = Clifford Stein
  | authorlink1 = Thomas H. Cormen
  | authorlink2 = Charles E. Leiserson
  | authorlink3 = Ronald L. Rivest
  | authorlink4 = Clifford Stein
  }}&lt;/ref&gt; Their representation uses an array indexed by vertex number, in which the array cell for each vertex points to a [[singly linked list]] of the neighboring vertices of that vertex. In this representation, the nodes of the singly linked list may be interpreted as edge objects; however, they do not store the full information about each edge (they only store one of the two endpoints of the edge) and in undirected graphs there will be two different linked list nodes for each edge (one within the lists for each of the two endpoints of the edge).
*The [[object oriented]] '''incidence list''' structure suggested by Goodrich and Tamassia has special classes of vertex objects and edge objects. Each vertex object has an instance variable pointing to a collection object that lists the neighboring edge objects. In turn, each edge object points to the two vertex objects at its endpoints.&lt;ref name=&quot;A&quot;&gt;{{cite book
  | author = [[Michael T. Goodrich]] and [[Roberto Tamassia]]
  | title = Algorithm Design: Foundations, Analysis, and Internet Examples
  | publisher = John Wiley &amp; Sons
  | year = 2002
  | isbn = 0-471-38365-1}}&lt;/ref&gt; This version of the adjacency list uses more memory than the version in which adjacent vertices are listed directly, but the existence of explicit edge objects allows it extra flexibility in storing additional information about edges.

{{clear}}

==Operations==
The main operation performed by the adjacency list data structure is to report a list of the neighbors of a given vertex. Using any of the implementations detailed above, this can be performed in constant time per neighbor. In other words, the total time to report all of the neighbors of a vertex ''v'' is proportional to the [[degree (graph theory)|degree]] of ''v''.

It is also possible, but not as efficient, to use adjacency lists to test whether an edge exists or does not exist between two specified vertices.  In an adjacency list in which the neighbors of each vertex are unsorted, testing for the existence of an edge may be performed in time proportional to the degree of one of the two given vertices, by using a [[sequential search]] through the neighbors of this vertex. If the neighbors are represented as a sorted array, [[binary search]] may be used instead, taking time proportional to the logarithm of the degree.

== Trade-offs ==
The main alternative to the adjacency list is the [[adjacency matrix]], a [[matrix (mathematics)|matrix]] whose rows and columns are indexed by vertices and whose cells contain a Boolean value that indicates whether an edge is present between the vertices corresponding to the row and column of the cell. For a [[sparse graph]] (one in which most pairs of vertices are not connected by edges) an adjacency list is significantly more space-efficient than an adjacency matrix (stored as an array): the space usage of the adjacency list is proportional to the number of edges and vertices in the graph, while for an adjacency matrix stored in this way the space is proportional to the square of the number of vertices. However, it is possible to store adjacency matrices more space-efficiently, matching the linear space usage of an adjacency list, by using a hash table indexed by pairs of vertices rather than an array.

The other significant difference between adjacency lists and adjacency matrices is in the efficiency of the operations they perform. In an adjacency list, the neighbors of each vertex may be listed efficiently, in time proportional to the degree of the vertex. In an adjacency matrix, this operation takes time proportional to the number of vertices in the graph, which may be significantly higher than the degree. On the other hand, the adjacency matrix allows testing whether two vertices are adjacent to each other in constant time; the adjacency list is slower to support this operation.

== Data Structures ==

For use as a data structure, the main alternative to the adjacency matrix is the adjacency list. Because each entry in the adjacency matrix requires only one bit, it can be represented in a very compact way, occupying only &lt;math&gt;|V|^2 / 8&lt;/math&gt; bytes of contiguous space. Besides avoiding wasted space, this compactness encourages locality of reference.

However, for a sparse graph, adjacency lists require less storage space, because they do not waste any space to represent edges that are not present. Using a naïve array implementation on a 32-bit computer, an adjacency list for an undirected graph requires about &lt;math&gt;8 |E|&lt;/math&gt; bytes of storage.

Noting that a simple graph can have at most &lt;math&gt;|V|^2&lt;/math&gt; edges, allowing loops, we can let &lt;math&gt;d = |E| / |V|^2&lt;/math&gt; denote the density of the graph. Then, &lt;math&gt;8 |E| &gt; |V|^2 / 8&lt;/math&gt;, or the adjacency list representation occupies more space precisely when &lt;math&gt;d &gt; 1/64&lt;/math&gt;. Thus a graph must be sparse indeed to justify an adjacency list representation.

Besides the space tradeoff, the different data structures also facilitate different operations. Finding all vertices adjacent to a given vertex in an adjacency list is as simple as reading the list. With an adjacency matrix, an entire row must instead be scanned, which takes &lt;math&gt;O(|V|)&lt;/math&gt; time. Whether there is an edge between two given vertices can be determined at once with an adjacency matrix, while requiring time proportional to the minimum degree of the two vertices with the adjacency list.

== References ==
{{Reflist}}

== Additional reading ==
* {{cite web
  | author = [[David Eppstein]]
  | year = 1996
  | title = ICS 161 Lecture Notes: Graph Algorithms
  | url = http://www.ics.uci.edu/~eppstein/161/960201.html}}

== External links ==
* The [[Boost Graph Library]] implements an efficient [http://www.boost.org/doc/libs/1_43_0/libs/graph/doc/index.html adjacency list]
* [http://opendatastructures.org/versions/edition-0.1e/ods-java/12_2_AdjacencyLists_Graph_a.html Open Data Structures - Section 12.2 - AdjacencyList: A Graph as a Collection of Lists]

[[Category:Graph data structures]]</text>
      <sha1>dk8v56cv3fr3tp5psk9s84lqsmhuwf6</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>And-inverter graph</title>
    <ns>0</ns>
    <id>4689919</id>
    <revision>
      <id>617862497</id>
      <parentid>617847962</parentid>
      <timestamp>2014-07-21T16:04:02Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{Refimprove}} {{Cn}}</comment>
      <text xml:space="preserve" bytes="7673">{{refimprove|date=July 2014}}
An '''and-inverter graph (AIG)''' is a directed, acyclic [[graph (mathematics)|graph]] that represents a structural implementation of the logical functionality of a [[digital circuit|circuit or network]].  An AIG consists of two-input nodes representing [[logical conjunction]], terminal nodes labeled with variable names, and edges optionally containing markers indicating [[logical negation]].  This representation of a logic function is rarely structurally efficient for large circuits, but is an efficient representation for manipulation of [[boolean function]]s. Typically, the abstract graph is represented as a [[data structure]] in software.
{| align=&quot;right&quot;
|-
| [[Image:And-inverter-graph.png|thumb|400px|Two structurally different AIGs for the function f(x1, x2, x3) = x2 * ( x1 + x3 )]]
|}

Conversion from the network of [[logic gate]]s to AIGs is fast and scalable. It only requires that every gate be expressed in terms of [[AND gate]]s and [[inverter (logic gate)|inverters]]. This conversion does not lead to unpredictable increase in memory use and runtime.  This makes the AIG an efficient representation in comparison with either the [[binary decision diagram]] (BDD) or the &quot;sum-of-product&quot; (ΣoΠ) form,{{cn|date=July 2014}} that is, the [[canonical form (Boolean algebra)|canonical form]] in [[Boolean algebra (logic)|Boolean algebra]] known as the [[disjunctive normal form]] (DNF). The BDD and DNF may also be viewed as circuits, but they involve formal constraints that deprive them of scalability.  For example, ΣoΠs are circuits with at most two levels while BDDs are canonical, that is, they require that input variables be evaluated in the same order on all paths.

Circuits composed of simple gates, including AIGs, are an &quot;ancient&quot; research topic. The interest in AIGs started in the late 1950s&lt;ref&gt;{{cite journal|author=L. Hellerman|title=A catalog of three-variable Or-Inverter and And-Inverter logical circuits|journal=IEEE Trans. Electron. Comput.|volume=EC-12|date=June 1963|pages=198&amp;ndash;223|doi=10.1109/PGEC.1963.263531|issue=3}}&lt;/ref&gt; and continued in the 1970s when various local transformations have been developed.  These transformations were implemented in several
logic synthesis and verification systems, such as Darringer et al.&lt;ref&gt;{{cite journal|author=A. Darringer, W. H. Joyner, Jr., C. L. Berman, L. Trevillyan|title=Logic synthesis through local transformations|journal=IBM J. of Research and Development|volume=25|issue=4|year=1981|pages=272&amp;ndash;280|doi=10.1147/rd.254.0272}}&lt;/ref&gt; and Smith et al.,&lt;ref&gt;{{cite journal|author=G. L. Smith, R. J. Bahnsen, H. Halliwell|title=Boolean comparison of hardware and flowcharts|journal=IBM J. of Research and Development|volume=26|issue=1|year=1982|pages=106&amp;ndash;116|doi=10.1147/rd.261.0106}}&lt;/ref&gt; which reduce circuits to improve area and delay during synthesis, or to speed up [[formal equivalence checking]]. Several important techniques were discovered early at [[IBM]], such as combining and reusing multi-input logic expressions and subexpressions, now known as [[structural hashing]].

Recently there has been a renewed interest in AIGs as a [[Functional Representation|functional representation]] for a variety of tasks in synthesis and verification. That is because representations popular in the 1990s (such as BDDs) have reached their limits of scalability in many of their applications.{{cn|date=July 2014}} Another important development was the recent emergence of much more efficient [[boolean satisfiability]] (SAT) solvers. When coupled with ''AIGs'' as the circuit representation, they lead to remarkable speedups in solving a wide variety of [[boolean problem]]s.{{cn|date=July 2014}}

AIGs found successful use in diverse [[Electronic design automation|EDA]] applications. A well-tuned combination of ''AIGs'' and [[boolean satisfiability]] made an impact on [[formal verification]], including both [[model checking]] and equivalence checking.&lt;ref&gt;{{cite journal|author=A. Kuehlmann, V. Paruthi, F. Krohm, and M. K. Ganai|title=Robust boolean reasoning for equivalence checking and functional property verification|journal=IEEE Trans. CAD|volume=21|issue=12|year=2002|pages=1377&amp;ndash;1394}}&lt;/ref&gt;  Another recent work shows that efficient circuit compression techniques can be developed using AIGs.&lt;ref&gt;{{cite conference|author=P. Bjesse and A. Boralv|title=DAG-aware circuit compression for formal verification|booktitle=Proc. ICCAD '04|pages=42&amp;ndash;49}}&lt;/ref&gt; There is a growing understanding that logic and [[physical synthesis]] problems can be solved using AIGs simulation and [[boolean satisfiability]] compute functional properties (such as symmetries&lt;ref&gt;{{cite conference|author=K.-H. Chang, I. L. Markov, V. Bertacco|title=Post-placement rewiring and rebuffering by exhaustive search for functional symmetries|booktitle=Proc. ICCAD '05`pages=56&amp;ndash;63}}&lt;/ref&gt;) and node flexibilities (such as [[don't-care]]s, [[resubstitution]]s, and [[SPFD]]s&lt;ref&gt;{{cite journal|author=A. Mishchenko, J. S. Zhang, S. Sinha, J. R. Burch, R. Brayton, and M. Chrzanowska-Jeske|title=Using simulation and satisfiability to compute flexibilities in Boolean networks|journal=IEEE Trans. CAD|volume=25|issue=5|date=May 2006|pages=743&amp;ndash;755.}}&lt;/ref&gt;). This work shows that AIGs are a promising ''unifying'' representation, which can bridge [[logic synthesis]], [[technology mapping]], physical synthesis, and formal verification.  This is, to a large extent, due to the simple and uniform structure of AIGs, which allow rewriting, simulation, mapping, placement, and verification to share the same data structure.

In addition to combinational logic, AIGs have also been applied to [[sequential logic]] and sequential transformations.  Specifically, the method of structural hashing was extended to work for AIGs with memory elements (such as [[flip-flop (electronics)#D flip-flop|D-type flip-flop]]s with an initial state,
which, in general, can be unknown) resulting in a data structure that is specifically tailored for applications related to [[retiming]].&lt;ref&gt;{{cite conference|author=J. Baumgartner and A. Kuehlmann|title=Min-area retiming on flexible circuit structures|booktitle= Proc. ICCAD'01|pages=176&amp;ndash;182}}&lt;/ref&gt;

Ongoing research includes implementing a modern logic synthesis system completely based on AIGs.  The prototype called [http://www.eecs.berkeley.edu/~alanmi/abc/ ABC] features an AIG package, several AIG-based synthesis and equivalence-checking techniques, as well as an experimental implementation of sequential synthesis. One such technique combines technology mapping and retiming in a single optimization step. These optimizations can be implemented using networks composed of arbitrary gates, but the use of AIGs makes them more scalable and easier to implement.

==Implementations==
* Logic Synthesis and Verification System [http://www.eecs.berkeley.edu/~alanmi/abc/ ABC]
* A set of utilities for AIGs [http://fmv.jku.at/aiger/index.html AIGER]
* [http://www.si2.org/openeda.si2.org/help/group_ld.php?group=73 OpenAccess Gear]

==References==
&lt;references /&gt;

==See also==
* [[Binary decision diagram]]
* [[Logical conjunction]]

----
''This article is adapted from a column in the ACM [http://www.sigda.org SIGDA] [http://www.sigda.org/newsletter/index.html e-newsletter] by [http://www.eecs.berkeley.edu/~alanmi/ Alan Mishchenko] &lt;br&gt;
Original text is available [http://archive.sigda.org/newsletter/2006/060215.txt here].''

[[Category:Graph data structures]]
[[Category:Diagrams]]
[[Category:Electrical circuits]]
[[Category:Electronic design automation]]
[[Category:Formal methods]]</text>
      <sha1>k1nllkgnkzn9cij3h8ls56g25hj3h0d</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Binary moment diagram</title>
    <ns>0</ns>
    <id>4056695</id>
    <revision>
      <id>451611787</id>
      <parentid>423280899</parentid>
      <timestamp>2011-09-21T03:16:27Z</timestamp>
      <contributor>
        <username>Headbomb</username>
        <id>1461430</id>
      </contributor>
      <minor/>
      <comment>Cleanup for [[Book:Data structures]] using [[Project:AWB|AWB]] (7840)</comment>
      <text xml:space="preserve" bytes="4059">A '''binary moment diagram (BMD)''' is a generalization of the [[binary decision diagram]] (BDD) to linear functions over domains such as booleans (like BDDs), but also to integers or to real numbers.

They can deal with boolean functions with complexity comparable to BDDs, but also some functions that are dealt with very inefficiently in a BDD are handled easily by BMD, most notably [[multiplication]].

The most important properties of BMD is that, like with BDDs, each function has exactly one canonical representation, and many operations can be efficiently performed on these representations.

The main features that differentiate BMDs from BDDs are using linear instead of pointwise diagrams, and having weighted edges.

The rules that ensure the canonicity of the representation are:
* Decision over variables higher in the ordering may only point to decisions over variables lower in the ordering.
* No two nodes may be identical (in normalization such nodes all references to one of these nodes should be replaced be references to another)
* No node may have all decision parts equivalent to 0 (links to such nodes should be replaced by links to their always part)
* No edge may have weight zero (all such edges should be replaced by direct links to&amp;nbsp;0)
* Weights of the edges should be [[coprime]]. Without this rule or some equivalent of it, it would be possible for a function to have many representations, for example 2''x''&amp;nbsp;+&amp;nbsp;2 could be represented as 2&amp;nbsp;·&amp;nbsp;(1&amp;nbsp;+&amp;nbsp;''x'') or 1&amp;nbsp;·&amp;nbsp;(2&amp;nbsp;+&amp;nbsp;2''x'').

== Pointwise and linear decomposition ==

In pointwise decomposition, like in BDDs, on each branch point we store result of all branches separately.  An example of such decomposition for an integer function (2''x''&amp;nbsp;+&amp;nbsp;''y'') is:

:&lt;math&gt;\begin{cases} \text{if } x
\begin{cases}
 \text{if } y , 3
\\
 \text{if } \neg y , 2
\end{cases}
\\ 
 \text{if } \neg x
\begin{cases}
 \text{if } y \text{ , } 1
\\
 \text{if } \neg y \text{ , } 0
\end{cases}
\end{cases}&lt;/math&gt;

In linear decomposition we provide instead a default value and a difference:

:&lt;math&gt;\begin{cases}
\text{always} 
\begin{cases}
\text{always } 0 \\
\text{if } y , +1
\end{cases}
\\
\text{if } x , +2
\end{cases}&lt;/math&gt;

It can easily be seen that the latter (linear) representation is much more efficient in case of additive functions, as when we add many elements the latter representation will have only O(''n'') elements, while the former (pointwise), even with sharing, exponentially many.

== Edge weights ==

Another extension is using weights for edges. A value of function at given node is a sum of the true nodes below it (the node under always, and possibly the decided node) times the edges' weights.

For example &lt;math&gt;(4x_2 + 2x_1 + x_0) (4y_2 + 2y_1 + y_0)&lt;/math&gt; can be represented as:
# Result node, always 1&amp;times; value of node 2, if &lt;math&gt;x_2&lt;/math&gt; add 4&amp;times; value of node 4
# Always 1&amp;times; value of node 3, if &lt;math&gt;x_1&lt;/math&gt; add 2&amp;times; value of node 4
# Always 0, if &lt;math&gt;x_0&lt;/math&gt; add 1&amp;times; value of node 4
# Always 1&amp;times; value of node 5, if &lt;math&gt;y_2&lt;/math&gt; add +4
# Always 1&amp;times; value of node 6, if &lt;math&gt;y_1&lt;/math&gt; add +2
# Always 0, if &lt;math&gt;y_0&lt;/math&gt; add +1

Without weighted nodes a much more complex representation would be required:
# Result node, always value of node 2, if &lt;math&gt;x_2&lt;/math&gt; value of node 4
# Always value of node 3, if &lt;math&gt;x_1&lt;/math&gt; value of node 7
# Always 0, if &lt;math&gt;x_0&lt;/math&gt; value of node 10
# Always value of node 5, if &lt;math&gt;y_2&lt;/math&gt; add +16
# Always value of node 6, if &lt;math&gt;y_1&lt;/math&gt; add +8
# Always 0, if &lt;math&gt;y_0&lt;/math&gt; add +4
# Always value of node 8, if &lt;math&gt;y_2&lt;/math&gt; add +8
# Always value of node 9, if &lt;math&gt;y_1&lt;/math&gt; add +4
# Always 0, if &lt;math&gt;y_0&lt;/math&gt; add +2
# Always value of node 11, if &lt;math&gt;y_2&lt;/math&gt; add +4
# Always value of node 12, if &lt;math&gt;y_1&lt;/math&gt; add +2
# Always 0, if &lt;math&gt;y_0&lt;/math&gt; add +1

==References==
*{{Unreferenced|date=January 2007}}

[[Category:Graph data structures]]
[[Category:Formal methods]]</text>
      <sha1>2395b7twputdvnxv5amj4pow3i1k5gl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Propositional directed acyclic graph</title>
    <ns>0</ns>
    <id>4477141</id>
    <revision>
      <id>561376261</id>
      <parentid>491665847</parentid>
      <timestamp>2013-06-24T15:51:22Z</timestamp>
      <contributor>
        <username>Mvinyals</username>
        <id>12354125</id>
      </contributor>
      <minor/>
      <comment>Fix abuse of mathcal symbols</comment>
      <text xml:space="preserve" bytes="2970">A '''propositional directed acyclic graph (PDAG)''' is a [[data structure]] that is used to represent a [[Boolean function]].  A Boolean function can be represented as a rooted, [[directed acyclic graph]] of the following form:
* Leaves are labeled with &lt;math&gt;\top&lt;/math&gt; (true), &lt;math&gt;\bot&lt;/math&gt; (false), or a Boolean variable.
* Non-leaves are &lt;math&gt;\bigtriangleup&lt;/math&gt; (logical and), &lt;math&gt;\bigtriangledown&lt;/math&gt; (logical or) and &lt;math&gt;\Diamond&lt;/math&gt; (logical not).
* &lt;math&gt;\bigtriangleup&lt;/math&gt;- and &lt;math&gt;\bigtriangledown&lt;/math&gt;-nodes have at least one child.
* &lt;math&gt;\Diamond&lt;/math&gt;-nodes have exactly one child.

Leaves labeled with &lt;math&gt;\top&lt;/math&gt; (&lt;math&gt;\bot&lt;/math&gt;) represent the constant Boolean function which always evaluates to 1 (0). A leaf labeled with a Boolean variable &lt;math&gt;x&lt;/math&gt; is interpreted as the assignment &lt;math&gt;x=1&lt;/math&gt;, i.e. it represents the Boolean function which evaluates to 1 if and only if &lt;math&gt;x=1&lt;/math&gt;. The Boolean function represented by a &lt;math&gt;\bigtriangleup&lt;/math&gt;-node is the one that evaluates to 1, if and only if the Boolean function of all its children evaluate to 1. Similarly, a &lt;math&gt;\bigtriangledown&lt;/math&gt;-node represents the Boolean function that evaluates to 1, if and only if the Boolean function of at least one child evaluates to 1. Finally, a &lt;math&gt;\Diamond&lt;/math&gt;-node represents the complemenatary Boolean function its child, i.e. the one that evaluates to 1, if and only if the Boolean function of its child evaluates to 0.

== PDAG, BDD, and NNF ==
Every '''[[Binary decision diagram|binary decision diagram (BDD)]]''' and every '''[[Negation normal form|negation normal form (NNF)]]''' are also a PDAG with some particular properties. The following pictures represent the Boolean function  &lt;math&gt;f(x1, x2, x3) = -x1 * -x2 * -x3  +  x1 * x2  +  x2 * x3&lt;/math&gt;:

{| align=&quot;center&quot;
|-
| [[File:BDD simple.svg|thumb|189px|BDD for the function f]]
| [[File:BDD2pdag.png|thumb|189px|PDAG for the function f obtained from the BDD]]
| [[File:BDD2pdag simple.svg|thumb|189px|PDAG for the function f]]
|}

== See also ==
* [[Data structure]]
* [[Boolean satisfiability problem]]
* [[Proposition (mathematics)|Proposition]]

== References ==
*  M. Wachter &amp; R. Haenni, &quot;Propositional DAGs: a New Graph-Based Language for Representing Boolean Functions&quot;, KR'06, 10th International Conference on Principles of Knowledge Representation and Reasoning, Lake District, UK, 2006.
*  M. Wachter &amp; R. Haenni, &quot;Probabilistic Equivalence Checking with Propositional DAGs&quot;, Technical Report iam-2006-001, Institute of Computer Science and Applied Mathematics, University of Bern, Switzerland, 2006.
*  M. Wachter, R. Haenni &amp; J. Jonczy,  &quot;Reliability and Diagnostics of Modular Systems: a New Probabilistic Approach&quot;, DX'06, 18th International Workshop on Principles of Diagnosis, Peñaranda de Duero, Burgos, Spain, 2006.

[[Category:Graph data structures]]
[[Category:Directed graphs]]
[[Category:Boolean algebra]]</text>
      <sha1>e2fxu239w90vbwghm9pwglheyrxg0g7</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Scene graph</title>
    <ns>0</ns>
    <id>161944</id>
    <revision>
      <id>618086925</id>
      <parentid>572911802</parentid>
      <timestamp>2014-07-23T06:46:50Z</timestamp>
      <contributor>
        <ip>37.110.57.35</ip>
      </contributor>
      <comment>/* Web sites and articles */</comment>
      <text xml:space="preserve" bytes="17026">{{this|general data structure for vector-based graphics|the software component used in user interfaces|Canvas (GUI)}}

A '''scene graph''' is a general [[data structure]] commonly used by [[vector graphics|vector-based graphics editing]] applications and modern computer games. Examples of such programs include [[Acrobat 3D]], [[Adobe Illustrator]], [[AutoCAD]], [[CorelDRAW]], [[OpenSceneGraph]], [[OpenSG]], [[VRML97]], and [[X3D]].

The scene graph is a structure that arranges the logical and often (but not necessarily) spatial representation of a graphical scene.
The definition of a scene graph is fuzzy because programmers who implement scene graphs in applications — and, in particular, the games industry — take the basic principles and adapt these to suit particular applications. This means there is no consensus as to what a scene graph should be.

A scene graph is a collection of nodes in a [[Graph (data structure)|graph]] or [[Tree (data structure)|tree]] structure. A tree node (in the overall tree structure of the scene graph) may have many children but often only a single parent, with the effect of a parent applied to all its child nodes; an operation performed on a group automatically propagates its effect to all of its members. In many programs, associating a geometrical [[transformation matrix]] (see also [[Transformation (mathematics)|transformation]] and [[matrix (math)|matrix]]) at each group level and concatenating such matrices together is an efficient and natural way to process such operations. A common feature, for instance, is the ability to group related shapes/objects into a compound object that can then be moved, transformed, selected, etc. as easily as a single object. 

It also happens that in some scene graphs, a node can have a relation to any node including itself, or at least an extension that refers to another node (for instance [[Pixar]]'s [[PhotoRealistic RenderMan]] because of its usage of [[Reyes rendering]] algorithm, or [[Adobe Systems]]'s [[Acrobat 3D]] for advanced interactive manipulation).

==Scene graphs in graphics editing tools==
In vector-based graphics editing, each [[leaf node]] in a scene graph represents some atomic unit of the document, usually a shape such as an [[ellipse]] or [[Bezier curve|Bezier path]]. Although shapes themselves (particularly paths) can be decomposed further into nodes such as [[spline (mathematics)|spline nodes]], it is practical to think of the scene graph as composed of shapes rather than going to a lower level of representation.

Another useful and user-driven node concept is the [[2D_computer_graphics#Layers|layer]]. A layer acts like a transparent sheet upon which any number of shapes and shape groups can be placed. The document then becomes a set of layers, any of which can be conveniently made invisible, dimmed, or locked (made read-only). Some applications place all layers in a linear list, while others support sublayers (i.e., layers within layers to any desired depth).

Internally, there may be no real structural difference between layers and groups at all, since they are both just nodes of a scene graph. If differences are needed, a common type declaration in [[C++]] would be to make a generic node class, and then derive layers and groups as subclasses. A visibility member, for example, would be a feature of a layer, but not necessarily of a group.

==Scene graphs in games and 3D applications==
Scene graphs are useful for modern games using [[3D graphics]] and increasingly large worlds or levels. In such applications, nodes in a scene graph (generally) represent entities or objects in the scene.

For instance, a game might define a logical relationship between a knight and a horse so that the knight is considered an extension to the horse. The scene graph would have a 'horse' node with a 'knight' node attached to it.

As well as describing the logical relationship, the scene graph may also describe the spatial relationship of the various entities: the knight moves through 3D space as the horse moves.

In these large applications, memory requirements are major considerations when designing a scene graph. For this reason, many large scene graph systems use instancing to reduce memory costs and increase speed. In our example above, each knight is a separate scene node, but the graphical representation of the knight (made up of a 3D mesh, textures, materials and shaders) is instanced. This means that only a single copy of the data is kept, which is then referenced by any 'knight' nodes in the scene graph. This allows a reduced memory budget and increased speed, since when a new knight node is created, the appearance data does not need to be duplicated.

==Scene graph implementation==
The simplest form of scene graph uses an [[Array data structure|array]] or [[linked list]] [[data structure]], and displaying its shapes is simply a matter of linearly iterating the nodes one by one. Other common operations, such as checking to see [[Point location|which shape intersects the mouse pointer]] (e.g., in a [[GUI]]-based [[application software|applications]]) are also done via linear searches. For small scene graphs, this tends to suffice.

Larger scene graphs cause linear operations to become noticeably slow and thus more complex underlying data structures are used, the most popular and common form being a [[tree data structure|tree]]. In these scene graphs, the [[composite pattern|composite design pattern]] is often employed to create the hierarchical representation of group nodes and leaf nodes.

'''[[Group node]]s''' — Can have any number of child nodes attached to it. Group nodes include transformations and switch nodes.

'''[[Leaf node]]s''' — Are nodes that are actually [[Rendering (computer graphics)|rendered]] or see the effect of an operation. These include objects, sprites, sounds, lights and anything that could be considered 'rendered' in some abstract sense.

===Scene graph operations and dispatch===
Applying an operation on a scene graph requires some way of dispatching an operation based on a node's type. For example, in a render operation, a transformation group node would accumulate its transformation by matrix multiplication, vector displacement, [[quaternions]] or [[Euler angles]].  After which a leaf node sends the object off for rendering to the renderer.  Some implementations might render the object directly, which invokes the underlying rendering [[API]], such as [[DirectX]] or [[OpenGL]]. But since the underlying implementation of the rendering API usually lacks portability, one might separate the scene graph and rendering systems instead. In order to accomplish this type of dispatching, several different approaches can be taken.

In object-oriented languages such as [[C++]], this can easily be achieved by [[virtual functions]], where each represents an operation that can be performed on a node. Virtual functions are simple to write, but it is usually impossible to add new operations to nodes without access to the source code. Alternatively, the '''[[visitor pattern]]''' can be used. This has a similar disadvantage in that it is similarly difficult to add new node types.

Other techniques involve the use of RTTI ([[Run-Time Type Information]]). The operation can be realised as a class that is passed to the current node; it then queries the node's type using RTTI and looks up the correct operation in an array of [[Callback (computer programming)|callback]]s or [[Function object|functor]]s. This requires that the map of types to callbacks or functors be initialized at runtime, but offers more flexibility, speed and extensibility.

Variations on these techniques exist, and new methods can offer added benefits. One alternative is scene graph rebuilding, where the scene graph is rebuilt for each of the operations performed. This, however, can be very slow, but produces a highly optimised scene graph. It demonstrates that a good scene graph implementation depends heavily on the application in which it is used.

====Traversals====
[[Tree traversal|Traversals]] are the key to the power of applying operations to scene graphs. A traversal generally consists of starting at some arbitrary node (often the root of the scene graph), applying the operation(s) (often the updating and rendering operations are applied one after the other), and recursively moving down the scene graph (tree) to the child nodes, until a leaf node is reached. At this point, many scene graph engines then traverse back up the tree, applying a similar operation. For example, consider a render operation that takes transformations into account: while recursively traversing down the scene graph hierarchy, a pre-render operation is called. If the node is a transformation node, it adds its own transformation to the current transformation matrix. Once the operation finishes traversing all the children of a node, it calls the node's post-render operation so that the transformation node can undo the transformation. This approach drastically reduces the necessary amount of matrix multiplication.

Some scene graph operations are actually more efficient when nodes are traversed in a different order — this is where some systems implement scene graph rebuilding to reorder the scene graph into an easier-to-parse format or tree.

For example, in 2D cases, scene graphs typically render themselves by starting at the tree's root node and then recursively draw the child nodes. The tree's leaves represent the most foreground objects. Since drawing proceeds from back to front with closer objects simply overwriting farther ones, the process is known as employing the [[Painter's algorithm]]. In 3D systems, which often employ [[depth buffer]]s, it is more efficient to draw the closest objects first, since farther objects often need only be depth-tested instead of actually rendered, because they are occluded by nearer objects.

==Scene graphs and bounding volume hierarchies (BVHs)==
[[Bounding volume hierarchy|Bounding Volume Hierarchies]] (BVHs) are useful for numerous tasks — including efficient culling and speeding up collision detection between objects. A BVH is a spatial structure, but doesn't have to partition the geometry (see [[Scene graph#Scene graphs and spatial partitioning|spatial partitioning]] below).

A BVH is a tree of [[bounding volume]]s (often spheres, axis-aligned [[bounding box]]es or oriented bounding boxes). At the bottom of the hierarchy, the size of the volume is just large enough to encompass a single object tightly (or possibly even some smaller fraction of an object in high resolution BVHs). As one ascends the hierarchy, each node has its own volume that tightly encompasses all the volumes beneath it. At the root of the tree is a volume that encompasses all the volumes in the tree (the whole scene).

BVHs are useful for speeding up collision detection between objects. If an object's bounding volume does not intersect a volume higher in the tree, it cannot intersect any object below that node (so they are all rejected very quickly).

Obviously, there are some similarities between BVHs and scene graphs. A scene graph can easily be adapted to include/become a BVH — if each node has a volume associated or there is a purpose-built 'bound node' added in at convenient location in the hierarchy. This may not be the typical view of a scene graph, but there are benefits to including a BVH in a scene graph.

==Scene graphs and spatial partitioning==
An effective way of combining [[Space partitioning|spatial partitioning]] and scene graphs is by creating a scene leaf node that contains the spatial partitioning data. This data is usually static and generally contains non-moving level data in some partitioned form. Some systems may have the systems and their rendering separately. This is fine and there are no real advantages to either method. In particular, it is bad to have the scene graph contained within the spatial partitioning system, as the scene graph is better thought of as the grander system to the spatial partitioning.

===When it is useful to combine them===
In short: Spatial partitioning will/should considerably speed up the processing and rendering time of the scene graph.

Very large drawings, or scene graphs that are generated solely at [[Run time (program lifecycle phase)|runtime]] (as happens in [[Ray tracing (graphics)|ray tracing]] [[Scanline rendering|rendering]] programs), require defining of group nodes in a more automated fashion. A raytracer, for example, will take a scene description of a [[dimension|3D]] model and build an internal representation that breaks up its individual parts into bounding boxes (also called bounding slabs). These boxes are grouped hierarchically so that ray intersection tests (as part of visibility determination) can be efficiently computed. A group box that does not intersect an eye ray, for example, can entirely skip testing any of its members.

A similar efficiency holds in 2D applications as well. If the user has magnified a document so that only part of it is visible on his computer screen, and then scrolls in it, it is useful to use a bounding box (or in this case, a bounding rectangle scheme) to quickly determine which scene graph elements are visible and thus actually need to be drawn.

Depending on the particulars of the application's drawing performance, a large part of the scene graph's design can be impacted by rendering efficiency considerations. In 3D video games such as [[Quake computer game|Quake]], for example, [[binary space partitioning]] (BSP) trees are heavily favored to minimize visibility tests. BSP trees, however, take a very long time to compute from design scene graphs, and must be recomputed if the design scene graph changes, so the levels tend to remain static, and dynamic characters aren't generally considered in the spatial partitioning scheme.

Scene graphs for dense regular objects such as [[heightfield]]s and polygon meshes tend to employ [[quadtree]]s and [[octree]]s, which are specialized variants of a 3D bounding box hierarchy. Since a heightfield occupies a box volume itself, recursively subdividing this box into eight subboxes (hence the 'oct' in octree) until individual heightfield elements are reached is efficient and natural. A quadtree is simply a 2D octree.

==Standards==
===PHIGS===
[[PHIGS]] was the first commercial scene graph specification, and became an ANSI standard in 1988. Disparate implementations were provided by [[Unix]] hardware vendors. The [[HOOPS 3D Graphics System]] appears to have been the first commercial scene graph library provided by a single software vendor. It was designed to run on disparate lower-level 2D and 3D interfaces, with the first major production version (v3.0) completed in 1991. Shortly thereafter, [[Silicon Graphics]] released [[IRIS Inventor]] 1.0 (1992), which was a scene graph built on top of the IRIS GL 3D API. It was followed up with [[Open Inventor]] in 1994, a portable scene graph built on top of OpenGL. More 3D scene graph libraries can be found in [[:Category:3D scenegraph APIs]].

===X3D===
[[X3D]] is a royalty-free open-standards file format and run-time architecture to represent and communicate 3D scenes and objects using [[XML]]. It is an [[International Standards Organization|ISO]]-ratified standard that provides a system for the storage, retrieval and playback of real-time graphics content embedded in applications, all within an open architecture to support a wide array of domains and user scenarios.

==See also ==
* [[Graph (data structure)]]
* [[Graph theory]]
* [[Space partitioning]]
* [[Tree (data structure)]]

==References==
===Books===
* Leler, Wm and Merry, Jim (1996) ''3D with HOOPS'', Addison-Wesley
* Wernecke, Josie (1994) ''The Inventor Mentor: Programming Object-Oriented 3D Graphics with Open Inventor'', Addison-Wesley, ISBN 0-201-62495-8 (Release 2)

===Web sites and articles===
* Bar-Zeev, Avi. [http://www.realityprime.com/scenegraph.php &quot;Scenegraphs: Past, Present, and Future&quot;]
* Carey, Rikk and Bell, Gavin (1997). [http://www.jwave.vt.edu/~engineer/vrml97book/ch1.htm &quot;The Annotated VRML 97 Reference Manual&quot;]
* Helman, Jim; Rohlf, John (1994). [http://portal.acm.org/citation.cfm?id=192262 &quot;IRIS Performer: A High Performance Multiprocessing Toolkit for Real-Time 3D Graphics&quot;]
* [http://java3d.dev.java.net Java3D]: [http://aviatrix3d.j3d.org Aviatrix3D], [https://lg3d.dev.java.net LG3D] * [[Jreality]]
* [http://www.opensg.org OpenSG]
* [http://www.openscenegraph.org OpenSceneGraph]
* [http://osgjs.org OSG.JS] Javascript Implementation of OpenSceneGraph
* PEXTimes [http://www.jch.com/jch/vrml/PEXTimes.txt]
* Strauss, Paul (1993). [http://portal.acm.org/citation.cfm?id=165889 &quot;IRIS Inventor, a 3D Graphics Toolkit&quot;]
* [http://www.visualizationlibrary.org/ Visualization Library]

&lt;!--Categories--&gt;
[[Category:Computer graphics data structures]]
[[Category:Graph data structures]]

&lt;!--Interwikies--&gt;</text>
      <sha1>0fjshtkguth51xukcztzg7rloba6inc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Doubly connected edge list</title>
    <ns>0</ns>
    <id>12015290</id>
    <revision>
      <id>610921970</id>
      <parentid>607274069</parentid>
      <timestamp>2014-05-31T12:39:38Z</timestamp>
      <contributor>
        <username>DVdm</username>
        <id>262666</id>
      </contributor>
      <minor/>
      <comment>/* Data structure */ Variable v was not declared.  Add possessive 's</comment>
      <text xml:space="preserve" bytes="2963">The '''doubly connected edge list''' ('''DCEL''') is a [[data structure]] to represent an [[Graph embedding|embedding]] of a [[planar graph]] in the [[plane (geometry)|plane]] and [[polytope]]s in [[three-dimensional space|3D]]. This data structure provides efficient manipulation of the topological information associated with the objects in question (vertices, edges, faces). It is used in many [[algorithm]]s of [[computational geometry]] to handle polygonal subdivisions of the plane, commonly called [[planar straight-line graph]]s (PSLG).&lt;ref&gt;The definition of a DCEL may be found in all major [[books in computational geometry]].&lt;/ref&gt; For example, a [[Voronoi diagram]] is commonly represented by a DCEL inside a bounding box. 

This data structure was originally suggested by Muller and Preparata&lt;ref&gt;Muller, D. E. ; Preparata, F. P.  &quot;Finding the Intersection of Two Convex Polyhedra&quot;, Tech. Rept. [[UIUC]], 1977, 38pp, also ''Theoretical Computer Science&quot;, Vol. 7, 1978, 217–236  &lt;/ref&gt; for representations of 3D [[convex polyhedra]]. 

Later a somewhat different data structuring was suggested, but the name &quot;DCEL&quot; was retained.

For simplicity, only [[connected graph]]s are considered, however the DCEL structure may be extended to handle disconnected graphs as well.

== Data structure ==
[[File:Dcel-halfedge-connectivity.svg|thumb|Each half-edge has exactly one previous half-edge, next half-edge and twin.]]
DCEL is more than just a [[doubly linked list]] of edges. In the general case, a DCEL contains a record for each edge, [[Vertex (graph theory)|vertex]] and [[Face (geometry)|face]] of the subdivision. Each record may contain additional information, for example, a face may contain the name of the area. Each edge usually bounds two faces and it is therefore convenient to regard each edge as two half-edges. Each half-edge bounds a single face and thus has a pointer to that face. A half-edge has a pointer to the next half-edge and previous half-edge of the same face. To reach the other face, we can go to the twin of the half-edge and then traverse the other face. Each half-edge also has a pointer to its origin vertex (the destination vertex can be obtained by querying the origin of its twin).

Each vertex contains the coordinates of the vertex and also stores a pointer to an arbitrary edge that has the vertex as its origin. Each face stores a pointer to some half-edge of its outer boundary (if the face is unbounded then pointer is null). It also has a list of half-edges, one for each hole that may be incident within the face. If the vertices or faces do not hold any interesting information, there is no need to store them, thus saving space and reducing the data structure's complexity.

==See also==
* [[Quad-edge data structure]]
* [[Doubly linked face list]]
* [[Winged edge]]

==References==
&lt;references/&gt;

[[Category:Graph data structures]]
[[Category:Geometric graph theory]]
[[Category:Geometric data structures]]</text>
      <sha1>hz74drtpoty6pjb4cifzzruw5cu9dy4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Fractional cascading</title>
    <ns>0</ns>
    <id>7543270</id>
    <revision>
      <id>600940345</id>
      <parentid>586830739</parentid>
      <timestamp>2014-03-23T22:00:21Z</timestamp>
      <contributor>
        <ip>188.134.40.82</ip>
      </contributor>
      <comment>8 -&gt; 8.0</comment>
      <text xml:space="preserve" bytes="24102">In [[computer science]], '''fractional cascading''' is a technique to speed up a sequence of [[binary search]]es for the same value in a sequence of related data structures. The first binary search in the sequence takes a logarithmic amount of time, as is standard for binary searches, but successive searches in the sequence are faster. The original version of fractional cascading, introduced in two papers by [[Bernard Chazelle|Chazelle]] and [[Leonidas J. Guibas|Guibas]] in 1986 ({{harvnb|Chazelle|Guibas|1986a}}; {{harvnb|Chazelle|Guibas|1986b}}), combined the idea of cascading, originating in [[range searching]] data structures of {{harvtxt|Lueker|1978}} and {{harvtxt|Willard|1978}}, with the idea of fractional sampling, which originated in {{harvtxt|Chazelle|1983}}. Later authors introduced more complex forms of fractional cascading that allow the data structure to be maintained as the data changes by a sequence of discrete insertion and deletion events.

==Example==
As a simple example of fractional cascading, consider the following problem. We are given as input a collection of ''k'' ordered lists ''L''&lt;sub&gt;''i''&lt;/sub&gt; of [[real number]]s, such that the total length Σ|''L''&lt;sub&gt;''i''&lt;/sub&gt;| of all lists is ''n'', and must process them so that we can perform binary searches for a query value ''q'' in each of the ''k'' lists. For instance, with ''k'' = 4 and ''n'' = 17,
:''L''&lt;sub&gt;1&lt;/sub&gt; = 2.4, 6.4, 6.5, 8.0, 9.3
:''L''&lt;sub&gt;2&lt;/sub&gt; = 2.3, 2.5, 2.6
:''L''&lt;sub&gt;3&lt;/sub&gt; = 1.3, 4.4, 6.2, 6.6
:''L''&lt;sub&gt;4&lt;/sub&gt; = 1.1, 3.5, 4.6, 7.9, 8.1
The simplest solution to this searching problem is just to store each list separately. If we do so, the space requirement is O(''n''), but the time to perform a query is O(''k'' log(''n''/''k'')), as we must perform a separate binary search in each of ''k'' lists.  The worst case for querying this structure occurs when each of the ''k'' lists has equal size ''n''/''k'', so each of the ''k'' binary searches involved in a query takes time O(log(''n''/''k'')).

A second solution allows faster queries at the expense of more space: we may merge all the ''k'' lists into a single big list ''L'', and associate
with each item ''x'' of ''L'' a list of the results of searching for ''x'' in each of the smaller lists ''L''&lt;sub&gt;''i''&lt;/sub&gt;. If we describe an element of this merged list as ''x''[''a'',''b'',''c'',''d''] where ''x'' is the numerical value and ''a'', ''b'', ''c'', and ''d'' are the positions (the first number has position 0) of the next element at least as large as ''x'' in each of the original input lists (or the position after the end of the list if no such element exists), then we would have
:''L'' = '''1.1'''[0,0,0,0], '''1.3'''[0,0,0,1], '''2.3'''[0,0,1,1], '''2.4'''[0,1,1,1], '''2.5'''[1,1,1,1], '''2.6'''[1,2,1,1],
::'''3.5'''[1,3,1,1], '''4.4'''[1,3,1,2], '''4.6'''[1,3,2,2], '''6.2'''[1,3,2,3], '''6.4'''[1,3,3,3], '''6.5'''[2,3,3,3],
::'''6.6'''[3,3,3,3], '''7.9'''[3,3,4,3], '''8.0'''[3,3,4,4], '''8.1'''[4,3,4,4], '''9.3'''[4,3,4,5]
This merged solution allows a query in time O(log ''n''&amp;nbsp;+&amp;nbsp;''k''): simply search for ''q'' in ''L'' and then report the results stored at the item ''x'' found by this search. For instance, if ''q'' = 5.0, searching for ''q'' in ''L'' finds the item 6.2[1,3,2,3], from which we return the results ''L''&lt;sub&gt;1&lt;/sub&gt;[1] = 6.4, ''L''&lt;sub&gt;2&lt;/sub&gt;[3] (a flag value indicating that ''q'' is past the end of ''L''&lt;sub&gt;2&lt;/sub&gt;), ''L''&lt;sub&gt;3&lt;/sub&gt;[2] = 6.2, and ''L''&lt;sub&gt;4&lt;/sub&gt;[3] = 7.9. However, this solution pays a high penalty in space complexity: it uses space O(''kn'') as each of the ''n'' items in ''L'' must store a list of ''k'' search results. 

Fractional cascading allows this same searching problem to be solved with time and space bounds meeting the best of both worlds: query time O(log ''n''&amp;nbsp;+&amp;nbsp;''k''), and space O(''n'').
The fractional cascading solution is to store a new sequence of lists ''M''&lt;sub&gt;''i''&lt;/sub&gt;. The final list in this sequence, ''M''&lt;sub&gt;''k''&lt;/sub&gt;, is equal to ''L''&lt;sub&gt;''k''&lt;/sub&gt;; each earlier list ''M''&lt;sub&gt;''i''&lt;/sub&gt; is formed by merging ''L''&lt;sub&gt;''i''&lt;/sub&gt; with every second item from ''M''&lt;sub&gt;''i''&amp;nbsp;+&amp;nbsp;1&lt;/sub&gt;. With each item ''x'' in this merged list, we store two numbers: the position resulting from searching for ''x'' in ''L''&lt;sub&gt;''i''&lt;/sub&gt; and the position resulting from searching for ''x'' in ''M''&lt;sub&gt;''i''&amp;nbsp;+&amp;nbsp;1&lt;/sub&gt;. For the data above, this would give us the following lists:
:''M''&lt;sub&gt;1&lt;/sub&gt; = '''2.4'''[0, 1], '''2.5'''[1, 1], '''3.5'''[1, 3], '''6.4'''[1, 5], '''6.5'''[2, 5], '''7.9'''[3, 5], '''8.0'''[3, 6], '''9.3'''[4, 6]
:''M''&lt;sub&gt;2&lt;/sub&gt; = '''2.3'''[0, 1], '''2.5'''[1, 1], '''2.6'''[2, 1], '''3.5'''[3, 1], '''6.2'''[3, 3], '''7.9'''[3, 5]
:''M''&lt;sub&gt;3&lt;/sub&gt; = '''1.3'''[0, 1], '''3.5'''[1, 1], '''4.4'''[1, 2], '''6.2'''[2, 3], '''6.6'''[3, 3], '''7.9'''[4, 3]  
:''M''&lt;sub&gt;4&lt;/sub&gt; = '''1.1'''[0, 0], '''3.5'''[1, 0], '''4.6'''[2, 0], '''7.9'''[3, 0], '''8.1'''[4, 0]
Suppose we wish to perform a query in this structure, for ''q'' = 5. We first do a standard binary search for ''q'' in ''M''&lt;sub&gt;1&lt;/sub&gt;, finding the value '''6.4'''[1,5]. The &quot;1&quot; in 6.4[1,5], tells us that the search for ''q'' in ''L''&lt;sub&gt;1&lt;/sub&gt; should return ''L''&lt;sub&gt;1&lt;/sub&gt;[1] = 6.4. The &quot;5&quot; in '''6.4'''[1,5] tells us that the approximate location of ''q'' in ''M''&lt;sub&gt;2&lt;/sub&gt; is position 5. More precisely, binary searching for ''q'' in ''M''&lt;sub&gt;2&lt;/sub&gt; would return either the value 7.9[3, 5] at position 5, or the value 6.2[3, 3] one place earlier. By comparing ''q'' to 6.2, and observing that it is smaller, we determine that the correct search result in ''M''&lt;sub&gt;2&lt;/sub&gt; is 6.2[3, 3]. The first &quot;3&quot; in 6.2[3, 3] tells us that the search for ''q'' in ''L''&lt;sub&gt;2&lt;/sub&gt; should return ''L''&lt;sub&gt;2&lt;/sub&gt;[3], a flag value meaning that ''q'' is past the end of list ''L''&lt;sub&gt;2&lt;/sub&gt;. The second &quot;3&quot; in 6.2[3, 3] tells us that the approximate location of ''q'' in ''M''&lt;sub&gt;3&lt;/sub&gt; is position 3. More precisely, binary searching for ''q'' in ''M''&lt;sub&gt;3&lt;/sub&gt; would return either the value 6.2[2, 3] at position 3, or the value 4.4[1, 2] one place earlier. A comparison of ''q'' with the smaller value 4.4 shows us that the correct search result in ''M''&lt;sub&gt;3&lt;/sub&gt; is 6.2[2,3]. The &quot;2&quot; in 6.2[2,3] tells us that the search for ''q'' in ''L''&lt;sub&gt;3&lt;/sub&gt; should return ''L''&lt;sub&gt;3&lt;/sub&gt;[2] = 6.2, and the &quot;3&quot; in 6.2[2,3] tells us that the result of searching for ''q'' in ''M''&lt;sub&gt;4&lt;/sub&gt; is either ''M''&lt;sub&gt;4&lt;/sub&gt;[3] = 7.9[3,0] or ''M''&lt;sub&gt;4&lt;/sub&gt;[2] = 4.6[2,0]; comparing ''q'' with 4.6 shows that the correct result is 7.9[3,0] and that the result of searching for ''q'' in ''L''&lt;sub&gt;4&lt;/sub&gt; is ''L''&lt;sub&gt;4&lt;/sub&gt;[3] = 7.9. Thus, we have found ''q'' in each of our four lists, by doing a binary search in the single list ''M''&lt;sub&gt;1&lt;/sub&gt; followed by a single comparison in each of the successive lists.

More generally, for any data structure of this type, we perform a query by doing a binary search for ''q'' in ''M''&lt;sub&gt;1&lt;/sub&gt;, and determining from the resulting value the position of ''q'' in ''L''&lt;sub&gt;1&lt;/sub&gt;. Then, for each ''i'' &gt; 1, we use the known position of ''q'' in ''M''&lt;sub&gt;''i''&lt;/sub&gt; to find its position in ''M''&lt;sub&gt;''i''&amp;nbsp;+&amp;nbsp;1&lt;/sub&gt;. The value associated with the position of ''q'' in ''M''&lt;sub&gt;''i''&lt;/sub&gt; points to a position in ''M''&lt;sub&gt;''i''&amp;nbsp;+&amp;nbsp;1&lt;/sub&gt; that is either the correct result of the binary search for ''q'' in ''M''&lt;sub&gt;''i''&amp;nbsp;+&amp;nbsp;1&lt;/sub&gt; or is a single step away from that correct result, so stepping from ''i'' to ''i''&amp;nbsp;+&amp;nbsp;1 requires only a single comparison. Thus, the total time for a query is O(log ''n''&amp;nbsp;+&amp;nbsp;''k'').

In our example, the fractionally cascaded lists have a total of 25 elements, less than twice that of the original input.
In general, the size of ''M''&lt;sub&gt;''i''&lt;/sub&gt; in this data structure is at most
:&lt;math&gt;|L_i|+\frac12|L_{i+1}|+\frac14|L_{i+2}|+\cdots+\frac1{2^j}|L_{i+j}|+\cdots,&lt;/math&gt;
as may easily be proven by induction. Therefore, the total size of the data structure is at most 
:&lt;math&gt;\sum|M_i|=\sum |L_i|(1+\frac12+\frac14+\cdots)\leq 2n=O(n),&lt;/math&gt;
as may be seen by regrouping the contributions to the total size coming from the same input list ''L&lt;sub&gt;i&lt;/sub&gt;'' together with each other.

==The general problem==

In general, fractional cascading begins with a ''catalog graph'', a [[directed graph]] in which each [[vertex (graph theory)|vertex]] is labeled with an ordered list. A query in this data structure consists of a [[path (graph theory)|path]] in the graph and a query value ''q''; the data structure must determine the position of ''q'' in each of the ordered lists associated with the vertices of the path. For the simple example above, the catalog graph is itself a path, with just four nodes. It is possible for later vertices in the path to be determined dynamically as part of a query, in response to the results found by the searches in earlier parts of the path.

To handle queries of this type, for a graph in which each vertex has at most ''d'' incoming and at most ''d'' outgoing edges for some constant ''d'', the lists associated with each vertex are augmented by a fraction of the items from each outgoing neighbor of the vertex; the fraction must be chosen to be smaller than 1/''d'', so that the total amount by which all lists are augmented remains linear in the input size. Each item in each augmented list stores with it the position of that item in the unaugmented list stored at the same vertex, and in each of the outgoing neighboring lists. In the simple example above, ''d'' = 1, and we augmented each list with a 1/2 fraction of the neighboring items.

A query in this data structure consists of a standard binary search in the augmented list associated with the first vertex of the query path, together with simpler searches at each successive vertex of the path. If a 1/''r'' fraction of items are used to augment the lists from each neighboring item, then each successive query result may be found within at most ''r'' steps of the position stored at the query result from the previous path vertex, and therefore may be found in constant time without having to perform a full binary search.

==Dynamic fractional cascading==

In ''dynamic fractional cascading'', the list stored at each node of the catalog graph may change dynamically, by a sequence of updates in which list items are inserted and deleted. This causes several difficulties for the data structure.

First, when an item is inserted or deleted at a node of the catalog graph, it must be placed within the augmented list associated with that node, and may cause changes to propagate to other nodes of the catalog graph. Instead of storing the augmented lists in arrays, they should be stored as binary search trees, so that these changes can be handled efficiently while still allowing binary searches of the augmented lists.

Second, an insertion or deletion may cause a change to the subset of the list associated with a node that is passed on to neighboring nodes of the catalog graph. It is no longer feasible, in the dynamic setting, for this subset to be chosen as the items at every ''d''th position of the list, for some ''d'', as this subset would change too drastically after every update. Rather, a technique closely related to [[B-tree]]s allows the selection of a fraction of data that is guaranteed to be smaller than 1/''d'', with the selected items guaranteed to be spaced a constant number of positions apart in the full list, and such that an insertion or deletion into the augmented list associated with a node causes changes to propagate to other nodes for a fraction of the operations that is less than 1/''d''. In this way, the distribution of the data among the nodes satisfies the properties needed for the query algorithm to be fast, while guaranteeing that the average number of binary search tree operations per data insertion or deletion is constant.

Third, and most critically, the static fractional cascading data structure maintains, for each element ''x'' of the augmented list at each node of the catalog graph, the index of the result that would be obtained when searching for ''x'' among the input items from that node and among the augmented lists stored at neighboring nodes. However, this information would be too expensive to maintain in the dynamic setting. Inserting or deleting a single value ''x'' could cause the indexes stored at an unbounded number of other values to change. Instead, dynamic versions of fractional cascading maintain several data structures for each node:
*A mapping of the items in the augmented list of the node to small integers, such that the ordering of the positions in the augmented list is equivalent to the comparison ordering of the integers, and a reverse map from these integers back to the list items. A technique of {{harvtxt|Dietz|1982}} allows this numbering to be maintained efficiently.
*An integer searching data structure such as a [[van Emde Boas tree]] for the numbers associated with the input list of the node. With this structure, and the mapping from items to integers, one can efficiently find for each element ''x'' of the augmented list, the item that would be found on searching for ''x'' in the input list.
*For each neighboring node in the catalog graph, a similar integer searching data structure for the numbers associated with the subset of the data propagated from the neighboring node. With this structure, and the mapping from items to integers, one can efficiently find for each element ''x'' of the augmented list, a position within a constant number of steps of the location of ''x'' in the augmented list associated with the neighboring node.

These data structures allow dynamic fractional cascading to be performed at a time of O(log&amp;nbsp;''n'') per insertion or deletion, and a sequence of ''k'' binary searches following a path of length ''k'' in the catalog graph to be performed in time O(log&amp;nbsp;''n''&amp;nbsp;+&amp;nbsp;''k''&amp;nbsp;log&amp;nbsp;log&amp;nbsp;''n'').

==Applications==
[[File:Convex layers halfspace.svg|thumb|200px|The convex layers of a point set, part of an efficient fractionally cascaded data structure for half-plane range reporting.]]
Typical applications of fractional cascading involve [[range search]] data structures in [[computational geometry]]. For example, consider the problem of ''half-plane range reporting'': that is, intersecting a fixed set of ''n'' points with a query [[half-plane]] and listing all the points in the intersection. The problem is to structure the points in such a way that a query of this type may be answered efficiently in terms of the intersection size ''h''. One structure that can be used for this purpose is the ''convex layers'' of the input point set, a family of nested [[convex polygon]]s consisting of the [[convex hull]] of the point set and the recursively-constructed convex layers of the remaining points. Within a single layer, the points inside the query half-plane may be found by performing a binary search for the half-plane boundary line's slope among the sorted sequence of convex polygon edge slopes, leading to the polygon vertex that is inside the query half-plane and farthest from its boundary, and then [[sequential search|sequentially searching]] along the polygon edges to find all other vertices inside the query half-plane. The whole half-plane range reporting problem may be solved by repeating this search procedure starting from the outermost layer and continuing inwards until reaching a layer that is disjoint from the query halfspace. Fractional cascading speeds up the successive binary searches among the sequences of polygon edge slopes in each layer, leading to a data structure for this problem with space O(''n'') and query time O(log&amp;nbsp;''n''&amp;nbsp;+&amp;nbsp;''h''). The data structure may be constructed in time O(''n''&amp;nbsp;log&amp;nbsp;''n'') by an algorithm of {{harvtxt|Chazelle|1985}}. As in our example, this application involves binary searches in a linear sequence of lists (the nested sequence of the convex layers), so the catalog graph is just a path.

Another application of fractional cascading in geometric data structures concerns [[point location]] in a monotone subdivision, that is, a partition of the plane into polygons such that any vertical line intersects any polygon in at most two points. As {{harvtxt|Edelsbrunner|Guibas|Stolfi|1986}} showed, this problem can be solved by finding a sequence of polygonal paths that stretch from left to right across the subdivision, and binary searching for the lowest of these paths that is above the query point. Testing whether the query point is above or below one of the paths can itself be solved as a binary search problem, searching for the x coordinate of the points among the x coordinates of the path vertices to determine which path edge might be above or below the query point. Thus, each point location query can be solved as an outer layer of binary search among the paths, each step of which itself performs a binary search among x coordinates of vertices. Fractional cascading can be used to speed up the time for the inner binary searches, reducing the total time per query to O(log&amp;nbsp;''n'') using a data structure with space O(''n''). In this application the catalog graph is a tree representing the possible search sequences of the outer binary search.

Beyond computational geometry, {{harvtxt|Lakshman|Stiliadis|1998}} and {{harvtxt|Buddhikot|Suri|Waldvogel|1999}} apply fractional cascading in the design of data structures for fast [[packet filter]]ing in [[internet router]]s. {{harvtxt|Gao|Guibas|Hershberger|Zhang|2004}} use fractional cascading as a model for data distribution and retrieval in [[sensor network]]s.

==References==
{{refbegin|colwidth=30em}}
*{{citation
 | first1 = Mikhail J. | last1 = Atallah
 | authorlink1 = Mikhail Atallah
 | first2 = Marina | last2 = Blanton
 | first3 = Michael T. | last3 = Goodrich | author3-link = Michael T. Goodrich
 | first4 = Stanislas | last4 = Polu
 | contribution = Discrepancy-sensitive dynamic fractional cascading, dominated maxima searching, and 2-d nearest neighbors in any Minkowski metric
 | title = [[Workshop on Algorithms and Data Structures|Algorithms and Data Structures, 10th International Workshop, WADS 2007]]
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | volume = 4619 | pages = 114–126 | year = 2007 | doi = 10.1007/978-3-540-73951-7_11
 | isbn = 978-3-540-73948-7}}.
*{{citation
 | first1 = Milind M. | last1 = Buddhikot
 | first2 = Subhash | last2 = Suri | author2-link = Subhash Suri
 | first3 = Marcel | last3 = Waldvogel
 | contribution = Space Decomposition Techniques for Fast Layer-4 Switching
 | title = Proceedings of the IFIP TC6 WG6.1 &amp; WG6.4 / IEEE ComSoc TC on on Gigabit Networking Sixth International Workshop on Protocols for High Speed Networks VI
 | year = 1999 | pages = 25–42
 | url = http://www1.bell-labs.com/user/mbuddhikot/psdocs/pfhsn99.pdf}}.
*{{citation
 | last1 = Chazelle | first1 = Bernard | authorlink1 = Bernard Chazelle
 | title = Filtering search: A new approach to query-answering
 | journal = Proc. 24 IEEE FOCS  | year = 1983 }}.
*{{citation
 | last1 = Chazelle | first1 = Bernard | authorlink1 = Bernard Chazelle
 | title = On the convex layers of a point set
 | journal = IEEE Transactions on Information Theory
 | volume = 31 | issue = 4 | year = 1985 | pages = 509–517 | url = http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1057060
 | doi = 10.1109/TIT.1985.1057060}}.
*{{citation
 | last1 = Chazelle | first1 = Bernard | authorlink1 = Bernard Chazelle
 | last2 = Guibas | first2 = Leonidas J. | authorlink2 = Leonidas J. Guibas
 | title = Fractional cascading: I. A data structuring technique
 | journal = Algorithmica
 | volume = 1 | issue = 1 | year = 1986 | pages = 133–162 | doi = 10.1007/BF01840440
 | url = http://www.cs.princeton.edu/~chazelle/pubs/FractionalCascading1.pdf
 | ref = {{harvid|Chazelle|Guibas|1986a}} }}.
*{{citation
 | last1 = Chazelle | first1 = Bernard | authorlink1 = Bernard Chazelle
 | last2 = Guibas | first2 = Leonidas J. | authorlink2 = Leonidas J. Guibas
 | title = Fractional cascading: II. Applications
 | journal = Algorithmica
 | volume = 1 | issue = 1 | year = 1986 | pages = 163–191 | doi = 10.1007/BF01840441
 | url = http://www.cs.princeton.edu/~chazelle/pubs/FractionalCascading2.pdf
 | ref = {{harvid|Chazelle|Guibas|1986b}} }}.
*{{citation
 | last1 = Chazelle | first1 = Bernard | authorlink1 = Bernard Chazelle
 | last2 = Liu | first2 = Ding
 | title = Lower bounds for intersection searching and fractional cascading in higher dimension
 | journal = Journal of Computer and System Sciences
 | volume = 68 | issue = 2 | year = 2004 | pages = 269–284 | doi = 10.1016/j.jcss.2003.07.003}}.
*{{citation
 | last = Dietz | first = F. Paul
 | contribution = Maintaining order in a linked list
 | title = 14th ACM Symp. Theory of Computing
 | year = 1982
 | pages = 122–127
 | doi = 10.1145/800070.802184
 | isbn = 0-89791-070-2}}.
*{{citation
 | last1 = Edelsbrunner | first1 = H. | authorlink1 = Herbert Edelsbrunner
 | last2 = Guibas | first2 = L. J.
 | last3 = Stolfi | first3 = J. | authorlink3 = Jorge Stolfi
 | title = Optimal point location in a monotone subdivision
 | journal = SIAM Journal on Computing
 | volume = 15 | pages = 317–340 | year = 1986
 | doi = 10.1137/0215023
 | issue = 2}}.
*{{citation
 | first1 = J. | last1 = Gao
 | first2 = L. J. |last2 = Guibas
 | first3 = J. | last3 = Hershberger
 | first4 = L. | last4 = Zhang
 | contribution = Fractionally cascaded information in a sensor network
 | title = Proc. of the 3rd International Symposium on Information Processing in Sensor Networks (IPSN'04)
 | pages = 311–319 | year = 2004 | doi = 10.1145/984622.984668
 | isbn = 1-58113-846-6}}.
*{{citation
 | last1 = JaJa | first1 = Joseph F.
 | last2 = Shi | first2 = Qingmin
 | title = Fast fractional cascading and its applications
 | publisher = Univ. of Maryland, Tech. Report UMIACS-TR-2003-71
 | url = http://www.umiacs.umd.edu/~joseph/ffc-and-apps-tr.pdf
 | year = 2003}}.
*{{citation
 | last1 = Lakshman | first1 = T. V.
 | last2 = Stiliadis | first2 = D.
 | contribution = High-speed policy-based packet forwarding using efficient multi-dimensional range matching
 | title = Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication
 | pages = 203–214 | year = 1998 | doi = 10.1145/285237.285283
 | isbn = 1-58113-003-1}}.
*{{citation
 | last = Lueker | first = George S.
 | contribution = A data structure for orthogonal range queries
 | doi = 10.1109/SFCS.1978.1
 | pages = 28–34
 | publisher = IEEE
 | title = Proc. 19th Symp. Foundations of Computer Science
 | year = 1978}}.
*{{citation
 | last1 = Mehlhorn | first1 = Kurt | authorlink1 = Kurt Mehlhorn
 | last2 = Näher | first2 = Stefan
 | title = Dynamic fractional cascading
 | journal = Algorithmica
 | volume = 5 | issue = 1 | year = 1990 | pages = 215–241 | doi = 10.1007/BF01840386}}.
*{{citation
 | last = Sen | first = S. D.
 | title = Fractional cascading revisited
 | journal = Journal of Algorithms
 | volume = 19 | issue = 2 | year = 1995 | pages = 161–172 | doi = 10.1006/jagm.1995.1032}}.
*{{citation
 | last = Willard | first = D. E. | authorlink = Dan Willard
 | publisher = Harvard University
 | series = Ph.D. thesis
 | title = Predicate-oriented database search algorithms
 | year = 1978}}.
*{{citation
 | last1 = Yap | first1 = Chee
 | last2 = Zhu | first2 = Yunyue
 | contribution = Yet another look at fractional cascading: B-graphs with application to point location
 | title = Proceedings of the 13th Canadian Conference on Computational Geometry (CCCG'01)
 | year = 2001
 | pages = 173–176
 | url = http://www.cccg.ca/proceedings/2001/yap-56333.ps.gz}}.
{{refend}}

[[Category:Graph data structures]]
[[Category:Geometric data structures]]
[[Category:Search algorithms]]</text>
      <sha1>ao8wb08wqbpictpvtw3o9d96lwzmvwl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Abstract semantic graph</title>
    <ns>0</ns>
    <id>1137033</id>
    <revision>
      <id>620461410</id>
      <parentid>616694880</parentid>
      <timestamp>2014-08-09T04:26:23Z</timestamp>
      <contributor>
        <username>PC-XT</username>
        <id>17821985</id>
      </contributor>
      <comment>/* top */ formatting: cite|coauthors=deprecated (using [[User:PC-XT/Advisor|Advisor.js]])</comment>
      <text xml:space="preserve" bytes="4638">{{semantics}}
In [[computer science]], an '''abstract semantic [[Graph (data structure)|graph]]''' ('''ASG''') or '''term graph''' is a form of [[abstract syntax]] in which an [[expression (computer science)|expression]] of a [[formal language|formal]] or [[programming language]] is represented by a [[graph (mathematics)|graph]] whose vertices are the expression's [[Term (mathematics)|subterm]]s. An ASG is at a higher [[abstraction (computer science)|level of abstraction]] than an [[abstract syntax tree]] (or AST), which is used to express the [[syntax|syntactic structure]] of an expression or [[program (computer science)|program]].

ASGs are more complex and concise than ASTs because they may contain shared subterms (also known as &quot;common subexpressions&quot;).&lt;ref name=Garner2011&gt;{{cite journal|last=Garner|first=Richard|title=An abstract view on syntax with sharing|publisher=[[Oxford University]] press|year=2011|doi=10.1093/logcom/exr021|quote=The notion of term graph encodes a reﬁnement of inductively generated syntax in which regard is paid to the sharing
and discard of subterms.}}&lt;/ref&gt; Abstract semantic graphs are often used as an [[intermediate representation]] by [[compilers]] to store the results of performing [[common subexpression elimination]] upon [[abstract syntax trees]]. ASTs are [[tree (computer science)|trees]] and are thus incapable of representing shared terms. ASGs are usually [[directed acyclic graph]]s however they may be [[cyclic graph|cyclic]], particularly in the field of [[graph rewriting]]. Cyclic graphs may represent [[recursive]] expressions which are commonly used to express [[iteration]] in [[functional programming language]]s without [[loop (computing)|loop]]ing constructs.

The nomenclature ''term graph'' is associated with the field of [[graph rewriting#Term graph rewriting|term graph rewriting]],&lt;ref name=Plump1999&gt;{{cite book|last=Plump D. (Hartmut Ehrig, G. Engels, Grzegorz Rozenberg, eds)|title=Handbook of Graph Grammars and Computing by Graph Transformation: applications, languages and tools. Vol. 2|year=1999|publisher=World Scientific|isbn=9789810228842|pages=9–13}}&lt;/ref&gt; which involves the transformation and processing of expressions by the specification of rewriting rules,&lt;ref name=Barendregt1987&gt;{{cite journal|last=Barendregt|first=H. P.|first2=M. C. J. D.|last2=van Eekelen|first3=J. R. W.|last3=Glauert|first4=J. R.|last4=Kennaway|first5=M. J.|last5=Plasmeijer|first6=M. R.|last6=Sleep|title=Term graph rewriting|journal=PARLE Parallel Architectures and Languages Europe (Lecture Notes in Computer Science)|year=1987|volume=259|pages=141–158|doi=10.1007/3-540-17945-3_8}}&lt;/ref&gt; whereas ''abstract semantic graph'' is used when discussing [[linguistics]], [[programming languages]], [[type systems]] and [[compiler|compilation]].

Abstract syntax trees are not capable of representing shared subexpressions due to their simplistic structure; this simplicity comes at a cost of efficiency due to redundant duplicate computations of identical terms. For this reason ASGs are often used as an [[intermediate language]] at a subsequent compilation stage to abstract syntax tree construction via parsing.

An abstract semantic graph is typically constructed from an abstract syntax tree by a process of enrichment and abstraction. The enrichment can for example be the addition of [[pointer (computer programming)|back-pointers]], [[edge (graph theory)|edges]] from an [[identifier]] node (where a [[Variable (programming)|variable]] is being used) to a node representing the [[declaration (computer science)|declaration]] of that variable. The abstraction can [[logical consequence|entail]] the removal of details which are relevant only in [[parsing]], not for semantics.

==See also==
*[[Ontology (computer science)]]
*[[Semantic Web]]
*[[Semantic Grid]]

==References==
{{reflist}}

==External links==
*Article &quot;[http://www.swag.uwaterloo.ca/cppx/old_cppx_site/ CPPX - C/C++ Fact Extractor]&quot; by [[Tom Dean]]{{dn|date=February 2014}}
*Paper &quot;[http://citeseer.ist.psu.edu/devanbu96generating.html Generating Testing and Analysis Tools with Aria]&quot; by [[Premkumar T. Devanbu]], [[David S. Rosenblum]] and [[Alexander L. Wolf]]
*Paper &quot;[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.6173&amp;rep=rep1&amp;type=pdf Towards Portable Source Code Representations Using XML]&quot; by [[Evan Mamas]] and [[Kostas Kontogiannis]]
*Paper &quot;[http://www.citeulike.org/user/hayashi/article/259537 Dex: a semantic-graph differencing tool for studying changes in large code bases]&quot;

[[Category:Graph data structures]]
[[Category:Formal languages]]


{{formalmethods-stub}}</text>
      <sha1>eiv2f7v1dukcbbs80s1zi9t580rxjp8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Binary decision diagram</title>
    <ns>0</ns>
    <id>576855</id>
    <revision>
      <id>626834498</id>
      <parentid>621558742</parentid>
      <timestamp>2014-09-24T00:23:25Z</timestamp>
      <contributor>
        <username>Mark viking</username>
        <id>17698045</id>
      </contributor>
      <comment>/* top */ Added wl</comment>
      <text xml:space="preserve" bytes="14413">In [[computer science]], a '''binary decision diagram''' ('''BDD''') or '''branching program''', like a [[negation normal form]] (NNF) or a [[propositional directed acyclic graph]] (PDAG), is a [[data structure]] that is used to represent a [[Boolean function]]. On a more abstract level, BDDs can be considered as a [[data compression|compressed]] representation of [[set (mathematics)|sets]] or [[relation (mathematics)|relation]]s. Unlike other compressed representations, operations are performed directly on the compressed representation, i.e. without decompression.

== Definition ==

A Boolean function can be represented as a rooted, directed, acyclic [[graph theory|graph]], which consists of several decision nodes and terminal nodes. There are two types of terminal nodes called 0-terminal and 1-terminal. Each decision node &lt;math&gt;N&lt;/math&gt; is labeled by  Boolean variable &lt;math&gt;V_N&lt;/math&gt; and has two [[child node]]s called low child and high child. The edge from node &lt;math&gt;V_N&lt;/math&gt; to a low (or high) child represents an assignment of &lt;math&gt;V_N&lt;/math&gt; to 0 (resp. 1).
Such a '''BDD''' is called 'ordered' if different variables appear in the same order on all paths from the root. A BDD is said to be 'reduced' if the following two rules have been applied to its graph:
* Merge any [[Graph isomorphism|isomorphic]] subgraphs.
* Eliminate any node whose two children are [[Graph isomorphism|isomorphic]].

In popular usage, the term '''BDD''' almost always refers to '''Reduced Ordered Binary Decision Diagram''' ('''ROBDD''' in the literature, used when the ordering and reduction aspects need to be emphasized). The advantage of an ROBDD is that it is canonical (unique) for a particular function and variable order.&lt;ref&gt;Graph-Based Algorithms
for Boolean Function Manipulation, Randal E. Bryant, 1986&lt;/ref&gt; This property makes it useful in [[functional equivalence]] checking and other operations like functional technology mapping.

A path from the root node to the 1-terminal represents a (possibly partial) variable assignment for which the represented Boolean function is true. As the path descends to a low (or high) child from a node, then that node's variable is assigned to 0 (resp. 1).

=== Example ===
The left figure below shows a binary [[decision tree|decision ''tree'']] (the reduction rules are not applied), and a [[truth table]], each representing the function f (x1, x2, x3).  In the tree on the left, the value of the function can be determined for a given variable assignment by following a path down the graph to a terminal. In the figures below, dotted lines represent edges to a low child, while solid lines represent edges to a high child. Therefore, to find (x1=0, x2=1, x3=1), begin at x1, traverse down the dotted line to x2 (since x1 has an assignment to 0), then down two solid lines (since x2 and x3 each have an assignment to one).  This leads to the terminal 1, which is the value of f (x1=0, x2=1, x3=1).

The binary decision ''tree'' of the left figure can be transformed into a binary decision ''diagram'' by maximally reducing it according to the two reduction rules. The resulting '''BDD''' is shown in the right figure.

{| align=&quot;center&quot;
|-
| [[File:BDD.png|thumb|546px|Binary decision tree and truth table for the function &lt;math&gt;f(x_1, x_2, x_3)=\bar{x}_1 \bar{x}_2 \bar{x}_3 + x_1 x_2 + x_2 x_3&lt;/math&gt;]]
| [[File:BDD simple.svg|thumb|189px|BDD for the function f]]
|}

== History ==
The basic idea from which the data structure was created is the [[Shannon expansion]]. A [[switching function]] is split into two sub-functions (cofactors) by assigning one variable (cf. ''if-then-else normal form'').  If such a sub-function is considered as a sub-tree, it can be represented by a ''binary decision tree''. Binary decision diagrams (BDD) were introduced by Lee,&lt;ref name=&quot;Lee&quot;&gt;C. Y. Lee. &quot;Representation of Switching Circuits by Binary-Decision Programs&quot;. Bell Systems Technical Journal, 38:985–999, 1959.&lt;/ref&gt; and further studied and made known by Akers&lt;ref name=&quot;Akers&quot;&gt;Sheldon B. Akers. Binary Decision Diagrams, IEEE Transactions on Computers, C-27(6):509–516, June 1978.&lt;/ref&gt; and Boute.&lt;ref&gt;Raymond T. Boute, &quot;The Binary Decision Machine as a programmable controller&quot;. [[EUROMICRO]] Newsletter, Vol. 1(2):16–22, January 1976.&lt;/ref&gt;

The full potential for efficient algorithms based on the data structure was investigated by [[Randal Bryant]] at [[Carnegie Mellon University]]: his key extensions were to use a fixed variable ordering (for canonical representation) and shared sub-graphs (for compression).  Applying these two concepts results in an efficient data structure and algorithms for the representation of sets and relations.&lt;ref name=&quot;Bryant-1986&quot;&gt;Randal E. Bryant. &quot;[http://www.cs.cmu.edu/~bryant/pubdir/ieeetc86.ps Graph-Based Algorithms for Boolean Function Manipulation]&quot;. IEEE Transactions on Computers, C-35(8):677–691, 1986.&lt;/ref&gt;&lt;ref name=&quot;Bryant-1992&quot;&gt;R. E. Bryant, &quot;[http://www.cs.cmu.edu/~bryant/pubdir/acmcs92.ps Symbolic Boolean Manipulation with Ordered Binary Decision Diagrams&quot;], ACM Computing Surveys, Vol. 24, No. 3 (September, 1992), pp. 293–318.
&lt;/ref&gt;  By extending the sharing to several BDDs, i.e. one sub-graph is used by several BDDs, the data structure ''Shared Reduced Ordered Binary Decision Diagram'' is defined.&lt;ref name=&quot;Brace&quot;&gt;Karl S. Brace, Richard L. Rudell and Randal E. Bryant. &quot;[http://portal.acm.org/citation.cfm?id=123222&amp;coll=portal&amp;dl=ACM Efficient Implementation of a BDD Package&quot;]. In Proceedings of the 27th ACM/IEEE Design Automation Conference (DAC 1990), pages 40–45. IEEE Computer Society Press, 1990.&lt;/ref&gt;  The notion of a BDD is now generally used to refer to that particular data structure.

In his video lecture ''Fun With Binary Decision Diagrams (BDDs)'',&lt;ref&gt;http://scpd.stanford.edu/knuth/index.jsp&lt;/ref&gt; [[Donald Knuth]] calls BDDs &quot;one of the only really fundamental data structures that came out in the last twenty-five years&quot; and mentions that Bryant's 1986 paper was for some time one of the most-cited papers in computer science.

== Applications ==

BDDs are extensively used in [[Computer Aided Design|CAD]] software to synthesize circuits ([[logic synthesis]]) and in [[formal verification]]. There are several lesser known applications of BDD, including [[fault tree]] analysis, [[Bayesian probability|Bayesian]] reasoning, product configuration, and [[private information retrieval]] &lt;ref name=&quot;Jensen&quot;&gt;R.M. Jensen. [http://www.cs.cmu.edu/~runej/data/papers/JSW04.pdf &quot;CLab: A C+ + library for fast backtrack-free interactive product configuration&quot;]. Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming, 2004.&lt;/ref&gt;
&lt;ref name=&quot;Lipmaa&quot;&gt;H.L. Lipmaa. [http://eprint.iacr.org/2009/395.pdf &quot;First CPIR Protocol with Data-Dependent Computation&quot;]. ICISC 2009.&lt;/ref&gt;{{Citation needed|reason=Please provide examples of these applications in the literature.|date=June 2010}}.

Every arbitrary BDD (even if it is not reduced or ordered) can be directly implemented by replacing each node with a 2 to 1 [[Multiplexer#Digital multiplexers|multiplexer]]; each multiplexer can be directly implemented by a 4-LUT in a [[FPGA]]. It is not so simple to convert from an arbitrary network of logic gates to a BDD{{Citation needed|date=March 2008}} (unlike the [[and-inverter graph]]).

== Variable ordering ==
The size of the BDD is determined both by the function being represented and the chosen ordering of the variables. There exist Boolean functions &lt;math&gt;f(x_1,\ldots, x_{n})&lt;/math&gt; for which depending upon the ordering of the variables we would end up getting a graph whose number of nodes would be linear (in&amp;nbsp;''n'') at the best and exponential at the worst case (e.g., a ripple carry adder).  Let us consider the Boolean function &lt;math&gt;f(x_1,\ldots, x_{2n}) = x_1x_2 + x_3x_4 + \cdots + x_{2n-1}x_{2n}.&lt;/math&gt;
Using the variable ordering &lt;math&gt;x_1 &lt; x_3 &lt; \cdots &lt; x_{2n-1} &lt; x_2 &lt; x_4 &lt; \cdots &lt; x_{2n}&lt;/math&gt;, the BDD needs 2&lt;sup&gt;''n''+1&lt;/sup&gt; nodes to represent the function.  Using the ordering &lt;math&gt;x_1 &lt; x_2 &lt; x_3 &lt; x_4 &lt; \cdots &lt; x_{2n-1} &lt; x_{2n}&lt;/math&gt;, the BDD consists of 2''n''&amp;nbsp;+&amp;nbsp;2 nodes.

{| align=&quot;center&quot;
|-
| [[File:BDD Variable Ordering Bad.svg|thumb|638px|BDD for the function ''&amp;fnof;''(''x''&lt;sub&gt;1&lt;/sub&gt;, ..., ''x''&lt;sub&gt;8&lt;/sub&gt;) = ''x''&lt;sub&gt;1&lt;/sub&gt;''x''&lt;sub&gt;2&lt;/sub&gt; + ''x''&lt;sub&gt;3&lt;/sub&gt;''x''&lt;sub&gt;4&lt;/sub&gt; + ''x''&lt;sub&gt;5&lt;/sub&gt;''x''&lt;sub&gt;6&lt;/sub&gt; + ''x''&lt;sub&gt;7&lt;/sub&gt;''x''&lt;sub&gt;8&lt;/sub&gt; using bad variable ordering]]
| [[File:BDD Variable Ordering Good.svg|thumb|156px|Good variable ordering]]
|}

It is of crucial importance to care about variable ordering when applying this data structure in practice.
The problem of finding the best variable ordering is [[NP-hard]].&lt;ref name=&quot;Bollig&quot;&gt;Beate Bollig, Ingo Wegener. {{doi-inline|10.1109/12.537122|Improving the Variable Ordering of OBDDs Is NP-Complete}}, IEEE Transactions on Computers, 45(9):993–1002, September 1996.
&lt;/ref&gt; For any constant ''c''&amp;nbsp;&gt;&amp;nbsp;1 it is even NP-hard to compute a variable ordering resulting in an OBDD with a size that is at most c times larger than an optimal one.&lt;ref name=&quot;Sieling&quot;&gt;Detlef Sieling. &quot;The nonapproximability of OBDD minimization.&quot; Information and Computation 172, 103–138. 2002.
&lt;/ref&gt; However there exist efficient heuristics to tackle the problem.&lt;ref&gt;{{cite web|last=Rice|first=Michael|title=A Survey of Static Variable Ordering Heuristics for Eﬃcient BDD/MDD Construction|url=http://alumni.cs.ucr.edu/~skulhari/StaticHeuristics.pdf}}&lt;/ref&gt;

There are functions for which the graph size is always exponential — independent of variable ordering. This holds e. g. for the multiplication function (an indication{{Citation needed|date=March 2007}} as to the apparent complexity of [[factorization]] ).

Researchers have of late suggested refinements on the BDD data structure giving way to a number of related graphs, such as BMD ([[binary moment diagram]]s), ZDD ([[zero-suppressed decision diagram]]), FDD ([[free binary decision diagram]]s), PDD ([[parity decision diagram]]s), and MTBDDs (multiple terminal BDDs).

== Logical operations on BDDs ==
Many logical operations on BDDs can be implemented by
polynomial-time graph manipulation algorithms.
* [[logical conjunction|conjunction]]
* [[logical disjunction|disjunction]]
* [[negation]]
* existential abstraction
* universal abstraction
However, repeating these operations several times, for example forming the conjunction or disjunction of a set of BDDs, may in the worst case result in an exponentially big BDD. This is because any of the preceding operations for two BDDs may result in a BDD with a size proportional to the product of the BDDs' sizes, and consequently for several BDDs the size may be exponential.

== See also ==
* [[Boolean satisfiability problem]]
* [[L/poly]], a [[complexity class]] that captures the complexity of problems with polynomially sized BDDs
* [[Model checking]]
* [[Radix tree]]
* [[Binary key]] – a method of species identification in biology using binary trees
* [[NC (complexity)#Barrington's theorem|Barrington's theorem]]

== References ==
&lt;references/&gt;
* R. Ubar, &quot;Test Generation for Digital Circuits Using Alternative Graphs (in Russian)&quot;, in Proc. Tallinn Technical University, 1976, No.409, Tallinn Technical University, Tallinn, Estonia, pp.&amp;nbsp;75–81.

== Further reading ==
* D. E. Knuth, &quot;The Art of Computer Programming Volume 4, Fascicle 1: Bitwise tricks &amp; techniques;  Binary Decision Diagrams&quot; (Addison–Wesley Professional, March 27, 2009) viii+260pp, ISBN 0-321-58050-8. [http://www-cs-faculty.stanford.edu/~knuth/fasc1b.ps.gz Draft of Fascicle 1b] available for download.
* H. R. Andersen &quot;[http://configit.com/configit_wordpress/wp-content/uploads/2013/07/bdd-eap.pdf An Introduction to Binary Decision Diagrams,]&quot; Lecture Notes, 1999, IT University of Copenhagen.
* Ch. Meinel, T. Theobald, &quot;[http://www.hpi.uni-potsdam.de/fileadmin/hpi/FG_ITS/books/OBDD-Book.pdf Algorithms and Data Structures in VLSI-Design: OBDD – Foundations and Applications&quot;], Springer-Verlag, Berlin, Heidelberg, New York, 1998. Complete textbook available for download.
* {{cite book|author1=Rüdiger Ebendt|author2=Görschwin Fey|author3=Rolf Drechsler|title=Advanced BDD optimization|year=2005|publisher=Springer|isbn=978-0-387-25453-1}}
* {{cite book|author1=Bernd Becker|author2=Rolf Drechsler|title=Binary Decision Diagrams: Theory and Implementation|year=1998|publisher=Springer|isbn=978-1-4419-5047-5}}

== External links ==
{{Commons category|Binary decision diagrams}}
* [http://myvideos.stanford.edu/player/slplayer.aspx?coll=ea60314a-53b3-4be2-8552-dcf190ca0c0b&amp;co=18bcd3a8-965a-4a63-a516-a1ad74af1119&amp;o=true Fun With Binary Decision Diagrams (BDDs)], lecture by [[Donald Knuth]]
'''Available OBDD packages'''
* [http://fmv.jku.at/abcd/ ABCD]: The ABCD package by Armin Biere, Johannes Kepler Universität, Linz.
* [http://www-2.cs.cmu.edu/~modelcheck/bdd.html CMU BDD], BDD package, Carnegie Mellon University, Pittsburgh
* [http://buddy.sourceforge.net/manual/ BuDDy]: A BDD package by Jørn Lind-Nielsen
* [http://biddy.meolic.com/ Biddy]: Academic multiplatform BDD package, University of Maribor
* [http://vlsi.colorado.edu/~fabio/CUDD/ CUDD]: BDD package, University of Colorado, Boulder
* [http://javabdd.sourceforge.net JavaBDD], a Java port of BuDDy that also interfaces to CUDD, CAL, and JDD
* [http://javaddlib.sourceforge.net/jdd/ JDD] is a pure java implementation of BDD and ZBDD. [http://javaddlib.sourceforge.net/jbdd/ JBDD] by the same author has a similar API but is a Java interface to BuDDy and CUDD
* The Berkeley [http://embedded.eecs.berkeley.edu/Research/cal_bdd/ CAL] package which does breadth-first manipulation
* [http://ddd.lip6.fr DDD]: A C++ library with support for integer valued and hierarchical decision diagrams.
* [http://www.jossowski.de/projects/jinc/jinc.html JINC]: A C++ library developed at University of Bonn, Germany, supporting several BDD variants and multi-threading.
* [https://github.com/cjdrake PyEDA] [http://pyeda.readthedocs.org/en/latest/bdd.html BDD] module: A Python implementation, by Chris Drake

{{Data structures}}

{{DEFAULTSORT:Binary Decision Diagram}}
[[Category:Diagrams]]
[[Category:Graph data structures]]
[[Category:Model checking]]
[[Category:Articles with example code]]
[[Category:Boolean algebra]]</text>
      <sha1>3e03cgbc0ed9h6xzlhfngczsr6vfuss</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>SPQR tree</title>
    <ns>0</ns>
    <id>11220797</id>
    <revision>
      <id>616019689</id>
      <parentid>616018991</parentid>
      <timestamp>2014-07-08T00:20:50Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>/* Structure */ describe different usages for the Q node case rather than insisting that one way is the right way</comment>
      <text xml:space="preserve" bytes="13103">[[File:SPQR tree 2.svg|thumb|360px|A graph and its SPQR tree. The dashed black lines connect pairs of virtual edges, shown as black; the remaining edges are colored according to the triconnected component they belong to.]]
In [[graph theory]], a branch of mathematics, the '''triconnected components''' of a [[biconnected graph]] are a system of smaller graphs that describe all of the 2-vertex cuts in the graph. An '''SPQR tree''' is a  [[tree data structure]] used in [[computer science]], and more specifically [[graph algorithm]]s, to represent the triconnected components of a graph. The SPQR tree of a graph may be constructed in [[linear time]]&lt;ref name=&quot;linear-construction&quot;&gt;{{harvtxt|Hopcroft|Tarjan|1973}}; {{harvtxt|Gutwenger|Mutzel|2001}}.&lt;/ref&gt; and has several applications in [[dynamic graph algorithm]]s and [[graph drawing]].

The basic structures underlying the SPQR tree, the triconnected components of a graph, and the connection between this decomposition and the planar embeddings of a [[planar graph]], were first investigated by {{harvs|authorlink=Saunders Mac Lane|first=Saunders|last=Mac Lane|year=1937|txt}}; these structures were used in efficient algorithms by several other researchers&lt;ref&gt;E.g., {{harvtxt|Hopcroft|Tarjan|1973}} and {{harvtxt|Bienstock|Monma|1988}}, both of which are cited as precedents by Di Battista and Tamassia.&lt;/ref&gt; prior to their formalization as the SPQR tree by {{harvs|last1=Di Battista|last2=Tamassia|year=1989|year2=1990|year3=1996|txt}}.

==Structure==
An SPQR tree takes the form of an [[Tree (graph theory)|unrooted tree]] in which for each node ''x'' there is associated an [[undirected graph]] or [[multigraph]] ''G''&lt;sub&gt;''x''&lt;/sub&gt;. The node, and the graph associated with it, may have one of four types, given the initials SPQR:
*In an S node, the associated graph is a [[cycle graph]] with three or more vertices and edges. This case is analogous to series composition in [[series-parallel graph]]s; the S stands for &quot;series&quot;.&lt;ref name=&quot;dbt89&quot;&gt;{{harvtxt|Di Battista|Tamassia|1989}}.&lt;/ref&gt;
*In a P node, the associated graph is a [[dipole graph]], a multigraph with two vertices and three or more edges, the [[dual graph|planar dual]] to a cycle graph. This case is analogous to parallel composition in [[series-parallel graph]]s; the P stands for &quot;parallel&quot;.&lt;ref name=&quot;dbt89&quot;/&gt;
*In a Q node, the associated graph has a single real edge. This trivial case is necessary to handle the graph that has only one edge. In some works on SPQR trees, this type of node does not appear in the SPQR trees of graphs with more than one edge; in other works, all non-virtual edges are required to be represented by Q nodes with one real and one virtual edge, and the edges in the other node types must all be virtual.
*In an R node, the associated graph is a 3-connected graph that is not a cycle or dipole. The R stands for &quot;rigid&quot;: in the application of SPQR trees in planar graph embedding, the associated graph of an R node has a unique planar embedding.&lt;ref name=&quot;dbt89&quot;/&gt;
Each edge ''xy'' between two nodes of the SPQR tree is associated with two directed ''virtual edges'', one of which is an edge in ''G&lt;sub&gt;x&lt;/sub&gt;'' and the other of which is an edge in ''G&lt;sub&gt;y&lt;/sub&gt;''. Each edge in a graph ''G&lt;sub&gt;x&lt;/sub&gt;'' may be a virtual edge for at most one SPQR tree edge.

An SPQR tree ''T'' represents a 2-connected graph ''G&lt;sub&gt;T&lt;/sub&gt;'', formed as follows. Whenever SPQR tree edge ''xy'' associates the virtual edge ''ab'' of ''G&lt;sub&gt;x&lt;/sub&gt;'' with the virtual edge ''cd'' of ''G&lt;sub&gt;y&lt;/sub&gt;'', form a single larger graph by merging ''a'' and ''c'' into a single supervertex, merging ''b'' and ''d'' into another single supervertex, and deleting the two virtual edges. That is, the larger graph is the [[clique-sum|2-clique-sum]] of ''G&lt;sub&gt;x&lt;/sub&gt;'' and ''G&lt;sub&gt;y&lt;/sub&gt;''. Performing this gluing step on each edge of the SPQR tree produces the graph ''G&lt;sub&gt;T&lt;/sub&gt;''; the order of performing the gluing steps does not affect the result. Each vertex in one of the graphs ''G&lt;sub&gt;x&lt;/sub&gt;'' may be associated in this way with a unique vertex in ''G&lt;sub&gt;T&lt;/sub&gt;'', the supervertex into which it was merged.

Typically, it is not allowed within an SPQR tree for two S nodes to be adjacent, nor for two P nodes to be adjacent, because if such an adjacency occurred the two nodes could be merged into a single larger node. With this assumption, the SPQR tree is uniquely determined from its graph. When a graph ''G'' is represented by an SPQR tree with no adjacent P nodes and no adjacent S nodes, then the graphs ''G''&lt;sub&gt;''x''&lt;/sub&gt; associated with the nodes of the SPQR tree are known as the triconnected components of ''G''.

==Construction==
The SPQR tree of a given 2-vertex-connected graph can be constructed in [[linear time]].&lt;ref name=&quot;linear-construction&quot;/&gt;

The problem of constructing the triconnected components of a graph was first solved in linear time by {{harvtxt|Hopcroft|Tarjan|1973}}. Based on this algorithm, {{harvtxt|Di Battista|Tamassia|1996}} suggested that the full SPQR tree structure, and not just the list of components, should be constructible in linear time. After an implementation of a slower algorithm for SPQR trees was provided as part of the GDToolkit library, {{harvtxt|Gutwenger|Mutzel|2001}} provided the first linear-time implementation. As part of this process of implementing this algorithm, they also corrected some errors in the earlier work of {{harvtxt|Hopcroft|Tarjan|1973}}.

The algorithm of {{harvtxt|Gutwenger|Mutzel|2001}} includes the following overall steps.
#Sort the edges of the graph by the pairs of numerical indices of their endpoints, using a variant of [[radix sort]] that makes two passes of [[bucket sort]], one for each endpoint. After this sorting step, parallel edges between the same two vertices will be adjacent to each other in the sorted list and can be split off into a P-node of the eventual SPQR tree, leaving the remaining graph simple.
#Partition the graph into split components; these are graphs that can be formed by finding a pair of separating vertices, splitting the graph at these two vertices into two smaller graphs (with a linked pair of virtual edges having the separating vertices as endpoints), and repeating this splitting process until no more separating pairs exist. The partition found in this way is not uniquely defined, because the parts of the graph that should become S-nodes of the SPQR tree will be subdivided into multiple triangles.
#Label each split component with a P (a two-vertex split component with multiple edges), an S (a split component in the form of a triangle), or an R (any other split component). While there exist two split components that share a linked pair of virtual edges, and both components have type S or both have type P, merge them into a single larger component of the same time.

To find the split components, {{harvtxt|Gutwenger|Mutzel|2001}} use [[depth-first search]] to find a structure that they call a palm tree; this is a [[trémaux tree|depth-first search tree]] with its edges [[orientation (graph theory)|oriented]] away from the root of the tree, for the edges belonging to the tree, and towards the root for all other edges. They then find a special [[preorder]] numbering of the nodes in the tree, and use certain patterns in this numbering to identify pairs of vertices that can separate the graph into smaller components. When a component is found in this way, a [[stack (abstract data type)|stack data structure]] is used to identify the edges that should be part of the new component.

==Usage==
===Finding 2-vertex cuts===
With the SPQR tree of a graph ''G'' (without Q nodes) it is straightforward to find every pair of vertices ''u'' and ''v'' in ''G'' such that removing ''u'' and ''v'' from ''G'' leaves a disconnected graph, and the connected components of the remaining graphs:
*The two vertices ''u'' and ''v'' may be the two endpoints of a virtual edge in the graph associated with an R node, in which case the two components are represented by the two subtrees of the SPQR tree formed by removing the corresponding SPQR tree edge.
*The two vertices ''u'' and ''v'' may be the two vertices in the graph associated with a P node that has two or more virtual edges. In this case the components formed by the removal of ''u'' and ''v'' are the represented by subtrees of the SPQR tree, one for each virtual edge in the node.
*The two vertices ''u'' and ''v'' may be two vertices in the graph associated with an S node such that either ''u'' and ''v'' are not adjacent, or the edge ''uv'' is virtual. If the edge is virtual, then the pair (''u'',''v'') also belongs to a node of type P and R and the components are as described above. If the two vertices are not adjacent then the two components are represented by two paths of the cycle graph associated with the S node and with the SPQR tree nodes attached to those two paths.

===Representing all embeddings of planar graphs===
If a planar graph is 3-connected, it has a unique planar embedding up to the choice of which face is the outer face and of orientation of the embedding: the faces of the embedding are exactly the nonseparating cycles of the graph. However, for a planar graph (with labeled vertices and edges) that is 2-connected but not 3-connected, there may be greater freedom in finding a planar embedding. Specifically, whenever two nodes in the SPQR tree of the graph are connected by a pair of virtual edges, it is possible to flip the orientation of one of the nodes relative to the other one. Additionally, in a P node of the SPQR tree, the different parts of the graph connected to virtual edges of the P node may be arbitrarily [[permutation|permuted]]. All planar representations may be described in this way.{{sfnp|Mac Lane|1937}}

==See also==
*[[Gomory–Hu tree]], a different tree structure that characterizes the edge connectivity of a graph
* [[Tree decomposition]], a generalization (no longer unique) to larger cuts

==Notes==
{{reflist}}

==References==
*{{citation
 | first1 = Daniel | last1 = Bienstock | first2 = Clyde L. | last2 = Monma
 | title = On the complexity of covering vertices by faces in a planar graph
 | journal = SIAM Journal on Computing
 | volume = 17 | issue = 1 | pages = 53–76 | year = 1988 | doi = 10.1137/0217004}}.
*{{citation
 | last1 = Di Battista | first1 = Giuseppe
 | last2 = Tamassia | first2 = Roberto | author2-link = Roberto Tamassia
 | year = 1989
 | contribution = Incremental planarity testing
 | title = [[Symposium on Foundations of Computer Science|Proc. 30th Annual Symposium on Foundations of Computer Science]]
 | pages = 436–441
 | doi = 10.1109/SFCS.1989.63515}}.
*{{citation
 | last1 = Di Battista | first1 = Giuseppe
 | last2 = Tamassia | first2 = Roberto | author2-link = Roberto Tamassia
 | year = 1990
 | contribution = On-line graph algorithms with SPQR-trees
 | title = [[International Colloquium on Automata, Languages and Programming|Proc. 17th International Colloquium on Automata, Languages and Programming]]
 | series = Lecture Notes in Computer Science
 | publisher = Springer-Verlag
 | volume = 443
 | pages = 598–611
 | doi = 10.1007/BFb0032061}}.
*{{citation
 | last1 = Di Battista | first1 = Giuseppe
 | last2 = Tamassia | first2 = Roberto | author2-link = Roberto Tamassia
 | year = 1996
 | title = On-line planarity testing
 | journal = SIAM Journal on Computing
 | volume = 25 | issue = 5 | pages = 956–997 | doi = 10.1137/S0097539794280736
 | url = http://cs.brown.edu/research/pubs/pdfs/1996/DiBattista-1996-OPT.pdf}}.
*{{citation
 | last1 = Gutwenger | first1 = Carsten
 | last2 = Mutzel | first2 = Petra | author2-link = Petra Mutzel
 | contribution = A linear time implementation of SPQR-trees
 | title = [[International Symposium on Graph Drawing|Proc. 8th International Symposium on Graph Drawing (GD 2000)]]
 | year = 2001
 | series = Lecture Notes in Computer Science
 | publisher = Springer-Verlag
 | volume = 1984
 | pages = 77–90
 | doi = 10.1007/3-540-44541-2_8}}.
*{{citation
 | last1 = Hopcroft | first1 = John | author1-link = John Hopcroft
 | last2 = Tarjan | first2 = Robert | author2-link = Robert Tarjan
 | title = Dividing a graph into triconnected components
 | journal = SIAM Journal on Computing
 | volume = 2 | issue = 3 | pages = 135–158 | year = 1973 | doi = 10.1137/0202012}}.
*{{citation
 | last = Mac Lane | first = Saunders | authorlink = Saunders Mac Lane
 | title = A structural characterization of planar combinatorial graphs
 | journal = Duke Mathematical Journal
 | volume = 3 | issue = 3 | year = 1937 | pages = 460–472
 | doi = 10.1215/S0012-7094-37-00336-3}}.

== External links ==
* [http://www.ogdf.net/doc-ogdf/classogdf_1_1_s_p_q_r_tree.html SPQR tree implementation] in the Open Graph Drawing Framework.
* [http://code.google.com/p/jbpt/ The tree of the triconnected components Java implementation] in the jBPT library (see TCTree class).

{{CS-Trees}}

{{DEFAULTSORT:Spqr Tree}}
[[Category:Trees (data structures)]]
[[Category:Graph connectivity]]
[[Category:Graph data structures]]</text>
      <sha1>lgc3la38ipw18f6l8hu7m1d9jlwa765</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Implicit graph</title>
    <ns>0</ns>
    <id>24109545</id>
    <revision>
      <id>607161263</id>
      <parentid>585232203</parentid>
      <timestamp>2014-05-05T12:11:41Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* Neighborhood representations */Added 1 dois to journal cites using [[Project:AWB|AWB]] (10094)</comment>
      <text xml:space="preserve" bytes="19255">In the study of [[graph algorithm]]s, an '''implicit graph representation''' (or more simply '''implicit graph''') is a [[graph (mathematics)|graph]] whose vertices or edges are not represented as explicit objects in a computer's memory, but rather are determined [[algorithm]]ically from some more concise input.

==Neighborhood representations==
The notion of an implicit graph is common in various [[search algorithm]]s which are described in terms of graphs. In this context, an implicit graph may be  defined as a set of rules to define all [[Neighborhood (graph theory)|neighbors]] for any specified vertex.&lt;ref&gt;{{citation
 | last = Korf | first = Richard E.
 | at = Article 26, 40pp
 | doi = 10.1145/1455248.1455250
 | issue = 6
 | journal = [[Journal of the ACM]]
 | mr = 2477486
 | title = Linear-time disk-based implicit graph search
 | volume = 55
 | year = 2008}}.&lt;/ref&gt; This type of implicit graph representation is analogous to an [[adjacency list]], in that it provides easy access to the neighbors of each vertex. For instance, in searching for a solution to a puzzle such as [[Rubik's Cube]], one may define an implicit graph in which each vertex represents one of the possible states of the cube, and each edge represents a move from one state to another. It is straightforward to generate the neighbors of any vertex by trying all possible moves in the puzzle and determining the states reached by each of these moves; however, an implicit representation is necessary, as the state space of Rubik's Cube is too large to allow an algorithm to list all of its states.&lt;ref&gt;{{citation|last=Korf|first=Richard E.|contribution=Minimizing disk I/O in two-bit breadth-first search|title=Proc. 23rd AAAI Conf. on Artificial Intelligence|year=2008|url=http://www.aaai.org/Papers/AAAI/2008/AAAI08-050.pdf|pages=317–324|quotation=The standard 3&amp;times;3&amp;times;3 Rubik’s Cube contains 4.3252&amp;nbsp;&amp;times;&amp;nbsp;10&lt;sup&gt;19&lt;/sup&gt; states, and is too large to search exhaustively.}}&lt;/ref&gt;

In [[computational complexity theory]], several [[complexity class]]es have been defined in connection with implicit graphs, defined as above by a rule or algorithm for listing the neighbors of a vertex. For instance, [[PPA (complexity)|PPA]] is the class of problems in which one is given as input an undirected implicit graph (in which vertices are {{mvar|n}}-bit binary strings, with a [[polynomial time]] algorithm for listing the neighbors of any vertex) and a vertex of odd degree in the graph, and must find a second vertex of odd degree. By the [[handshaking lemma]], such a vertex exists; finding one is a problem in [[NP (complexity)|NP]], but the problems that can be defined in this way may not necessarily be [[NP-complete]], as it is unknown whether PPA&amp;nbsp;=&amp;nbsp;NP. [[PPAD (complexity)|PPAD]] is an analogous class defined on implicit [[directed graph]]s that has attracted attention in [[algorithmic game theory]] because it contains the problem of computing a [[Nash equilibrium]].&lt;ref&gt;{{citation | first = Christos | last = Papadimitriou | authorlink = Christos Papadimitriou | year = 1994 | title = On the complexity of the parity argument and other inefficient proofs of existence | journal = [[Journal of Computer and System Sciences]] | volume = 48 | issue = 3 | pages = 498–532 | url = http://www.cs.berkeley.edu/~christos/papers/On%20the%20Complexity.pdf | doi = 10.1016/S0022-0000(05)80063-7}}&lt;/ref&gt; The problem of testing [[reachability]] of one vertex to another in an implicit graph may also be used to characterize space-bounded nondeterministic complexity classes including [[NL (complexity)|NL]] (the class of problems that may be characterized by reachability in implicit directed graphs whose vertices are {{math|O(log ''n'')}}-bit bitstrings), [[SL (complexity)|SL]] (the analogous class for undirected graphs), and [[PSPACE]] (the class of problems that may be characterized by reachability in implicit graphs with polynomial-length bitstrings). In this complexity-theoretic context, the vertices of an implicit graph may represent the states of a [[nondeterministic Turing machine]], and the edges may represent possible state transitions, but implicit graphs may also be used to represent many other types of combinatorial structure.&lt;ref&gt;{{citation|title=Descriptive Complexity|contribution=Exercise 3.7 (Everything is a Graph)|first=Neil|last=Immerman|authorlink=Neil Immerman|page=48|url=http://books.google.com/books?id=kWSZ0OWnupkC&amp;pg=PA48|series=Graduate Texts in Computer Science|year=1999|publisher=Springer-Verlag|isbn= 978-0-387-98600-5}}.&lt;/ref&gt; [[PLS (complexity)|PLS]], another complexity class, captures the complexity of finding local optima in an implicit graph.&lt;ref&gt;{{Citation | last1=Yannakakis | first1=Mihalis | author1-link=Mihalis Yannakakis | title=Equilibria, fixed points, and complexity classes | year=2009 | journal=Computer Science Review | volume=3 | issue=2 | pages=71–85 | doi=10.1016/j.cosrev.2009.03.004}}.&lt;/ref&gt;

Implicit graph models have also been used as a form of [[relativization]] in order to prove separations between complexity classes that are stronger than the known separations for non-relativized models. For instance, Childs et al. used neighborhood representations of implicit graphs to define a graph traversal problem that can be solved in polynomial time on a [[quantum computer]] but that requires exponential time to solve on any classical computer.&lt;ref&gt;{{citation
 | last1 = Childs | first1 = Andrew M.
 | last2 = Cleve | first2 = Richard
 | last3 = Deotto | first3 = Enrico
 | last4 = Farhi | first4 = Edward
 | last5 = Gutmann | first5 = Sam
 | last6 = Spielman | first6 = Daniel A.
 | contribution = Exponential algorithmic speedup by a quantum walk
 | doi = 10.1145/780542.780552
 | location = New York
 | mr = 2121062
 | pages = 59–68
 | publisher = ACM
 | title = [[Symposium on Theory of Computing|Proceedings of the Thirty-Fifth Annual ACM Symposium on Theory of Computing]]
 | year = 2003}}.&lt;/ref&gt;

==Adjacency labeling schemes==
In the context of efficient representations of graphs, J. H. Muller defined a ''local structure'' or ''adjacency labeling scheme'' for a graph {{mvar|G}} in a given family {{mvar|F}} of graphs to be an assignment of an {{math|''O''(log ''n'')}}-bit identifier to each vertex of {{mvar|G}}, together with an algorithm (that may depend on {{mvar|F}} but is independent of the individual graph {{mvar|G}}) that takes as input two vertex identifiers and determines whether or not they are the endpoints of an edge in {{mvar|G}}. That is, this type of implicit representation is analogous to an [[adjacency matrix]]: it is straightforward to check whether two vertices are adjacent but finding the neighbors of any vertex requires a search through all possible vertices.&lt;ref name=&quot;muller&quot;&gt;{{citation
 | last = Muller | first = John Harold
 | publisher = Georgia Institute of Technology
 | series = Ph.D. thesis
 | title = Local structure in graph classes
 | year = 1988}}.&lt;/ref&gt;

Graph families with adjacency labeling schemes include:
*'''Sparse graphs'''. If every vertex in {{mvar|G}} has at most {{mvar|d}} neighbors, one may number the vertices of {{mvar|G}} from 1 to {{mvar|n}} and let the identifier for a vertex be the {{math|(''d'' + 1)}}-tuple of its own number and the numbers of its neighbors. Two vertices are adjacent when the first numbers in their identifiers appear later in the other vertex's identifier. More generally, the same approach can be used to provide an implicit representation for graphs with bounded [[arboricity]] or bounded [[degeneracy (graph theory)|degeneracy]], including the [[planar graph]]s and the graphs in any [[Robertson–Seymour theorem|minor-closed graph family]].&lt;ref name=&quot;knr&quot;/&gt;&lt;ref&gt;{{citation
 | last1 = Chrobak | first1 = Marek
 | last2 = Eppstein | first2 = David | author2-link = David Eppstein
 | doi = 10.1016/0304-3975(91)90020-3
 | issue = 2
 | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]
 | pages = 243–266
 | title = Planar orientations with low out-degree and compaction of adjacency matrices
 | url = http://www.ics.uci.edu/~eppstein/pubs/ChrEpp-TCS-91.pdf
 | volume = 86
 | year = 1991}}.&lt;/ref&gt;
*'''Intersection graphs'''. An [[interval graph]] is the [[intersection graph]] of a set of [[line segment]]s in the [[real line]]. It may be given an adjacency labeling scheme in which the points that are endpoints of line segments are numbered from 1 to 2''n'' and each vertex of the graph is represented by the numbers of the two endpoints of its corresponding interval. With this representation, one may check whether two vertices are adjacent by comparing the numbers that represent them and verifying that these numbers define overlapping intervals. The same approach works for other geometric intersection graphs including the graphs of bounded [[boxicity]] and the [[circle graph]]s, and subfamilies of these families such as the [[distance-hereditary graph]]s and [[cograph]]s.&lt;ref name=&quot;knr&quot;/&gt;&lt;ref name=&quot;spinrad&quot;/&gt; However, a geometric intersection graph representation does not always imply the existence of an adjacency labeling scheme, because it may require more than a logarithmic number of bits to specify each geometric object; for instance, representing a graph as a [[unit disk graph]] may require exponentially many bits for the coordinates of the disk centers.&lt;ref&gt;{{citation|url=http://homepages.cwi.nl/~mueller/Papers/SphericityDotproduct.pdf|last1=Kang|first1=Ross J.|last2=Müller|first2=Tobias|title=Sphere and dot product representations of graphs|year=2011}}.&lt;/ref&gt;
*'''Low-dimensional comparability graphs'''. The [[comparability graph]] for a [[partially ordered set]] has a vertex for each set element and an edge between two set elements that are related by the partial order. The [[order dimension]] of a partial order is the minimum number of linear orders whose intersection is the given partial order. If a partial order has bounded order dimension, then an adjacency labeling scheme for the vertices in its comparability graph may be defined by labeling each vertex with its position in each of the defining linear orders, and determining that two vertices are adjacent if each corresponding pair of numbers in their labels has the same order relation as each other pair. In particular, this allows for an adjacency labeling scheme for the [[chordal graph|chordal]] [[comparability graph]]s, which come from partial orders of dimension at most four.&lt;ref&gt;{{citation
 | last1 = Ma | first1 = Tze Heng
 | last2 = Spinrad | first2 = Jeremy P.
 | doi = 10.1007/BF00385814
 | issue = 1
 | journal = [[Order (journal)|Order]]
 | mr = 1129614
 | pages = 49–61
 | title = Cycle-free partial orders and chordal comparability graphs
 | volume = 8
 | year = 1991}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last1 = Curtis | first1 = Andrew R.
 | last2 = Izurieta | first2 = Clemente
 | last3 = Joeris | first3 = Benson
 | last4 = Lundberg | first4 = Scott
 | last5 = McConnell | first5 = Ross M.
 | doi = 10.1016/j.dam.2010.01.005
 | issue = 8
 | journal = Discrete Applied Mathematics
 | mr = 2602811
 | pages = 869–875
 | title = An implicit representation of chordal comparability graphs in linear time
 | volume = 158
 | year = 2010}}.&lt;/ref&gt;

Not all graph families have local structures. For some families, a simple counting argument proves that adjacency labeling schemes do not exist: only {{math|''O''(''n'' log ''n'')}} bits may be used to represent an entire graph, so a representation of this type can only exist when the number of {{mvar|n}}-vertex graphs in the given family {{mvar|F}} is at most {{math|2&lt;sup&gt;''O''(''n'' log ''n'')&lt;/sup&gt;}}. Graph families that have larger numbers of graphs than this, such as the [[bipartite graph]]s or the [[triangle-free graph]]s, do not have adjacency labeling schemes.&lt;ref name=&quot;knr&quot;/&gt;&lt;ref name=&quot;spinrad&quot;&gt;{{citation|first=Jeremy P.|last=Spinrad|title=Efficient Graph Representations|year=2003|isbn=0-8218-2815-0|chapter=2. Implicit graph representation|pages=17–30|url=http://books.google.com/books?id=RrtXSKMAmWgC&amp;pg=PA17}}.&lt;/ref&gt; However, even families of graphs in which the number of graphs in the family is small might not have an adjacency labeling scheme; for instance, the family of graphs with fewer edges than vertices has {{math|2&lt;sup&gt;''O''(''n'' log ''n'')&lt;/sup&gt;}} {{mvar|n}}-vertex graphs but does not have an adjacency labeling scheme, because one could transform any given graph into a larger graph in this family by adding a new isolated vertex for each edge, without changing its labelability.&lt;ref name=&quot;muller&quot;/&gt;&lt;ref name=&quot;spinrad&quot;/&gt; Kannan et al. asked whether having a [[Forbidden graph characterization|forbidden subgraph characterization]] and having at most  {{math|2&lt;sup&gt;''O''(''n'' log ''n'')&lt;/sup&gt;}} {{mvar|n}}-vertex graphs are together enough to guarantee the existence of an adjacency labeling scheme; this question, which Spinrad restated as a conjecture, remains open.&lt;ref name=&quot;knr&quot;/&gt;&lt;ref name=&quot;spinrad&quot;/&gt;

If a graph family {{mvar|F}} has an adjacency labeling scheme, then the {{mvar|n}}-vertex graphs in {{mvar|F}} may be represented as [[induced subgraph]]s of a common [[universal graph]] of polynomial size, the graph consisting of all possible vertex identifiers. Conversely, if a universal graph of this type can be constructed, then the identities of its vertices may be used as labels in an adjacency labeling scheme.&lt;ref name=&quot;knr&quot;&gt;{{citation
 | last1 = Kannan | first1 = Sampath
 | last2 = Naor | first2 = Moni | author2-link = Moni Naor
 | last3 = Rudich | first3 = Steven | author3-link = Steven Rudich
 | doi = 10.1137/0405049
 | issue = 4
 | journal = [[SIAM Journal on Discrete Mathematics]]
 | mr = 1186827
 | pages = 596–603
 | title = Implicit representation of graphs
 | volume = 5
 | year = 1992}}.&lt;/ref&gt; For this application of implicit graph representations, it is important that the labels use as few bits as possible, because the number of bits in the labels translates directly into the number of vertices in the universal graph. Alstrup and Rauhe showed that any tree has an adjacency labeling scheme with {{math|log&lt;sub&gt;2&lt;/sub&gt; ''n'' + ''O''({{log-star}} ''n'')}} bits per label, from which it follows that any graph with [[arboricity]] ''k'' has a scheme with {{math|''k'' log&lt;sub&gt;2&lt;/sub&gt; ''n'' + ''O''({{log-star}} ''n'')}} bits per label and a universal graph with {{math|''n''&lt;sup&gt;''k''&lt;/sup&gt;2&lt;sup&gt;''O''({{log-star}} ''n'')&lt;/sup&gt;}} vertices. In particular, planar graphs have arboricity at most three, so they have universal graphs with a nearly-cubic number of vertices.&lt;ref&gt;{{citation
 | last1 = Alstrup | first1 = Stephen
 | last2 = Rauhe | first2 = Theis
 | doi = 10.1109/SFCS.2002.1181882
 | journal = [[Symposium on Foundations of Computer Science|Proceedings of the 43rd Annual IEEE Symposium on Foundations of Computer Science]]
 | pages = 53–62
 | title = Small induced-universal graphs and compact implicit graph representations
 | url = http://www.it-c.dk/research/algorithms/Kurser/AD/2002E/Uge7/parent.pdf
 | year = 2002}}.&lt;/ref&gt;
For the family of planar graphs, Gavoille and Labourel showed a labeling scheme with  {{math|2log(n) + O(log log (n))}}   bits per label.
&lt;ref&gt;{{citation
 | last1 = Arnaud  | first1 = Labourel
 | last2 = Gavoille | first2 = Cyril
 | doi = 10.1007/978-3-540-75520-3_52
 | journal = [[European Symposium on Algorithms|Proceedings of the 15th annual European Symposium on Algorithms]]
 | pages = 582-593
 | title = Shorter Implicit Representation for Planar Graphs and Bounded Treewidth Graphs
 | url = http://dept-info.labri.fr/~gavoille/article/GL07.pdf
 | year = 2007}}.&lt;/ref&gt;

==Evasiveness==
The [[Aanderaa–Karp–Rosenberg conjecture]] concerns implicit graphs given as a set of labeled vertices with a black-box rule for determining whether any two vertices are adjacent; this differs from an adjacency labeling scheme in that the rule may be specific to a particular graph rather than being a generic rule that applies to all graphs in a family. This difference allows every graph to have an implicit representation: for instance, the rule could be to look up the pair of vertices in a separate adjacency matrix. However, an algorithm that is given as input an implicit graph of this type must operate on it only through the implicit adjacency test, without reference to the implementation of that test.

A ''graph property'' is the question of whether a graph belongs to a given family of graphs; the answer must remain invariant under any relabeling of the vertices. In this context, the question to be determined is how many pairs of vertices must be tested for adjacency, in the worst case, before the property of interest can be determined to be true or false for a given implicit graph. Rivest and Vuillemin proved that any deterministic algorithm for any nontrivial graph property must test a quadratic number of pairs of vertices;&lt;ref&gt;{{Citation
| doi = 10.1145/800116.803747
| pages = 6&amp;ndash;11
| last1 = Rivest
| first1 = Ronald L.
| authorlink = Ron Rivest
| first2 = Jean | last2 = Vuillemin
| contribution = A generalization and proof of the Aanderaa-Rosenberg conjecture
| title = [[Symposium on Theory of Computing|Proc. 7th ACM Symposium on Theory of Computing]]
| location = Albuquerque, New Mexico, United States
| year = 1975
}}.&lt;/ref&gt; the full Aanderaa–Karp–Rosenberg conjecture is that any deterministic algorithm for a monotonic graph property (one that remains true if more edges are added to a graph with the property) must in some cases test every possible pair of vertices. Several cases of the conjecture have been proven to be true—for instance, it is known to be true for graphs with a prime number of vertices&lt;ref&gt;{{Citation
| publisher = IEEE Computer Society
| doi = 10.1109/SFCS.1983.4
| pages = 31&amp;ndash;33
| last1 = Kahn
| first1 = Jeff
| author2-link = Michael Saks (mathematician) | first2 = Michael | last2 = Saks
| first3 = Dean | last3 = Sturtevant
| contribution = A topological approach to evasiveness
| title = [[Symposium on Foundations of Computer Science]]
| location = Los Alamitos, CA, USA
| year = 1983
}}.&lt;/ref&gt;—but the full conjecture remains open. Variants of the problem for randomized algorithms and quantum algorithms have also been studied.

Bender and Ron have shown that, in the same model used for the evasiveness conjecture, it is possible in only constant time to distinguish [[directed acyclic graph]]s from graphs that are very far from being acyclic. In contrast, such a fast time is not possible in neighborhood-based implicit graph models,&lt;ref&gt;{{citation
 | last1 = Bender | first1 = Michael A.
 | last2 = Ron | first2 = Dana | author2-link = Dana Ron
 | contribution = Testing acyclicity of directed graphs in sublinear time
 | doi = 10.1007/3-540-45022-X_68
 | location = Berlin
 | mr = 1795937
 | pages = 809–820
 | publisher = Springer
 | series = Lecture Notes in Comput. Sci.
 | title = Automata, languages and programming (Geneva, 2000)
 | volume = 1853
 | year = 2000}}.&lt;/ref&gt;

==See also==
*[[Matroid oracle]], an implicit model for [[matroid]] algorithms

==References==
{{reflist|colwidth=30em}}

[[Category:Graph theory]]
[[Category:Graph data structures]]</text>
      <sha1>a4y8ebtsm6hof8y2psm8mrpu890aaj2</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Deterministic acyclic finite state automaton</title>
    <ns>0</ns>
    <id>6022680</id>
    <revision>
      <id>613308832</id>
      <parentid>611286726</parentid>
      <timestamp>2014-06-17T17:25:37Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* Comparison to tries */Task 5: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <text xml:space="preserve" bytes="6491">[[Image:Trie-vs-minimal-acyclic-fa.svg|thumb|right|250px|The strings &quot;tap&quot;, &quot;taps&quot;, &quot;top&quot;, and &quot;tops&quot; stored in a [[Trie]] (left) and a DAFSA (right), &lt;tt&gt;EOW&lt;/tt&gt; stands for End-of-word.]]

In [[computer science]], a '''deterministic acyclic finite state automaton''' ('''DAFSA'''),&lt;ref name=&quot;daciuk&quot;&gt;Jan Daciuk, Stoyan Mihov, Bruce Watson and Richard Watson (2000). Incremental construction of minimal acyclic finite state automata. Computational Linguistics '''26'''(1):3-16.&lt;/ref&gt;
also called a '''directed acyclic word graph''' ('''DAWG'''; though that name also refers to a [[Directed acyclic word graph|related data structure]] that functions as a suffix index&lt;ref&gt;{{DADS|directed acyclic word graph|directedAcyclicWordGraph}}&lt;/ref&gt;)
is a [[data structure]] that represents a set of [[String (computer science)|strings]], and allows for a query operation that tests whether a given string belongs to the set in time proportional to its length. In these respects, a DAFSA is very similar to a [[trie]], but it is much more space efficient.

A DAFSA is a special case of a [[finite state recognizer]] that takes the form of a [[directed acyclic graph]] with a single source vertex (a vertex with no incoming edges), in which each edge of the graph is labeled by a letter or symbol, and in which each vertex has at most one outgoing edge for each possible letter or symbol. The strings represented by the DAFSA are formed by the symbols on paths in the graph from the source vertex to any sink vertex (a vertex with no outgoing edges). In fact, a [[deterministic finite state automaton]] is acyclic [[if and only if]] it recognizes a [[finite language|finite set of strings]].&lt;ref name=&quot;daciuk&quot;/&gt;

==Comparison to tries==
By allowing the same vertices to be reached by multiple paths, a DAFSA may use significantly fewer vertices than the strongly related trie data structure. Consider, for example, the four English words &quot;tap&quot;, &quot;taps&quot;, &quot;top&quot;, and &quot;tops&quot;. A trie for those four words would have 11 vertices, one for each of the strings formed as a prefix of one of these words, or for one of the words followed by the end-of-string marker. However, a DAFSA can represent these same four words using only six vertices ''v&lt;sub&gt;i&lt;/sub&gt;'' for 0&amp;nbsp;≤&amp;nbsp;''i''&amp;nbsp;≤&amp;nbsp;5, and the following edges: an edge from ''v''&lt;sub&gt;0&lt;/sub&gt; to ''v''&lt;sub&gt;1&lt;/sub&gt; labeled &quot;t&quot;, two edges from ''v''&lt;sub&gt;1&lt;/sub&gt; to ''v''&lt;sub&gt;2&lt;/sub&gt; labeled &quot;a&quot; and &quot;o&quot;, an edge from ''v''&lt;sub&gt;2&lt;/sub&gt; to ''v''&lt;sub&gt;3&lt;/sub&gt; labeled &quot;p&quot;, an edge ''v''&lt;sub&gt;3&lt;/sub&gt; to ''v''&lt;sub&gt;4&lt;/sub&gt; labeled &quot;s&quot;, and edges from ''v''&lt;sub&gt;3&lt;/sub&gt; and ''v''&lt;sub&gt;4&lt;/sub&gt; to ''v''&lt;sub&gt;5&lt;/sub&gt; labeled with the end-of-string marker. There is a tradeoff between memory and functionality, because a standard DAFSA can tell you if a word exists within it, but it cannot point you to auxiliary information about that word, whereas a trie can.

The primary difference between DAFSA and trie is the elimination of suffix and infix redundancy in storing strings. The trie eliminates prefix redundancy since all common prefixes are shared between strings, such as between ''doctors'' and ''doctorate'' the ''doctor'' prefix is shared. In a DAFSA common suffixes are also shared, for words that have the same set of possible suffixes as each other. For dictionary sets of common English words, this translates into major memory usage reduction.

Because the terminal nodes of a DAFSA can be reached by multiple paths, a DAFSA cannot directly store auxiliary information relating to each path, e.g. a word's frequency in the English language. However, if for each node we store the number of unique paths through that point in the structure, we can use it to retrieve the index of a word, or a word given its index.&lt;ref name=&quot;kowaltowski1993&quot;&gt;{{cite journal| pages = 15–30| last = Kowaltowski| first = T.|author2=CL Lucchesi | title = Applications of finite automata representing large vocabularies| journal = Software-Practice and Experience| year = 1993| volume = 1993| id = {{citeseerx|10.1.1.56.5272}}}}&lt;/ref&gt; The auxiliary information can then be stored in an array.

==References==
{{Reflist}}

*{{citation | last1=Blumer | first1=A. | last2=Blumer | first2=J. | last3=Haussler | first3=D. | last4=Ehrenfeucht | first4=A. | last5=Chen | first5=M.T. | last6=Seiferas | first6=J.| title=The smallest automation recognizing the subwords of a text | year=1985 | journal=Theoretical computer science | pages=31–55 | volume=40 | doi=10.1016/0304-3975(85)90157-4}}
*{{citation | first1=Andrew | last1=Appel | first2=Guy | last2=Jacobsen | title= The World's Fastest Scrabble Program | year=1988 | journal=Communications of the ACM | url=http://www.cs.cmu.edu/afs/cs/academic/class/15451-s06/www/lectures/scrabble.pdf | format=PDF }}. One of the early mentions of the data structure.
*{{citation | first1=Cees J. A. | last1=Jansen | first2=Dick E. | last2=Boekee | contribution=On the significance of the directed acyclic word graph in cryptology | series=Lecture Notes in Computer Science | publisher=[[Springer-Verlag]] | title=Advances in Cryptology — AUSCRYPT '90 | volume=453 | pages=318–326 | doi=10.1007/BFb0030372 | year=1990 | isbn=3-540-53000-2 }}.
*{{citation | last1=Epifanio | first1=Chiara | last2=Mignosi | first2=Filippo | last3=Shallit | first3=Jeffrey | last4=Venturini | first4=Ilaria | chapter=Sturmian graphs and a conjecture of Moser | pages=175–187 | editor1-last=Calude | editor1-first=Cristian S. | editor2-last=Calude | editor2-first=Elena | editor3-last=Dineen | editor3-first=Michael J. | title=Developments in language theory.  Proceedings, 8th international conference (DLT 2004), Auckland, New Zealand, December 2004 | year=2004 | publisher=[[Springer-Verlag]] | series=Lecture Notes in Computer Science | volume=3340 | isbn=3-540-24014-4 | zbl=1117.68454 }}

==External links==
* [http://pages.pathcom.com/~vadco/dawg.html http://pages.pathcom.com/~vadco/dawg.html] - JohnPaul Adamovsky teaches how to construct a DAFSA using an array of integers.
* [http://pages.pathcom.com/~vadco/cwg.html http://pages.pathcom.com/~vadco/cwg.html] - JohnPaul Adamovsky teaches how to construct a DAFSA hash function using a novel encoding with multiple integer arrays.  This encoding is called the Caroline Word Graph (CWG).

{{Data structures}}
{{formal languages and grammars|state=collapsed}}

[[Category:Graph data structures]]
[[Category:String data structures]]

[[hr:Aciklički deterministički konačni automat]]</text>
      <sha1>aapna75032g5mmg8ptkzyz6vlkn4rus</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Call graph</title>
    <ns>0</ns>
    <id>3838181</id>
    <revision>
      <id>622595202</id>
      <parentid>617696070</parentid>
      <timestamp>2014-08-24T12:44:24Z</timestamp>
      <contributor>
        <username>Phaghio</username>
        <id>22312694</id>
      </contributor>
      <minor/>
      <comment>Added tceetree project</comment>
      <text xml:space="preserve" bytes="11417">[[File:A Call Graph generated by pycallgraph.png|thumb|A call graph generated for a simple computer program in Python.]]

A '''call graph''' (also known as a '''call [[multigraph]]'''&lt;ref name=&quot;KhedkerSanyal2009&quot;&gt;{{cite book|author1=Uday Khedker|author2=Amitabha Sanyal|author3=Bageshri Sathe|title=Data Flow Analysis: Theory and Practice|year=2009|publisher=CRC Press|isbn=978-0-8493-3251-7|page=234}}&lt;/ref&gt;) is a [[directed graph]] (and more specifically a [[flow graph]]&lt;ref name=&quot;Jalote1997&quot;&gt;{{cite book|author=Pankaj Jalote|title=An Integrated Approach to Software Engineering|year=1997|publisher=Springer Science &amp; Business Media|isbn=978-0-387-94899-7|page=372}}&lt;/ref&gt;) that represents calling relationships between [[subroutine]]s in a [[computer program]]. Specifically, each node represents a procedure and each edge ''(f, g)'' indicates that procedure ''f'' calls procedure ''g''. Thus, a cycle in the graph indicates recursive procedure calls.

Call graphs are a basic program analysis result that can be used for human understanding of programs, or as a basis for further analyses, such as an analysis that tracks the flow of values between procedures. One simple application of call graphs is finding procedures that are never called.

Call graphs can be '''dynamic''' or '''static'''. A dynamic call graph is a record of an execution of the program, e.g., as output by a profiler. Thus, a dynamic call graph can be exact, but only describes one run of the program. A static call graph is a call graph intended to represent every possible run of the program. The exact static call graph is an [[undecidable problem]], so static call graph algorithms are generally overapproximations. That is, every call relationship that occurs is represented in the graph, and possibly also some call relationships that would never occur in actual runs of the program.

Call graphs can be defined to represent varying degrees of precision. A more precise call graph more precisely approximates the behavior of the real program, at the cost of taking longer to compute and more memory to store. The most precise call graph is fully '''context-sensitive''', which means that for each procedure, the graph contains a separate node for each [[call stack]] that procedure can be activated with. A fully context-sensitive call graph is called '''calling context tree'''. A [[calling context tree]] can be computed dynamically easily, although it may take up a large amount of memory. Calling context trees are usually not computed statically, because it would take too long for a large program. The least precise call graph is '''context-insensitive''', which means that there is only one node for each procedure.

With languages that feature [[dynamic dispatch]], such as [[Java (programming language)|Java]] and [[C++]], computing a static call graph precisely requires [[alias analysis]] results. Conversely, computing precise aliasing requires a call graph. Many static analysis systems solve the apparent infinite regress by computing both simultaneously.

This term is frequently used in the [[compiler]] and [[binary translation]] community.  By tracking a call graph, it may be possible to detect anomalies of program execution or code injection attacks{{Citation needed|date=February 2007}}.

==Software==

=== [[Free software]] call-graph generators ===

Run-time call-graph (most of tools listed are profilers with callgraph functionality):

* [[gprof]] : included in BSD or part of the [[GNU Binary Utilities]]
* [http://kcachegrind.sourceforge.net KCachegrind] : powerful tool to generate and analyze call graphs based on data generated by [[Valgrind]]'s callgrind tool.
* Mac OS X Activity Monitor : Apple GUI process monitor Activity Monitor has a built-in call graph generator that can sample  processes and return a call graph. This function is only available in [[Mac OS X]] Leopard
* [[OpenPAT]] : includes the &lt;code&gt;control_flow&lt;/code&gt; tool which automatically creates a [[Graphviz]] call-graph picture from runtime measurements.
* [http://goog-perftools.sourceforge.net/doc/cpu_profiler.html pprof tool], part of open-source google-perftools.
* [[CodeAnalyst]] from [[AMD]] (released under GPL)
* [http://makepp.sourceforge.net/gallery/ makeppgraph] is a dependency graph generator (at module level) for builds performed with [[Make (software)#Modern versions|makepp]].

Static (for C language), for getting call graphs without running of application:

* [[doxygen]] : Uses [[graphviz]] to generate static call/inheritance diagrams
* [[GNU cflow|cflow]] : GNU cflow is able to generate the direct and inverted call graph of a C program
* [http://www.gson.org/egypt/ egypt] : a small [[Perl]] script that uses gcc and [[Graphviz]] to generate the static call graph of a C program.
* [http://www.vim.org/scripts/script.php?script_id=2368 CCTree] : Native [[Vim]] plugin that can display static call graphs by reading a [[cscope]] database. Works for C programs.
* [http://www.csn.ul.ie/~mel/projects/codeviz/ codeviz] : a static call graph generator (the program is ''not'' run). Implemented as a patch to [[GNU Compiler Collection|gcc]]; works for C and C++ programs.
* [http://toolchainguru.blogspot.com/2011/03/c-calltrees-in-bash-revisited.html calltree.sh] : Bash shell functions that glue together cscope, graphviz, and a sampling of dot-rendering tools to display &quot;caller&quot; and &quot;callee&quot; relationships above, below, and/or between the C functions you specify.
* [http://sourceforge.net/projects/tceetree/ tceetree] : like calltree.sh, it connects [[Cscope]] and [[Graphviz]], but it is an executable rather than a bash script.

PHP, perl, python
* [https://metacpan.org/module/Devel::NYTProf Devel::NYTProf] : a perl performance analyser and call chart generator
* [http://phpcallgraph.sourceforge.net/ phpCallGraph] : a call graph generator for PHP programs that uses [[Graphviz]]. It is written in PHP and requires at least PHP 5.2.
* [http://pycallgraph.slowchop.com/ pycallgraph] : a call graph generator for Python programs that uses [[Graphviz]].
* [http://code.google.com/p/jrfonseca/wiki/Gprof2Dot gprof2dot] : A call graph generator.  Converts profiling data for many languages/runtimes to a [[Graphviz]] callgraph.  Written in Python, but does callgraphs for a lot more than Python.
* [https://github.com/scottrogowski/code2flow code2flow]: A call graph generator for Python and Javascript programs that uses [[Graphviz]]

=== Proprietary call-graph generators ===

; [[Project Analyzer]] : Static code analyzer and call graph generator for Visual Basic code
; [[VTune|Intel VTune Performance Analyzer]] : Instrumenting profiler to show call graph and execution statistics
; [[DMS Software Reengineering Toolkit]] : Customizable program analysis tool with static whole-program global call graph extraction for C, Java and COBOL
; [http://www.codeprophet.co.uk CodeProphet Profiler] : Callgraph Profiler for native C/C++ code under Windows x86, x64 and Windows Mobile.

=== Other, related tools ===

; [[Graphviz]] : Turns a text representation of any graph (including a call graph) into a picture. Must be used together with &lt;code&gt;[[gprof]]&lt;/code&gt; (and a glue script, such as [http://code.google.com/p/google-gprof2dot/ gprof2dot]), which in accordance to the [[Unix philosophy]] doesn't handle graphics by itself.

==Sample graph==
A sample call graph generated from [[gprof]] analyzing itself:
&lt;pre&gt;
index    called     name                              |index    called     name
      72384/72384       sym_id_parse [54]             |       1508/1508        cg_dfn [15]
[3]   72384             match [3]                     |[13]   1508             pre_visit [13]
----------------------                                |----------------------
          4/9052        cg_tally [32]                 |       1508/1508        cg_assemble [38]
       3016/9052        hist_print [49]               |[14]   1508             propagate_time [14]
       6032/9052        propagate_flags [52]          |----------------------
[4]    9052             sym_lookup [4]                |          2             cg_dfn [15]
----------------------                                |       1507/1507        cg_assemble [38]
       5766/5766        core_create_function_syms [41]|[15]   1507+2           cg_dfn [15]
[5]    5766             core_sym_class [5]            |       1509/1509        is_numbered [9]
----------------------                                |       1508/1508        is_busy [11]
         24/1537        parse_spec [19]               |       1508/1508        pre_visit [13]
       1513/1537        core_create_function_syms [41]|       1508/1508        post_visit [12]
[6]    1537             sym_init [6]                  |          2             cg_dfn [15]
----------------------                                |----------------------
       1511/1511        core_create_function_syms [41]|       1505/1505        hist_print [49]
[7]    1511             get_src_info [7]              |[16]   1505             print_line [16]
----------------------                                |          2/9           print_name_only [25]
          2/1510        arc_add [31]                  |----------------------
       1508/1510        cg_assemble [38]              |       1430/1430        core_create_function_syms [41]
[8]    1510             arc_lookup [8]                |[17]   1430             source_file_lookup_path [17]
----------------------                                |----------------------
       1509/1509        cg_dfn [15]                   |         24/24          sym_id_parse [54]
[9]    1509             is_numbered [9]               |[18]     24             parse_id [18]
----------------------                                |         24/24          parse_spec [19]
       1508/1508        propagate_flags [52]          |----------------------
[10]   1508             inherit_flags [10]            |         24/24          parse_id [18]
----------------------                                |[19]     24             parse_spec [19]
       1508/1508        cg_dfn [15]                   |         24/1537        sym_init [6]
[11]   1508             is_busy [11]                  |----------------------
----------------------                                |         24/24          main [1210]
       1508/1508        cg_dfn [15]                   |[20]     24             sym_id_add [20]
[12]   1508             post_visit [12]               |
&lt;/pre&gt;

==See also==
*[[Dependency graph]]

==References==
{{Reflist}}
*Ryder, B.G., &quot;Constructing the Call Graph of a Program,&quot; Software Engineering, IEEE Transactions on, vol. SE-5, no.3pp. 216– 226, May 1979 [http://ieeexplore.ieee.org/xpls/abs_all.jsp?isnumber=35910&amp;arnumber=1702621&amp;count=17&amp;index=5]
*Grove, D., DeFouw, G., Dean, J., and Chambers, C. 1997. Call graph construction in [[object-oriented language]]s. SIGPLAN Not. 32, 10 (Oct. 1997), 108-124. [http://doi.acm.org/10.1145/263700.264352]
*Callahan, D.; Carle, A.; Hall, M.W.; Kennedy, K., &quot;Constructing the procedure call multigraph,&quot; Software Engineering, IEEE Transactions on, vol.16, no.4pp.483–487, Apr 1990 [http://ieeexplore.ieee.org/xpls/abs_all.jsp?isnumber=1950&amp;arnumber=54302&amp;count=13&amp;index=12]

[[Category:Compiler construction]]
[[Category:Documentation generators]]
[[Category:Static program analysis]]
[[Category:Graph data structures]]</text>
      <sha1>ioijhmwh1lm0mlsp8td58epqhjuyuh8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Adjacency matrix</title>
    <ns>0</ns>
    <id>244463</id>
    <revision>
      <id>625834738</id>
      <parentid>625792639</parentid>
      <timestamp>2014-09-16T16:57:18Z</timestamp>
      <contributor>
        <username>Wcherowi</username>
        <id>13428914</id>
      </contributor>
      <minor/>
      <comment>Reverted 2 edits by [[Special:Contributions/27.251.158.66|27.251.158.66]] ([[User talk:27.251.158.66|talk]]) to last revision by LokiClock. ([[WP:TW|TW]])</comment>
      <text xml:space="preserve" bytes="10595">In [[mathematics]] and [[computer science]], an '''adjacency matrix''' is a means of representing which [[Vertex (graph theory)|vertices]] (or nodes) of a [[graph (mathematics)|graph]] are [[adjacent (graph theory)|adjacent]] to which other vertices. Another matrix representation for a graph is the [[incidence matrix]].

Specifically, the adjacency matrix of a [[finite set|finite]] graph '''G''' on ''n'' vertices is the ''n &amp;times; n'' matrix where the non-diagonal entry ''a''&lt;sub&gt;''ij''&lt;/sub&gt; is the number of edges from vertex ''i'' to vertex ''j'', and the diagonal entry ''a''&lt;sub&gt;''ii''&lt;/sub&gt;, depending on the convention, is either once or twice the number of edges (loops) from vertex ''i'' to itself.  Undirected graphs often use the latter convention of counting loops twice, whereas directed graphs typically use the former convention.  There exists a unique adjacency matrix for each isomorphism class of graphs (up to permuting rows and columns), and it is not the adjacency matrix of any other isomorphism class of graphs. In the special case of a finite [[simple graph]], the adjacency matrix is a [[(0,1)-matrix]] with zeros on its diagonal. If the graph is undirected, the adjacency matrix is [[symmetric matrix|symmetric]]. 

The relationship between a graph and the [[eigenvalue]]s and [[eigenvector]]s of its adjacency matrix is studied in [[spectral graph theory]].

==Examples==
The convention followed here is that an adjacent edge counts 1 in the matrix for an undirected graph. 

{|class=&quot;wikitable&quot; style=&quot;text-align: center; width: 700px; height: 1100px;&quot;
![[Labeled graph]]
!Adjacency matrix
|-
|[[Image:6n-graph2.svg|200px]]
|&lt;math&gt;\begin{pmatrix}
1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
\end{pmatrix}&lt;/math&gt;

Coordinates are 1-6.
|-
|[[File:Symmetric group 4; Cayley graph 1,5,21 (Nauru Petersen); numbers.svg|250px]]
&lt;br&gt;The [[Nauru graph]]
|[[File:Symmetric group 4; Cayley graph 1,5,21 (adjacency matrix).svg|250px]]
&lt;br&gt;Coordinates are 0-23.&lt;br&gt;White fields are zeros, colored fields are ones.
|-
|[[File:Symmetric group 4; Cayley graph 4,9; numbers.svg|250px]]
&lt;br&gt;[[Directed graph|Directed]] [[Cayley graph]] of [[Symmetric group|S]]&lt;sub&gt;4&lt;/sub&gt;
|[[File:Symmetric group 4; Cayley graph 4,9 (adjacency matrix).svg|250px]]
&lt;br&gt;As the graph is directed,
&lt;br&gt;the matrix is not [[Symmetric matrix|symmetric]].
|}

* The adjacency matrix of a [[complete graph]] contains all ones except along the diagonal where there are only zeros.
* The adjacency matrix of an [[empty graph]] is a [[zero matrix]].

==Adjacency matrix of a bipartite graph==&lt;!-- [[Adjacency matrix of a bipartite graph]] &amp; [[Biadjacency matrix]] redirect here --&gt;

The adjacency matrix &lt;math&gt;A&lt;/math&gt; of a [[bipartite graph]] whose parts have &lt;math&gt;r&lt;/math&gt; and &lt;math&gt;s&lt;/math&gt; vertices has the form 
:&lt;math&gt;A = \begin{pmatrix} 0_{r,r} &amp; B \\ B^T &amp; 0_{s,s} \end{pmatrix},&lt;/math&gt;
where &lt;math&gt;B&lt;/math&gt; is an &lt;math&gt;r \times s&lt;/math&gt; matrix, and &lt;math&gt;0&lt;/math&gt; represents the zero matrix. Clearly, the matrix &lt;math&gt;B&lt;/math&gt; uniquely represents the bipartite graphs.  It is sometimes called the biadjacency matrix. 
Formally, let &lt;math&gt;G = (U, V, E)&lt;/math&gt;  be a [[bipartite graph]] with parts &lt;math&gt;U={u_1,..., u_r}&lt;/math&gt; and &lt;math&gt;V={v_1,..., v_s}&lt;/math&gt;. The '''biadjacency matrix''' is the &lt;math&gt;r \times s&lt;/math&gt; 0-1 matrix &lt;math&gt;B&lt;/math&gt; in which &lt;math&gt;b_{i,j} = 1&lt;/math&gt; iff &lt;math&gt;(u_i, v_j) \in E&lt;/math&gt;. 

If &lt;math&gt;G&lt;/math&gt; is a bipartite [[multigraph]] or [[weighted graph]] then the elements &lt;math&gt;b_{i,j}&lt;/math&gt; are taken to be the number of edges between the vertices or the weight of the edge &lt;math&gt;(u_i, v_j),&lt;/math&gt; respectively.

==Properties==
The adjacency matrix of an undirected simple graph is [[symmetric matrix|symmetric]], and therefore has a complete set of [[real number|real]] [[eigenvalue]]s and an orthogonal [[eigenvector]] basis. The set of eigenvalues of a graph is the '''spectrum''' of the graph.

Suppose two directed or undirected graphs &lt;math&gt;G_1&lt;/math&gt; and &lt;math&gt;G_2&lt;/math&gt; with adjacency matrices &lt;math&gt;A_1&lt;/math&gt; and &lt;math&gt;A_2&lt;/math&gt; are given. &lt;math&gt;G_1&lt;/math&gt; and &lt;math&gt;G_2&lt;/math&gt; are [[graph isomorphism|isomorphic]] if and only if there exists a [[permutation matrix]] &lt;math&gt;P&lt;/math&gt; such that

:&lt;math&gt;P A_1 P^{-1} = A_2.&lt;/math&gt; 

In particular, &lt;math&gt;A_1&lt;/math&gt; and &lt;math&gt;A_2&lt;/math&gt; are [[similar (linear algebra)|similar]] and therefore have the same [[Minimal polynomial (linear algebra)|minimal polynomial]], [[characteristic polynomial]], eigenvalues, [[determinant]] and [[trace (matrix)|trace]]. These can therefore serve as isomorphism invariants of graphs. However, two graphs may possess the same set of eigenvalues but not be isomorphic. &lt;ref&gt;[[Chris Godsil|Godsil, Chris]]; [[Gorden Royle|Royle, Gordon]] ''Algebraic Graph Theory'', Springer (2001), ISBN 0-387-95241-1, p.164&lt;/ref&gt;

If ''A'' is the adjacency matrix of the directed or undirected graph ''G'', then the matrix ''A&lt;sup&gt;n&lt;/sup&gt;'' (i.e., the [[matrix multiplication|matrix product]] of ''n'' copies of ''A'') has an interesting interpretation: the entry in row ''i'' and column ''j'' gives the number of (directed or undirected) walks of length ''n'' from vertex ''i'' to vertex ''j''. This implies, for example, that the number of triangles in an undirected graph ''G'' is exactly the [[Trace (linear algebra)|trace]] of ''A&lt;sup&gt;3&lt;/sup&gt;'' divided by 6.

The main diagonal of every adjacency matrix corresponding to a graph without loops has all zero entries. Note that here 'loops' means, for example A→A, not 'cycles' such as A→B→A.

For &lt;math&gt;\left( d \right)&lt;/math&gt; -regular graphs, d is also an eigenvalue of A for the vector &lt;math&gt;v=\left( 1,\dots,1 \right)&lt;/math&gt;, and &lt;math&gt;G&lt;/math&gt; is connected if and only if the multiplicity of &lt;math&gt;d&lt;/math&gt; is 1. It can be shown that &lt;math&gt;-d&lt;/math&gt; is also an eigenvalue of A if G is a connected [[bipartite graph]]. The above are results of [[Perron–Frobenius theorem]].

==Variations==
An '''(''a'', ''b'', ''c'')-adjacency matrix''' ''A'' of a simple graph has ''A''&lt;sub&gt;''ij''&lt;/sub&gt; = ''a'' if ''ij'' is an edge, ''b'' if it is not, and ''c'' on the diagonal. The [[Seidel adjacency matrix]] is a '''(−1,1,0)-adjacency matrix'''. This matrix is used in studying [[strongly regular graph]]s and [[two-graph]]s.&lt;ref&gt;{{cite journal |last=Seidel |first=J. J. |title=Strongly Regular Graphs with (−1,1,0) Adjacency Matrix Having Eigenvalue 3 |journal=[[Linear Algebra and its Applications|Lin. Alg. Appl.]] |volume=1 |issue=2 |pages=281–298 |year=1968 |doi=10.1016/0024-3795(68)90008-6 }}&lt;/ref&gt;

The '''[[distance matrix]]''' has in position (''i'',''j'') the distance between vertices ''v&lt;sub&gt;i&lt;/sub&gt;'' and ''v&lt;sub&gt;j&lt;/sub&gt;''&amp;nbsp;.  The distance is the length of a shortest path connecting the vertices. Unless lengths of edges are explicitly provided, the length of a path is the number of edges in it.  The distance matrix resembles a high power of the adjacency matrix, but instead of telling only whether or not two vertices are connected (i.e., the connection matrix, which contains boolean values), it gives the exact distance between them.

==Data structures==

{{ref improve section|date=May 2012}}
For use as a [[data structure]], the main alternative to the adjacency matrix is the [[adjacency list]]. Because each entry in the adjacency matrix requires only one bit, it can be represented in a very compact way, occupying only &lt;math&gt;{n^2} / 8&lt;/math&gt; bytes of contiguous space, where &lt;math&gt;n&lt;/math&gt; is the number of vertices. Besides avoiding wasted space, this compactness encourages [[locality of reference]].

However, for a [[sparse graph]], adjacency lists require less storage space, because they do not waste any space to represent edges that are ''not'' present. Using a naïve [[array data structure|array]] implementation on a 32-bit computer, an adjacency list for an undirected graph requires about &lt;math&gt;8 e&lt;/math&gt; bytes of storage, where &lt;math&gt;e&lt;/math&gt; is the number of edges.

Noting that a simple graph can have at most &lt;math&gt;n^2&lt;/math&gt; edges, allowing loops, we can let &lt;math&gt;d = e / n^2&lt;/math&gt; denote the ''density'' of the graph. Then, &lt;math&gt;8 e &gt; n^2 / 8&lt;/math&gt;, or the adjacency list representation occupies more space precisely when &lt;math&gt;d &gt; 1/64&lt;/math&gt;. Thus a graph must be sparse indeed to justify an adjacency list representation.

Besides the space tradeoff, the different data structures also facilitate different operations. Finding all vertices adjacent to a given vertex in an adjacency list is as simple as reading the list. With an adjacency matrix, an entire row must instead be scanned, which takes ''[[Big O notation|O]](n)'' time. Whether there is an edge between two given vertices can be determined at once with an adjacency matrix, while requiring time proportional to the minimum degree of the two vertices with the adjacency list.

==References==
{{Reflist}}

==Further reading==
*{{cite book |authorlink=Thomas H. Cormen |first=Thomas H. |last=Cormen |authorlink2=Charles E. Leiserson |first2=Charles E. |last2=Leiserson |authorlink3=Ronald L. Rivest |first3=Ronald L. |last3=Rivest |authorlink4=Clifford Stein |first4=Clifford |last4=Stein |year=2001 |title=[[Introduction to Algorithms]] |edition=Second |publisher=MIT Press and McGraw-Hill |isbn=0-262-03293-7 |chapter=Section 22.1: Representations of graphs |pages=527–531 }}
*{{cite book |authorlink=Chris Godsil |first=Chris |last=Godsil |authorlink2=Gordon Royle |first2=Gordon |last2=Royle |year=2001 |title=Algebraic Graph Theory |location=New York |publisher=Springer |isbn=0-387-95241-1 }}

==External links==
{{Commons category|Adjacency matrices of graphs}}
* [http://www.x2d.org/java/projects/fluffschack.jnlp Fluffschack] &amp;mdash; an educational Java web start game demonstrating the relationship between adjacency matrices and graphs.
* [http://opendatastructures.org/versions/edition-0.1e/ods-java/12_1_AdjacencyMatrix_Repres.html Open Data Structures - Section 12.1 - AdjacencyMatrix: Representing a Graph by a Matrix]
* {{cite web|first1=Brendan | last1=McKay |title=Description of graph6 and sparse6 encodings|url=http://cs.anu.edu.au/~bdm/data/formats.txt}}
* [http://www.cafemath.fr/mathblog/article.php?page=GoodWillHunting.php Café math : Adjacency Matrices of Graphs] : Application of the adjacency matrices to the computation generating series of walks.

{{DEFAULTSORT:Adjacency Matrix}}
[[Category:Algebraic graph theory]]
[[Category:Matrices]]
[[Category:Graph data structures]]</text>
      <sha1>9lk4yqxdz9919e4dj5li64zyb3tyrg9</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Graph-structured stack</title>
    <ns>0</ns>
    <id>672499</id>
    <revision>
      <id>543790277</id>
      <parentid>399640593</parentid>
      <timestamp>2013-03-13T07:48:54Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q3775803]]</comment>
      <text xml:space="preserve" bytes="1095">{{Unreferenced|auto=yes|date=December 2009}}

In [[computer science]], a '''graph-structured stack''' is a [[directed acyclic graph]] where each directed [[Path (graph theory)|path]] represents a [[Stack (data structure)|stack]].
The graph-structured stack is an essential part of [[GLR parser|Tomita's algorithm]], where it replaces the usual [[Stack (data structure)|stack]] of a [[pushdown automaton]]. This allows the algorithm to encode the nondeterministic choices in parsing an [[ambiguous grammar]], sometimes with greater efficiency. 

In the following diagram, there are four stacks: {7,3,1,0}, {7,4,1,0}, {7,5,2,0}, and {8,6,2,0}.
:[[Image:Graph-structured stack 1 - jaredwf.png|Graph-structured stack 1 - jaredwf.png]]

Another way to simulate nondeterminism would be to duplicate the stack as needed.  The duplication would be less efficient since vertices would not be shared.  For this example, 16 vertices would be needed instead of 9.
:[[Image:Stacks jaredwf.png|Stacks jaredwf.png]]

{{DEFAULTSORT:Graph-Structured Stack}}
[[Category:Graph data structures]]


{{Comp-sci-stub}}</text>
      <sha1>rjeve41w0mcugew9mphqj9v1nadwhmt</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Navigation mesh</title>
    <ns>0</ns>
    <id>2083415</id>
    <revision>
      <id>590972178</id>
      <parentid>587928609</parentid>
      <timestamp>2014-01-16T14:42:13Z</timestamp>
      <contributor>
        <ip>130.226.142.243</ip>
      </contributor>
      <comment>Just modified a line because hard to understand. Missing a comma and using an implied &quot;that&quot; was doing the sentence too complex to be understood, at least for a non English mother togue</comment>
      <text xml:space="preserve" bytes="1368">{{no footnotes|date=December 2012}}

A '''navigation mesh''', or '''navmesh''', is an [[abstract data structure|abstract]] [[data structure]] used in [[artificial intelligence]] applications to aid [[intelligent agent|agent]]s in [[search algorithm|path-finding]] through large spaces. Meshes that do not map to static obstacles in the environment that they model, offer the additional advantage that agents with access to the mesh will not consider these obstacles in path-finding, reducing computational effort and making [[collision detection]] between agents and static obstacles moot. Meshes are typically implemented as [[graph (data structure)|graph]]s, opening their use to a large number of [[tree search algorithm|algorithms]] defined on these structures.

One of the most common uses of a navigation mesh is in video games, to describe the paths that a computer-controlled character can follow. It is usually represented as a volume or [[brush (video game)|brush]] that is processed and computed during level compilation.

==References==
{{refbegin}}
*{{cite web|title=Navigation Mesh Reference|url=http://udn.epicgames.com/Three/NavigationMeshReference.html|accessdate=2012-12-16}}
{{refend}}

{{DEFAULTSORT:Navigation Mesh}}
[[Category:Graph data structures]]
[[Category:Video game development]]
[[Category:Computational physics]]


{{Datastructure-stub}}</text>
      <sha1>oeiz8zru6jplc1necnesqyga8xouhio</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Combinatorial map</title>
    <ns>0</ns>
    <id>21105530</id>
    <revision>
      <id>580573097</id>
      <parentid>558140298</parentid>
      <timestamp>2013-11-07T09:27:48Z</timestamp>
      <contributor>
        <username>Pierrekraemer</username>
        <id>16427784</id>
      </contributor>
      <text xml:space="preserve" bytes="7297">A '''combinatorial map''' is a combinatorial object modelling topological structures with subdivided objects. Historically, the concept was introduced informally by J. Edmonds for polyhedral surfaces &lt;ref&gt;Edmonds J., A Combinatorial Representation for Polyhedral Surfaces, Notices Amer. Math. Soc., vol. 7, 1960&lt;/ref&gt; which are [[planar graph]]s. It was given its first definite formal expression under the name &quot;Constellations&quot; by A. Jacques &lt;ref&gt;Jacques A., Constellations et Graphes Topologiques, Colloque Math. Soc. János Bolyai, p. 657-672, 1970&lt;/ref&gt; but the concept was already extensively used under the name &quot;rotation&quot; by [[Gerhard Ringel]]&lt;ref&gt;Ringel G., Map Color Theorem, Springer-Verlag, Berlin 1974&lt;/ref&gt; and J.W.T. Youngs in their famous solution of the Heawood map-coloring problem. The term &quot;constellation&quot; was not retained and instead &quot;combinatorial map&quot; was favored. The concept was later extended to represent higher-dimensional orientable subdivided objects. Combinatorial maps are used as efficient data structures in image representation and processing, in geometrical modeling. This model is related to [[simplicial complex]]es and to [[combinatorial topology]]. Note that combinatorial maps were extended to [[generalized maps]] that allow also to represent non-orientable objects like the [[Möbius strip]] and the [[Klein bottle]].  A combinatorial map is a [[boundary representation]] model; it represents object by its boundaries.

==Motivation==
Several applications require a data structure to represent the subdivision of an object. For example, a 2D object can be decomposed into vertices (0-cells), edges (1-cells), and faces (2-cells).  More generally, an n-dimensional object is composed with cells of dimension 0 to n. Moreover, it is also often necessary to represent neighboring relations between these cells.

Thus, we want to describe all the cells of the subdivision, plus all the incidence and adjacency relations between these cells.  When all the represented cells are simplexes, a [[simplicial complex]] can be used, but when we want to represent any type of cells, we need to use cellular topological model, like combinatorial maps or generalized maps.

==Planar graph representation==
The first works about combinatorial maps &lt;ref&gt;Jacques, A. Constellations et propriétés algébriques des graphes topologiques, Ph.D. thesis, Paris 1969&lt;/ref&gt;
&lt;ref&gt;Cori R., Un code pour les graphes planaires et ses applications, Astérisque, vol. 27, 1975&lt;/ref&gt;
develop combinatorial representations of graphs on surfaces which includes [[planar graph]]s: 
A 2-dimensional combinatorial map (or 2-map) is a triplet ''M''&amp;nbsp;=&amp;nbsp;(''D'',&amp;nbsp;''σ'',&amp;nbsp;''α'') such that:
* ''D'' is a finite set of darts;
* ''σ'' is a [[Permutation#Permutations in group theory|permutation]] on ''D'';
* ''α'' is an [[Involution (mathematics)|involution]] on ''D'' with no fixed point.

Intuitively, a 2-map corresponds to a planar graph where each edge is subdivided into two darts (sometimes also called half-edges).  The permutation ''σ'' gives, for each dart, the next dart by turning around the vertex in the positive orientation; the other permutation ''α'' gives, for each dart, the other dart of the same edge.

''α'' allows one to retrieve edges ('''a'''lpha for '''a'''rête in French), and ''σ'' allows one to retrieve vertices ('''s'''igma for '''s'''ommet in French).  We define ''φ''&amp;nbsp;=&amp;nbsp;''σ'' o ''α'' which gives, for each dart, the next dart of the same face ('''p'''hi for '''f'''ace also in French).

So, there are two ways to represent a combinatorial map depending if the permutation is ''σ'' or ''φ'' (see example below). These two representations are dual to each other: vertices and faces are exchanged.

&lt;center&gt;
{| border=&quot;0&quot;
|+'''Combinatorial maps example''': a plane graph and the two combinatorial maps depending if we use the notation (''D'',&amp;nbsp;''σ'',&amp;nbsp;''α'') or (''D'',&amp;nbsp;''φ'',&amp;nbsp;''α'').
|-
| valign=&quot;top&quot;|
[[Image:Combinatorial map planar graph example.svg|thumb|center|200px|A plane graph]]
| valign=&quot;top&quot;|
[[Image:Combinatorial map example.svg|thumb|center|200px|Corresponding combinatorial map (''D'',&amp;nbsp;''&amp;sigma;'',&amp;nbsp;''&amp;alpha;''). Darts are represented by numbered segments, ''&amp;sigma;'' by gray arrows (example ''&amp;sigma;''(1)=7), two darts linked by ''&amp;alpha;'' are drawn consecutively and separated by a small bar (example ''&amp;alpha;''(1)=2).]]
| valign=&quot;top&quot;|
[[Image:Combinatorial map dual example.svg|thumb|center|200px|Corresponding combinatorial map (''D'',&amp;nbsp;''&amp;phi;'',&amp;nbsp;''&amp;alpha;''). Darts are represented by numbered arrows, two darts linked by ''&amp;phi;'' are drawn consecutively (example ''&amp;phi;''(1)=3) and two darts linked by ''&amp;alpha;'' are drawn parallel and in reverse orientation (example ''&amp;alpha;''(1)=2.]]
|}
&lt;/center&gt;

==General definition==
The definition of combinatorial map in any dimension is given in &lt;ref&gt;Lienhardt P., Topological models for Boundary Representation : a comparison with ''n''-dimensional generalized maps, ''Computer-Aided Design'', Vol. 23, no.1, pp. 59-82, 1991&lt;/ref&gt; and:&lt;ref&gt;Lienhardt P., N-dimensional generalized combinatorial maps and cellular quasi-manifolds, International Journal on Computational Geometry and Applications, Vol. 4, no. 3, pp. 275-324, 1994&lt;/ref&gt;

An ''n''-dimensional combinatorial map (or ''n''-map) is a (''n''&amp;nbsp;+&amp;nbsp;1)-tuple ''M''&amp;nbsp;=&amp;nbsp;(''D'',&amp;nbsp;''β''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''β''&lt;sub&gt;''n''&lt;/sub&gt;) such that:
* ''D'' is a finite set of darts;
* ''β&lt;sub&gt;1&lt;/sub&gt; is a [[Permutation#Permutations in group theory|permutation]] on ''D'';
* ''β&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''β''&lt;sub&gt;''n''&lt;/sub&gt; are [[Involution (mathematics)|involutions]] on ''D'';
* ''β''&lt;sub&gt;''i''&lt;/sub&gt;&amp;nbsp;o&amp;nbsp;''β''&lt;sub&gt;''j''&lt;/sub&gt; is an involution if ''i''&amp;nbsp;+&amp;nbsp;2&amp;nbsp;≤&amp;nbsp;''j'' (''i'',&amp;nbsp;''j'' ∈ {&amp;nbsp;1,&amp;nbsp;,...,&amp;nbsp;''n''&amp;nbsp;}).

An ''n''-dimensional combinatorial map represents the subdivision of a closed orientable ''n''-dimensional space.  A dart is an abstract element which is only required to define one-to-one mappings.  The last line of this definition fixes constraints which guarantee the topological validity of the represented object: a combinatorial map represents a quasi-manifold subdivision. The initial definition of 2-dimensional combinatorial maps can be retrieved by fixing ''n''&amp;nbsp;=&amp;nbsp;2 and renaming ''σ'' by ''β&lt;sub&gt;1&lt;/sub&gt; and ''α'' by ''β&lt;sub&gt;2&lt;/sub&gt;.

==See also==
* [[Boundary representation]]
* [[Generalized maps]]
* [[Quad-edge data structure]]
* [[Rotation system]]
* [[Simplicial complex]]
* [[Winged edge]]

==References==
&lt;references /&gt;

== External links ==
* Combinatorial maps in [[CGAL]], the Computational Geometry Algorithms Library:
** {{cite web
 | last = Damiand | first = Guillaume
 | title = Combinatorial maps
 | url = http://www.cgal.org/Pkg/CombinatorialMaps
 | accessdate = October 2011
}}

* Combinatorial maps in [http://cgogn.unistra.fr CGoGN], '''C'''ombinatorial and '''G'''eometric m'''o'''deling with '''G'''eneric '''N'''-dimensional '''M'''aps

[[Category:Algebraic topology]]
[[Category:Topological graph theory]]
[[Category:Computer graphics data structures|Data structures]]
[[Category:Graph data structures|Graphics]]</text>
      <sha1>rn8qqqn4dgpogs72urwkk0k7wq0nsja</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>GADDAG</title>
    <ns>0</ns>
    <id>28134188</id>
    <revision>
      <id>535022000</id>
      <parentid>495631902</parentid>
      <timestamp>2013-01-26T18:36:45Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>fixed header names + [[WP:GENFIXES|general fixes]] using [[Project:AWB|AWB]] (8863)</comment>
      <text xml:space="preserve" bytes="3771">A '''GADDAG''' is a [[data structure]] presented by Steven Gordon in 1994, for use in generating moves for [[Scrabble]] and other word-generation games where such moves require words that &quot;hook into&quot; existing words.  It is often in contrast to move-generation algorithms using a [[DAWG|Directed Acyclic Word Graph (DAWG)]], such as the one used by [[Maven (Scrabble)|Maven]].  It is generally twice as fast as the traditional DAWG algorithms, but take about 5 times as much space for regulation Scrabble dictionaries.&lt;ref name = Gordon&gt;{{cite journal|last=Gordon|first=Steven|title=A Faster Scrabble Move Generation Algorithm|year=1994}}&lt;/ref&gt;

[http://people.csail.mit.edu/jasonkb/quackle/ Quackle] uses a GADDAG to generate moves.

==Description==
A GADDAG is a specialization of a [[Trie]], containing states and branches to other GADDAGs.  It is distinct for its storage of every reversed prefix of every word in a dictionary.  This means every word has as many representations as it does letters;  since the average word in most Scrabble regulation dictionaries is 5 letters long, this makes the GADDAG about 5 times as big as a simple [[DAWG]].

===Definition===
For any word in a dictionary that is formed by a non-empty prefix ''x'' and a suffix ''y,'' a GADDAG contains a direct, deterministic path for any string '''REV'''(''x'')৳''y'', where ৳ is a concatenation operator.

For example, for the word &quot;''explain'',&quot; a GADDAG will contain direct paths to the strings &quot;''e৳xplain'',&quot; &quot;''xe৳plain'',&quot; &quot;''pxe৳lain'',&quot; &quot;''lpxe৳ain'',&quot; &quot;''alpxe৳in'',&quot; &quot;''ialpxe৳n'',&quot; and &quot;''nialpxe''.&quot;

===Use in Move Generation===
Any move-generation algorithm must adhere to three types of constraints:

* '''Board constraints''':  You may only build by 'hooking' onto existing letters of the board.  Additionally, you may only place tiles on empty squares.
* '''Rack constraints''': You may only place tiles with letters on your rack.
* '''Dictionary constraint''': All words resulting from the placement of tiles exist in the game's dictionary.

The DAWG algorithms speed up and take advantage of the second and third constraint:  the DAWG is built around the dictionary, and you only traverse it using tiles from your rack.  It fails, however, to address the first constraint:  supposing you want to 'hook into' the letter '''P''' in '''HARPY''', and the board has 2 spaces before the P, you must search the dictionary for all words containing letters from your rack where the third letter is '''P'''.  This is non-deterministic when searching through the DAWG, as many searches through the trie will be fruitless.

This is addressed by the GADDAG's storage of prefixes:  by traversing the '''P''' branch of a GADDAG, you see all words that have a '''P''' somewhere in their composition, and can &quot;travel up&quot; the prefix to form the word with tiles in your rack.  To use the example from the definition section, to hook onto the '''P''', you will immediately see a path for ''pxe৳lain''.  You add tiles from your rack while appropriate, traveling backwards through the word until you encounter the ৳, meaning you've completed the prefix.  You complete the move by adding to the front of the word with the suffix.

==See also==
* [[Directed acyclic word graph]] (DAWG)
* [[Suffix tree|Suffix Tree]]
* [[Trie]]
* [[Scrabble]]
* [[Prefix Hash Tree]]

==References==
{{Reflist}}

==External links==
* [http://www.ericsink.com/downloads/faster-scrabble-gordon.pdf Steven Gordon's 1993 Paper describing the GADDAG.]
* [http://people.csail.mit.edu/jasonkb/quackle/doc/how_quackle_plays_scrabble.html Mention of GADDAG in Quackle.]

[[Category:Scrabble]]
[[Category:Graph data structures]]
[[Category:String data structures]]
[[Category:Game artificial intelligence]]</text>
      <sha1>ict7thpitha6tyyfhpvjs07epccwmhv</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Zero-suppressed decision diagram</title>
    <ns>0</ns>
    <id>4057707</id>
    <revision>
      <id>618984105</id>
      <parentid>618981962</parentid>
      <timestamp>2014-07-29T15:18:50Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <text xml:space="preserve" bytes="2541">A '''zero-suppressed decision diagram''' ('''ZSDD''' or '''ZDD''') is a type of [[binary decision diagram]] (BDD) where instead of nodes being introduced when the positive and the negative part are different, they are introduced when negative part is different from constant 0. A [[Zero suppression|zero-suppressed]] decision diagram is also commonly referred to as a '''zero-suppressed binary decision diagram''' (ZBDD).

They are useful when dealing with functions that are almost everywhere&amp;nbsp;0.

In a 2011 talk &quot;All Questions Answered&quot;,&lt;ref&gt;{{cite web|title=&quot;All Questions Answered&quot; by Donald Knuth|url=https://www.youtube.com/watch?v=xLBvCB2kr4Q|work=YouTube.com|accessdate=12 June 2013}}&lt;/ref&gt; [[Donald Knuth]] referred to ZDD as the most beautiful construct in computer science.

In ''[[The Art of Computer Programming]]'', volume 4, Knuth introduces his [[Knuth's Simpath algorithm|Simpath algorithm]] for constructing a ZDD representing all simple paths between two vertices in a graph.

== Available packages==
* [http://vlsi.colorado.edu/~fabio/CUDD/ CUDD]: A BDD package written in C that implements BDDs and ZBDDs, University of Colorado, Boulder
* [http://javaddlib.sourceforge.net/jdd/ JDD], A java library that implements common BDD and ZBDD operations
* [https://github.com/takemaru/graphillion Graphillion], A ZDD software implementation based on Python 

== References ==
&lt;references/&gt;
* Shin-ichi Minato, &quot;[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1600231 Zero-suppressed BDDs for set manipulation in combinatorial problems]&quot;, DAC '93: Proceedings of the 30th international conference on Design automation, 1993
* Ch. Meinel, T. Theobald, &quot;[http://www.hpi.uni-potsdam.de/fileadmin/hpi/FG_ITS/books/OBDD-Book.pdf Algorithms and Data Structures in VLSI-Design: OBDD – Foundations and Applications&quot;], Springer-Verlag, Berlin, Heidelberg, New York, 1998.

==External links==
* Alan Mishchenko, [http://www.eecs.berkeley.edu/~alanmi/publications/2001/tech01_zdd.pdf An Introduction to Zero-Suppressed Binary Decision Diagrams]
* [[Donald Knuth]], [http://myvideos.stanford.edu/player/slplayer.aspx?coll=ea60314a-53b3-4be2-8552-dcf190ca0c0b&amp;co=af52aca1-9c60-4a7b-a10b-0e543f4f3451&amp;o=true Fun With Zero-Suppressed Binary Decision Diagrams (ZDDs)] (video lecture, 2008)
* [[Minato Shin-ichi]], [https://www.youtube.com/watch?v=Q4gTV4r0zRs Counting paths in graphs (fundamentals of ZDD)] (video illustration produced on Miraikan)
{{DEFAULTSORT:Zero-Suppressed Decision Diagram}}
[[Category:Graph data structures]]</text>
      <sha1>q4qq9ooj2bo1ajsqoh2i73dhkzllbqc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Incidence matrix</title>
    <ns>0</ns>
    <id>420919</id>
    <revision>
      <id>622332654</id>
      <parentid>622332563</parentid>
      <timestamp>2014-08-22T12:49:22Z</timestamp>
      <contributor>
        <username>Prashanth4321</username>
        <id>22292500</id>
      </contributor>
      <minor/>
      <text xml:space="preserve" bytes="7077">In [[mathematics]], an '''incidence matrix''' is a [[matrix (mathematics)|matrix]] that shows the relationship between two classes of objects.  If the first class is ''X'' and the second is ''Y'', the matrix has one row for each element of ''X'' and one column for each element of ''Y''.  The entry in row ''x'' and column ''y'' is 1 if ''x'' and ''y'' are related (called '''incident''' in this context) and 0 if they are not. There are variations; see below.

==Graph theory==

Incidence matrices are mostly used in [[graph theory]].

===Undirected and directed graphs===
[[Image:Labeled_undirected_graph.svg|thumb|250px|An undirected graph]]
In [[graph theory]] an [[undirected graph]] ''G'' has two kinds of incidence matrices: unoriented and oriented. 
The '''incidence matrix''' (or '''unoriented incidence matrix''') of ''G'' is a ''n'' &amp;times; ''m'' [[matrix (math)|matrix]] &lt;math&gt;(b_{ij})&lt;/math&gt;, where ''n'' and ''m'' are the numbers of [[vertex (graph theory)|vertices]] and [[Edge (graph theory)|edge]]s respectively, such that &lt;math&gt;b_{ij} = 1&lt;/math&gt; if the vertex &lt;math&gt;v_i&lt;/math&gt; and edge &lt;math&gt;x_j&lt;/math&gt; are incident and 0 otherwise. 

For example the incidence matrix of the undirected graph shown on the right is a matrix consisting of 4 rows (corresponding to the four vertices, 1-4) and 4 columns (corresponding to the four edges, e1-e4):
:&lt;math&gt;
\begin{pmatrix}
  1 &amp; 1 &amp; 1 &amp; 0 \\
  1 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 1 &amp; 0 &amp; 1 \\
  0 &amp; 0 &amp; 1 &amp; 1 \\
\end{pmatrix}
&lt;/math&gt;
If we look at the incidence matrix, we see that the sum of each column is equal to 2. This is because each edge has a vertex connected to each end.

The '''incidence matrix''' of a [[directed graph]] ''D'' is a ''n'' &amp;times; ''m'' matrix &lt;math&gt;[b_{ij}]&lt;/math&gt; where ''n'' and ''m'' are the number of vertices and edges respectively, such that &lt;math&gt;b_{ij} = -1&lt;/math&gt; if the edge &lt;math&gt;x_j&lt;/math&gt; leaves vertex &lt;math&gt;v_i&lt;/math&gt;, &lt;math&gt;1&lt;/math&gt; if it enters vertex &lt;math&gt;v_i&lt;/math&gt; and 0 otherwise (Note that many authors use the opposite sign convention.). 

An '''oriented incidence matrix''' of an undirected graph ''G'' is the incidence matrix, in the sense of directed graphs, of any [[Orientation (graph theory)|orientation]] of ''G''.  That is, in the column of edge ''e'', there is one +1 in the row corresponding to one vertex of ''e'' and one −1 in the row corresponding to the other vertex of ''e'', and all other rows have 0.  All oriented incidence matrices of ''G'' differ only by negating some set of columns.  In many uses, this is an insignificant difference, so one can speak of ''the'' oriented incidence matrix, even though that is technically incorrect.

The oriented or unoriented incidence matrix of a graph ''G'' is related to the [[adjacency matrix]] of its [[line graph]] ''L''(''G'') by the following theorem:

:&lt;math&gt;
A(L(G)) = B(G)^{T}B(G) - 2I_m\ 
&lt;/math&gt;

where &lt;math&gt;A(L(G))&lt;/math&gt; is the adjacency matrix of the line graph of ''G'', ''B''(''G'') is the incidence matrix, and &lt;math&gt;I_m&lt;/math&gt; is the [[identity matrix]] of dimension m.

The discrete [[Kirchhoff matrix|Laplacian]] (or Kirchhoff matrix) is obtained from the oriented incidence matrix ''M''(''G'') by the formula

:&lt;math&gt;
M(G) M(G)^{T}. 
&lt;/math&gt; 

The integral [[cycle space]] of a graph is equal to the [[null space]] of its oriented incidence matrix, viewed as a matrix over the [[integers]] or [[real numbers|real]] or [[complex numbers]].  The binary cycle space is the null space of its oriented or unoriented incidence matrix, viewed as a matrix over the two-element [[field (mathematics)|field]].

===Signed and bidirected graphs===

The incidence matrix of a [[signed graph]] is a generalization of the oriented incidence matrix.  It is the incidence matrix of any [[bidirected graph]] that orients the given signed graph.  The column of a positive edge has a +1 in the row corresponding to one endpoint and a −1 in the row corresponding to the other endpoint, just like an edge in an ordinary (unsigned) graph.  The column of a negative edge has either a +1 or a −1 in both rows.  The line graph and Kirchhoff matrix properties generalize to signed graphs.

===Multigraphs===

The definitions of incidence matrix apply to graphs with [[loop (graph theory)|loops]] and [[multiple edges]].  The column of an oriented incidence matrix that corresponds to a loop is all zero, unless the graph is signed and the loop is negative; then the column is all zero except for ±2 in the row of its incident vertex.

===Hypergraphs===
Because the edges of ordinary graphs can only have two vertices (one at each end), the column of an incidence matrix for graphs can only have two non-zero entries. By contrast, a [[hypergraph]] can have multiple vertices assigned to one edge; thus, a general matrix of non-negative integers describes a hypergraph.

==Incidence structures==

The '''incidence matrix''' of an [[incidence structure]] ''C'' is a ''p'' &amp;times; ''q'' matrix &lt;math&gt;[b_{ij}]&lt;/math&gt;, where ''p'' and ''q'' are the number of '''points''' and '''lines''' respectively, such that &lt;math&gt;b_{ij} = 1&lt;/math&gt; if the point &lt;math&gt;p_i&lt;/math&gt; and line &lt;math&gt;L_j&lt;/math&gt; are incident and 0 otherwise. In this case the incidence matrix is also a [[biadjacency matrix]] of the [[Levi graph]] of the structure. As there is a [[hypergraph]] for every Levi graph, and ''vice-versa'', the incidence matrix of an incidence structure describes a hypergraph.

===Finite geometries===

An important example is a [[finite geometry]].  For instance, in a finite plane, ''X'' is the set of points and ''Y'' is the set of lines.  In a finite geometry of higher dimension, ''X'' could be the set of points and ''Y'' could be the set of subspaces of dimension one less than the dimension of ''Y''; or ''X'' could be the set of all subspaces of one dimension ''d'' and ''Y'' the set of all subspaces of another dimension ''e''.

===Block designs===

Another example is a [[block design]].  Here ''X'' is a finite set of &quot;points&quot; and ''Y'' is a class of subsets of ''X'', called &quot;blocks&quot;, subject to rules that depend on the type of design.  The incidence matrix is an important tool in the theory of block designs.  For instance, it is used to prove the fundamental theorem of symmetric 2-designs, that the number of blocks equals the number of points.

==References==
*{{citation | last=Diestel | first=Reinhard | title=Graph Theory | series=Graduate Texts in Mathematics | volume=173 | edition=3rd | date=2005 | publisher=Springer-Verlag | isbn=3-540-26183-4}}.
* [[Coxeter|Coxeter, H.S.M.]] ''[[Regular Polytopes (book)|Regular Polytopes]]'', (3rd edition, 1973), Dover edition, ISBN 0-486-61480-8 (Section 9.2 Incidence matrices, pp. 166-171)
* Jonathan L Gross, Jay Yellen, ''Graph Theory and its applications'', second edition, 2006 (p 97, Incidence Matrices for undirect graphs; p 98, incidence matrices for digraphs)
*{{mathworld | urlname = IncidenceMatrix  | title = Incidence matrix }}

[[Category:Algebraic graph theory]]
[[Category:Combinatorics]]
[[Category:Matrices]]
[[Category:Graph data structures]]</text>
      <sha1>o7680g6lih70gwwo1gd80pg4p1nj8nz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Skip graph</title>
    <ns>0</ns>
    <id>31169226</id>
    <revision>
      <id>618257664</id>
      <parentid>560126892</parentid>
      <timestamp>2014-07-24T11:08:01Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>Task 5: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <text xml:space="preserve" bytes="1873">'''Skip graphs''' are a kind of distributed data structure based on [[skip list]]s.  They have the full functionality of a [[balanced tree]] in a [[distributed system]].  Skip graphs are mostly used in searching [[peer-to-peer network]]s. As they provide the ability to [[query]] by [[key ordering]], they improve other search tools based on the [[hash table]] functionality only. In contrast to [[skip list]]s and other [[tree (data structure)|tree data structures]], they are very resilient and can tolerate a large fraction of [[Node (graph theory)|node]] fails. Also, constructing, inserting, searching and repairing a skip graph that was disturbed by failing nodes can be done by simple and straightforward algorithms.&lt;ref&gt;{{cite web
| location = http://www.cs.yale.edu/
| publisher = Computer Science&amp;nbsp;– Yale University
| author = James Aspnes
|author2=Gauri Shah
 | title = Skip Graphs
| url = http://www.cs.yale.edu/homes/aspnes/papers/skip-graphs-journal.pdf
| quote = Skip graphs are a novel distributed data structure, based on skip lists, that provide the full functionality of a balanced tree in a distributed system where resources are stored in separate nodes that may fail at any time. They are designed for use in searching peer-to-peer systems, and by providing the ability to perform queries based on key ordering, they improve on existing search tools that provide only hash table functionality. Unlike skip lists or other tree data structures, skip graphs are highly resilient, tolerating a large fraction of failed nodes without losing connectivity. In addition, simple and straightforward algorithms can be used to construct a skip graph, insert new nodes into it, search it, and detect and repair errors in a skip graph introduced due to node failures.}}&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Graph data structures]]

{{computer-stub}}</text>
      <sha1>1ivz6b5uzcf85bz5o4cdnuhwyknozg9</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Directed acyclic word graph</title>
    <ns>0</ns>
    <id>38022094</id>
    <revision>
      <id>611991366</id>
      <parentid>609722361</parentid>
      <timestamp>2014-06-07T20:31:26Z</timestamp>
      <contributor>
        <username>RussBot</username>
        <id>279219</id>
      </contributor>
      <minor/>
      <comment>Robot: Editing intentional link to disambiguation page in hatnote per [[WP:INTDABLINK]] [[User:RussBot#About the hatnote task|(explanation)]]</comment>
      <text xml:space="preserve" bytes="2492">{{Redirect|DAWG|other uses of this letter group|Dawg (disambiguation){{!}}Dawg}}
{{for|the US Department of Defense review panel|Deputy's Advisory Working Group}}
{{for|the dictionary data structure that is sometimes called a DAWG|Deterministic acyclic finite state automaton}}

In [[computer science]], a '''directed acyclic word graph''' ('''DAWG''') is a data structure that represents the set of suffixes of a string. As its name implies, a DAWG takes the form of a [[directed acyclic graph]].

==See also==
* [[GADDAG]]
* [[Suffix array]]
* [[Suffix tree]]

==References==
*{{citation | doi=10.1109/SPIRE.2001.989743|last1= Inenaga|first1= S.|last2=    Hoshino|first2=H.|last3=    Shinohara|first3= A.  |last4=  Takeda|first4= M. |last5=   Arikawa|first5= S. |contribution= On-line construction of symmetric compact directed acyclic word graphs|title=Proc. 8th Int. Symp. String Processing and Information Retrieval, 2001. SPIRE 2001|year=2001|pages=96–110|url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=989743|isbn= 0-7695-1192-9}}.
*{{citation | first1=Maxime | last1=Crochemore | first2=Renaud | last2=Vérin | contribution=Direct construction of compact directed acyclic word graphs | series=Lecture Notes in Computer Science | publisher=Springer-Verlag | title=Combinatorial Pattern Matching | year=1997 | pages=116–129 | doi=10.1007/3-540-63220-4_55 }}.
*{{citation | last1=Epifanio | first1=Chiara | last2=Mignosi | first2=Filippo | last3=Shallit | first3=Jeffrey | last4=Venturini | first4=Ilaria | chapter=Sturmian graphs and a conjecture of Moser | pages=175–187 | editor1-last=Calude | editor1-first=Cristian S. | editor2-last=Calude | editor2-first=Elena | editor3-last=Dineen | editor3-first=Michael J. | title=Developments in language theory.  Proceedings, 8th international conference (DLT 2004), Auckland, New Zealand, December 2004 | year=2004 | publisher=[[Springer-Verlag]] | series=Lecture Notes in Computer Science | volume=3340 | isbn=3-540-24014-4 | zbl=1117.68454 }}
*{{citation | first1=H.H. | last1=Do | first2=W.K. | last2=Sung | contribution=Compressed Directed Acyclic Word Graph with Application in Local Alignment | series=Lecture Notes in Computer Science | publisher=[[Springer-Verlag]] | title=Computing and Combinatorics | volume=6842 | pages=503–518 | doi=10.1007/978-3-642-22685-4_44 | year=2011 | isbn=978-3-642-22684-7 }}

{{Data structures}}

[[Category:Graph data structures]]
[[Category:String data structures]]

{{compu-stub}}</text>
      <sha1>b07c0gru5nes1kghab2y5t55fejj18h</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cause–effect graph</title>
    <ns>0</ns>
    <id>9707263</id>
    <revision>
      <id>606271859</id>
      <parentid>587242763</parentid>
      <timestamp>2014-04-29T02:32:21Z</timestamp>
      <contributor>
        <username>Rich Farmbrough</username>
        <id>82835</id>
      </contributor>
      <text xml:space="preserve" bytes="2280">In [[software testing]], a '''cause–effect graph''' is a [[directed graph]] that maps a set of causes to a set of effects.  The causes may be thought of as the input to the program, and the effects may be thought of as the output.  Usually the graph shows the nodes representing the causes on the left side and the nodes representing the effects on the right side.  There may be intermediate nodes in between that combine inputs using logical operators such as AND and OR.

Constraints may be added to the causes and effects.  These are represented as edges labeled with the constraint symbol using a dashed line.  For causes, valid constraint symbols are E (exclusive), O (one and only one), I (at least one), and R (Requires). The exclusive constraint states that at most one of the causes 1 and 2 can be true, i.e. both cannot be true simultaneously. The Inclusive (at least one) constraint states that at least one of the causes 1, 2 or 3 must be true, i.e. all cannot be false simultaneously. The one and only one (OaOO or simply O) constraint states that only one of the causes 1, 2 or 3 can be true. The Requires constraint states that if cause 1 is true, then cause 2 must be true, and it is impossible for 1 to be true and 2 to be false.

For effects, valid constraint symbol is M (Mask). The mask constraint states that if effect 1 is true then effect 2 is false. Note that the mask constraint relates to the effects and not the causes like the other constraints. 

The graph's direction is as follows:

 Causes --&gt; intermediate nodes --&gt; Effects

The graph can always be rearranged so there is only one node between any input and any output.  See [[conjunctive normal form]] and [[disjunctive normal form]].

A cause–effect graph is useful for generating a reduced [[decision table]].

== See also ==
* [[Causal diagram]]
* [[Decision table]]
* [[Why–because graph]]

== Further reading ==
{{Refbegin}}
* {{cite book |first=Glenford J. |last=Myers |title=The Art of Software Testing |isbn=0-471-04328-1 |publisher=John Wiley &amp; Sons |year=1979}}
{{Refend}}

{{DEFAULTSORT:Cause-effect graph}}
[[Category:Causal diagrams]]
[[Category:Graph data structures]]
[[Category:Decision theory]]
[[Category:Software testing]]
[[Category:Directed graphs]]


{{soft-eng-stub}}</text>
      <sha1>c50bcku370xvp4cowlnanhy94x0rjf9</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Graph (abstract data type)</title>
    <ns>0</ns>
    <id>557931</id>
    <revision>
      <id>626561640</id>
      <parentid>624764352</parentid>
      <timestamp>2014-09-22T02:26:25Z</timestamp>
      <contributor>
        <ip>98.156.88.11</ip>
      </contributor>
      <comment>Spelling change and added some information.</comment>
      <text xml:space="preserve" bytes="6003">[[Image:6n-graf.svg|thumb|250px|A labeled graph of 6 vertices and 7 edges.]]
{{refimprove|date=October 2010}}
In [[computer science]], a '''graph''' is an [[abstract data type]] that is meant to implement the [[graph (mathematics)|graph]] and [[hypergraph]] concepts from [[mathematics]].

A graph data structure consists of a finite (and possibly mutable) [[set (computer science)|set]] of ordered pairs, called '''edges''' or '''arcs''', of certain entities called '''nodes''' or '''vertices'''. As in mathematics, an edge (''x'',''y'') is said to '''point''' or '''go''' '''from''' ''x'' '''to''' ''y''. The nodes may be part of the graph structure, or may be external entities represented by integer indices or [[reference (computer science)|references]].

A graph data structure may also associate to each edge some '''edge value''', such as a symbolic label or a numeric attribute (cost, capacity, length, etc.).

==Algorithms==
Graph algorithms are a significant field of interest within computer science. Typical higher-level operations associated with graphs are: finding a path between two nodes, like [[depth-first search]] and [[breadth-first search]] and finding the shortest path from one node to another, like [[Dijkstra's algorithm]]. A solution to finding the shortest path from each node to every other node also exists in the form of the [[Floyd–Warshall algorithm]].

==Operations==
The basic operations provided by a graph data structure ''G'' usually include:
* &lt;code&gt;adjacent&lt;/code&gt;(''G'', ''x'', ''y''): tests whether there is an edge from node ''x'' to node ''y''.
* &lt;code&gt;neighbors&lt;/code&gt;(''G'', ''x''): lists all nodes ''y'' such that there is an edge from ''x'' to ''y''.
* &lt;code&gt;add&lt;/code&gt;(''G'', ''x'', ''y''): adds to ''G'' the edge from ''x'' to ''y'', if it is not there.
* &lt;code&gt;delete&lt;/code&gt;(''G'', ''x'', ''y''): removes the edge from ''x'' to ''y'', if it is there.
* &lt;code&gt;get_node_value&lt;/code&gt;(''G'', ''x''): returns the value associated with the node ''x''.
* &lt;code&gt;set_node_value&lt;/code&gt;(''G'', ''x'', ''a''): sets the value associated with the node ''x'' to ''a''.

Structures that associate values to the edges usually also provide:
* &lt;code&gt;get_edge_value&lt;/code&gt;(''G'', ''x'', ''y''): returns the value associated to the edge (''x'',''y'').
* &lt;code&gt;set_edge_value&lt;/code&gt;(''G'', ''x'', ''y'', ''v''): sets the value associated to the edge (''x'',''y'') to ''v''.

==Representations==
Different data structures for the representation of graphs are used in practice:
; [[Adjacency list]] : Vertices are stored as records or objects, and every vertex stores a [[list (computing)|list]] of adjacent vertices. This data structure allows the storage of additional data on the vertices. Additional data can be stored if edges are also stored as objects, in which case each vertex stores its incident edges and each edge stores its incident vertices.
; [[Adjacency matrix]] : A two-dimensional matrix, in which the rows represent source vertices and columns represent destination vertices. Data on edges and vertices must be stored externally. Only the cost for one edge can be stored between each pair of vertices.
; [[Incidence matrix]] : A two-dimensional Boolean matrix, in which the rows represent the vertices and columns represent the edges. The entries indicate whether the vertex at a row is incident to the edge at a column.

The following table gives the [[time complexity]] cost of performing various operations on graphs, for each of these representations.{{Citation needed|reason=Reliable source needed for the entire table below.|date=November 2011}} In the matrix representations, the entries encode the cost of following an edge. The cost of edges that are not present are assumed to be &lt;math&gt; \infty &lt;/math&gt;.

{| class=&quot;wikitable&quot;
|-
!
! Adjacency list
! Adjacency matrix
! Incidence matrix
|-
| Storage
| &lt;math&gt; O(|V|+|E|) &lt;/math&gt;
| &lt;math&gt; O(|V|^2) &lt;/math&gt;
| &lt;math&gt; O(|V|\cdot|E|) &lt;/math&gt;
|-
| Add vertex
| &lt;math&gt; O(1) &lt;/math&gt;
| &lt;math&gt; O(|V|^2) &lt;/math&gt;
| &lt;math&gt; O(|V|\cdot|E|) &lt;/math&gt;
|-
| Add edge
| &lt;math&gt; O(1) &lt;/math&gt;
| &lt;math&gt; O(1) &lt;/math&gt;
| &lt;math&gt; O(|V|\cdot|E|) &lt;/math&gt;
|-
| Remove vertex
| &lt;math&gt; O(|E|) &lt;/math&gt;
| &lt;math&gt; O(|V|^2) &lt;/math&gt;
| &lt;math&gt; O(|V|\cdot|E|) &lt;/math&gt;
|-
| Remove edge
| &lt;math&gt; O(|E|) &lt;/math&gt;
| &lt;math&gt; O(1) &lt;/math&gt;
| &lt;math&gt; O(|V|\cdot|E|) &lt;/math&gt;
|-
| Query: are vertices u, v adjacent? (Assuming that the storage positions for u, v are known)
| &lt;math&gt; O(|V|) &lt;/math&gt;
| &lt;math&gt; O(1) &lt;/math&gt;
| &lt;math&gt; O(|E|) &lt;/math&gt;
|-
| Remarks
| When removing edges or vertices, need to find all vertices or edges
| Slow for add/remove vertices, because matrix must be resized/copied
| Slow to add or remove vertices and edges, because matrix must be resized/copied
|}

Adjacency lists are generally preferred because they efficiently represent [[sparse graph]]s. An adjacency matrix is preferred if the graph is dense, that is the number of edges |''E''| is close to the number of vertices squared, |''V''|&lt;sup&gt;2&lt;/sup&gt;, or if one must be able to quickly look up if there is an edge connecting two vertices.&lt;ref&gt;Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). Introduction to Algorithms (2nd ed.). MIT Press and McGraw–Hill. ISBN 0-262-53196-8.&lt;/ref&gt;

==See also==
* [[Graph traversal]] for graph walking strategies
* [[Graph database]] for graph (data structure) persistency
* [[Graph rewriting]] for rule based transformations of graphs (graph data structures)
* [[GraphStream]]
* [[Graphviz]]
* [[yEd]] Graph Editor – Java-based diagram editor for creating and editing graphs

==References==
{{Reflist}}

==External links==
* [http://www.boost.org/libs/graph Boost Graph Library: a powerful C++ graph library]
* [http://networkx.github.com/ Networkx: a Python graph library]

{{Data structures}}

&lt;!--Categories--&gt;
{{DEFAULTSORT:Graph (Abstract Data Type)}}
[[Category:Graph theory]]
[[Category:Graph data structures| ]]
[[Category:Abstract data types]]
[[Category:Graphs]]
[[Category:Hypergraphs]]</text>
      <sha1>dkbku9ifqm07gc7birliynibltd3jot</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Merkle tree</title>
    <ns>0</ns>
    <id>2497388</id>
    <revision>
      <id>626662691</id>
      <parentid>619727486</parentid>
      <timestamp>2014-09-22T19:06:16Z</timestamp>
      <contributor>
        <ip>68.203.10.93</ip>
      </contributor>
      <comment>/* Overview */</comment>
      <text xml:space="preserve" bytes="8421">[[Image:Hash Tree.svg|thumb|300px|right|A binary hash tree]]

In [[cryptography]] and [[computer science]] a '''hash tree''' or '''Merkle tree''' is a tree in which every non-leaf node is labelled with the [[Hash function|hash]] of the labels of its children nodes. Hash trees are useful because they allow efficient and secure verification of the contents of larger data structures.  Hash trees are a generalization of [[hash list]]s and [[hash chain]]s.

Demonstrating that a leaf node is a part of the given hash tree requires processing an amount of data proportional to the [[logarithm]] of the number of nodes of the tree;&lt;ref&gt;{{cite web
 | url = http://www.emsec.rub.de/media/crypto/attachments/files/2011/04/becker_1.pdf
 | title = Merkle Signature Schemes, Merkle Trees and Their Cryptanalysis
 | date = 2008-07-18 | accessdate = 2013-11-20
 | format = PDF | page = 16
 | author = Georg Becker | publisher = Ruhr-Universit ̈at Bochum
}}&lt;/ref&gt; this contrasts with hash lists, where the amount is proportional to the number of nodes. The concept is named after [[Ralph Merkle]].

== Uses ==
Hash trees can be used to verify any kind of data stored, handled and transferred in and between computers. Currently the main use of hash trees is to make sure that data blocks received from other peers in a [[Peer-to-peer|peer-to-peer network]] are received undamaged and unaltered, and even to check that the other peers do not lie and send fake blocks. Suggestions have been made to use hash trees in [[trusted computing]] systems.&lt;ref&gt;J. Kilian. Improved efﬁcient arguments (preliminary version). In CRYPTO, 1995.&lt;/ref&gt; Hash Trees are used in the [[ZFS]] file system,&lt;ref&gt;Jeff Bonwick's Blog ''[https://blogs.oracle.com/bonwick/entry/zfs_end_to_end_data ZFS End-to-End Data Integrity]''&lt;/ref&gt; [[BitTorrent]] protocol, [[Apache Wave]] protocol,&lt;ref&gt;Google Wave Federation Protocol ''[http://www.waveprotocol.org/protocol/whitepapers/wave-protocol-verification Wave Protocol Verification Paper'']&lt;/ref&gt; [[Git (software)|Git]] distributed revision control system, the [[Tahoe-LAFS]] backup system, the [[Bitcoin]] peer-to-peer network, and a number of [[NoSQL]] systems like [[Apache Cassandra]] and [[Riak]].&lt;ref&gt;&quot;When a replica is down for an extended period of time, or the machine storing hinted handoffs for an unavailable replica goes down as well, replicas must synchronize from one another. In this case, Cassandra and Riak implement a Dynamo-inspired process called anti-entropy. In anti-entropy, replicas exchange Merkle trees to identify parts of their replicated key ranges which are out of sync. A Merkle tree is a hierarchical hash verification: if the hash over the entire keyspace is not the same between two replicas, they will exchange hashes of smaller and smaller portions of the replicated keyspace until the out-of-sync keys are identified. This approach reduces unnecessary data transfer between replicas which contain mostly similar data.&quot; http://www.aosabook.org/en/nosql.html&lt;/ref&gt;

Hash trees were patented in 1979 by [[Ralph Merkle]].&lt;ref&gt;{{cite doi|10.1007/3-540-48184-2_32|noedit}}&lt;/ref&gt; The original purpose was to make it possible to efficiently handle many [[Lamport signature|Lamport one-time signatures]]. Unfortunately each Lamport key can only be used to sign a single message. But combined with hash trees they can be used for many messages and then become a [[Merkle signature scheme|fairly efficient digital signature scheme]].

== Overview ==
A hash tree is a [[Binary tree|tree]] of [[Hash function|hashes]] in which the leaves are hashes of data blocks in, for instance, a file or set of files. Nodes further up in the tree are the hashes of their respective children. For example, in the picture ''hash 0'' is the result of hashing the result of [[Concatenation|concatenating]] ''hash 0-0'' and ''hash 0-1''. That is, ''hash 0 = hash( hash 0-0 + hash 0-1 )'' where + denotes concatenation.

Most hash tree implementations are binary (two child nodes under each node) but they can just as well use many more child nodes under each node.

Usually, a [[cryptographic hash function]] such as [[SHA-2]] or [[SHA-3]] is used for the hashing. If the hash tree only needs to protect against unintentional damage, much less secure [[checksum]]s such as [[Cyclic redundancy check|CRCs]] can be used.

In the top of a hash tree there is a ''top hash'' (or ''root hash'' or ''master hash''). Before downloading a file on a p2p network, in most cases the top hash is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top hash is available, the hash tree can be received from any non-trusted source, like any peer in the p2p network. Then, the received hash tree is checked against the trusted top hash, and if the hash tree is damaged or fake, another hash tree from another source will be tried until the program finds one that matches the top hash.

The main difference from a [[hash list]] is that one branch of the hash tree can be downloaded at a time and the integrity of each branch can be checked immediately, even though the whole tree is not available yet. For example, in the picture, the integrity of ''data block 2'' can be verified immediately if the tree already contains ''hash 0-0'' and ''hash 1'' by hashing the data block and iteratively combining the result with ''hash 0-0'' and then ''hash 1'' and finally comparing the result with the ''top hash''. Similarly, the integrity of ''data block 3'' can be verified if the tree already has ''hash 1-1'' and ''hash 0''.  This can be an advantage since it is efficient to split files up in very small data blocks so that only small blocks have to be re-downloaded if they get damaged. If the hashed file is very big, such a hash tree or hash list becomes fairly big. But if it is a tree, one small branch can be downloaded quickly, the integrity of the branch can be checked, and then the downloading of data blocks can start.

There are several additional tricks, benefits and details regarding hash trees. See the references and external links below for more in-depth information.

=== Tiger tree hash ===
The Tiger tree hash is a widely used form of hash tree. It uses a binary hash tree (two child nodes under each node), usually has a data block size of 1024-[[byte]]s and uses the cryptographically secure [[Tiger (hash)|Tiger hash]].

Tiger tree hashes are used in [[Gnutella]], [[Gnutella2]], and [[Direct Connect (file sharing)|Direct Connect]] [[Peer-to-peer|P2P]] file sharing protocols and in [[file sharing]] applications such as [[Phex]], [[BearShare]], [[LimeWire]], [[Shareaza]], [[DCPlusPlus|DC++]]&lt;ref&gt;[http://dcplusplus.sourceforge.net/features.html &quot;DC++'s feature list&quot;]&lt;/ref&gt; and [[Valknut (software)|Valknut]].{{Citation needed|date=May 2010}}

== See also ==
* [[Hash table]]
* [[Hash trie]]
* [[Distributed hash table]]
* [[Linked Timestamping]]

== References ==
{{Reflist|30em}}

== Further reading ==
* {{US patent|4309569|Merkle tree patent 4,309,569}} – Explains both the hash tree structure and the use of it to handle many one-time signatures.
* [http://web.archive.org/web/20080316033726/http://www.open-content.net/specs/draft-jchapweske-thex-02.html Tree Hash EXchange format (THEX)] – A detailed description of Tiger trees.
* [http://www.emc.com/emc-plus/rsa-labs/historical/recent-improvements-efficient-use-merkle-trees.htm Efficient Use of Merkle Trees] – [[RSA Security|RSA labs]] explanation of the original purpose of Merkle trees: To handle many Lamport one-time signatures.

== External links ==
* http://www.codeproject.com/cs/algorithms/thexcs.asp – Tiger Tree Hash (TTH) source code in C# – by Gil Schmidt
* http://sourceforge.net/projects/tigertree/ – Tiger Tree Hash (TTH) implementations in C and Java
* [http://rhash.sourceforge.net/ RHash], an [[open source]] command-line tool, which can calculate TTH and magnet links with TTH.
* [http://www.guardtime.com/educational-series-on-hashes/ &quot;Series of mini-lectures about cryptographic hash functions&quot;]; includes application in time-stamping and provable security; by A. Buldas, 2011.

{{Spoken Wikipedia|en-Merkle_Tree.ogg|2013-09-17}}

{{Cryptography navbox}}
{{CS-Trees}}

[[Category:Error detection and correction]]
[[Category:Cryptographic hash functions]]
[[Category:Hashing]]
[[Category:Trees (data structures)]]</text>
      <sha1>mt8wcrdyacslss8jzhnmizpcdes5hkr</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hash list</title>
    <ns>0</ns>
    <id>2497149</id>
    <revision>
      <id>555197087</id>
      <parentid>544099757</parentid>
      <timestamp>2013-05-15T10:52:40Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <minor/>
      <comment>/* See also */ lf</comment>
      <text xml:space="preserve" bytes="3117">{{Unreferenced|date=March 2009}} 

In [[computer science]], a '''hash list''' is typically a [[List (computing)|list]] of [[Hash function|hashes]] of the data blocks in a file or set of files. Lists of hashes are used for many different purposes, such as fast table lookup ([[hash table]]s) and distributed databases ([[distributed hash table]]s). This article covers hash lists that are used to guarantee data integrity.

[[Image:Hash list.svg|thumb|350px|right|A hash list with a top hash]]

A hash list is an extension of the old concept of hashing an item (for instance, a file). A hash list is usually sufficient for most needs, but a more advanced form of the concept is a [[hash tree]].

Hash lists can be used to protect any kind of data stored, handled and transferred in and between computers. An important use of hash lists is to make sure that data blocks received from other peers in a [[Peer-to-peer|peer-to-peer network]] are received undamaged and unaltered, and to check that the other peers do not &quot;lie&quot; and send fake blocks. 

Usually a [[cryptographic hash function]] such as [[SHA-1]] is used for the hashing. If the hash list only needs to protect against unintentional damage less secure [[checksum]]s such as [[Cyclic redundancy check|CRCs]] can be used.

Hash lists are better than a simple hash of the entire file since, in the case of a data block being damaged, this is noticed, and only the damaged block needs to be redownloaded. With only a hash of the file, many undamaged blocks would have to be redownloaded, and the file reconstructed and tested until the correct hash of the entire file is obtained. Hash lists also protect against nodes that try to sabotage by sending fake blocks, since in such a case the damaged block can be acquired from some other source.

==Root hash==

Often, an additional hash of the hash list itself (a ''top hash'', also called ''root hash'' or ''master hash'') is used. Before downloading a file on a p2p network, in most cases the top hash is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top hash is available, the hash list can be received from any non-trusted source, like any peer in the p2p network. Then the received hash list is checked against the trusted top hash, and if the hash list is damaged or fake, another hash list from another source will be tried until the program finds one that matches the top hash.

In some systems (like for example [[BitTorrent (protocol)|BitTorrent]]), instead of a top hash the whole hash list is available on a web site in a small file. Such a &quot;[[torrent file]]&quot; contains a description, file names, a hash list and some additional data.

== See also ==

* [[Merkle tree|Hash tree]]
* [[Hash table]]
* [[Hash chain]]
* [[Ed2k: URI scheme]], which uses an MD4 top hash of an MD4 hash list to uniquely identify a file
* [[Cryptographic hash function]]
* [[List (computing)|List]]

{{Cryptography navbox}}

[[Category:Error detection and correction]]
[[Category:Cryptographic hash functions]]
[[Category:Hashing]]</text>
      <sha1>iuvv0pr1m7y4rya1tq5a8bcia48di41</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Double hashing</title>
    <ns>0</ns>
    <id>1583816</id>
    <revision>
      <id>608293366</id>
      <parentid>608236966</parentid>
      <timestamp>2014-05-12T23:06:08Z</timestamp>
      <contributor>
        <username>Tcl16</username>
        <id>6617698</id>
      </contributor>
      <comment>Undid revision 608236966 by [[Special:Contributions/138.38.233.228|138.38.233.228]] ([[User talk:138.38.233.228|talk]])</comment>
      <text xml:space="preserve" bytes="5413">'''Double hashing''' is a [[computer programming]] technique used in [[hash table]]s to resolve [[hash collision]]s, cases when two different values to be searched for produce the same hash key.  It is a popular [[hash collision|collision]]-resolution technique in [[open addressing|open-addressed]] hash tables. Double hashing is implemented in many popular [[library (computing)|libraries]].

Like [[linear probing]], it uses one hash value as a starting point and then repeatedly steps forward an interval until the desired value is located, an empty location is reached, or the entire table has been searched; but this interval is decided using a second, independent hash function (hence the name double hashing). Unlike [[linear probing]] and [[quadratic probing]], the interval depends on the data, so that even values mapping to the same location have different bucket sequences; this minimizes repeated collisions and the effects of clustering. 

Given two randomly, uniformly, and independently selected hash functions &lt;math&gt;h_1&lt;/math&gt; and &lt;math&gt;h_2&lt;/math&gt;, the ''i''th location in the bucket sequence for value ''k'' in a hash table &lt;math&gt;T&lt;/math&gt; is: &lt;math&gt;h(i,k)=(h_1(k) + i \cdot h_2(k))\mod |T|.&lt;/math&gt;
Generally, &lt;math&gt;h_1&lt;/math&gt; and &lt;math&gt;h_2&lt;/math&gt; are selected from a set of [[universal hash]] functions.

== Classical applied data structure ==
Double hashing with open addressing is a classical data structure on a table &lt;math&gt;T&lt;/math&gt;. Let &lt;math&gt;n&lt;/math&gt; be the number of elements stored in &lt;math&gt;T&lt;/math&gt;, then &lt;math&gt;T&lt;/math&gt;'s load factor is &lt;math&gt;\alpha = \frac{n}{|T|}&lt;/math&gt;. 

Double hashing approximates uniform open address hashing.
That is, start by randomly, uniformly and independently selecting two [[universal hash]] functions &lt;math&gt;h_1&lt;/math&gt; and &lt;math&gt;h_2&lt;/math&gt; to build a double hashing table &lt;math&gt;T&lt;/math&gt;.

All elements are put in &lt;math&gt;T&lt;/math&gt; by double hashing using &lt;math&gt;h_1&lt;/math&gt; and &lt;math&gt;h_2&lt;/math&gt;.
Given a key &lt;math&gt;k&lt;/math&gt;, determining the &lt;math&gt;(i+1)&lt;/math&gt;-st hash location is computed by:

&lt;math&gt; h(i,k) = ( h_1(k) + i \cdot h_2(k) ) \mod |T|.&lt;/math&gt;

Let &lt;math&gt;T&lt;/math&gt; have fixed load factor &lt;math&gt;\alpha: 1 &gt; \alpha &gt; 0&lt;/math&gt;.
Bradford and [[Michael N. Katehakis |Katehakis]]&lt;ref&gt;
P. G. Bradford and M. Katehakis:
''A Probabilistic Study on Combinatorial Expanders and Hashing'', SIAM Journal on Computing 2007 (37:1), 83-111.
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.2647 
&lt;/ref&gt;
showed the expected number of probes for an unsuccessful search in &lt;math&gt;T&lt;/math&gt;, still using these initially chosen hash functions, is &lt;math&gt;\frac{1}{1-\alpha}&lt;/math&gt; regardless of the distribution of the inputs.
More precisely, these two uniformly, randomly and independently chosen hash functions are chosen from a set of [[universal hash]] functions where pair-wise independence suffices.

Previous results include:
Guibas and Szemerédi&lt;ref&gt;
L. Guibas and E. Szemerédi:
''The Analysis of Double Hashing'', Journal of Computer and System Sciences, 1978, 16, 226-274.
&lt;/ref&gt;
showed &lt;math&gt;\frac{1}{1-\alpha}&lt;/math&gt; holds for unsuccessful search for load factors
&lt;math&gt;\alpha &lt; 0.319&lt;/math&gt;.
Also, Lueker and Molodowitch&lt;ref&gt;
G. S. Lueker and M. Molodowitch:
''More Analysis of Double Hashing'', Combinatorica, 1993, 13(1), 83-96.
&lt;/ref&gt;
showed this held assuming ideal randomized functions.
Schmidt and Siegel&lt;ref&gt;
J. P. Schmidt and A. Siegel:
''Double Hashing is Computable and Randomizable with Universal Hash Functions'', manuscript.
&lt;/ref&gt;
showed this with &lt;math&gt;k&lt;/math&gt;-wise
independent and uniform functions (for &lt;math&gt;k = c \log n&lt;/math&gt;, and suitable constant &lt;math&gt;c&lt;/math&gt;).

== Implementation details for caching ==

Linear probing and, to a lesser extent, quadratic probing are able to take advantage of the data cache by accessing locations that are close together. Double hashing has, on average, larger intervals and is not able to achieve this advantage.  To avoid this situation, store your data with the second key as the row, and your first key as the column.  Doing this allows you to iterate on the column, thus preventing cache problems.  This also prevents the need to rehash the second key.

For instance:

&lt;source lang=c&gt;
pData[hk_2][hk_1]

int hv_1 = Hash(v)
int hv_2 = Hash2(v)

int original_hash = hv_1
while(pData[hv_2][hv_1]){
  hv_1 = hv_1 + 1
}
&lt;/source&gt;

Like all other forms of open addressing, double hashing becomes linear as the hash table approaches maximum capacity.  The only solution to this is to rehash to a larger size, as with all other open addressing schemes.

On top of that, it is possible for the secondary hash function to evaluate to zero. For example, if we choose k=5 with the following function:

&lt;math&gt; h_2(k) = 5 - (k\mod 7) &lt;/math&gt;

The resulting sequence will always remain at the initial hash value. One possible solution is to change the secondary hash function to:

&lt;math&gt; h_2(k) = (k\mod 7) + 1&lt;/math&gt;

This ensures that the secondary hash function will always be non zero.

==See also==
* [[Hash collision]]
* [[Hash function]]
* [[Linear probing]]
* [[Cuckoo hashing]]

==Notes==
{{reflist}}

==External links==
*[http://www.siam.org/meetings/alenex05/papers/13gheileman.pdf How Caching Affects Hashing] by Gregory L. Heileman and Wenbin Luo 2005.
*[http://www.cs.pitt.edu/~kirk/cs1501/animations/Hashing.html Hash Table Animation]

[[Category:Search algorithms]]
[[Category:Hashing]]</text>
      <sha1>3koz9kjkrte0v337075whmygz07uxgd</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Linear hashing</title>
    <ns>0</ns>
    <id>2981764</id>
    <revision>
      <id>614685210</id>
      <parentid>604820524</parentid>
      <timestamp>2014-06-27T21:13:12Z</timestamp>
      <contributor>
        <username>Olleicua</username>
        <id>339605</id>
      </contributor>
      <text xml:space="preserve" bytes="6356">'''Linear hashing''' is a dynamic [[hash table]] algorithm invented by Witold Litwin (1980),&lt;ref&gt;{{Citation | first1=Witold | last1=Litwin | title=Linear hashing: A new tool for file and table addressing | journal=Proc. 6th Conference on Very Large Databases | pages=212–223 | year=1980 | url=http://www.cs.cmu.edu/afs/cs.cmu.edu/user/christos/www/courses/826-resources/PAPERS+BOOK/linear-hashing.PDF|format=PDF}}&lt;/ref&gt; and later popularized by [[Paul Larson]]. Linear hashing allows for the expansion of the hash table one slot at a time.
The frequent single slot expansion can very effectively control the length of
the collision chain.  The cost of hash table expansion is spread out across each
hash table insertion operation, as opposed to being incurred all at once.&lt;ref&gt;{{Citation | first1=Per-Åke | last1=Larson | title=Dynamic Hash Tables | journal=Communications of the ACM | pages=446–457 | date=April 1988 | volume=31 | number=4 | doi=10.1145/42404.42410}}&lt;/ref&gt; Linear hashing is therefore well suited for interactive applications.

==Algorithm Details==


First the initial hash table is set up with some arbitrary initial number of buckets.  The following values need to be kept track of:

* &lt;math&gt;N&lt;/math&gt;: The initial number of buckets.
* &lt;math&gt;L&lt;/math&gt;: The current level which is an integer that indicates on a logarithmic scale approximately how many buckets the table has grown by.  This is initially &lt;math&gt;0&lt;/math&gt;.
* &lt;math&gt;S&lt;/math&gt;: The step pointer which points to a bucket.  It initially points to the first bucket in the table.

Bucket collisions can be handled in a variety of ways but it is typical to have space for two items in each bucket and to add more buckets whenever a bucket overflows. More than two items can be used once the implementation is debugged.   Addresses are calculated in the following way:

* Apply a [[hash function]] to the key and call the result &lt;math&gt;H&lt;/math&gt;.
* If &lt;math&gt;H \bmod N \times 2^L&lt;/math&gt; is an address that comes before &lt;math&gt;S&lt;/math&gt;, the address is &lt;math&gt;H \bmod N \times 2^{L+1}&lt;/math&gt;.
* If &lt;math&gt;H \bmod N \times 2^L&lt;/math&gt; is &lt;math&gt;S&lt;/math&gt; or an address that comes after &lt;math&gt;S&lt;/math&gt;, the address is &lt;math&gt;H \bmod N \times 2^L&lt;/math&gt;.

To add a bucket:

* Allocate a new bucket at the end of the table.
* If &lt;math&gt;S&lt;/math&gt; points to the &lt;math&gt;N \times 2^L&lt;/math&gt;th bucket in the table, reset &lt;math&gt;S&lt;/math&gt; and increment &lt;math&gt;L&lt;/math&gt;.
* Otherwise increment &lt;math&gt;S&lt;/math&gt;.

The effect of all of this is that the table is split into three sections; the section before &lt;math&gt;S&lt;/math&gt;, the section from &lt;math&gt;S&lt;/math&gt; to &lt;math&gt;N \times 2^L&lt;/math&gt;, and the section after &lt;math&gt;N \times 2^L&lt;/math&gt;.  The first and last sections are stored using &lt;math&gt;H \bmod N \times 2^{L+1}&lt;/math&gt; and the middle section is stored using &lt;math&gt;H \bmod N \times 2^L&lt;/math&gt;.  Each time &lt;math&gt;S&lt;/math&gt; reaches &lt;math&gt;N \times 2^L&lt;/math&gt; the table has doubled in size.

=== Points to ponder over ===

: 
* Full buckets are not necessarily split, and an overflow space for temporary overflow buckets is required. In external storage, this could mean a second file. 
* Buckets split are not necessarily full
* Every bucket will be split sooner or later and so all Overflows will be reclaimed and rehashed.
* Split pointer s decides which bucket to split
** s is independent to overflowing bucket
** At level i, s is between 0 and 2^i
** s is incremented and if at end, is reset to 0.
** since a bucket at s is split then s is in incremented, only buckets before s have the second doubled hash space .
** a large good pseudo random number is first obtained , and then is bit-masked with either (2^i) -1 or (2^(i+1)) -1, but the latter only applies if x, the random number, bit-masked with the former , (2^i) - 1, is less than S, so the larger range of hash values only apply to buckets that have already been split.  
** e.g. To bit-mask a number , use x &amp; 0111 , where &amp; is the AND operator, 111 is binary 7 , where 7 = 8 - 1 and 8 is 2^3 and i = 3.
** what if s lands on a bucket which has 1 or more full overflow buckets ? The split will only reduce the overflow bucket count by 1, and the remaining overflow buckets will have to be recreated by seeing which of the new 2 buckets, or their overflow buckets,  the overflow entries belong.
* h&lt;sub&gt;i&lt;/sub&gt; (k)= h(k) mod(2^i n)
* h&lt;sub&gt;i+1&lt;/sub&gt; doubles the range of h&lt;sub&gt;i&lt;/sub&gt;

=== Algorithm  for inserting ‘k’ and checking overflow condition ===

* b = h&lt;sub&gt;0&lt;/sub&gt;(k)
* if b &lt; split-pointer then
* b = h&lt;sub&gt;1&lt;/sub&gt;(k)

=== Searching in the hash table for ‘k’ ===

* b = h&lt;sub&gt;0&lt;/sub&gt;(k)
* if b &lt; split-pointer then
* b = h&lt;sub&gt;1&lt;/sub&gt;(k)
* read bucket b and search there

==Adoption in language systems==
Griswold and Townsend &lt;ref&gt;{{Citation | title=The Design and Implementation of Dynamic Hashing for Sets and Tables in Icon | first1=William G. | last1=Griswold | author1-link = Bill Griswold | first2=Gregg M. | last2=Townsend | journal=Software - Practice and Experience | volume=23 | issue=4 | date=April 1993 | pages=351–367 | url=http://citeseer.ist.psu.edu/griswold93design.html}}&lt;/ref&gt; discussed the adoption of linear hashing in the [[Icon language]]. They discussed the implementation alternatives of [[dynamic array]] algorithm used in linear hashing, and presented performance comparisons using a list of Icon benchmark applications.

==Adoption in database systems==
Linear hashing is used in the BDB Berkeley database system, which in turn is used by many software systems such as OpenLDAP, using a C implementation derived from the CACM article and first published on the Usenet in 1988 by Esmond Pitt.

==References==
{{Reflist}}

==External links==
*[http://www.concentric.net/~Ttwang/tech/sorthash.htm Sorted Linear Hash Table, C++ implementation of a Linear Hashtable]
*[http://tommyds.sourceforge.net/ TommyDS, C implementation of a Linear Hashtable]
*[http://hackthology.com/an-in-memory-go-implementation-of-linear-hashing.html An in Memory Go Implementation with Explanation]
* {{DADS|linear hashing|linearHashing}}
*[https://github.com/KevinStern/index-cpp/blob/master/src/linear_hashing_table.h A C++ Implementation of Linear Hashtable which Supports Both Filesystem and In-Memory Storage]

==See also==
* [[Extendible hashing]]
* [[Consistent hashing]]

[[Category:Search algorithms]]
[[Category:Hashing]]</text>
      <sha1>l1eopsu79ralt9y088u4yxhvdq1xqil</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Coalesced hashing</title>
    <ns>0</ns>
    <id>2381605</id>
    <revision>
      <id>604942554</id>
      <parentid>576809861</parentid>
      <timestamp>2014-04-19T22:49:13Z</timestamp>
      <contributor>
        <username>StewieK</username>
        <id>6402461</id>
      </contributor>
      <text xml:space="preserve" bytes="6046">{{Lead too long|date=April 2014}}
[[Image:CoalescedHash.jpg|frame|right|Coalesced Hashing example. For purposes of this example, collision buckets are allocated in increasing order, starting with bucket 0.]]
'''Coalesced hashing''', also called '''coalesced chaining''', is a strategy of collision resolution in a [[hash table]] that forms a hybrid of [[separate chaining]] and [[open addressing]]. In a separate chaining hash table, items that hash to the same address are placed on a list (or &quot;chain&quot;) at that address. This technique can result in a great deal of wasted memory because the table itself must be large enough to maintain a load factor that performs well (typically twice the expected number of items), and extra memory must be used for all but the first item in a chain (unless list headers are used, in which case extra memory must be used for all items in a chain).

Given a sequence &quot;qrj,&quot; &quot;aty,&quot; &quot;qur,&quot; &quot;dim,&quot; &quot;ofu,&quot; &quot;gcl,&quot; &quot;rhv,&quot; &quot;clq,&quot; &quot;ecd,&quot; &quot;qsu&quot; of randomly generated three character long strings, the following table would be generated (using [http://burtleburtle.net/bob/ Bob Jenkins' One-at-a-Time hash algorithm]) with a table of size 10:

{|border=&quot;1&quot;
| (null)
|-
| &quot;clq&quot;
|-
| &quot;qur&quot;
|-
| (null)
|-
| (null)
|-
| &quot;dim&quot;
|-
| &quot;aty&quot; || &quot;qsu&quot;
|-
| &quot;rhv&quot;
|-
| &quot;qrj&quot; || &quot;ofu&quot; || &quot;gcl&quot; || &quot;ecd&quot;
|-
| (null)
|}

This strategy is effective, efficient, and very easy to implement. However, sometimes the extra memory use might be prohibitive, and the most common alternative, open addressing, has uncomfortable disadvantages that decrease performance.  The primary disadvantage of open addressing is primary and secondary clustering, in which searches may access long sequences of used buckets that contain items with different hash addresses; items with one hash address can thus lengthen searches for items with other hash addresses.  

One solution to these issues is coalesced hashing. Coalesced hashing uses a similar technique as separate chaining, but instead of allocating new nodes for the linked list, buckets in the actual table are used. The first empty bucket in the table at the time of a collision is considered the collision bucket. When a collision occurs anywhere in the table, the item is placed in the collision bucket and a link is made between the chain and the collision bucket. It is possible for a newly inserted item to collide with items with a different hash address, such as the case in the example above when item &quot;clq&quot; is inserted.  The chain for &quot;clq&quot; is said to &quot;coalesce&quot; with the chain of &quot;qrj,&quot; hence the name of the algorithm.  However, the extent of coalescing is minor compared with the clustering exhibited by open addressing.  For example, when coalescing occurs, the length of the chain grows by only 1, whereas in open addressing, search sequences of arbitrary length may combine.

An important optimization, to reduce the effect of coalescing, is to restrict the address space of the hash function to only a subset of the table.  For example, if the table has size ''M'' with buckets numbered from 0 to ''M &amp;minus; 1'', we can restrict the address space so that the hash function only assigns addresses to the first ''N'' locations in the table.  The remaining ''M &amp;minus; N'' buckets, called the ''cellar'', are used exclusively for storing items that collide during insertion.  No coalescing can occur until the cellar is exhausted.  

The optimal choice of ''N'' relative to ''M'' depends upon the load factor (or fullness) of the table.  A careful analysis shows that the value ''N = 0.86 &amp;times; M'' yields near-optimum performance for most load factors.&lt;ref name=&quot;VitterChen&quot;&gt;J. S. Vitter and W.-C. Chen, ''Design and Analysis of Coalesced Hashing'', Oxford University Press, New York, NY, 1987, ISBN 0-19-504182-8&lt;/ref&gt; Other variants for insertion are also possible that have improved search time.  Deletion algorithms have been developed that preserve randomness, and thus the average search time analysis still holds after deletions.&lt;ref name=&quot;VitterChen&quot;/&gt;

Insertion in [[C (programming language)|C]]:
&lt;source lang=&quot;c&quot;&gt;
/* htab is the hash table,
   N is the size of the address space of the hash function, and
   M is the size of the entire table including the cellar.
   Collision buckets are allocated in decreasing order, starting with bucket M-1. */

int insert ( char key[] )
{
  unsigned h = hash ( key, strlen ( key ) ) % N;

  if ( htab[h] == NULL ) {
    /* Make a new chain */
    htab[h] = make_node ( key, NULL );
  } else {
    struct node *it;
    int cursor = M-1;

    /* Find the first empty bucket */
    while ( cursor &gt;= 0 &amp;&amp; htab[cursor] != NULL )
      --cursor;

    /* The table is full, terminate unsuccessfully */
    if ( cursor == -1 )
      return -1;

    htab[cursor] = make_node ( key, NULL );
    
    /* Find the last node in the chain and point to it */
    it = htab[h];

    while ( it-&gt;next != NULL )
      it = it-&gt;next;

    it-&gt;next = htab[cursor];
  }

  return 0;
}
&lt;/source&gt;
One benefit of this strategy is that the search algorithm for separate chaining can be used without change in a coalesced hash table.

Lookup in C:
&lt;source lang=&quot;c&quot;&gt;
char *find ( char key[] )
{
  unsigned h = hash ( key, strlen ( key ) ) % N;

  if ( htab[h] != NULL ) {
    struct node *it;

    /* Search the chain at index h */
    for ( it = htab[h]; it != NULL; it = it-&gt;next ) {
      if ( strcmp ( key, it-&gt;data ) == 0 )
        return it-&gt;data;
    }
  }

  return NULL;
}
&lt;/source&gt;

== Performance ==
Coalesced chaining avoids the effects of primary and secondary clustering, and as a result can take advantage of the efficient search algorithm for separate chaining. If the chains are short, this strategy is very efficient and can be highly condensed, memory-wise. As in open addressing, deletion from a coalesced hash table is awkward and potentially expensive, and resizing the table is terribly expensive and should be done rarely, if ever.

== References ==
&lt;references/&gt;

[[Category:Hashing]]
[[Category:Articles with example C code]]</text>
      <sha1>hwcrugeczl9ukbd7clvfk2mqrrt4kh4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cuckoo hashing</title>
    <ns>0</ns>
    <id>4015872</id>
    <revision>
      <id>617213157</id>
      <parentid>612684155</parentid>
      <timestamp>2014-07-16T18:35:42Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* Generalizations and applications */Task 2: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <text xml:space="preserve" bytes="12378">[[Image:cuckoo.svg|thumb|Cuckoo hashing example. The arrows show the alternative location of each key. A new item would be inserted in the location of A by moving A to its alternative location, currently occupied by B, and moving B to its alternative location which is currently vacant. Insertion of a new item in the location of H would not succeed: Since H is part of a cycle (together with W), the new item would get kicked out again.]]

'''Cuckoo hashing''' is a scheme in [[computer programming]] for resolving [[hash collision]]s of values of [[hash function]]s in a [[hash table|table]], with [[worst case analysis|worst-case]] [[constant time|constant]] lookup time. The name derives from the behavior of some species of [[cuckoo]], where the cuckoo chick pushes the other eggs or young out of the nest when it hatches; analogously, inserting a new key into a cuckoo hashing table may push an older key to a different location in the table.

==History==
Cuckoo hashing was first described by [[Rasmus Pagh]] and [[Flemming Friche Rodler]] in 2001.&lt;ref name=&quot;Cuckoo&quot;&gt;{{cite doi|10.1007/3-540-44676-1_10}}&lt;/ref&gt;

==Theory==
The basic idea is to use two hash functions instead of only one. This provides two possible locations in the hash table for each [[unique key|key]]. In one of the commonly used variants of the algorithm, the hash table is split into two smaller tables of equal size, and each hash function provides an index into one of these two tables.

When a new key is inserted, a [[greedy algorithm]] is used: The new key is inserted in one of its two possible locations, &quot;kicking out&quot;, that is, displacing, any key that might already reside in this location. This displaced key is then inserted in its alternative location, again kicking out any key that might reside there, until a vacant position is found, or the procedure would enter an [[infinite loop]]. In the latter case, the [[hash table]] is rebuilt [[in-place algorithm|in-place]] using new [[hash function]]s:{{quote | text = There is no need to allocate new tables for the rehashing: We may simply run through the tables to delete and perform the usual insertion procedure on all keys found not to be at their intended position in the table. | sign = Pagh &amp; Rodler | source = &quot;Cuckoo Hashing&quot;&lt;ref name=Cuckoo/&gt; }}

Lookup requires inspection of just two locations in the hash table, which takes constant time in the worst case (''see'' [[Big O notation]]). This is in contrast to many other hash table algorithms, which may not have a constant worst-case bound on the time to do a lookup.

It can also be shown that insertions succeed in expected constant time,&lt;ref name=&quot;Cuckoo&quot; /&gt; even considering the possibility of having to rebuild the table, as long as the number of keys is kept below half of the capacity of the hash table, i.e., the load factor is below 50%. One method of proving this uses the theory of [[random graph]]s: one may form an [[undirected graph]] called the &quot;Cuckoo Graph&quot; that has a vertex for each hash table location, and an edge for each hashed value, with the endpoints of the edge being the two possible locations of the value. Then, the greedy insertion algorithm for adding a set of values to a cuckoo hash table succeeds if and only if the Cuckoo Graph for this set of values is a [[pseudoforest]], a graph with at most one cycle in each of its [[connected component (graph theory)|connected component]]s, as any vertex-induced subgraph with more edges than vertices corresponds to a set of keys for which there are an insufficient number of slots in the hash table. This property is true with high probability for a random graph in which the number of edges is less than half the number of vertices.&lt;ref&gt;{{Cite web|first=Reinhard|last=Kutzelnigg|url=http://www.dmtcs.org/dmtcs-ojs/index.php/proceedings/article/viewFile/590/1710|contribution=Bipartite random graphs and cuckoo hashing|title=Fourth Colloquium on Mathematics and Computer Science|series=Discrete Mathematics and Theoretical Computer Science|year=2006|volume=AG|pages=403–406|postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt;

== Example ==
The following hashfunctions are given:

&lt;math&gt;h\left(k\right)=k\mod 11&lt;/math&gt;&lt;br/&gt;
&lt;math&gt;h'\left(k\right)=\left\lfloor\frac{k}{11}\right\rfloor\mod 11&lt;/math&gt;

&lt;div style=&quot;float:left; margin-right:1em&quot;&gt;
{| class=&quot;wikitable&quot;
|-
! k !! h(k) !! h'(k)
|-
| 20 || 9 || 1
|-
| 50 || 6 || 4
|-
| 53 || 9 || 4
|-
| 75 || 9 || 6
|-
| 100 || 1 || 9
|-
| 67 || 1 || 6
|-
| 105 || 6 || 9
|-
| 3 || 3 || 0
|-
| 36 || 3 || 3
|-
| 39 || 6 || 3
|}
&lt;/div&gt;

Columns in the following two tables show the state of the hash tables over time as the elements are inserted.

&lt;div style=&quot;float:left; margin-right:1em&quot;&gt;
{| class=&quot;wikitable&quot;
|-
! colspan=&quot;11&quot; | 1. table for h(k)
|-
| || 20 || 50 || 53 || 75 || 100 || 67 || 105 || 3 || 36 || 39
|-
| 0 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 
|-
| 1 ||  ||  ||  ||  || 100 || 67 || 67 || 67 || 67 || 100
|-
| 2 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 
|-
| 3 ||  ||  ||  ||  ||  ||  ||  || 3 || 3 || 36
|-
| 4 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 
|-
| 5 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 
|-
| 6 ||  || 50 || 50 || 50 || 50 || 50 || 105 || 105 || 105 || 50
|-
| 7 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 
|- 
| 8 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 
|-
| 9 || 20 || 20 || 20 || 20 || 20 || 20 || 53 || 53 || 53 || 75
|-
| 10 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 
|}
&lt;/div&gt;

&lt;div style=&quot;float:left&quot;&gt;
{| class=&quot;wikitable&quot;
|-
! colspan=&quot;11&quot; | 2. table for h'(k)
|-
| || 20 || 50 || 53 || 75 || 100 || 67 || 105 || 3 || 36 || 39
|-
| 0 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 3
|-
| 1 ||  ||  ||  ||  ||  ||  || 20 || 20 || 20 || 20
|-
| 2 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 
|-
| 3 ||  ||  ||  ||  ||  ||  ||  ||  || 36 || 39
|-
| 4 ||  ||  || 53 || 53 || 53 || 53 || 50 || 50 || 50 || 53
|-
| 5 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 
|-
| 6 ||  ||  ||  || 75 || 75 || 75 || 75 || 75 || 75 || 67
|-
| 7 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 
|-
| 8 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 
|-
| 9 ||  ||  ||  ||  ||  || 100 || 100 || 100 || 100 || 105
|-
| 10 ||  ||  ||  ||  ||  ||  ||  ||  ||  || 
|}
&lt;/div&gt;
&lt;div style=&quot;clear:both;&quot; /&gt;

=== Cycle ===
If you now wish to insert the element 6, then you get into a cycle. In the last row of the table we find the same initial situation as at the beginning again.

&lt;math&gt;h\left(6\right)=6\mod 11=6&lt;/math&gt;&lt;br/&gt;
&lt;math&gt;h'\left(6\right)=\left\lfloor\frac{6}{11}\right\rfloor\mod 11=0&lt;/math&gt;&lt;br/&gt;
{| class=&quot;wikitable&quot;
|-
| considered key || ! colspan=&quot;2&quot; | table 1 || ! colspan=&quot;2&quot; | table 2
|-
| || old value || new value || old value || new value
|-
| 6 || 50 || 6 || 53 || 50
|-
| 53 || 75 || 53 || 67 || 75
|-
| 67 || 100 || 67 || 105 || 100
|-
| 105 || 6 || 105 || 3 || 6
|-
| 3 || 36 || 3 || 39 || 36
|-
| 39 || 105 || 39 || 100 || 105
|-
| 100 || 67 || 100 || 75 || 67
|-
| 75 || 53 || 75 || 50 || 53
|-
| 50 || 39 || 50 || 36 || 39
|-
| 36 || 3 || 36 || 6 || 3
|-
| 6 || 50 || 6 || 53 || 50
|}

==Generalizations and applications==
Generalizations of cuckoo hashing that use more than 2 alternative hash functions can be expected to utilize a larger part of the capacity of the hash table efficiently while sacrificing some lookup and insertion speed. Using just three hash functions increases the load to 91%.&lt;ref name=&quot;mitzenmacher2009survey&quot;&gt;{{cite journal | last=Mitzenmacher | first=Michael | authorlink=Michael Mitzenmacher|title=Some Open Questions Related to Cuckoo Hashing &amp;#124; Proceedings of ESA 2009 | date=2009-09-09|url=http://www.eecs.harvard.edu/~michaelm/postscripts/esa2009.pdf | format=PDF | accessdate=2010-11-10 }}&lt;/ref&gt; Another generalization of cuckoo hashing consists in using more than one key per bucket. Using just 2 keys per bucket permits a load factor above 80%.

Other algorithms that use multiple hash functions include the [[Bloom filter]]. A Cuckoo Filter employs Cuckoo hashing principles to implement a data structure equivalent to a Bloom filter.&lt;ref name=cuckoofilter2013&gt;{{cite journal|last1=Fan|first1=Bin|last2=Kaminsky|first2=Michael|last3=Andersen|first3=David|title=Cuckoo Filter: Better Than Bloom|journal=;login:|date=August 2013|volume=38|issue=4|page=36-40|url=http://www.cs.cmu.edu/~binfan/papers/login_cuckoofilter.pdf|accessdate=12 June 2014|publisher=USENIX|language=English|format=PDF}}&lt;/ref&gt; A simplified generalization of cuckoo hashing called [[Associative cache|skewed-associative cache]] is used in some CPU caches.{{Citation needed|date=February 2011}}

A study by Zukowski et al.&lt;ref&gt;{{cite journal | last=Zukowski | first=Marcin |author2=Heman, Sandor |author3=Boncz, Peter  | title=Architecture-Conscious Hashing | publisher=Proceedings of the International Workshop on Data Management on New Hardware (DaMoN) | date=June 2006 | doi= | url=http://www.cs.cmu.edu/~damon2006/pdf/zukowski06archconscioushashing.pdf | format=PDF  | accessdate=2008-10-16}}&lt;/ref&gt; has shown that cuckoo hashing is much faster than [[Separate chaining|chained hashing]] for small, [[CPU cache|cache]]-resident hash tables on modern processors. Kenneth Ross&lt;ref&gt;{{cite journal | last=Ross | first=Kenneth | title=Efficient Hash Probes on Modern Processors | publisher=IBM Research Report RC24100 | date=2006-11-08 |url=http://domino.research.ibm.com/library/cyberdig.nsf/papers/DF54E3545C82E8A585257222006FD9A2/$File/rc24100.pdf | format=PDF | id=RC24100 | accessdate=2008-10-16 }}&lt;/ref&gt; has shown bucketized versions of cuckoo hashing (variants that use buckets that contain more than one key) to be faster than conventional methods also for large hash tables, when space utilization is high. The performance of the bucketized cuckoo hash table was investigated further by Askitis,&lt;ref&gt;{{Cite book | title=Fast and Compact Hash Tables for Integer Keys | first1=Nikolas | last1=Askitis | year=2009 | isbn=978-1-920682-72-9 | url=http://crpit.com/confpapers/CRPITV91Askitis.pdf | pages=113–122 | journal=Proceedings of the 32nd Australasian Computer Science Conference (ACSC 2009) | volume=91}}&lt;/ref&gt;
with its performance compared against alternative hashing schemes.

A survey by [[Michael Mitzenmacher|Mitzenmacher]]&lt;ref name=&quot;mitzenmacher2009survey&quot; /&gt; presents open problems related to cuckoo hashing as of 2009.

== See also ==

* [[Perfect hashing]]
* [[Linear probing]]
* [[Double hashing]]
* [[Hash collision]]
* [[Hash function]]
* [[Quadratic probing]]
* [[Hopscotch hashing]]

== References ==
{{Reflist}}

==External links==
* [http://www.ru.is/faculty/ulfar/CuckooHash.pdf A cool and practical alternative to traditional hash tables], U. Erlingsson, M. Manasse, F. Mcsherry, 2006.
* [http://www.it-c.dk/people/pagh/papers/cuckoo-undergrad.pdf Cuckoo Hashing for Undergraduates, 2006], R. Pagh, 2006.
* [http://mybiasedcoin.blogspot.com/2007/06/cuckoo-hashing-theory-and-practice-part.html Cuckoo Hashing, Theory and Practice] (Part 1, [http://mybiasedcoin.blogspot.com/2007/06/cuckoo-hashing-theory-and-practice-part_15.html Part 2] and [http://mybiasedcoin.blogspot.com/2007/06/cuckoo-hashing-theory-and-practice-part_19.html Part 3]), Michael Mitzenmacher, 2007.
* {{cite conference | last = Naor | first = Moni | coauthors = Segev, Gil; Wieder, Udi | author2-link = http://www.wisdom.weizmann.ac.il/~gils/ | title = History-Independent Cuckoo Hashing | booktitle = International Colloquium on Automata, Languages and Programming (ICALP) | place = Reykjavik, Iceland | year = 2008 | url = http://www.wisdom.weizmann.ac.il/~naor/PAPERS/cuckoo_hi_abs.html | accessdate = 2008-07-21 }}

===Examples===
*[http://github.com/efficient/libcuckoo Concurrent high-performance Cuckoo hashtable written in C++]
* [http://sourceforge.net/projects/cuckoo-cpp/ Cuckoo hash map written in C++]
* [http://www.theiling.de/projects/lookuptable.html Static cuckoo hashtable generator for C/C++]
* [http://lmonson.com/blog/?p=100 Cuckoo hashtable written in Java]
* [http://github.com/joacima/Cuckoo-hash-map/blob/master/CuckooHashMap.java Generic Cuckoo hashmap in Java]
* [http://hackage.haskell.org/packages/archive/hashtables/latest/doc/html/Data-HashTable-ST-Cuckoo.html Cuckoo hash table written in Haskell]

[[Category:Search algorithms]]
[[Category:Hashing]]

[[pl:Tablica mieszająca#Haszowanie kuku.C5.82cze]]</text>
      <sha1>nl3s128ekha42atrhbkv4rw4c54ees6</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Extendible hashing</title>
    <ns>0</ns>
    <id>8019200</id>
    <revision>
      <id>589287141</id>
      <parentid>585017205</parentid>
      <timestamp>2014-01-05T13:26:56Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor/>
      <comment>fixed [[:Category:CS1 errors: dates|CS1 errors: dates]] &amp; [[WP:AWB/GF|General fixes]] using [[Project:AWB|AWB]] (9832)</comment>
      <text xml:space="preserve" bytes="9851">'''Extendible hashing''' is a type of [[hash function|hash]] system which treats a hash as a bit string, and uses a [[trie]] for bucket lookup.&lt;ref&gt;{{Citation | title=Extendible Hashing - A Fast Access Method for Dynamic Files | journal=ACM Transactions on Database Systems | volume=4 | issue=3 | date=September 1979 | first1=R. | last1=Fagin | first2=J. | last2=Nievergelt | first3=N. | last3=Pippenger | first4=H. R. | last4=Strong | pages=315–344 | doi=10.1145/320083.320092 }}&lt;/ref&gt; Because of the hierarchical nature of the system, re-hashing is an incremental operation (done one bucket at a time, as needed).  This means that time-sensitive applications are less affected by table growth than by standard full-table rehashes.

==Example==

This is an example from {{harvtxt|Fagin|Nievergelt|Pippenger|Strong|1979}}.

Assume that the hash function &lt;math&gt;h(k)&lt;/math&gt; returns a binary number. The first i bits of each string will be used as indices to figure out where they will go in the &quot;directory&quot; (hash table). Additionally, i is the smallest number such that the first i bits of all keys are different.

Keys to be used:

&lt;math&gt;h(k_1)&lt;/math&gt; = 100100&lt;br&gt;
&lt;math&gt;h(k_2)&lt;/math&gt; = 010110&lt;br&gt;
&lt;math&gt;h(k_3)&lt;/math&gt; = 110110

Let's assume that for this particular example, the bucket size is 1. The first two keys to be inserted, k&lt;sub&gt;1&lt;/sub&gt; and k&lt;sub&gt;2&lt;/sub&gt;, can be distinguished by the most significant bit, and would be inserted into the table as follows:

[[Image:Extendible hashing 1.svg|200px]]

Now, if k&lt;sub&gt;3&lt;/sub&gt; were to be hashed to the table, it wouldn't be enough to distinguish all three keys by one bit (because k&lt;sub&gt;3&lt;/sub&gt; and k&lt;sub&gt;1&lt;/sub&gt; have 1 as their leftmost bit. Also, because the bucket size is one, the table would overflow. Because comparing the first two most significant bits would give each key a unique location, the directory size is doubled as follows:

[[Image:Extendible hashing 2.svg|200px]]

And so now k&lt;sub&gt;1&lt;/sub&gt; and k&lt;sub&gt;3&lt;/sub&gt; have a unique location, being distinguished by the first two leftmost bits. Because k&lt;sub&gt;2&lt;/sub&gt; is in the top half of the table, both 00 and 01 point to it because there is no other key to compare to that begins with a 0.

=== Further detail ===

&lt;math&gt;h(k_4)&lt;/math&gt; = 011110

Now, k&lt;sub&gt;4&lt;/sub&gt; needs to be inserted, and it has the first two bits as 01..(1110), and using a 2 bit depth in the directory, this maps from 01 to Bucket A. Bucket A is full (max size 1), so it must be split; because there is more than one pointer to Bucket A, there is no need to increase the directory size.

What is needed is information about:

# The key size that maps the directory (the global depth), and
# The key size that has previously mapped the bucket (the local depth)

In order to distinguish the two action cases:

# Doubling the directory when a bucket becomes full
# Creating a new bucket, and re-distributing the entries between the old and the new bucket

Examining the initial case of an extendible hash structure, if each directory entry points to one bucket, then the local depth should be equal to the global depth.

The number of directory entries is equal to 2&lt;sup&gt;global depth&lt;/sup&gt;, and the initial number of buckets
is equal to 2&lt;sup&gt;local depth&lt;/sup&gt;.

Thus if global depth = local depth = 0, then 2&lt;sup&gt;0&lt;/sup&gt; = 1, so an initial directory of one pointer to one bucket.

Back to the two action cases:

If the local depth is equal to the global depth, then there is only one pointer to the bucket, and there is no other directory pointers that can map to the bucket, so the directory must be doubled (case1).

If the bucket is full, if the local depth is less than the global depth, 
then there exists more than one pointer from the directory to the bucket, and the bucket can be split (case 2).

[[Image:Extendible hashing 3.svg|200px]]

Key 01  points to Bucket A, and Bucket A's local depth of 1 is less than the directory's global depth of 2, which means keys hashed to Bucket A have only used a 1 bit prefix (i.e. 0), and the bucket needs to have its contents split using keys 1 + 1 = 2 bits in length; in general, for any local depth d where d is less than D, the global depth, then d must be incremented after a bucket split, and the new d used as the number of bits of each entry's key to redistribute the entries of the former bucket into the new buckets.

[[Image:Extendible hashing 4.svg|200px]]

Now, 
&lt;math&gt;h(k_4)&lt;/math&gt; = 011110&lt;br&gt;
is tried again, with 2 bits  01.., and now key 01 points to a new bucket but there is still k2 in it (&lt;math&gt;h(k2)&lt;/math&gt; = 010110 and also begins with 01).

If k2 had been 000110, with key 00, there would have been no problem, because k2 would have remained in the new bucket A' and bucket D would have been empty.

(This would have been the most likely case by far when buckets are of greater size than 1 and the newly split buckets would be exceedingly unlikely to overflow, unless all the entries were all rehashed to one bucket again. But just to emphasize the role of the depth information, the example will be pursued logically to the end.)

So Bucket D needs to be split, but a check of its local depth, which is 2, is the same as the global depth, which is 2, so the directory must be split again, in order to hold keys of sufficient detail, e.g. 3 bits.

[[Image:Extendible hashing 5.svg|200px]]

# Bucket D needs to split due to being full.
# As D's local depth = the global depth, the directory must double to increase bit detail of keys.
# Global depth has incremented after directory split to 3.
# The new entry k4 is rekeyed with global depth 3 bits and ends up in D which has local depth 2, which can now be incremented to 3 and D can be split to D' and E.
# The contents of the split bucket D, k2, has been re-keyed with 3 bits, and it ends up in D.
# K4 is retried and it ends up in E which has a spare slot.

[[Image:Extendible hashing 6.svg|200px]]

Now, &lt;math&gt;h(k2)&lt;/math&gt; = 010110 is in D and &lt;math&gt;h(k_4)&lt;/math&gt; = 011110 is tried again, with 3 bits  011..,  and it points to bucket D which already contains k2 so is full; D's local depth is 2 but now the global depth is 3 after the directory doubling, so now D can be split into bucket's D' and E, the contents of D, k2 has its  &lt;math&gt;h(k2)&lt;/math&gt; retried with a new global depth bitmask of 3 and k2 ends up in D', then the new entry k4 is retried with &lt;math&gt;h(k_4)&lt;/math&gt; bitmasked using the new global depth bit count of 3 and this gives 011 which now points to a new bucket E which is empty. So K4 goes in Bucket E.

==Example implementation ==

Below is the extendible hashing algorithm in [[Python (programming language)|Python]], with the disc block / memory page association, caching and consistency issues removed. Note a problem exists if the depth exceeds the bit size of an integer, because then doubling of the directory or splitting of a bucket won't allow entries to be rehashed to different buckets.

The code uses the ''least significant bits'', which makes it more efficient to expand the table, as the entire directory can be copied as one block ({{harvtxt|Ramakrishnan|Gehrke|2003}}).

=== Python example ===
&lt;source lang=&quot;python&quot;&gt;
PAGE_SZ = 20

class Page(object):

    def __init__(self):
        self.m = {}
        self.d = 0

    def full(self):
        return len(self.m) &gt;= PAGE_SZ

    def put(self,k,v):
        self.m[k] = v

    def get(self,k):
        return self.m.get(k)

class EH(object):

    def __init__(self):
        self.gd = 0 
        p = Page()
        self.pp= [p]

    def get_page(self,k):
        h = hash(k) 
        p = self.pp[ h &amp; (( 1 &lt;&lt; self.gd) -1)]
        return p

    def  put(self, k, v):
        p = self.get_page(k)
        if p.full() and p.d == self.gd:
            self.pp *= 2
            self.gd += 1
        
        if p.full() and p.d &lt; self.gd:
            p.put(k,v);
            p1 = Page()
            p2 = Page()
            for k2,v2 in p.m.items():
                h = hash(k2)
                h = h &amp; ((1 &lt;&lt; self.gd) -1)
                if (h &gt;&gt; p.d) &amp; 1 == 1:
                    p2.put(k2,v2)
                else:
                    p1.put(k2,v2)
            for i,x in enumerate(self.pp):
                if x == p:
                    if (i &gt;&gt; p.d) &amp; 1 == 1:
                        self.pp[i] = p2
                    else:
                        self.pp[i] = p1

            p2.d = p1.d = p.d + 1
        else:    
            p.put(k,  v)

    def get(self, k):
        p = self.get_page(k)
        return p.get(k)

if __name__ == &quot;__main__&quot;:
    eh = EH()
    N = 10088
    l = list(range(N))

    import random
    random.shuffle(l)
    for x in l:
        eh.put(x,x)
    print l

    for i in range(N):
        print eh.get(i)

&lt;/source&gt;

==Notes==
{{Reflist}}

== See also ==
* [[Trie]]
* [[Hash table]]
* [[Stable hashing]]
* [[Consistent hashing]]
* [[Linear hashing]]

==References==
* {{Citation | title=Extendible Hashing - A Fast Access Method for Dynamic Files | journal=ACM Transactions on Database Systems | volume=4 | issue=3 | date=September 1979 | first1=R. | last1=Fagin | first2=J. | last2=Nievergelt | first3=N. | last3=Pippenger | first4=H. R. | last4=Strong | pages=315–344 | doi=10.1145/320083.320092}}
* {{Citation | title=Database Management Systems, 3rd Edition: Chapter 11, Hash-Based Indexing | year=2003 | first1=R. | last1=Ramakrishnan | first2=J. | last2=Gehrke | pages=373–378 }}

== External links ==
* {{DADS|Extendible hashing|extendibleHashing}}
* [http://www.isqa.unomaha.edu/haworth/isqa3300/fs009.htm Extendible Hashing] at University of Nebraska
* [http://www.csm.astate.edu/~rossa/datastruc/Extend.html Extendible Hashing notes] at Arkansas State University
* [http://www.smckearney.com/adb/notes/lecture.extendible.hashing.pdf Extendible hashing notes]

[[Category:Search algorithms]]
[[Category:Hashing]]</text>
      <sha1>4i4cyf223k55k4mpw53mwq3f4ulo2j3</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>2-choice hashing</title>
    <ns>0</ns>
    <id>4436335</id>
    <revision>
      <id>610661510</id>
      <parentid>610661464</parentid>
      <timestamp>2014-05-29T16:14:30Z</timestamp>
      <contributor>
        <username>Thomasda</username>
        <id>2387809</id>
      </contributor>
      <comment>/* Performance */</comment>
      <text xml:space="preserve" bytes="2643">{{multiple issues|
{{original research|date=May 2013}}
{{orphan|date=December 2008}}
}}

'''2-choice hashing''', also known as '''2-choice chaining''', is a variant of a [[hash table]] in which keys are added by hashing with two [[hash function]]s. The key is put in the array position with the fewer (colliding) keys. Some [[collision resolution scheme]] is needed, unless keys are kept in buckets. The [[average-case cost]] of a successful search is [[Big O notation|O(2 + (m-1)/n)]], where m is the number of keys and n is the size of the array. The most collisions is &lt;math&gt;\log_2 \ln n + \theta(m/n)&lt;/math&gt; with high probability.

==How It Works==

2-choice hashing utilizes two hash functions h1(x) and h2(x) which work as hash functions are expected to work (i.e. mapping integers from the universe into a specified range). The two hash functions should be independent and have no correlation to each other. Having two hash functions allows any integer x to have up to two potential locations to be stored based on the values of the respective outputs, h1(x) and h2(x). It is important to note that, although there are two hash functions, there is only one table; both hash functions map to locations on that table.

==Implementation==

The most important functions of the hashing implementation in this case are insertion and search.

'''Insertion:''' When inserting the values of both hash functions are computed for the to-be-inserted object. The object is then placed in the bucket which contains fewer objects. If the buckets are equal in size, the default location is the h1(x) value.

'''Search:''' Effective searches are done by looking in both buckets, that is, the bucket locations which h1(x) and h2(x) mapped to for the desire value.

==Performance==

As is true with all hash tables, the performance is based on the largest bucket. Although there are instances where bucket sizes happen to be large based on the values and the hash functions used, this is rare. Having two hash functions and, therefore, two possible locations for any one value, makes the possibility of large buckets even more unlikely to happen.

The expected bucket size while using 2-choice hashing is: '''θ(log(log(n)))'''. This massive improvement is due to the randomized concept known as The [[Power of Two Choices]].

'''Note: The idea of multiple hash functions is optimized using 2 hash functions. There is no improvement found if the number of hash functions is increased - that is 3 hash functions does not have better performance than 2. '''

==See also==
*[[2-left hashing]]

{{DADS|2-choice hashing|twoChoiceHashing}}

[[Category:Hashing]]</text>
      <sha1>a2hloiyewwfu1o6zkiw1e20v6q6lllf</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Open addressing</title>
    <ns>0</ns>
    <id>1583843</id>
    <revision>
      <id>609294602</id>
      <parentid>609294528</parentid>
      <timestamp>2014-05-19T21:42:48Z</timestamp>
      <contributor>
        <username>Nicmcd</username>
        <id>21437963</id>
      </contributor>
      <minor/>
      <comment>Added content to code comment to make it clear.</comment>
      <text xml:space="preserve" bytes="6172">[[Image:HASHTB12.svg|thumb|362px|right|Hash collision resolved by linear probing (interval=1).]]
'''Open addressing''', or '''closed hashing''', is a method of [[Hash_table#Collision_resolution|collision resolution in hash tables]].  With this method a hash collision is resolved by '''probing''', or searching through alternate locations in the array (the ''probe sequence'') until either the target record is found, or an unused array slot is found, which indicates that there is no such key in the table.&lt;ref name=&quot;tenenbaum90&quot;&gt;{{Citation | title=Data Structures Using C | first1=Aaron M. | last1=Tenenbaum | first2=Yedidyah | last2=Langsam | first3=Moshe J. | last3=Augenstein | publisher=Prentice Hall | year=1990 | isbn=0-13-199746-7 | pages=456–461, pp. 472}}&lt;/ref&gt; Well known probe sequences include:

; [[Linear probing]] : in which the interval between probes is fixed — often at 1.
; [[Quadratic probing]] : in which the interval between probes increases linearly (hence, the indices are described by a quadratic function).
; [[Double hashing]] : in which the interval between probes is fixed for each record but is computed by another hash function.

The main tradeoffs between these methods are that linear probing has the best [[Locality of reference|cache performance]] but is most sensitive to clustering, while double hashing has poor cache performance but exhibits virtually no clustering; quadratic probing falls in-between in both areas. Double hashing can also require more computation than other forms of probing.  Some open addressing methods, such as
[[last-come-first-served hashing]] and [[cuckoo hashing]] move existing keys around in the array to make room for the new key. This gives better maximum search times than the methods based on probing.

A critical influence on performance of an open addressing hash table is the ''load factor''; that is, the proportion of the slots in the array that are used. As the load factor increases towards 100%, the number of probes that may be required to find or insert a given key rises dramatically. Once the table becomes full, probing algorithms may even fail to terminate. Even with good hash functions, load factors are normally limited to 80%. A poor hash function can exhibit poor performance even at very low load factors by generating significant clustering. What causes hash functions to cluster is not well understood {{Citation needed|date=September 2009}}, and it is easy to unintentionally write a hash function which causes severe clustering.

==Example pseudo code==

The following [[pseudocode]] is an implementation of an open addressing hash table with linear probing and single-slot stepping, a common approach that is effective if the hash function is good. Each of the '''lookup''', '''set''' and '''remove''' functions use a common internal function '''find_slot''' to locate the array slot that either does or should contain a given key.

  '''record''' pair { key, value }
  '''var''' ''pair array'' slot[0..num_slots-1]

  '''function''' find_slot(key)
      i := hash(key) modulo num_slots
      ''// search until we either find the key, or find an empty slot.
      '''while''' (slot[i] is occupied) and ( slot[i].key ≠ key )
          i = (i + 1) modulo num_slots
      '''return''' i

  '''function''' lookup(key)
      i := find_slot(key)
      '''if''' slot[i] is occupied   ''// key is in table''
          '''return''' slot[i].value
      '''else'''                     ''// key is not in table''
          '''return''' not found

  '''function''' set(key, value)
      i := find_slot(key)
      '''if''' slot[i] is occupied   ''// we found our key''
          slot[i].value = value
          '''return'''
      '''if''' the table is almost full
          rebuild the table larger ''(note 1)''
          i = find_slot(key)
      slot[i].key   = key
      slot[i].value = value

; note 1 : Rebuilding the table requires allocating a larger array and recursively using the '''set''' operation to insert all the elements of the old array into the new larger array. It is common to increase the array size [[exponential growth|exponentially]], for example by doubling the old array size.

  '''function''' remove(key)
      i := find_slot(key)
      '''if''' slot[i] is unoccupied
          return   ''// key is not in the table''
      j := i
      '''loop'''
          mark slot[i] as unoccupied
         r2: ''(note 2)''
          j := (j+1) modulo num_slots
          '''if''' slot[j] is unoccupied
              '''exit loop'''
          k := hash(slot[j].key) modulo num_slots
          // determine if k lies cyclically in ]i,j]
          // |    i.k.j |
          // |....j i.k.| or  |.k..j i...|
          if ( (i&lt;=j) ? ((i&lt;k)&amp;&amp;(k&lt;=j)) : ((i&lt;k)||(k&lt;=j)) )
              goto r2;
          slot[i] := slot[j]
          i := j

; note 2 : For all records in a cluster, there must be no vacant slots between their natural hash position and their current position (else lookups will terminate before finding the record). At this point in the pseudocode, ''i'' is a vacant slot that might be invalidating this property for subsequent records in the cluster. ''j'' is such a subsequent record. ''k'' is the raw hash where the record at ''j'' would naturally land in the hash table if there were no collisions. This test is asking if the record at ''j'' is invalidly positioned with respect to the required properties of a cluster now that ''i'' is vacant.

Another technique for removal is simply to mark the slot as deleted. However this eventually requires rebuilding the table simply to remove deleted records. The methods above provide O(1) updating and removal of existing records, with occasional rebuilding if the high-water mark of the table size grows.

The O(1) remove method above is only possible in linearly probed hash tables with single-slot stepping.  In the case where many records are to be deleted in one operation, marking the slots for deletion and later rebuilding may be more efficient.

==See also==
* [[Lazy deletion]] - a method of deleting from a hash table using open addressing.

==References==
&lt;references /&gt;

[[Category:Hashing]]</text>
      <sha1>ok7cuh8xa2rhz7qttmkgeh7yn99queg</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Koorde</title>
    <ns>0</ns>
    <id>14947011</id>
    <revision>
      <id>568905267</id>
      <parentid>535377189</parentid>
      <timestamp>2013-08-17T08:28:09Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fix.  Category problem. Syntax fixes.  Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (9421)</comment>
      <text xml:space="preserve" bytes="3693">In [[peer-to-peer]] networks, '''Koorde''' is a [[Distributed hash table]] (DHT) system based on the [[Chord (DHT)|Chord DHT]] and the [[De Bruijn graph]] ([[De Bruijn sequence]]). Inheriting the simplicity of Chord, Koorde meets O(log n) hops per node (where n is the number of nodes in the DHT), and O(log n/ log log n) hops per lookup request with O(log n) neighbors per node.

The Chord concept is based on a wide range of identifiers (e.g. 2^160) in a structure of a ring where an identifier can stand for both node and data. Node-successor is responsible for the whole range of IDs between itself and its predecessor.

==De Bruijn's graphs==
[[File:De bruijn graph-for binary sequence of order 4.svg|right|thumb|A de Bruijn's 3-dimensional graph]]

Koorde is based on Chord but also on [[De Bruijn graph]] ([[De Bruijn sequence]]).
In a d-dimensional de Bruijn graph, there are 2&lt;sup&gt;d&lt;/sup&gt; nodes, each of which has a unique d-bit ID. The node with ID i is connected to nodes 2i modulo 2&lt;sup&gt;d&lt;/sup&gt; and 2i+1 modulo 2&lt;sup&gt;d&lt;/sup&gt;. Thanks to this property, the routing algorithm can route to any destination in d hops by successively &quot;shifting in&quot; the bits of the destination ID but only if the dimensions of the distance between modulo 1d and 3d are equal.

Routing a message from node m to node k is accomplished by taking the number m and shifting in the bits of k one at a time until the number has been replaced by k. Each shift corresponds to a routing hop to the next intermediate address; the hop is valid because each node's neighbors are the two possible outcomes of shifting a 0 or 1 onto its own address. Because of the structure of de Bruijn graphs, when the last bit of k has been shifted, the query will be at node k. Node k responds whether key k exists.

==Routing example==
[[Image:Koorde lookup routing.JPG|250px|right|thumb|Example of the way Koorde routes from Node2 to Node6 using a 3-dimensional, binary graph.]]

For example, when a message needs to be routed from node “2” (which is “010”) to “6” (which is “110”), the steps are following:

Step 1)
Node #2 routes the message to Node #5 (using its connection to 2i+1 mod8), shifts the bits left and puts “1” as the youngest bit (right side).

Step 2)
Node #5 routes the message to Node #3 (using its connection to 2i+1 mod8), shifts the bits left and puts “1” as the youngest bit (right side).

Step 3)
Node #3 routes the message to Node #6 (using its connection to 2i mod8), shifts the bits left and puts “0” as the youngest bit (right side).

==Non-constant degree Koorde==
[[Image:Koorde lookup code.JPG|250px|right|thumb|Koorde lookup algorithm.]]

The d-dimensional de Bruijn can be generalized to base k, in which case node i is connected to nodes k * i + j modulo kd, 0 ≤ j &lt; k. The diameter is reduced to Θ(logk n). Koorde node i maintains pointers to k consecutive nodes beginning at the predecessor of k * i modulo kd. Each de Bruijn routing step can be emulated with an expected constant number of messages, so routing uses O(logk n) expected hops- For k = Θ(log n), we get Θ(log n) degree and Θ(log n/ log log n) diameter.

==References==
*&quot;Internet Algorithms&quot; by Greg Plaxton, Fall 2003: [http://web.archive.org/web/20040929211835/http://www.cs.utexas.edu/users/plaxton/c/395t/slides/DynamicTopologies-2.pdf]

*&quot;Koorde: A simple degree-optimal distributed hash table&quot; by M. Frans Kaashoek and David R. Karger: [http://iptps03.cs.berkeley.edu/final-papers/koorde.ps]

*Chord and Koorde descriptions: [http://www.cs.jhu.edu/~scheideler/courses/600.348_F03/lecture_10.pdf]

[[Category:File sharing networks]]
[[Category:Distributed data storage]]
[[Category:Hashing]]</text>
      <sha1>m99dx9h4z180556kqpgqko8vb0c1088</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>SUHA (computer science)</title>
    <ns>0</ns>
    <id>15713616</id>
    <revision>
      <id>586072740</id>
      <parentid>536837711</parentid>
      <timestamp>2013-12-14T17:53:49Z</timestamp>
      <contributor>
        <ip>24.7.25.79</ip>
      </contributor>
      <comment>removed incorrect statement that SUHA is same as uniform hashing assumption (see talk page)</comment>
      <text xml:space="preserve" bytes="3917">{{redirect|SUHA|other uses|Suha (disambiguation){{!}}Suha}}
In [[computer science]], '''SUHA''' ('''S'''imple [[Uniform distribution (discrete)|'''U'''niform]] '''H'''ashing '''A'''ssumption) is a basic assumption that facilitates the mathematical analysis of [[Hash_Table|hash tables]].  The assumption states that a hypothetical [[hashing function]] will evenly distribute items into the slots of a hash table.  Moreover, each item to be hashed has an equal [[probability]] of being placed into a slot, regardless of the other elements already placed.  This assumption generalizes the details of the hash function and allows for certain assumptions about the stochastic system. 

==Applications==
SUHA is most commonly used as a foundation for mathematical proofs describing the properties and behavior of hash tables in [[theoretical computer science]].  Minimizing [[Hash_collision|hashing collisions]] can be achieved with a uniform hashing function.   These functions often rely on the specific input data set and can be quite difficult to implement.  Assuming uniform hashing allows hash table analysis to be made without exact knowledge of the input or the hash function used.

==Mathematical implications==
Certain properties of hash tables can be derived once uniform hashing is assumed.

===Uniform distribution===
Under the assumption of uniform hashing, given a hash function '''''h''''', and a hash table of size '''''m''''', the probability that two non-equal elements will hash to the same slot is
:&lt;math&gt;P(h(a) = h(b)) =  \frac{1}{m}.&lt;/math&gt;

===Collision chain length===
Under the assumption of uniform hashing, the [[load factor (computer science)|load factor]] &lt;math&gt;\alpha&lt;/math&gt; and the [[Average_case|average]] chain length of a hash table of size '''''m''''' with '''''n''''' elements will be
:&lt;math&gt;\alpha = \tfrac{n}{m}&lt;/math&gt;

===Successful lookup===
Under the assumption of uniform hashing, the average time (in [[Big O notation|big-O notation]]) to successfully find an element in a hash table using [[Hash_table#Separate_chaining|chaining]] is
:&lt;math&gt;\Theta(\alpha + 1)\,&lt;/math&gt;

===Unsuccessful lookup===
Under the assumption of uniform hashing, the average time (in big-O notation) to unsuccessfully find an element in a hash table using chaining is
:&lt;math&gt;\Theta(\alpha + 1)\,&lt;/math&gt;

==Example==
A simple example of using SUHA can be seen while observing an arbitrary hash table of size 10 and a data set of 30 unique elements.  If chaining is used to deal with collisions, the average chain length of this hash table may be a desirable value.  Without any assumptions and with no more additional information about the data or hash function, the chain length cannot be estimated.  With SUHA however, we can state that because of an assumed uniform hashing, each element has an equal probability of mapping to a slot.  Since no particular slot should be favored over another, the 30 elements should hash into the 10 slots uniformly.  This will produce a hash table with, on average, 10 chains each of length 3
:&lt;math&gt;\alpha = \tfrac{n}{m}&lt;/math&gt;

:&lt;math&gt;\alpha = \tfrac{30}{10}&lt;/math&gt;

:&lt;math&gt;\alpha = 3\,&lt;/math&gt;

==See also==
*[[Hash Table]]
*[[Hash_collision|Hash Collision]]
*[[Perfect Hashing]]

==References==
===General===
* {{cite book
  | last = Collins
  | first = William
  | title = Data Structures and the Java Collections Framework
  | publisher = McGraw-Hill
  | date=  2004
  | chapter = Section 14.3.2: The Uniform Hashing Assumption
  | pages = 608
  | id = ISBN 0-07-282379-8 }}
* {{cite book
  | last = Cormen
  | first = Thomas H.
  | authorlink = Thomas H. Cormen
  | coauthors = [[Charles E. Leiserson]], [[Ronald L. Rivest]], [[Clifford Stein]]
  | title = [[Introduction to Algorithms]]
  | publisher = MIT Press and McGraw-Hill
  | date=  2001
  | chapter = Section 11.2: Hash Tables
  | pages = 226–228
  | id = ISBN 0-262-03293-7 }}


[[Category:Hashing]]</text>
      <sha1>ghqvp6vvzw8en33hn7rm944lnj7pj41</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Comparison of cryptographic hash functions</title>
    <ns>0</ns>
    <id>20844795</id>
    <revision>
      <id>601196354</id>
      <parentid>593837729</parentid>
      <timestamp>2014-03-25T14:23:06Z</timestamp>
      <contributor>
        <ip>192.118.35.248</ip>
      </contributor>
      <comment>/* Cryptanalysis */ Change MD4 results to reflect those in http://eprint.iacr.org/2010/016.pdf which is the source linked. It didn't make sense that a 2nd preimage would be harder than a preimage attack.</comment>
      <text xml:space="preserve" bytes="11897">The following tables compare general and technical information for a number of [[cryptographic hash function]]s.&lt;ref&gt;See the individual functions' articles for further information. This article is not all-inclusive or necessarily up-to-date.&lt;/ref&gt;

== General information ==
Basic general information about the [[cryptographic hash function]]s: year, designer, references, etc. 
{| class=&quot;wikitable sortable&quot; style=&quot;text-align: center&quot;
|-
! Function
! Year&lt;ref group=&quot;gi&quot;&gt;It refers to the first official description of the algorithm, not designed date.&lt;/ref&gt;
! Designer
! Derived from
! Reference
|-
| [[HAVAL]]
| 1992
| [[Yuliang Zheng]]&lt;br /&gt;[[Josef Pieprzyk]]&lt;br /&gt;[[Jennifer Seberry]]
|
| [http://labs.calyptix.com/haval.php Website]
|-
| [[MD2 (cryptography)|MD2]]
| 1989
| rowspan=&quot;4&quot;|[[Ronald Rivest]]
| 
| RFC 1319
|-
| [[MD4]]
| 1990
| 
| RFC 1320
|-
| [[MD5]]
| 1992
| [[MD4]]&lt;br /&gt;RFC 1321 page 1
| RFC 1321
|-
| [[MD6]]
| 2008
|
| [http://groups.csail.mit.edu/cis/md6/submitted-2008-10-27/Supporting_Documentation/md6_report.pdf md6_report.pdf]
|-
| [[RIPEMD]]
| 1990
| The RIPE Consortium [http://homes.esat.kuleuven.be/~bosselae/ripemd160.html#What]
| [[MD4]]
|
|-
| [[RIPEMD-128]]&lt;br /&gt;[[RIPEMD-256]]&lt;br /&gt;[[RIPEMD-160]]&lt;br /&gt;[[RIPEMD-320]]
| 1996
| [[Hans Dobbertin]]&lt;br /&gt;[[Antoon Bosselaers]]&lt;br /&gt;[[Bart Preneel]]
| [[RIPEMD]][http://homes.esat.kuleuven.be/~bosselae/ripemd160.html#What]
| [http://homes.esat.kuleuven.be/~bosselae/ripemd160.html Website]
|-
| [[SHA-0]]
| 1993
| rowspan=&quot;4&quot;|[[National Security Agency|NSA]]
|
| [http://w2.eff.org/Privacy/Digital_signature/?f=fips_sha_shs.info.txt SHA-0]
|-
| [[SHA-1]]
| 1995
| [[SHA-0]]
| rowspan=&quot;3&quot;|[http://csrc.nist.gov/publications/fips/fips180-3/fips180-3_final.pdf FIPS 180--3]
|-
| [[SHA-256]]&lt;br /&gt;[[SHA-512]]&lt;br /&gt;[[SHA-384]]
| 2002
| rowspan=&quot;2&quot;|
|-
| [[SHA-224]]
| 2004
|-
| [[GOST (hash function)|GOST R 34.11-94]]
| 1994
| [[FAPSI]] and VNIIstandart
| [[GOST (block cipher)|GOST 28147-89]]
| RFC 5831, RFC 4357
|-
| [[Tiger (cryptography)|Tiger]]
| 1995
| [[Ross J. Anderson|Ross Anderson]]&lt;br /&gt;[[Eli Biham]]
|
| [http://www.cs.technion.ac.il/~biham/Reports/Tiger/ Website]
|-
| [[Whirlpool (cryptography)|Whirlpool]]
| 2004
| [[Vincent Rijmen]]&lt;br /&gt;[[Paulo S. L. M. Barreto|Paulo Barreto]]
|
| [http://www.larc.usp.br/~pbarreto/WhirlpoolPage.html Website]
|-
| [[SHA-3]] (Keccak)
| 2008&lt;ref&gt;{{Citation |title=Keccak sponge function family main document |url=http://keccak.noekeon.org/Keccak-main-1.0.pdf |first1=Guido |last1=Bertoni |first2=Joan |last2=Daemen |first3=Michael |last3=Peeters |first4=Gilles Van |last4=Assche |version=1.0 |date=Oct 2008 |accessdate=2013-07-30 }}&lt;/ref&gt;
| [[Guido Bertoni]]&lt;br /&gt;[[Joan Daemen]]&lt;br /&gt;[[Michaël Peeters]]&lt;br /&gt;[[Gilles Van Assche]]
|
| [http://keccak.noekeon.org Website]
|}

=== Notes ===
&lt;references group=&quot;gi&quot; /&gt;

== Compression function ==
The following tables compare technical information for [[One-way compression function|compression function]]s of [[cryptographic hash function]]s. The information comes from the specifications, please refer to them for more details. 
{| class=&quot;wikitable&quot; style=&quot;text-align: center&quot;
|-
! rowspan=&quot;2&quot; | Function
! colspan=&quot;6&quot; | Size ([[bit]]s)&lt;ref group=&quot;cf&quot;&gt;The omitted multiplicands are word sizes.&lt;/ref&gt;
! rowspan=&quot;2&quot; | Words × &lt;br /&gt;Passes = &lt;br /&gt;Rounds&lt;ref group=&quot;cf&quot;&gt;Some authors interchange passes and rounds.&lt;/ref&gt;
! rowspan=&quot;2&quot; | Operations&lt;ref group=&quot;cf&quot;&gt;A: addition, subtraction; B: [[bitwise operation]]; L: [[lookup table]]; S: [[Bitwise operations#Bit shifts|shift, rotation]].&lt;/ref&gt;
! rowspan=&quot;2&quot; | [[Endianness|Endian]]&lt;ref group=&quot;cf&quot;&gt;It refers to ''byte'' endianness only. If the operations consist of bitwise operations and lookup tables only, the endianness is irrelevant.&lt;/ref&gt;
! rowspan=&quot;2&quot; | Specification
|-
! [[Word (data type)|Word]]
! [[Cryptographic hash function|Digest]]
! [[Merkle–Damgård construction|Chaining &lt;br /&gt;values]]&lt;ref group=&quot;cf&quot;&gt;The size of message digest equals to the size of chaining values usually. In truncated versions of certain cryptographic hash functions such as SHA-384, the former is less than the latter.&lt;/ref&gt;
! Computation&lt;br /&gt;values&lt;ref group=&quot;cf&quot;&gt;The size of chaining values equals to the size of computation values usually. In certain cryptographic hash functions such as RIPEMD-160, the former is less than the latter because RIPEMD-160 use two sets of parallel computaion values and then combine into a single set of chaining values.&lt;/ref&gt;
! [[Merkle–Damgård construction|Block]]
! [[Merkle–Damgård construction#Security characteristics|Length]]&lt;br /&gt;&lt;ref group=&quot;cf&quot;&gt;The maximum input size = 2&lt;sup&gt;length size&lt;/sup&gt; − 1 [[bit]]s. For example, the maximum input size of SHA-1 = 2&lt;sup&gt;64&lt;/sup&gt; − 1 bits.&lt;/ref&gt;
|-
|                [[HAVAL|HAVAL-3-128]]
| rowspan=&quot;15&quot; | 32
|                {{nowrap|×4 {{=}} 128}}
| colspan=&quot;2&quot; rowspan=&quot;15&quot; | {{nowrap|×8 {{=}} 256}}
| rowspan=&quot;15&quot; | {{nowrap|×32 {{=}} 1,024}}
| rowspan=&quot;15&quot; | 64
| rowspan=&quot;5&quot;  | {{nowrap|32 × 3 {{=}} 96}}
| rowspan=&quot;15&quot; | A B S
| rowspan=&quot;15&quot; | Little
| rowspan=&quot;15&quot; | [http://labs.calyptix.com/files/haval-paper.pdf HAVAL]
|-
| [[HAVAL|HAVAL-3-160]]
| {{nowrap|×5 {{=}} 160}}
|-
| [[HAVAL|HAVAL-3-192]]
| {{nowrap|×6 {{=}} 192}}
|-
| [[HAVAL|HAVAL-3-224]]
| {{nowrap|×7 {{=}} 224}}
|-
| [[HAVAL|HAVAL-3-256]]
| {{nowrap|×8 {{=}} 256}}
|-
| [[HAVAL|HAVAL-4-128]]
| {{nowrap|×4 {{=}} 128}}
| rowspan=&quot;5&quot; | {{nowrap|32 × 4 {{=}} 128}}
|-
| [[HAVAL|HAVAL-4-160]]
| {{nowrap|×5 {{=}} 160}}
|-
| [[HAVAL|HAVAL-4-192]]
| {{nowrap|×6 {{=}} 192}}
|-
| [[HAVAL|HAVAL-4-224]]
| {{nowrap|×7 {{=}} 224}}
|-
| [[HAVAL|HAVAL-4-256]]
| {{nowrap|×8 {{=}} 256}}
|-
| [[HAVAL|HAVAL-5-128]]
| {{nowrap|×4 {{=}} 128}}
| rowspan=&quot;5&quot; | {{nowrap|32 × 5 {{=}} 160}}
|-
| [[HAVAL|HAVAL-5-160]]
| {{nowrap|×5 {{=}} 160}}
|-
| [[HAVAL|HAVAL-5-192]]
| {{nowrap|×6 {{=}} 192}}
|-
| [[HAVAL|HAVAL-5-224]]
| {{nowrap|×7 {{=}} 224}}
|-
| [[HAVAL|HAVAL-5-256]]
| {{nowrap|×8 {{=}} 256}}
|-
| [[MD2 (cryptography)|MD2]]
| 8
| {{nowrap|×16 {{=}} 128}}
| {{nowrap|×32 {{=}} 256}}
| {{nowrap|×48 {{=}} 384}}
| {{nowrap|×16 {{=}} 128}}
| None
| {{nowrap|48 × 18 {{=}} 864}}
| B
| N/A
| RFC 1319
|-
| [[MD4]]
| rowspan=&quot;2&quot; | 32
| colspan=&quot;3&quot; rowspan=&quot;2&quot;| {{nowrap|×4 {{=}} 128}}
| rowspan=&quot;2&quot; | {{nowrap|×16 {{=}} 512}}
| rowspan=&quot;2&quot; | 64
|               {{nowrap|16 × 3 {{=}} 48}}
| rowspan=&quot;2&quot; | A B S
| rowspan=&quot;2&quot; | Little
| RFC 1320
|-
| [[MD5]]
| {{nowrap|16 × 4 {{=}} 64}}
| RFC 1321
|-
| [[RIPEMD]]
| rowspan=&quot;5&quot; | 32
| colspan=&quot;2&quot; rowspan=&quot;2&quot; | {{nowrap|×4 {{=}} 128}}
| rowspan=&quot;3&quot; | {{nowrap|×8 {{=}} 256}}
| rowspan=&quot;5&quot; | {{nowrap|×16 {{=}} 512}}
| rowspan=&quot;5&quot; | 64
| {{nowrap|16 × 3 {{=}} 48}}
| rowspan=&quot;5&quot; | A B S
| rowspan=&quot;5&quot; | Little
|
|-
| [[RIPEMD-128]]
| rowspan=&quot;2&quot; | {{nowrap|16 × 4 {{=}} 64}}
| rowspan=&quot;4&quot; | [http://www.esat.kuleuven.ac.be/~cosicart/pdf/AB-9601/AB-9601.pdf RIPEMD-160]
|-
| [[RIPEMD-256]]
| colspan=&quot;2&quot; | {{nowrap|×8 {{=}} 256}}
|-
| [[RIPEMD-160]]
| colspan=&quot;2&quot; | {{nowrap|×5 {{=}} 160}}
| rowspan=&quot;2&quot; | {{nowrap|×10 {{=}} 320}}
| rowspan=&quot;2&quot; | {{nowrap|16 × 5 {{=}} 80}}
|-
| [[RIPEMD-320]]
| colspan=&quot;2&quot; | {{nowrap|×10 {{=}} 320}}
|-
| [[SHA-0]]
| rowspan=&quot;4&quot; | 32
| colspan=&quot;3&quot; rowspan=&quot;2&quot; | {{nowrap|×5 {{=}} 160}}
| rowspan=&quot;4&quot; | {{nowrap|×16 {{=}} 512}}
| rowspan=&quot;4&quot; | 64
| rowspan=&quot;2&quot; | {{nowrap|16 × 5 {{=}} 80}}
| rowspan=&quot;6&quot; | A B S
| rowspan=&quot;6&quot; | Big
|-
| [[SHA-1]]
| rowspan=&quot;5&quot; | [http://csrc.nist.gov/publications/fips/fips180-3/fips180-3_final.pdf FIPS 180--3]
|-
| [[SHA-256]]
| {{nowrap|×8 {{=}} 256}}
| colspan=&quot;2&quot; rowspan=&quot;2&quot; | {{nowrap|×8 {{=}} 256}}
| rowspan=&quot;2&quot; | {{nowrap|16 × 4 {{=}} 64}}
|-
| [[SHA-224]]
| {{nowrap|×7 {{=}} 224}}
|-
| [[SHA-512]]
| rowspan=&quot;2&quot; | 64
| {{nowrap|×8 {{=}} 512}}
| colspan=&quot;2&quot; rowspan=&quot;2&quot; | {{nowrap|×8 {{=}} 512}}
| rowspan=&quot;2&quot; | {{nowrap|×16 {{=}} 1024}}
| rowspan=&quot;2&quot; | 128
| rowspan=&quot;2&quot; | {{nowrap|16 × 5 {{=}} 80}}
|-
| [[SHA-384]]
| {{nowrap|×6 {{=}} 384}}
|-
| [[GOST (hash function)|GOST R 34.11-94]]
| 32
| colspan=&quot;3&quot; | {{nowrap|×8 {{=}} 256}}
| {{nowrap|×8 {{=}} 256}}
| 32
| 4
| A B L S
| Little
| RFC 5831
|-
| [[Tiger (cryptography)|Tiger-192]]
| rowspan=&quot;3&quot; | 64
| {{nowrap|×3 {{=}} 192}}
| colspan=&quot;2&quot; rowspan=&quot;3&quot; | {{nowrap|×3 {{=}} 192}}
| rowspan=&quot;3&quot; | {{nowrap|×8 {{=}} 512}}
| rowspan=&quot;3&quot; | 64
| rowspan=&quot;3&quot; | {{nowrap|8 × 3 {{=}} 24}}
| rowspan=&quot;3&quot; | A B L S
| rowspan=&quot;3&quot; | Little
| rowspan=&quot;3&quot; | [http://www.cs.technion.ac.il/~biham/Reports/Tiger/tiger/node3.html#SECTION00030000000000000000 Tiger]
|-
| [[Tiger (cryptography)|Tiger-160]]
| ×2.5=160
|-
| [[Tiger (cryptography)|Tiger-128]]
| {{nowrap|×2 {{=}} 128}}
|}

=== Notes ===
&lt;references group=&quot;cf&quot; /&gt;

== Cryptanalysis ==
The following tables compare [[cryptanalysis]] status of [[cryptographic hash function]]s. This table is probably out of date. (last edited May, 2013)
{| class=&quot;wikitable&quot; style=&quot;text-align: center&quot;
|-
! rowspan=&quot;2&quot;|Function
! rowspan=&quot;2&quot;|Digest &lt;br /&gt;size
! rowspan=&quot;2&quot;|Rounds
! colspan=&quot;3&quot;|Best known attacks &lt;br /&gt;(complexity:rounds)&lt;ref group=&quot;c&quot;&gt;When omitted, rounds are full number.&lt;/ref&gt;
|-
! [[Birthday attack|Collision]]
! [[Preimage attack|Second&lt;br /&gt;preimage]]
! [[Preimage attack|Preimage]]
|-
| [[RIPEMD]]
| 128
| 48
| [http://www.springerlink.com/content/n5vrtdha97a2udkx/ 2&lt;sup&gt;18&lt;/sup&gt;]
|
|
|-
| [[RIPEMD|RIPEMD-160]]
| 160
| 80
| 
|
|
|-
| [[RIPEMD|RIPEMD-160-Reduced]]
| 160
| 80
| [http://www.springerlink.com/content/3540l03h1w31n6w7 2&lt;sup&gt;51&lt;/sup&gt;:48]
|
|
|-
| [[MD2 (cryptography)|MD2]]
| 128
| 864
| [http://www.springerlink.com/content/n5vrtdha97a2udkx/ 2&lt;sup&gt;63.3&lt;/sup&gt;]
|
| [http://eprint.iacr.org/2008/089.pdf 2&lt;sup&gt;73&lt;/sup&gt;]
|-
| [[MD4]]
| 128
| 48
| [http://www.springerlink.com/content/v6526284mu858v37/ 3]
| [http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;69.4&lt;/sup&gt;]
| [http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;78.4&lt;/sup&gt;]
|-
| [[MD5]]
| 128
| 64
| [http://eprint.iacr.org/2009/223.pdf 2&lt;sup&gt;20.96&lt;/sup&gt;]
|
| [http://springerlink.com/content/d7pm142n58853467/ 2&lt;sup&gt;123.4&lt;/sup&gt;]
|-
| [[SHA-0]]
| 160
| 80
| [http://www.springerlink.com/content/3810jp9730369045/ 2&lt;sup&gt;33.6&lt;/sup&gt;]
|
|
|-
| [[SHA-1]]
| 160
| 80
| [http://2012.sharcs.org/slides/stevens.pdf 2&lt;sup&gt;60&lt;/sup&gt;]
|
|
|-
| [[SHA-256]]
| 256
| 64
| 
|
| 
|-
| [[SHA-256-Reduced]]
| 256
| 64
| [http://dx.doi.org/10.1007/978-3-642-38348-9_16 2&lt;sup&gt;65.5&lt;/sup&gt;:31]
|
| [http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;248.4&lt;/sup&gt;:42]
|-
| [[SHA-512]]
| 512
| 80
| 
|
| 
|-
| [[SHA-512-Reduced]]
| 512
| 80
| [http://eprint.iacr.org/2008/270.pdf 2&lt;sup&gt;32.5&lt;/sup&gt;:24]
|
| [http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;494.6&lt;/sup&gt;:42]
|-
| [[GOST (hash function)|GOST]]
| 256
| 256
| [http://www.springerlink.com/content/2514122231284103/ 2&lt;sup&gt;105&lt;/sup&gt;]
| [http://www.springerlink.com/content/2514122231284103/ 2&lt;sup&gt;192&lt;/sup&gt;]
| [http://www.springerlink.com/content/2514122231284103/ 2&lt;sup&gt;192&lt;/sup&gt;]
|-
| [[Tiger (cryptography)|Tiger]]
| 192
| 24
| [http://www.springerlink.com/content/u762587644802p38/ 2&lt;sup&gt;62&lt;/sup&gt;:19]
|
| [http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;184.3&lt;/sup&gt;]
|}

=== Notes ===
&lt;references group=&quot;c&quot; /&gt;

== See also ==
*[[List of hash functions]]
*[[Word (data type)]]

==Notes==
&lt;references/&gt;

==References==
{{Reflist}}

== External links ==
# [http://www.larc.usp.br/~pbarreto/hflounge.html The Hash Function Lounge] - A list of hash functions and known attacks, by Paulo Barreto
# [http://ehash.iaik.tugraz.at/wiki/The_eHash_Main_Page The eHash Main Page] - A wiki for cryptographic hash functions
# [http://csrc.nist.gov/groups/ST/hash/sha-3/index.html The NIST Hash Competition Main Page] - The competition to become SHA-3

{{Cryptography navbox | hash}}

[[Category:Cryptography]]
[[Category:Cryptographic primitives]]
[[Category:Cryptographic hash functions| ]]
[[Category:Hashing]]
[[Category:Cryptography lists and comparisons|Hash functions]]</text>
      <sha1>a7rcf2ijmvoo9p1568mirs61vur8goe</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hash consing</title>
    <ns>0</ns>
    <id>21898171</id>
    <revision>
      <id>578756758</id>
      <parentid>561688035</parentid>
      <timestamp>2013-10-25T23:22:24Z</timestamp>
      <contributor>
        <username>ChrisGualtieri</username>
        <id>16333418</id>
      </contributor>
      <minor/>
      <comment>General Fixes using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="3400">In [[computer science]], particularly in [[functional programming]], '''hash consing''' is a technique used to share values that are structurally equal. The term ''hash consing'' originates from implementations of [[Lisp (programming language)|Lisp]]&lt;ref&gt;{{Cite journal|first=Eiichi|last=Goto|authorlink=Eiichi Goto|title=Monocopy and associative algorithms in extended Lisp|place=Tokyo|publisher=[[University of Tokyo]] Technical Report TR 74-03|year=1974}}&lt;/ref&gt; that attempt to reuse [[cons]] cells that have been constructed before, avoiding the penalty of [[memory allocation]]. Hash consing is most commonly implemented with [[hash table]]s storing [[weak reference]]s that may be [[garbage collection (computer science)|garbage-collected]] when the data stored therein contains no [[reference (computer science)|reference]]s from outside the table.&lt;ref&gt;{{cite book|author=Allen, John|title=Anatomy of Lisp|publisher=[[McGraw Hill]]|year=1978|isbn=0-07-001115-X}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last1=Fillâtre|first1=Jean-Christophe|last2=Conchon|first2=Sylvain|contribution=Type-Safe Modular Hash-Consing|title=Workshop on ML|publisher=[[Association for Computing Machinery|ACM]]|year=2006}}&lt;/ref&gt; Hash consing has been shown to give dramatic performance improvements&amp;mdash;both space and time&amp;mdash;for [[symbolic computation|symbolic]] and [[dynamic programming]] algorithms.{{Citation needed|date=March 2009}}

In other communities a similar idea is known as the [[Flyweight pattern]]. When applied to [[string (computer science)|string]]s this technique is also known as ''[[string interning]]''.

== Examples ==

===Scheme===

Simple, not very efficient, but suitable for demonstration of the concept implementation of a [[memoization|memoizer]] by means of hash table and weak references in [[Scheme (programming language)|Scheme]]:

&lt;source lang=&quot;scheme&quot;&gt;
;; weak hashes
;;
(require 'hash-table)

(define (make-weak-table . args)
  (apply make-hash-table args))

(define (weak-table-set! table key data)
  (let ((w (hash-table-ref table key #f)))
    (if w
	(vector-set! w 0 data)
	(let ((w (make-weak-vector 1)))
	  (vector-set! w 0 data)
	  (hash-table-set! table key w)))))

(define (weak-table-ref table key)
  (let ((w (hash-table-ref table key #f)))
    (if w
	(vector-ref w 0)
	#f)))

;; memoizer factory: for given (side-effect-free) procedure,
;; return a procedure which does the same memoizing some of results
;; in the sense of equal? on the whole list of args
;;
(define (make-weak-memoizer proc)
  (let ((cache (make-weak-table equal?)))
    (lambda args
      (let ((x (weak-table-ref cache args)))
	(if (bwp-object? x)
	  (let ((r (apply proc args)))
	    (weak-table-set! cache args r)
	    r)
	  x)))))
&lt;/source&gt;

== References ==
{{reflist}}

==Further reading==
* {{cite journal|doi=10.1145/368892.368907|author=Ershov, A.P.|title=On programming of arithmetic operations|journal=[[Communications of the ACM]]|volume=1|issue=8|pages=3&amp;ndash;6|year=1958}}
* Jean Goubault. Implementing Functional Languages with Fast Equality, Sets and Maps: an Exercise in Hash Consing. In ''[[Journées Francophones des Langages Applicatifs]]'' (JFLA’93), pages 222–238, Annecy, February 1993.

[[Category:Implementation of functional programming languages]]
[[Category:Hashing]]
[[Category:Software design patterns]]
[[Category:Articles with example Scheme code]]


{{comp-sci-stub}}</text>
      <sha1>btfqimmz6c14j9yue8rvslxsvty18vh</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hash join</title>
    <ns>0</ns>
    <id>1983584</id>
    <revision>
      <id>626214249</id>
      <parentid>620052330</parentid>
      <timestamp>2014-09-19T14:34:03Z</timestamp>
      <contributor>
        <username>Rowena-san</username>
        <id>19978450</id>
      </contributor>
      <minor/>
      <comment>Changed predicate to [[predicate]]</comment>
      <text xml:space="preserve" bytes="7178">The '''hash join''' is an example of a [[Join (SQL)|join algorithm]] and is used in the implementation of a [[relational database|relational]] [[database management system]].

The task of a join algorithm is to find, for each distinct value of the join attribute, the set of [[Tuple#Relational model|tuples]] in each relation which have that value.

Hash joins require an [[equijoin]] predicate (a [[predicate]] comparing values from one table with values from the other table using the equals operator '=').

== Classic hash join ==
The classic hash join algorithm for an [[Join_(SQL)#Inner_join|inner join]] of two relations proceeds as follows:
* First prepare a [[hash table]] of the smaller relation. The [[hash table]] entries consist of the join attribute and its row. Because the hash table is accessed by applying a [[hash function]] to the join attribute, it will be much quicker to find a given join attribute's rows by using this table than by scanning the original relation.
* Once the [[hash table]] is built, scan the larger relation and find the relevant rows from the smaller relation by looking in the [[hash table]].
The first phase is usually called the '''&quot;build&quot; phase''', while the second is called the '''&quot;probe&quot; phase'''. Similarly, the join relation on which the hash table is built is called the &quot;build&quot; input, whereas the other input is called the &quot;probe&quot; input.It is like merge join algorithm. 

This algorithm is simple, but it requires that the smaller join relation fits into memory, which is sometimes not the case. A simple approach to handling this situation proceeds as follows:

# For each tuple &lt;math&gt;r&lt;/math&gt; in the build input &lt;math&gt;R&lt;/math&gt;
## Add &lt;math&gt;r&lt;/math&gt; to the in-memory hash table
## If the size of the hash table equals the maximum in-memory size:
### Scan the probe input &lt;math&gt;S&lt;/math&gt;, and add matching join tuples to the output relation
### Reset the hash table
# Do a final scan of the probe input &lt;math&gt;S&lt;/math&gt; and add the resulting join tuples to the output relation

This is essentially the same as the [[block nested loop]] join algorithm. This algorithm scans &lt;math&gt;S&lt;/math&gt; more times than necessary.

== Grace hash join ==
A better approach is known as the &quot;grace hash join&quot;, after the GRACE database machine for which it was first implemented.

This algorithm avoids rescanning the entire &lt;math&gt;S&lt;/math&gt; relation by first partitioning both &lt;math&gt;R&lt;/math&gt; and &lt;math&gt;S&lt;/math&gt; via a hash function, and writing these partitions out to disk. The algorithm then loads pairs of partitions into memory, builds a hash table for the smaller partitioned relation, and probes the other relation for matches with the current hash table. Because the partitions were formed by hashing on the join key, it must be the case that any join output tuples must belong to the same partition.

It is possible that one or more of the partitions still does not fit into the available memory, in which case the algorithm is recursively applied: an additional orthogonal hash function is chosen to hash the large partition into sub-partitions, which are then processed as before. Since this is expensive, the algorithm tries to reduce the chance that it will occur by forming as many partitions as possible during the initial partitioning phase.

== Hybrid hash join ==
The hybrid hash join algorithm&lt;ref&gt;{{cite journal &lt;!-- This is a conference proceedings, but it's also a journal. --&gt;
  | last=DeWitt
  | first=D.J.
  |author2=Katz, R. |author3=Olken, F. |author4=Shapiro, L. |author5=Stonebraker, M. |author6= Wood, D. 
  | title=Implementation techniques for main memory database systems
  | volume=14
  | issue=4
  | pages=1–8
  | journal=Proc. ACM SIGMOD Conf
  | doi=10.1145/971697.602261
  |date=June 1984 }}&lt;/ref&gt; is a refinement of the grace hash join which takes advantage of more available memory. During the partitioning phase, the hybrid hash join uses the available memory for two purposes:
# To hold the current output buffer page for each of the &lt;math&gt;k&lt;/math&gt; partitions
# To hold an entire partition in-memory, known as &quot;partition 0&quot;
Because partition 0 is never written to or read from disk, the hybrid hash join typically performs fewer I/O operations than the grace hash join. Note that this algorithm is memory-sensitive, because there are two competing demands for memory (the hash table for partition 0, and the output buffers for the remaining partitions). Choosing too large a hash table might cause the algorithm to recurse because one of the non-zero partitions is too large to fit into memory.

== Hash anti-join ==
Hash joins can also be evaluated for an anti-join predicate (a predicate selecting values from one table when no related values are found in the other). Depending on the sizes of the tables, different algorithms can be applied:

=== Hash left anti-join ===

* Prepare a [[hash table]] for the '''NOT IN''' side of the join.
* Scan the other table, selecting any rows where the join attribute hashes to an empty entry in the hash table.

This is more efficient when the '''NOT IN''' table is smaller than the '''FROM''' table

=== Hash right anti-join ===

* Prepare a hash table for the '''FROM''' side of the join.
* Scan the '''NOT IN''' table, removing the corresponding records from the hash table on each hash hit
* Return everything that left in the hash table

This is more efficient when the '''NOT IN''' table is larger than the '''FROM''' table

== Hash semi-join ==

Hash semi-join is used to return the records found in the other table. Unlike plain join, it returns each matching record from the leading table only once, not regarding how many matches are there in the '''IN''' table.

As with the anti-join, semi-join can also be left and right:

=== Hash left semi-join ===

* Prepare a hash table for the '''IN''' side of the join.
* Scan the other table, returning any rows that produce a hash hit.

The records are returned right after they produced a hit. The actual records from the hash table are ignored.

This is more efficient when the '''IN''' table is smaller than the '''FROM''' table

=== Hash right semi-join ===

* Prepare a hash table for the '''FROM''' side of the join.
* Scan the '''IN''' table, returning the corresponding records from the hash table and removing them

With this algorithm, each record from the hash table (that is, '''FROM''' table) can only be returned once, since it's removed after being returned.

This is more efficient when the '''IN''' table is larger than the '''FROM''' table

==References==
&lt;references /&gt;

==External links==
* {{Cite journal |title=An Adaptive Hash Join Algorithm for Multiuser Environments
 |url=http://www.vldb.org/conf/1990/P186.PDF |format=PDF|author1=Hansjörg Zeller |author2=Jim Gray |author2-link=Jim Gray (computer scientist) |journal=Proceedings of the 16th VLDB conference |place=Brisbane |year=1990 |pages=186–197 |accessdate=2008-09-21 |postscript=&lt;!--None--&gt; |archiveurl=https://web.archive.org/web/20120311211953/http://www.vldb.org/conf/1990/P186.PDF |archivedate=2012-03-11}}

==See also==
[[Symmetric Hash Join]]

{{DEFAULTSORT:Hash Join}}
[[Category:Hashing]]
[[Category:Join algorithms]]</text>
      <sha1>dojeci78v89m1oiszxsrzdht1hlbs8h</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hopscotch hashing</title>
    <ns>0</ns>
    <id>25097895</id>
    <revision>
      <id>595674951</id>
      <parentid>568905852</parentid>
      <timestamp>2014-02-16T02:29:27Z</timestamp>
      <contributor>
        <ip>81.205.191.223</ip>
      </contributor>
      <comment>/* External links */</comment>
      <text xml:space="preserve" bytes="5629">'''Hopscotch hashing''' is a scheme in [[computer programming]] for resolving [[hash collision]]s of values of [[hash function]]s in a [[hash table|table]] using [[open addressing]]. It is also well suited for implementing a [[concurrent hash table]]. Hopscotch hashing was introduced by [[Maurice Herlihy]], [[Nir Shavit]] and Moran Tzafrir in 2008.&lt;ref&gt;{{cite conference | author = Herlihy, Maurice and Shavit, Nir and Tzafrir, Moran | title = Hopscotch Hashing | booktitle = DISC '08: Proceedings of the 22nd international symposium on Distributed Computing | year = 2008 | pages = 350–364 | location = Arcachon, France | publisher =Springer-Verlag | url=http://mcg.cs.tau.ac.il/papers/disc2008-hopscotch.pdf }}&lt;/ref&gt; The name is derived from the sequence of hops that characterize the table's insertion algorithm.

[[Image:hopscotch-wiki-example.gif|thumb|upright=2.5| Hopscotch hashing.  Here, ''H''  is 4. Gray entries are occupied.  In part (a), the item ''x'' is added with a hash value of 6. A linear probe finds that entry 13 is empty. Because 13 is more than 4 entries away from 6, the algorithm looks for an earlier entry to swap with 13. The first place to look in is ''H-1 = 3'' entries before, at entry 10. That entry's hop information bit-map indicates that ''d'', the item at entry 11, can be displaced to 13. After displacing ''d'', Entry 11 is still too far from entry 6, so the algorithm examines entry 8. The hop information bit-map indicates that item ''c'' at entry 9 can be moved to entry 11. Finally, ''a'' is moved to entry 9. Part (b) shows the table state just after adding ''x''.]]

The algorithm uses a single array of ''n'' buckets. For each bucket, its ''neighborhood'' is a small collection of nearby consecutive buckets (i.e. one with close indexes to the original hashed bucket). The desired property of the neighborhood is that the cost of finding an item in the buckets of the neighborhood is close to the cost of finding it in the bucket itself (for example, by having buckets in the neighborhood fall within the same [[cache line]]). The size of the neighborhood must be sufficient to accommodate a logarithmic number of items in the worst case (i.e. it must accommodate log(n) items), but only a constant number on average. If some bucket's neighborhood is filled, the table is resized.

In hopscotch hashing, as in [[cuckoo hashing]], and unlike in [[linear probing]], a given item will always be inserted-into and found-in the neighborhood of its hashed bucket. In other words, it will always be found either in its original hashed array entry, or in one of the next ''H-1'' neighboring entries. ''H'' could, for example, be 32, the standard machine word size. The neighborhood is thus a &quot;virtual&quot; bucket that has fixed size and overlaps with the next ''H-1'' buckets. To speed the search, each bucket (array entry) includes a &quot;hop-information&quot; word, an ''H''-bit bitmap that indicates which of the next ''H-1'' entries contain items that hashed to the current entry's virtual bucket. In this way, an item can be found quickly by looking at the word to see which entries belong to the bucket, and then scanning through the constant number of entries (most modern processors support special bit manipulation operations that make the lookup in the &quot;hop-information&quot; bitmap very fast).

Here is how to add item ''x'' which was hashed to bucket ''i'':

# If the entry ''i'' is empty, add ''x'' to ''i'' and return.
# Starting at entry ''i'', use a linear probe to find an empty entry at index ''j''.
# If the empty entry's index ''j'' is within ''H-1'' of entry ''i'', place ''x'' there and return. Otherwise, entry ''j'' is too far from ''i''. To create an empty entry closer to ''i'', find an item ''y'' whose hash value lies between ''i'' and ''j'', but within ''H-1'' of ''j''. Displacing ''y'' to ''j'' creates a new empty slot closer to ''i''. Repeat until  the empty entry is within ''H-1'' of entry ''i'', place ''x'' there and return. If no such item ''y'' exists, or if the bucket ''i'' already contains ''H'' items, resize and rehash the table.

The idea is that hopscotch hashing &quot;moves the empty slot towards the desired bucket&quot;. This distinguishes it from [[linear probing]] which leaves the empty slot where it was found, possibly far away from  the original bucket, or from [[cuckoo hashing]] that, in order to create a  free bucket, moves an item out of one of the desired buckets in the target arrays, and only then tries to find the displaced item a new place.

To remove an item from the table, one simply removes it from the table entry. If the neighborhood buckets are cache aligned, then one could apply a reorganization operation in which items are moved into the now vacant location in order to improve alignment.

One advantage of hopscotch hashing is that it provides good performance at very high table load factors, even ones exceeding 0.9. Part of this efficiency is due to using a linear probe only to find an empty slot  during insertion, not for every lookup as in the original [[linear probing]] hash table algorithm.  Another advantage is that one can use any hash function, in particular simple ones that are close-to-universal.

== See also ==

* [[Cuckoo hashing]]
* [[Hash collision]]
* [[Hash function]]
* [[Linear probing]]
* [[Hash_table#Open_addressing|Open addressing]]
* [[Perfect hashing]]
* [[Quadratic probing]]

== References ==

{{refbegin}} &lt;references /&gt; {{refend}}

== External links ==
*[https://code.google.com/p/libhhash/wiki/Intro libhhash - a C hopscotch hashing implementation]

[[Category:Search algorithms]]
[[Category:Hashing]]</text>
      <sha1>6zq40tng93r8q5p5xdix4738b1s932v</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Stable hashing</title>
    <ns>0</ns>
    <id>6635042</id>
    <revision>
      <id>591239250</id>
      <parentid>573721408</parentid>
      <timestamp>2014-01-18T07:24:11Z</timestamp>
      <contributor>
        <username>Rpyle731</username>
        <id>46515</id>
      </contributor>
      <minor/>
      <comment>stub sort</comment>
      <text xml:space="preserve" bytes="416">{{Unreferenced stub|auto=yes|date=December 2009}}
'''Stable hashing''' is a tool used to implement randomized [[Load balancing (computing)|load balancing]] and [[distributed lookup]] in [[peer-to-peer]] computer systems.

==See also==
* [[Hash function]]
* [[Consistent hashing]]

{{DEFAULTSORT:Stable Hashing}}
[[Category:Hashing]]
[[Category:Peer-to-peer computing]]


{{Datastructure-stub}}
{{compu-network-stub}}</text>
      <sha1>cp2fwbs5kh9uq8grwgxguvetj9r2e89</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Linear probing</title>
    <ns>0</ns>
    <id>1852304</id>
    <revision>
      <id>617276154</id>
      <parentid>617275904</parentid>
      <timestamp>2014-07-17T06:14:57Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>/* Algorithm */ step size is usually one</comment>
      <text xml:space="preserve" bytes="4797">'''Linear probing ''' is a scheme in [[computer programming]] for resolving [[hash collision]]s of values of [[hash function]]s by sequentially searching the [[hash table]] for a free location.&lt;ref&gt;{{cite book |last=Dale |first=Nell |title=C++ Plus Data Structures |year=2003 |publisher=Jones and Bartlett Computer Science |location=Sudbury, MA |isbn=0-7637-0481-4 }}&lt;/ref&gt;

==Algorithm==
Linear probing is accomplished using two values - one as a starting value and one as an interval between successive values in [[modular arithmetic]]. The second value, which is the same for all keys and known as the ''stepsize'', is repeatedly added to the starting value until a free space is found, or the entire table is traversed.  (In order to traverse the entire table the stepsize should be [[relatively prime]] to the arraysize, which is why the array size is often chosen to be a prime number.)

::newLocation = (startingValue + stepSize) % arraySize

Given an ordinary hash function ''H(x)'', a linear probing function (''H(x, i)'') would be:
::&lt;math&gt; H(x, i) =  (H(x) + i) \pmod n.\, &lt;/math&gt;
Here ''H(x)'' is the starting value, ''n'' the size of the hash table, and the ''stepsize'' is ''i'' in this case.

Often, the step size is one; that is, the array cells that are probed are consecutive in the hash table. [[Double hashing]] is a variant of the same method in which the step size is itself computed by a hash function.

==Properties==
This algorithm, which is used in open-addressed [[hash table]]s, provides good memory caching (if stepsize is equal to one), through good locality of reference, but also results in clustering, an unfortunately high [[probability]] that where there has been one collision there will be more. The performance of linear probing is also more sensitive to input distribution when compared to [[double hashing]], where the stepsize is determined by another hash function applied to the value instead of a fixed stepsize as in linear probing.

==Dictionary operation in constant time==
Using linear probing, dictionary operation can be implemented in constant time. In other words, insert, remove and find operations can be implemented in O(1), as long as the [[load factor (computer science)|load factor]] of the hash table is a constant strictly less than one.&lt;ref&gt;{{citation|first=Donald|last=Knuth|authorlink=Donald Knuth|title=Notes on &quot;Open&quot; Addressing|year=1963|url=http://algo.inria.fr/AofA/Research/11-97.html}}&lt;/ref&gt; This analysis makes the (unrealistic) assumption that the hash function is completely random, but can be extended also to [[K-independent hashing|5-independent hash functions]].&lt;ref&gt;{{citation
 | last1 = Pagh | first1 = Anna
 | last2 = Pagh | first2 = Rasmus
 | last3 = Ružić | first3 = Milan
 | doi = 10.1137/070702278
 | issue = 3
 | journal = SIAM Journal on Computing
 | mr = 2538852
 | pages = 1107–1120
 | title = Linear probing with constant independence
 | volume = 39
 | year = 2009}}&lt;/ref&gt; Weaker properties, such as [[universal hashing]], are not strong enough to ensure the constant-time operation of linear probing,&lt;ref&gt;{{citation
 | last1 = Pătraşcu | first1 = Mihai | author1-link = Mihai Pătraşcu
 | last2 = Thorup | first2 = Mikkel | author2-link = Mikkel Thorup
 | contribution = On the k-independence required by linear probing and minwise independence
 | doi = 10.1007/978-3-642-14165-2_60
 | pages = 715–726
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Automata, Languages and Programming, 37th International Colloquium, ICALP 2010, Bordeaux, France, July 6-10, 2010, Proceedings, Part I
 | url = http://people.csail.mit.edu/mip/papers/kwise-lb/kwise-lb.pdf
 | volume = 6198
 | year = 2010}}&lt;/ref&gt; but one practical method of hash function generation, [[tabulation hashing]], again leads to a guaranteed constant expected time performance despite not being 5-independent.&lt;ref&gt;{{citation
 | last1 = Pătraşcu | first1 = Mihai | author1-link = Mihai Pătraşcu
 | last2 = Thorup | first2 = Mikkel | author2-link = Mikkel Thorup
 | arxiv = 1011.5200
 | contribution = The power of simple tabulation hashing
 | doi = 10.1145/1993636.1993638
 | pages = 1–10
 | title = Proceedings of the 43rd annual ACM Symposium on Theory of Computing (STOC '11)
 | year = 2011}}&lt;/ref&gt;

==See also==
* [[Quadratic probing]]
* [[Hash_table#Collision_resolution|Collision resolution]]

==References==
{{reflist}}

==External links==
*[http://www.siam.org/meetings/alenex05/papers/13gheileman.pdf How Caching Affects Hashing] by Gregory L. Heileman and Wenbin Luo 2005.
*[http://opendatastructures.org/versions/edition-0.1e/ods-java/5_2_LinearHashTable_Linear_.html Open Data Structures - Section 5.2 - LinearHashTable: Linear Probing]

[[Category:Search algorithms]]
[[Category:Hashing]]</text>
      <sha1>rbu4q5isg4nuhpac1sofbb2kgpgq7vr</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Perfect hash function</title>
    <ns>0</ns>
    <id>268162</id>
    <revision>
      <id>623132858</id>
      <parentid>600387735</parentid>
      <timestamp>2014-08-28T04:42:51Z</timestamp>
      <contributor>
        <ip>5.172.141.17</ip>
      </contributor>
      <comment>Fix link</comment>
      <text xml:space="preserve" bytes="6782">A '''perfect hash function''' for a set S is a [[hash function]] that maps distinct elements in S to a set of integers, with no [[hash collision|collisions]]. A perfect hash function has many of the same [[Hash function#Applications|applications]] as other hash functions, but with the advantage that no collision resolution has to be implemented. In mathematical terms, it is a [[Partial_function#Total_function|total]] [[injective function]].

== Properties and uses ==
A perfect hash function for a specific set S that can be evaluated in constant time, and with values in a small range, can be found by a [[randomized algorithm]] in a number of operations that is proportional to the size of S.&lt;ref name=&quot;inventor&quot;&gt;{{cite doi|10.1145/828.1884}}&lt;/ref&gt; Any perfect hash functions suitable for use with a [[hash table]] require at least a number of bits that is proportional to the size of S.

A perfect hash function with values in a limited [[range (mathematics)|range]] can be used for efficient lookup operations, by placing keys from S (or other associated values) in a [[lookup table|table]] indexed by the output of the function. Using a perfect hash function is best in situations where there is a frequently queried large set, S, which is seldom updated. Efficient solutions to performing updates are known as [[dynamic perfect hashing]], but these methods are relatively complicated to implement. A simple alternative to perfect hashing, which also allows dynamic updates, is [[cuckoo hashing]].

== Minimal perfect hash function ==
A '''minimal perfect hash function''' is a perfect hash function that maps ''n'' keys to ''n'' consecutive integers—usually [0..''n''−1] or [1..''n''].  A more formal way of expressing this is:  Let ''j'' and ''k'' be elements of some finite set '''K'''.  F is a minimal perfect hash function [[iff]] F(''j'') =F(''k'') implies ''j''=''k'' ([[injectivity]]) and there exists an integer ''a'' such that the range of F is ''a''..''a''+|'''K'''|−1. It has been proved that a general purpose minimal perfect hash scheme requires at least 1.44 bits/key.&lt;ref&gt;{{Cite doi|10.1007/978-3-642-04128-0_61}}&lt;/ref&gt; The best currently known minimal perfect hashing schemes use around 2.6 bits/key.&lt;ref&gt;{{citation|contribution=Searching|first1=Ricardo|last1=Baeza-Yates|author1-link=Ricardo Baeza-Yates|first2=Patricio V.|last2=Poblete|title=Algorithms and Theory of Computation Handbook: General Concepts and Techniques|edition=2nd|editor1-first=Mikhail J.|editor1-last=Atallah|editor1-link=Mikhail Atallah|editor2-first=Marina|editor2-last=Blanton|publisher=CRC Press|year=2010|isbn=9781584888239}}. See in particular [http://books.google.com/books?id=5uA1c8TuOC0C&amp;pg=SA2-PA10 p.&amp;nbsp;2-10].&lt;/ref&gt;

A minimal perfect hash function F is '''order preserving''' if keys are given in some order ''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt;, ..., ''a''&lt;sub&gt;''n''&lt;/sub&gt; and for any keys ''a''&lt;sub&gt;''j''&lt;/sub&gt; and ''a''&lt;sub&gt;''k''&lt;/sub&gt;, ''j''&amp;lt;''k'' implies F(''a''&lt;sub&gt;''j''&lt;/sub&gt;)&amp;lt;F(''a''&lt;sub&gt;''k''&lt;/sub&gt;).&lt;ref&gt;{{Citation |first=Bob |last=Jenkins |contribution=order-preserving minimal perfect hashing |title=Dictionary of Algorithms and Data Structures |editor-first=Paul E. |editor-last=Black |publisher=U.S. National Institute of Standards and Technology |date=14 April 2009 |accessdate=2013-03-05 |url=http://www.nist.gov/dads/HTML/orderPreservMinPerfectHash.html}}&lt;/ref&gt; Order-preserving minimal perfect hash functions require necessarily Ω(''n'' log ''n'') bits to be represented.&lt;ref&gt;{{Cite doi|10.1145/96749.98233}}&lt;/ref&gt;

A minimal perfect hash function F is '''monotone''' if it preserves the [[lexicographical order]] of the keys. In this case, the function value is just the position of each key in the sorted ordering of all of the keys. If the keys to be hashed are themselves stored in a sorted array, it is possible to store a small number of additional bits per key in a data structure that can be used to compute hash values quickly.&lt;ref&gt;{{citation
 | last1 = Belazzougui | first1 = Djamal
 | last2 = Boldi | first2 = Paolo
 | last3 = Pagh | first3 = Rasmus
 | last4 = Vigna | first4 = Sebastiano
 | date = November 2008
 | doi = 10.1145/1963190.2025378
 | journal = Journal of Experimental Algorithmics
 | at = Art. no. 3.2, 26pp
 | title = Theory and practice of monotone minimal perfect hashing
 | volume = 16}}.&lt;/ref&gt;

== See also ==
*[[Dynamic perfect hashing]]
*[[Pearson hashing]]
*[[Universal hashing]]

== References ==
{{reflist|30em}}

== Further reading ==
*Richard J. Cichelli. ''Minimal Perfect Hash Functions Made Simple'', Communications of the ACM, Vol. 23, Number 1, January 1980.
* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 11.5: Perfect hashing, pp.&amp;nbsp;245&amp;ndash;249.
* Fabiano C. Botelho, Rasmus Pagh and Nivio Ziviani. [http://arxiv.org/pdf/cs/0702159 &quot;Perfect Hashing for Data Management Applications&quot;].
* Fabiano C. Botelho and [[Nivio Ziviani]]. [http://homepages.dcc.ufmg.br/~nivio/papers/cikm07.pdf &quot;External perfect hashing for very large key sets&quot;]. 16th ACM Conference on Information and Knowledge Management (CIKM07), Lisbon, Portugal, November 2007.
* Djamal Belazzougui, Paolo Boldi, Rasmus Pagh, and Sebastiano Vigna. [http://vigna.dsi.unimi.it/ftp/papers/MonotoneMinimalPerfectHashing.pdf &quot;Monotone minimal perfect hashing: Searching a sorted table with O(1) accesses&quot;]. In Proceedings of the 20th Annual ACM-SIAM Symposium On Discrete Mathematics (SODA), New York, 2009. ACM Press.
* Djamal Belazzougui, Paolo Boldi, Rasmus Pagh, and Sebastiano Vigna. [http://www.siam.org/proceedings/alenex/2009/alx09_013_belazzouguid.pdf &quot;Theory and practise of monotone minimal perfect hashing&quot;]. In Proceedings of the Tenth Workshop on Algorithm Engineering and Experiments (ALENEX). SIAM, 2009.
* Douglas C. Schmidt, [http://www.cs.wustl.edu/~schmidt/PDF/gperf.pdf GPERF: A Perfect Hash Function Generator], C++ Report, SIGS, Vol. 10, No. 10, November/December, 1998.

== External links ==
*[http://burtleburtle.net/bob/hash/perfect.html Minimal Perfect Hashing] by Bob Jenkins
*[http://www.gnu.org/software/gperf/ gperf] is an [[Open Source]] C and C++ perfect hash generator
*[http://cmph.sourceforge.net/index.html cmph] is [[Open Source]] implementing many perfect hashing methods
*[http://sux.di.unimi.it/ Sux4J] is [[Open Source]] implementing perfect hashing, including monotone minimal perfect hashing in Java
*[http://www.dupuis.me/node/9 MPHSharp] is [[Open Source]] implementing many perfect hashing methods in C#

{{DEFAULTSORT:Perfect Hash Function}}
[[Category:Hashing]]
[[Category:Hash functions]]
[[Category:Search algorithms]]</text>
      <sha1>sr1ijhd0uioluv7pidblwu7gpueg3om</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cryptographic hash function</title>
    <ns>0</ns>
    <id>439526</id>
    <revision>
      <id>624695751</id>
      <parentid>619961631</parentid>
      <timestamp>2014-09-08T17:27:39Z</timestamp>
      <contributor>
        <username>Cpiral</username>
        <id>2517237</id>
      </contributor>
      <comment>/* See also */ +  [[Security of cryptographic hash functions]]</comment>
      <text xml:space="preserve" bytes="31938">[[Image:Cryptographic Hash Function.svg|thumb|375px|right|A cryptographic hash function (specifically, [[SHA-1]]) at work. Note that even small changes in the source input (here in the word &quot;over&quot;) drastically change the resulting output, by the so-called [[avalanche effect]].]]
A '''cryptographic hash function''' is a [[hash function]] which is considered [[One-way function|practically impossible to invert]], that is, to recreate the input data from its hash value alone. The input data is often called the ''message'', and the hash value is often called the ''message digest'' or simply the ''digest''.

The ideal cryptographic hash function has four main properties:
* it is easy to compute the hash value for any given message
* it is [[Computational complexity theory#Intractability|infeasible]] to generate a message that has a given hash
* it is infeasible to modify a message without changing the hash
* it is infeasible to find two different messages with the same hash.

Cryptographic hash functions have many [[information security]] applications, notably in [[digital signature]]s, [[message authentication codes]] (MACs), and other forms of [[authentication]]. They can also be used as ordinary [[hash function]]s, to index data in [[hash table]]s, for [[fingerprint (computing)|fingerprinting]], to detect duplicate data or uniquely identify files, and as [[checksum]]s to detect accidental data corruption. Indeed, in information security contexts, cryptographic hash values are sometimes called (''digital'') ''fingerprints'', ''checksums'', or just ''hash values'', even though all these terms stand for more general functions with rather different properties and purposes.

==Properties==
Most cryptographic hash functions are designed to take a [[string (computer science)|string]] of any length as input and produce a fixed-length hash value.

A cryptographic hash function must be able to withstand all known [[Cryptanalysis#Types of cryptanalytic attack|types of cryptanalytic attack]]. At a minimum, it must have the following properties:

* ''Pre-image resistance''
*: Given a hash ''h'' it should be difficult to find any message ''m'' such that ''h = hash(m)''. This concept is related to that of [[one-way function]]. Functions that lack this property are vulnerable to [[preimage attack]]s.
* ''Second pre-image resistance''
*: Given an input ''m''&lt;sub&gt;1&lt;/sub&gt; it should be difficult to find another input ''m''&lt;sub&gt;2&lt;/sub&gt; such that {{nowrap|1=''m''&lt;sub&gt;1&lt;/sub&gt; ≠ ''m''&lt;sub&gt;2&lt;/sub&gt;}} and {{nowrap|1=''hash''(''m''&lt;sub&gt;1&lt;/sub&gt;) = ''hash''(''m''&lt;sub&gt;2&lt;/sub&gt;)}}. Functions that lack this property are vulnerable to [[preimage attack|second-preimage attacks]].
* ''[[Collision resistance]]''
*: It should be difficult to find two different messages ''m''&lt;sub&gt;1&lt;/sub&gt; and ''m''&lt;sub&gt;2&lt;/sub&gt; such that {{nowrap|1=''hash''(''m''&lt;sub&gt;1&lt;/sub&gt;) = ''hash''(''m''&lt;sub&gt;2&lt;/sub&gt;)}}. Such a pair is called a cryptographic [[hash collision]]. This property is sometimes referred to as ''strong collision resistance.'' It requires a hash value at least twice as long as that required for preimage-resistance; otherwise collisions may be found by a [[birthday attack]].

These properties imply that a [[adversary (cryptography)|malicious adversary]] cannot replace or modify the input data without changing its digest. Thus, if two strings have the same digest, one can be very confident that they are identical.

A function meeting these criteria may still have undesirable properties. Currently popular cryptographic hash functions are vulnerable to [[Length extension attack|''length-extension'' attacks]]: given {{nowrap|''hash''(''m'')}} and {{nowrap|''len''(''m'')}} but not ''m'', by choosing a suitable ''m''{{'}} an attacker can calculate {{nowrap|''hash''(''m'' {{!}}{{!}} ''m''{{'}})}} where || denotes [[concatenation]].&lt;ref&gt;{{cite web|url=http://vnhacker.blogspot.com/2009/09/flickrs-api-signature-forgery.html|title=Flickr's API Signature Forgery Vulnerability |publisher=Thai Duong and Juliano Rizzo}}&lt;/ref&gt; This property can be used to break naive authentication schemes based on hash functions. The [[HMAC]] construction works around these problems.

Ideally, one may wish for even stronger conditions. It should be impossible for an adversary to find two messages with substantially similar digests; or to infer any useful information about the data, given only its digest. Therefore, a cryptographic hash function should behave as much as possible like a [[random function]] while still being deterministic and efficiently computable.

Checksum algorithms, such as [[CRC32]] and other [[cyclic redundancy check]]s, are designed to meet much weaker requirements, and are generally unsuitable as cryptographic hash functions. For example, a CRC was used for message integrity in the [[Wired Equivalent Privacy|WEP]] encryption standard, but an attack was readily discovered which exploited the linearity of the checksum.

===Degree of difficulty===
In cryptographic practice, “difficult” generally means “almost certainly beyond the reach of any adversary who must be prevented from breaking the system for as long as the security of the system is deemed important”. The meaning of the term is therefore somewhat dependent on the application, since the effort that a malicious agent may put into the task is usually proportional to his expected gain. However, since the needed effort usually grows very quickly with the digest length, even a thousand-fold advantage in processing power can be neutralized by adding a few dozen bits to the latter.

In some [[Computational complexity theory|theoretical analyses]] “difficult” has a specific mathematical meaning, such as &quot;not solvable in [[asymptotic computational complexity|asymptotic]] [[polynomial time]]&quot;. Such interpretations of ''difficulty'' are important in the study of [[provably secure cryptographic hash function]]s but do not usually have a strong connection to practical security. For example, an [[exponential time]] algorithm can sometimes still be fast enough to make a feasible attack. Conversely, a polynomial time algorithm (e.g., one that requires ''n''&lt;sup&gt;20&lt;/sup&gt; steps for ''n''-digit keys) may be too slow for any practical use.

==Illustration==
An illustration of the potential use of a cryptographic hash is as follows: [[Alice and Bob|Alice]] poses a tough math problem to [[Alice and Bob|Bob]] and claims she has solved it. Bob would like to try it himself, but would yet like to be sure that Alice is not bluffing. Therefore, Alice writes down her solution, computes its hash and tells Bob the hash value (whilst keeping the solution secret). Then, when Bob comes up with the solution himself a few days later, Alice can prove that she had the solution earlier by revealing it and having Bob hash it and check that it matches the hash value given to him before. (This is an example of a simple [[commitment scheme]]; in actual practice, Alice and Bob will often be computer programs, and the secret would be something less easily spoofed than a claimed puzzle solution).

==Applications==

===Verifying the integrity of files or messages===
{{ main | File verification }}

An important application of secure hashes is verification of [[message integrity]]. Determining whether any changes have been made to a message (or a [[computer file|file]]), for example, can be accomplished by comparing message digests calculated before, and after, transmission (or any other event).

For this reason, most [[digital signature]] algorithms only confirm the authenticity of a hashed digest of the message to be &quot;signed&quot;. Verifying the authenticity of a hashed digest of the message is considered proof that the message itself is authentic.

[[MD5]], [[SHA1]], or [[SHA2]] hashes are sometimes posted along with files on websites or forums to allow verification of integrity.&lt;ref&gt;{{cite web | url=http://www.techrepublic.com/blog/security/use-md5-hashes-to-verify-software-downloads/374 | title=Use MD5 hashes to verify software downloads | publisher=TechRepublic | date=December 5, 2007 | accessdate=March 2, 2013 | last = Perrin | first = Chad}}&lt;/ref&gt; This practice establishes a [[chain of trust]] so long as the hashes are posted on a site authenticated by [[HTTPS]].

===Password verification===
A related application is [[password]] verification (first invented by [[Roger Needham]]). Storing all user passwords as [[cleartext]] can result in a massive security breach if the password file is compromised. One way to reduce this danger is to only store the hash digest of each password. To authenticate a user, the password presented by the user is hashed and compared with the stored hash.  (Note that this approach prevents the original passwords from being retrieved if forgotten or lost, and they have to be replaced with new ones.) The password is often concatenated with a random, non-secret [[Salt (cryptography)|salt]] value before the hash function is applied. The salt is stored with the password hash. Because users have different salts, it is not feasible to store tables of [[precomputation|precomputed]] hash values for common passwords. [[Key stretching]] functions, such as [[PBKDF2]], [[Bcrypt]] or [[Scrypt]], typically use repeated invocations of a cryptographic hash to increase the time required to perform [[brute force attack]]s on stored password digests.

{{main|Password cracking}}

In 2013 a long-term [[Password Hashing Competition]] was announced to choose a new, standard algorithm for password hashing.&lt;ref&gt;{{cite web | url=https://password-hashing.net/call.html | title=Password Hashing Competition | accessdate=March 3, 2013}}&lt;/ref&gt;

===File or data identifier===

A message digest can also serve as a means of reliably identifying a file; several [[Source Code Management|source code management]] systems, including [[Git (software)|Git]], [[Mercurial (software)|Mercurial]] and [[Monotone (software)|Monotone]], use the [[sha1sum]] of various types of content (file content, directory trees, ancestry information, etc.) to uniquely identify them. Hashes are used to identify files on [[peer-to-peer]] [[filesharing]] networks. For example, in an [[ed2k link]], an [[MD4]]-variant hash is combined with the file size, providing sufficient information for locating file sources, downloading the file and verifying its contents. [[Magnet URI scheme|Magnet links]] are another example. Such file hashes are often the top hash of a [[hash list]] or a [[Merkle tree|hash tree]] which allows for additional benefits.

One of the main applications of a [[hash function]] is to allow the fast look-up of a data in a [[hash table]]. Being hash functions of a particular kind, cryptographic hash functions lend themselves well to this application too.

However, compared with standard hash functions, cryptographic hash functions tend to be much more expensive computationally. For this reason, they tend to be used in contexts where it is necessary for users to protect themselves against the possibility of forgery (the creation of data with the same digest as the expected data) by potentially malicious participants.

===Pseudorandom generation and key derivation===

Hash functions can also be used in the generation of [[pseudorandom]] bits, or to [[Key derivation function|derive new keys or passwords]] from a single, secure key or password.

==Hash functions based on block ciphers==
There are several methods to use a [[block cipher]] to build a cryptographic hash function, specifically a [[one-way compression function]].

The methods resemble the [[block cipher modes of operation]] usually used for encryption. All well-known hash functions, including [[MD4]], [[MD5]], [[SHA-1]] and [[SHA-2]] are built from block-cipher-like components designed for the purpose, with feedback to ensure that the resulting function is not invertible.  [[NIST hash function competition|SHA-3]] finalists included functions with block-cipher-like components (e.g., [[Skein hash function|Skein]], BLAKE) though the function finally selected, [[Keccak]], was built on a [[sponge function|cryptographic sponge]] instead.

A standard block cipher such as [[Advanced Encryption Standard|AES]] can be used in place of these custom block ciphers; that might be useful when an [[embedded system]] needs to implement both encryption and hashing with minimal code size or hardware area.  However, that approach can have costs in efficiency and security.  The ciphers in hash functions are built for hashing: they use large keys and blocks, can efficiently change keys every block, and have been designed and vetted for resistance to [[related-key attack]]s.  General-purpose ciphers tend to have different design goals.  In particular, AES has key and block sizes that make it nontrivial to use to generate long hash values; AES encryption becomes less efficient when the key changes each block; and related-key attacks make it potentially less secure for use in a hash function than for encryption.

==Merkle–Damgård construction==
{{Main|Merkle–Damgård construction}}
[[Image:Merkle-Damgard hash big.svg|thumb|400px|right|The Merkle–Damgård hash construction.]]
&lt;!--This section is largely duplicated in [[one-way compression function]]. Both should be moved to [[Merkle–Damgård construction]]--&gt;
A hash function must be able to process an arbitrary-length message into a fixed-length output. This can be achieved by breaking the input up into a series of equal-sized blocks, and operating on them in sequence using a one-way compression function. The compression function can either be specially designed for hashing or be built from a block cipher. A hash function built with the Merkle–Damgård construction is as resistant to collisions as is its compression function; any collision for the full hash function can be traced back to a collision in the compression function.

The last block processed should also be unambiguously [[Padding (cryptography)|length padded]]; this is crucial to the security of this construction. This construction is called the [[Merkle–Damgård construction]]. Most widely used hash functions, including [[SHA-1]] and [[MD5]], take this form.

The construction has certain inherent flaws, including length-extension and generate-and-paste attacks, and cannot be parallelized. As a result, many entrants in the current [[NIST hash function competition]] are built on different, sometimes novel, constructions.

==Use in building other cryptographic primitives==
Hash functions can be used to build other cryptographic primitives. For these other primitives to be cryptographically secure, care must be taken to build them correctly.

[[Message authentication code]]s (MACs) (also called keyed hash functions) are often built from hash functions. [[HMAC]] is such a MAC.

Just as [[block cipher]]s can be used to build hash functions, hash functions can be used to build block ciphers.  [[Luby-Rackoff]] constructions using hash functions can be provably secure if the underlying hash function is secure.  Also, many hash functions (including [[SHA-1]] and [[SHA-2]]) are built by using a special-purpose block cipher in a [[One-way compression function#Davies.E2.80.93Meyer|Davies-Meyer]] or other construction.  That cipher can also be used in a conventional mode of operation, without the same security guarantees.  See [[SHACAL]], [[BEAR (cipher)|BEAR]] and [[LION (cipher)|LION]].

[[Pseudorandom number generator]]s (PRNGs) can be built using hash functions.  This is done by combining a (secret) random seed with a counter and hashing it.

Some hash functions, such as [[Skein (hash function)|Skein]], [[Keccak]], and [[RadioGatún]] output an arbitrarily long stream and can be used as a [[stream cipher]], and stream ciphers can also be built from fixed-length digest hash functions. Often this is done by first building a [[cryptographically secure pseudorandom number generator]] and then using its stream of random bytes as [[keystream]]. [[SEAL (cipher)|SEAL]] is a stream cipher that uses [[SHA-1]] to generate internal tables, which are then used in a keystream generator more or less unrelated to the hash algorithm.  SEAL is not guaranteed to be as strong (or weak) as SHA-1. Similarly, the key expansion of the [[HC-256|HC-128 and HC-256]] stream ciphers makes heavy use of the [[SHA-2|SHA256]] hash function.

==Concatenation of cryptographic hash functions==
Concatenating outputs from multiple hash functions provides collision resistance as good as the strongest of the algorithms included in the concatenated result.  For example, older versions of [[Transport Layer Security|TLS/SSL]] use concatenated [[MD5]] and [[SHA-1]] sums—this ensures that a method to find collisions in one of the functions doesn't allow forging traffic protected with both functions.

For Merkle-Damgård hash functions, the concatenated function is as collision-resistant as its strongest component,&lt;ref&gt;Note that any two messages that collide the concatenated function also collide each component function, by the nature of concatenation.  For example, if concat(sha1(message1), md5(message1)) == concat(sha1(message2), md5(message2)) then sha1(message1) == sha1(message2) and md5(message1)==md5(message2).  The concatenated function could have other problems that the strongest hash lacks -- for example, it might leak information about the message when the strongest component does not, or it might be detectably nonrandom when the strongest component is not -- but it can't be less collision-resistant.&lt;/ref&gt; but not more collision-resistant.&lt;ref&gt;More generally, if an attack can produce a collision in one hash function's ''internal state,'' attacking the combined construction is only as difficult as a [[birthday attack]] against the other function(s).  For the detailed argument, see the Joux and Finney references that follow.&lt;/ref&gt; Joux&lt;ref&gt;[[Antoine Joux]]. ''Multicollisions in Iterated Hash Functions. Application to Cascaded Constructions''. LNCS 3152/2004, pages 306-316 [http://www.springerlink.com/index/DWWVMQJU0N0A3UGJ.pdf Full text].&lt;/ref&gt; noted that 2-collisions lead to n-collisions: if it is feasible to find two messages with the same MD5 hash, it is effectively no more difficult to find as many messages as the attacker  desires with identical MD5 hashes.  Among the n messages with the same MD5 hash, there is likely to be a collision in SHA-1.  The additional work needed to find the SHA-1 collision (beyond the exponential birthday search) is [[Polynomial time|polynomial]]. This argument is summarized by [http://article.gmane.org/gmane.comp.encryption.general/5154 Finney].  A more current paper and full proof of the security of such a combined construction gives a clearer and more complete explanation of the above.&lt;ref&gt;{{cite paper | first1 = Jonathan J. | last1 = Hoch | first2 = Adi | last2 = Shamir |date=2008-02-20 |title=On the Strength of the Concatenated Hash Combiner when all the Hash Functions are Weak |url=http://eprint.iacr.org/2008/075.pdf }}&lt;/ref&gt;

==Cryptographic hash algorithms==
There is a long list of cryptographic hash functions, although many have been found to be vulnerable and should not be used.  Even if a hash function has never been broken, a [[Cryptographic attack#Amount of information available to the attacker|successful attack]] against a weakened variant thereof may undermine the experts' confidence and lead to its abandonment.   For instance, in August 2004 weaknesses were found in a number of hash functions that were popular at the time, including SHA-0, RIPEMD, and MD5.  This has called into question the long-term security of later algorithms which are derived from these hash functions&amp;nbsp;— in particular, SHA-1 (a strengthened version of SHA-0), RIPEMD-128, and RIPEMD-160 (both strengthened versions of RIPEMD). Neither SHA-0 nor RIPEMD are widely used since they were replaced by their strengthened versions.

As of 2009, the two most commonly used cryptographic hash functions are [[MD5]] and [[SHA-1]].  However, MD5 has been broken; an attack against it was used to break [[Transport Layer Security|SSL]] in 2008.&lt;ref&gt;Alexander Sotirov, Marc Stevens, Jacob Appelbaum, Arjen Lenstra, David Molnar, Dag Arne Osvik, Benne de Weger, [http://www.win.tue.nl/hashclash/rogue-ca/ MD5 considered harmful today: Creating a rogue CA certificate], accessed March 29, 2009&lt;/ref&gt;

The [[SHA-0]] and [[SHA-1]] hash functions were developed by the [[NSA]].

On 12 August 2004, a collision for the full SHA-0 algorithm was announced by Joux, Carribault, Lemuet, and Jalby. This was done by using a generalization of the Chabaud and Joux attack. Finding the collision had complexity 2&lt;sup&gt;51&lt;/sup&gt; and took about 80,000 CPU hours on a [[supercomputer]] with 256 [[Itanium 2]] processors. (Equivalent to 13 days of full-time use of the computer.)

In February 2005, an attack on SHA-1 was reported that would find collision in about 2&lt;sup&gt;69&lt;/sup&gt; hashing operations, rather than the 2&lt;sup&gt;80&lt;/sup&gt; expected for a 160-bit hash function. In August 2005, another attack on SHA-1 was reported that would find collisions in 2&lt;sup&gt;63&lt;/sup&gt; operations. Though theoretical weaknesses of SHA-1 exist,&lt;ref&gt;Xiaoyun Wang, Yiqun Lisa Yin, and Hongbo Yu, [http://people.csail.mit.edu/yiqun/SHA1AttackProceedingVersion.pdf Finding Collisions in the Full SHA-1]&lt;/ref&gt;&lt;ref&gt;Bruce Schneier, [http://www.schneier.com/blog/archives/2005/02/cryptanalysis_o.html Cryptanalysis of SHA-1] (summarizes Wang et al. results and their implications)&lt;/ref&gt; no collision (or near-collision) has yet to be found. Nonetheless, it is often suggested that it may be practical to break within years, and that new applications can avoid these problems by using later members of the SHA family, such as [[SHA-2]], or using techniques such as randomized hashing&lt;ref&gt;Shai Halevi, Hugo Krawczyk, [http://csrc.nist.gov/groups/ST/hash/documents/HALEVI_UpdateonRandomizedHashing0824.pdf Update on Randomized Hashing]&lt;/ref&gt;&lt;ref&gt;Shai Halevi and Hugo Krawczyk, [http://www.ee.technion.ac.il/~hugo/rhash/ Randomized Hashing and Digital Signatures]&lt;/ref&gt; that do not require collision resistance.

However, to ensure the long-term robustness of applications that use hash functions, there was a [[NIST hash function competition|competition]] to design a replacement for SHA-2. On October 2, 2012, Keccak was selected as the winner of the [[National Institute of Standards and Technology|NIST]] hash function competition. A version of this algorithm is expected to become a [[Federal Information Processing Standard|FIPS]] standard in 2014 under the name [[SHA-3]].&lt;ref&gt;[http://csrc.nist.gov/groups/ST/hash/sha-3/timeline_fips.html NIST.gov - Computer Security Division - Computer Security Resource Center]&lt;/ref&gt;

Some of the following algorithms are used often in cryptography; consult the article for each specific algorithm for more information on the status of each algorithm.  Note that this list does not include candidates in the current NIST hash function competition.  For additional hash functions see the box at the bottom of the page.

{| class=&quot;wikitable&quot;
|-
! rowspan=&quot;2&quot;|Algorithm
! rowspan=&quot;2&quot;|Output size (bits)
! rowspan=&quot;2&quot;|Internal state size&lt;ref group=&quot;c&quot;&gt;The ''internal state'' here means the &quot;internal hash sum&quot; after each compression of a data block. Most hash algorithms also internally use some additional variables such as length of the data compressed so far since that is needed for the length padding in the end. See the [[Merkle-Damgård construction]] for details.&lt;/ref&gt;
! rowspan=&quot;2&quot;|Block size
! rowspan=&quot;2&quot;|Length size
! rowspan=&quot;2&quot;|Word size
! rowspan=&quot;2&quot;|Rounds
! colspan=&quot;3&quot;|Best known attacks &lt;br /&gt;(complexity:rounds)&lt;ref group=&quot;c&quot;&gt;When omitted, rounds are full number.&lt;/ref&gt;
|-
! [[Collision attack|Collision]]
! [[Preimage attack|Second&lt;br /&gt;preimage]]
! [[Preimage attack|Preimage]]

|- style=&quot;text-align:center;&quot;
| '''[[GOST (hash function)|GOST]]'''
| 256
| 256
| 256
| 256
| 32
| 256
| {{bad|Yes}} ([http://www.springerlink.com/content/2514122231284103/ 2&lt;sup&gt;105&lt;/sup&gt;])
| {{depends|Yes}}&lt;ref group=&quot;c&quot; name=&quot;second_preimage&quot;&gt;There isn't a unique second preimage attack against this hash.  However, the second preimage challenge reduces to the ordinary preimage attack by simply constructing a hash of the given message.&lt;/ref&gt; ([http://www.springerlink.com/content/2514122231284103/ 2&lt;sup&gt;192&lt;/sup&gt;])
| {{bad|Yes}} ([http://www.springerlink.com/content/2514122231284103/ 2&lt;sup&gt;192&lt;/sup&gt;])

|- style=&quot;text-align:center;&quot;
| '''[[HAVAL]]'''
| 256/224/192/160/128
| 256
| 1,024
| 64
| 32
| 160/128/96
| {{bad|Yes}}
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[MD2 (cryptography)|MD2]]'''
| 128
| 384
| 128
| -
| 32
| 864
| {{bad|Yes}} ([http://www.springerlink.com/content/n5vrtdha97a2udkx/ 2&lt;sup&gt;63.3&lt;/sup&gt;])
| {{depends|Yes}}&lt;ref group=&quot;c&quot; name=&quot;second_preimage&quot; /&gt; ([http://eprint.iacr.org/2008/089.pdf 2&lt;sup&gt;73&lt;/sup&gt;])
| {{bad|Yes}} ([http://eprint.iacr.org/2008/089.pdf 2&lt;sup&gt;73&lt;/sup&gt;])

|- style=&quot;text-align:center;&quot;
| '''[[MD4]]'''
| 128
| 128
| 512
| 64
| 32
| 48
| {{bad|Yes}} ([http://www.springerlink.com/content/v6526284mu858v37/ 3])
| {{bad|Yes}} ([http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;64&lt;/sup&gt;])
| {{bad|Yes}} ([http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;78.4&lt;/sup&gt;])

|- style=&quot;text-align:center;&quot;
| '''[[MD5]]'''
| 128
| 128
| 512
| 64
| 32
| 64
| {{bad|Yes}} ([http://eprint.iacr.org/2009/223.pdf 2&lt;sup&gt;20.96&lt;/sup&gt;])
| {{depends|Yes}}&lt;ref group=&quot;c&quot; name=&quot;second_preimage&quot; /&gt; ([http://springerlink.com/content/d7pm142n58853467/ 2&lt;sup&gt;123.4&lt;/sup&gt;])
| {{bad|Yes}} ([http://springerlink.com/content/d7pm142n58853467/ 2&lt;sup&gt;123.4&lt;/sup&gt;])

|- style=&quot;text-align:center;&quot;
| '''[[PANAMA]]'''
| 256
| 8,736
| 256
| -
| 32
| -
| {{bad|Yes}}
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[RadioGatún]]'''
| Up to 608/1,216 (19 words)
| 58 words
| 3 words
| -
| 1–64
| -
| {{bad|With flaws}} ([http://eprint.iacr.org/2008/515 2&lt;sup&gt;352&lt;/sup&gt; or 2&lt;sup&gt;704&lt;/sup&gt;])
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[RIPEMD]]'''
| 128
| 128
| 512
| 64
| 32
| 48
| {{bad|Yes}} ([http://www.springerlink.com/content/n5vrtdha97a2udkx/ 2&lt;sup&gt;18&lt;/sup&gt;])
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[RIPEMD|RIPEMD-128/256]]'''
| 128/256
| 128/256
| 512
| 64
| 32
| 64
| {{good|No}}
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[RIPEMD|RIPEMD-160]]'''
| 160
| 160
| 512
| 64
| 32
| 80
| {{depends|Yes}}&lt;ref group=&quot;c&quot; name=&quot;partial_rounds&quot; /&gt; ([http://www.springerlink.com/content/3540l03h1w31n6w7 2&lt;sup&gt;51&lt;/sup&gt;:48])
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[RIPEMD|RIPEMD-320]]'''
| 320
| 320
| 512
| 64
| 32
| 80
| {{good|No}}
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[SHA-1|SHA-0]]'''
| 160
| 160
| 512
| 64
| 32
| 80
| {{bad|Yes}} ([http://www.springerlink.com/content/3810jp9730369045/ 2&lt;sup&gt;33.6&lt;/sup&gt;])
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[SHA-1]]'''
| 160
| 160
| 512
| 64
| 32
| 80
| {{depends|Theoretical}} ([http://eprint.iacr.org/2008/469.pdf 2&lt;sup&gt;51&lt;/sup&gt;])
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[SHA-2|SHA-224, SHA-256]]'''
| 256/224
| 256
| 512
| 64
| 32
| 64
| {{depends|Theoretical}}&lt;ref group=&quot;c&quot; name=&quot;partial_rounds&quot;&gt;There is no known attack against the full version of this hash function, however there is an attack against this hashing scheme when the number of rounds is reduced.&lt;/ref&gt; ([http://eprint.iacr.org/2008/270.pdf 2&lt;sup&gt;28.5&lt;/sup&gt;:24])
| {{depends|Theoretical}}&lt;ref group=&quot;c&quot; name=&quot;second_preimage&quot; /&gt; &lt;ref group=&quot;c&quot; name=&quot;partial_rounds&quot; /&gt; ([http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;248.4&lt;/sup&gt;:42])
| {{depends|Theoretical}}&lt;ref group=&quot;c&quot; name=&quot;partial_rounds&quot; /&gt; ([http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;248.4&lt;/sup&gt;:42])

|- style=&quot;text-align:center;&quot;
| '''[[SHA-2|SHA-384, SHA-512, SHA-512/224, SHA-512/256]]'''
| 384/512/224/256
| 512
| 1,024
| 128
| 64
| 80
| {{depends|Theoretical}}&lt;ref group=&quot;c&quot; name=&quot;partial_rounds&quot; /&gt; ([http://eprint.iacr.org/2008/270.pdf 2&lt;sup&gt;32.5&lt;/sup&gt;:24])
| {{depends|Theoretical}}&lt;ref group=&quot;c&quot; name=&quot;second_preimage&quot; /&gt; &lt;ref group=&quot;c&quot; name=&quot;partial_rounds&quot; /&gt; ([http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;494.6&lt;/sup&gt;:42])
| {{depends|Theoretical}}&lt;ref group=&quot;c&quot; name=&quot;partial_rounds&quot; /&gt; ([http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;494.6&lt;/sup&gt;:42])

|- style=&quot;text-align:center;&quot;
| '''[[SHA-3]]'''
| 224/256/384/512&lt;ref group=&quot;c&quot;&gt;Although the underlying algorithm [[Keccak]] has arbitrary hash lengths, the NIST specified 224, 256, 384 and 512 bits output as valid modes for SHA-3.&lt;/ref&gt;
| 1600
| 1600-2*bits
| -
| 64
| 24
| {{good|No}}
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[SHA-3|SHA3]]'''-224
| 224
| 1600
| 1152
| -
| 64
| 24
| {{good|No}}
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[SHA-3|SHA3]]'''-256
| 256
| 1600
| 1088
| -
| 64
| 24
| {{good|No}}
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[SHA-3|SHA3]]'''-384
| 384
| 1600
| 832
| -
| 64
| 24
| {{good|No}}
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[SHA-3|SHA3]]'''-512
| 512
| 1600
| 576
| -
| 64
| 24
| {{good|No}}
| {{good|No}}
| {{good|No}}

|- style=&quot;text-align:center;&quot;
| '''[[Tiger (cryptography)|Tiger(2)-192/160/128]]'''
| 192/160/128
| 192
| 512
| 64
| 64
| 24
| {{depends|Yes}}&lt;ref group=&quot;c&quot; name=&quot;partial_rounds&quot; /&gt; ([http://www.springerlink.com/content/u762587644802p38/ 2&lt;sup&gt;62&lt;/sup&gt;:19])
| {{depends|Yes}}&lt;ref group=&quot;c&quot; name=&quot;second_preimage&quot; /&gt; ([http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;184.3&lt;/sup&gt;])
| {{bad|Yes}} ([http://eprint.iacr.org/2010/016.pdf 2&lt;sup&gt;184.3&lt;/sup&gt;])

|- style=&quot;text-align:center;&quot;
| '''[[WHIRLPOOL]]'''
| 512
| 512
| 512
| 256
| 8
| 10
| {{depends|Yes}}&lt;ref group=&quot;c&quot; name=&quot;partial_rounds&quot; /&gt; ([https://www.cosic.esat.kuleuven.be/fse2009/slides/2402_1150_Schlaeffer.pdf 2&lt;sup&gt;120&lt;/sup&gt;:4.5])
| {{good|No}}
| {{good|No}}

|}

===Notes===
&lt;references group=&quot;c&quot; /&gt;

==See also==

{{col-begin}}
{{col-break}}
* [[Avalanche effect]]
* [[Comparison of cryptographic hash functions]]
* [[Security of cryptographic hash functions]]
* [[CRYPTREC]] and [[NESSIE]] - Projects which recommend hash functions
* [[Keyed-hash message authentication code]]
* [[MD5CRK]]
* [[Message authentication code]]
{{col-break}}
* [[PGP word list]]
* [[Provably secure cryptographic hash function]]
* [[SHA-3]]
* [[UOWHF]] - Universal One Way Hash Functions
* [[Hash chain]]
{{col-break}}
{{Portal|Cryptography}}
{{col-end}}

==References==
{{reflist}}

==External links==
* {{cite book | first1 = Christof | last1 = Paar | first2 = Jan | last2 = Pelzl | url = http://wiki.crypto.rub.de/Buch/movies.php | chapter = 11: Hash Functions | title = Understanding Cryptography, A Textbook for Students and Practitioners | publisher = [[Springer Science+Business Media|Springer]] | date = 2009 }} (companion web site contains online cryptography course that covers hash functions)
* {{cite web | url = http://ehash.iaik.tugraz.at/wiki/The_eHash_Main_Page | title = The ECRYPT Hash Function Website }}
* {{cite paper | url = http://www.guardtime.com/educational-series-on-hashes/ | title = Series of mini-lectures about cryptographic hash functions | first = A. | last = Buldas | date = 2011 }}
* {{cite paper | id = {{citeseerx|10.1.1.3.6200}} | title = Cryptographic Hash-Function Basics: Definitions, Implications, and Separations for Preimage Resistance, Second-Preimage Resistance, and Collision Resistance | first1 = P. | last1 = Rogaway | first2 = T. | last2 = Shrimpton | date = 2004 }}

{{Cryptography navbox | hash}}

{{DEFAULTSORT:Cryptographic Hash Function}}
[[Category:Cryptography]]
[[Category:Cryptographic primitives]]
[[Category:Cryptographic hash functions| ]]
[[Category:Hashing]]

[[pl:Funkcja skrótu#Kryptograficzne funkcje skrótu]]</text>
      <sha1>nbyd0xx3q3bdfmrk0uluuweqsufrlh6</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Primary clustering</title>
    <ns>0</ns>
    <id>2343202</id>
    <revision>
      <id>479640877</id>
      <parentid>401033803</parentid>
      <timestamp>2012-03-01T12:25:44Z</timestamp>
      <contributor>
        <username>Pjoef</username>
        <id>3804503</id>
      </contributor>
      <comment>Disambiguated: [[load factor]] → [[load factor (computer science)]]</comment>
      <text xml:space="preserve" bytes="1407">{{Context|date=October 2009}}
'''Primary clustering''' is the tendency for certain open-addressing [[hash table]]s collision resolution schemes to create long sequences of filled slots. It is most commonly referred to in the context of problems with [[linear probing]].

These long chains degrade the hash-table performance closer to O(n) performance instead of closer to O(1).

Consider the linear probing [[hash function]] h(k, i) : (h`(k) + i)mod N. With k being the key, i the probing-iteration, N being the number of slots in the hash-table and h`(k) being the secondary-hash function.

Thus the probing sequence for key k is: {h`(k), h`(k) + 1, h`(k) + 2, ..., h`(k) + n}. It is easy to see that the probing sequences for two different keys may overlap and create clustering.

Also since the probability of each slot being already full is equal to the [[load factor (computer science)|load-factor]] of the hash-table, as the factor approaches 1.0 so does the chance that the next slot in the probing sequence will be full, this degrades performance of the hash-table to linear time.

Note: clusters of filled slots can grow exponentially in some cases.  For example, imagine two large clusters of filled slots separated by one empty slot.  Placing an entry into this slot will double the cluster size.

&lt;!-- Hmmm there is no algorithm or database stubs cat--&gt;


[[Category:Hashing]]

{{Comp-sci-stub}}</text>
      <sha1>gpyw1c89odv2h35cadqo123syrjf2fl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Dynamic perfect hashing</title>
    <ns>0</ns>
    <id>22348966</id>
    <revision>
      <id>623326404</id>
      <parentid>587546557</parentid>
      <timestamp>2014-08-29T15:55:33Z</timestamp>
      <contributor>
        <username>Mircea85</username>
        <id>2822929</id>
      </contributor>
      <comment>/* Details */ [Delete] Corrected hj (x) in subtable Tj instead of h(x) in subtable Tj</comment>
      <text xml:space="preserve" bytes="8892">In [[computer science]], '''dynamic perfect hashing''' is a programming technique for resolving [[collision (computer science)|collisions]] in a [[hash table]] [[data structure]].&lt;ref name=&quot;inventor&quot;&gt;Fredman, M. L., Komlós, J., and Szemerédi, E. 1984. Storing a Sparse Table with 0(1) Worst Case Access Time. J. ACM 31, 3 (Jun. 1984), 538-544 http://portal.acm.org/citation.cfm?id=1884#&lt;/ref&gt;&lt;ref name=&quot;dietzfelbinger&quot;&gt;Dietzfelbinger, M., Karlin, A., Mehlhorn, K., Meyer auf der Heide, F., Rohnert, H., and Tarjan, R. E. 1994. Dynamic Perfect Hashing: Upper and Lower Bounds. SIAM J. Comput. 23, 4 (Aug. 1994), 738-761. http://portal.acm.org/citation.cfm?id=182370#&lt;/ref&gt;&lt;ref&gt;Erik Demaine, Jeff Lind. 6.897: Advanced Data Structures. MIT Computer Science and Artificial Intelligence Laboratory. Spring 2003. http://courses.csail.mit.edu/6.897/spring03/scribe_notes/L2/lecture2.pdf&lt;/ref&gt; This technique is useful for situations where fast queries, insertions, and deletions must be made on a large set of elements.

==Details==

In this method, the entries that hash to the same slot of the table are organized as separate second-level hash table.  If there are ''k'' entries in this set ''S'', the second-level table is allocated with ''k''&lt;sup&gt;2&lt;/sup&gt; slots, and its [[hash function]] is selected at random from a [[universal hash function]] set so that it is collision-free (i.e. a [[perfect hash function]]). Therefore, the look-up cost is guaranteed to be [[big O notation|O(1)]] [[worst-case complexity|in the worst-case]].&lt;ref name=&quot;dietzfelbinger&quot;/&gt;

 '''function''' Locate(''x'') '''is'''
        ''j'' = h('''x''');
        '''if''' (position h&lt;sub&gt;j&lt;/sub&gt;(''x'') of subtable ''T&lt;sub&gt;j&lt;/sub&gt;'' contains ''x'' (not deleted))
           '''return''' (''x'' is in ''S'');
        '''end if'''
        '''else''' 
           '''return''' (''x'' is not in ''S'');
        '''end else'''
 '''end'''

Although each second-level table requires quadratic space, if the keys inserted into the first-level hash table are [[Uniform distribution (discrete)|uniformly distributed]], the structure as a whole occupies expected O(''n'') space, since bucket sizes are small with high [[probability]].&lt;ref name=&quot;inventor&quot;/&gt;

During the insertion of a new entry ''x'' at ''j'', the global operations counter, ''count'', is incremented. If ''x'' exists at ''j'' but is marked as deleted then the mark is removed. If ''x'' exists at ''j'', or at the subtable ''T&lt;sub&gt;j&lt;/sub&gt;'',  but is not marked as deleted then a collision is said to occur and the ''j''&lt;sup&gt;th&lt;/sup&gt; bucket's second-level table ''T&lt;sub&gt;j&lt;/sub&gt;'' is rebuilt with a different randomly selected hash function ''h&lt;sub&gt;j&lt;/sub&gt;''. Because the [[load factor (hash table)|load factor]] of the second-level table is kept low (1/''k''), rebuilding is infrequent, and the [[amortized analysis|amortized]] cost of insertions is O(1).&lt;ref name=&quot;dietzfelbinger&quot;/&gt;

 '''function''' Insert(''x'') '''is'''
        ''count'' = ''count'' + 1;
        '''if''' (''count'' &gt; ''M'') 
           FullRehash(''x'');
        '''end if'''
        '''else'''
           ''j'' = h(''x'');
           '''if''' (Position h&lt;sub&gt;''j''&lt;/sub&gt;(x) of subtable ''T&lt;sub&gt;j&lt;/sub&gt;'' contains ''x'')
              '''if''' (''x'' is marked deleted) 
                 remove the delete marker;
              '''end if'''
           '''end if'''
           '''else'''
              ''b&lt;sub&gt;j&lt;/sub&gt;'' = ''b&lt;sub&gt;j&lt;/sub&gt;'' + 1;
              '''if''' (''b&lt;sub&gt;j&lt;/sub&gt;'' &lt;= ''m&lt;sub&gt;j&lt;/sub&gt;'') 
                 '''if''' position h&lt;sub&gt;''j''&lt;/sub&gt;(''x'') of ''T&lt;sub&gt;j&lt;/sub&gt;'' is empty 
                    store ''x'' in position h&lt;sub&gt;''j''&lt;/sub&gt;(''x'') of ''T&lt;sub&gt;j&lt;/sub&gt;'';
                 '''end if'''
                 '''else'''
                    Put all unmarked elements of ''T&lt;sub&gt;j&lt;/sub&gt;'' in list ''L&lt;sub&gt;j&lt;/sub&gt;'';
                    Append ''x'' to list ''L&lt;sub&gt;j&lt;/sub&gt;'';
                    ''b&lt;sub&gt;j&lt;/sub&gt;'' = length of ''L&lt;sub&gt;j&lt;/sub&gt;'';
                    '''repeat''' 
                       ''h&lt;sub&gt;j&lt;/sub&gt;'' = randomly chosen function in ''H&lt;sub&gt;sj&lt;/sub&gt;'';
                    '''until''' ''h&lt;sub&gt;j&lt;/sub&gt;'' is injective on the elements of ''L&lt;sub&gt;j&lt;/sub&gt;'';
                    '''for''' all ''y'' on list ''L&lt;sub&gt;j&lt;/sub&gt;''
                       store ''y'' in position h&lt;sub&gt;''j''&lt;/sub&gt;(''y'') of ''T&lt;sub&gt;j&lt;/sub&gt;'';
                    '''end for'''
                 '''end else'''
              '''end if'''
              '''else'''
                 ''m&lt;sub&gt;j&lt;/sub&gt;'' = 2 * max{1, ''m&lt;sub&gt;j&lt;/sub&gt;''};
                 ''s&lt;sub&gt;j&lt;/sub&gt;'' = 2 * ''m&lt;sub&gt;j&lt;/sub&gt;'' * (''m&lt;sub&gt;j&lt;/sub&gt;'' - 1);
                 '''if''' the sum total of all s&lt;sub&gt;j&lt;/sub&gt; ≤ 32 * ''M''&lt;sup&gt;2&lt;/sup&gt; / ''s''(''M'') + 4 * ''M'' 
                    Allocate ''s&lt;sub&gt;j&lt;/sub&gt;'' cells for ''T&lt;sub&gt;j&lt;/sub&gt;'';
                    Put all unmarked elements of ''T&lt;sub&gt;j&lt;/sub&gt;'' in list ''L&lt;sub&gt;j&lt;/sub&gt;'';
                    Append ''x'' to list ''L&lt;sub&gt;j&lt;/sub&gt;'';
                    ''b&lt;sub&gt;j&lt;/sub&gt;'' = length of ''L&lt;sub&gt;j&lt;/sub&gt;'';
                    '''repeat''' 
                       ''h&lt;sub&gt;j&lt;/sub&gt;'' = randomly chosen function in ''H&lt;sub&gt;sj&lt;/sub&gt;'';
                    '''until''' ''h&lt;sub&gt;j&lt;/sub&gt;'' is injective on the elements of ''L&lt;sub&gt;j&lt;/sub&gt;'';
                    '''for''' all ''y'' on list ''L&lt;sub&gt;j&lt;/sub&gt;''
                       store ''y'' in position h&lt;sub&gt;''j''&lt;/sub&gt;(''y'') of ''T&lt;sub&gt;j&lt;/sub&gt;'';
                    '''end for'''
                 '''end if'''
                 '''else'''
                    FullRehash(''x'');
                 '''end else'''
              '''end else'''
           '''end else'''
        '''end else'''
 '''end'''

Deletion of ''x'' simply flags ''x'' as deleted without removal and increments ''count''. In the case of both insertions and deletions, if ''count'' reaches a threshold ''M'' the entire table is rebuilt, where ''M'' is some constant multiple of the size of S at the start of a new ''phase''. Here ''phase'' refers to the time between full rebuilds. The amortized cost of delete is O(1).&lt;ref name=&quot;dietzfelbinger&quot;/&gt; Note that here the -1 in &quot;Delete(''x'')&quot; is a representation of an element which is not in the set of all possible elements ''U''.

 '''function''' Delete(''x'') '''is'''
        ''count'' = ''count'' + 1;
        ''j'' = h(''x'');
        '''if''' position h&lt;sub&gt;j&lt;/sub&gt;(''x'') of subtable ''Tj'' contains ''x''
           mark ''x'' as deleted;
        '''end if'''
        '''else''' 
           '''return''' (x is not a member of S);
        '''end else'''
        '''if''' (''count'' &gt;= ''M'')
           FullRehash(-1);
        '''end if'''
 '''end'''

A full rebuild of the table of ''S'' first starts by removing all elements marked as deleted and then setting the next threshold value ''M'' to some constant multiple of the size of ''S''. A hash function, which partitions ''S'' into ''s''(''M'') subsets, where the size of subset ''j'' is ''s&lt;sub&gt;j&lt;/sub&gt;'', is repeatedly randomly chosen until:

&lt;math&gt;\sum_{0\le j\le s(M)} s_j \le \frac{32M^2}{s(M)} + 4M.&lt;/math&gt;

Finally, for each subtable ''T&lt;sub&gt;j&lt;/sub&gt;'' a hash function ''h&lt;sub&gt;j&lt;/sub&gt;'' is repeatedly randomly chosen from ''H&lt;sub&gt;sj&lt;/sub&gt;'' until ''h&lt;sub&gt;j&lt;/sub&gt;'' is injective on the elements of ''T&lt;sub&gt;j&lt;/sub&gt;''. The expected time for a full rebuild of the table of ''S'' with size ''n'' is O(''n'').&lt;ref name=&quot;dietzfelbinger&quot;/&gt; 

 '''function''' FullRehash(''x'') '''is'''
        Put all unmarked elements of ''T'' in list ''L'';
        '''if''' (''x'' is in ''U'') 
           append ''x'' to ''L'';
        '''end if'''
        ''count'' = length of list ''L'';
        ''M'' = (1 + ''c'') * max{''count'', 4};
        '''repeat''' 
           h = randomly chosen function in ''H&lt;sub&gt;s(M)&lt;/sub&gt;'';
           '''for''' all ''j'' &lt; ''s''(''M'') 
              form a list ''L&lt;sub&gt;j&lt;/sub&gt;'' for h(''x'') = ''j'';
              ''b&lt;sub&gt;j&lt;/sub&gt;'' = length of ''L&lt;sub&gt;j&lt;/sub&gt;''; 
              ''m&lt;sub&gt;j&lt;/sub&gt;'' = 2 * ''b&lt;sub&gt;j&lt;/sub&gt;''; 
              ''s&lt;sub&gt;j&lt;/sub&gt;'' = 2 * ''m&lt;sub&gt;j&lt;/sub&gt;'' * (''m&lt;sub&gt;j&lt;/sub&gt;'' - 1);
           '''end for'''
        '''until''' the sum total of all s&lt;sub&gt;j&lt;/sub&gt; ≤ 32 * ''M''&lt;sup&gt;2&lt;/sup&gt; / ''s''(''M'') + 4 * ''M''
        '''for''' all ''j'' &lt; ''s''(''M'') 
           Allocate space ''s&lt;sub&gt;j&lt;/sub&gt;'' for subtable ''T&lt;sub&gt;j&lt;/sub&gt;'';
           '''repeat''' 
              ''h&lt;sub&gt;j&lt;/sub&gt;'' = randomly chosen function in ''H&lt;sub&gt;sj&lt;/sub&gt;'';
           '''until''' ''h&lt;sub&gt;j&lt;/sub&gt;'' is injective on the elements of list ''L&lt;sub&gt;j&lt;/sub&gt;'';
        '''end for'''
        '''for''' all ''x'' on list ''L&lt;sub&gt;j&lt;/sub&gt;'' 
           store ''x'' in position h&lt;sub&gt;''j''&lt;/sub&gt;(''x'') of ''T&lt;sub&gt;j&lt;/sub&gt;'';
        '''end for'''
 '''end'''

==See also==
*[[Perfect hashing]]

==References==
{{reflist}}

{{DEFAULTSORT:Dynamic Perfect Hashing}}
[[Category:Hashing]]
[[Category:Search algorithms]]</text>
      <sha1>mudx2hnzy59h1sn1nyswcj1ujkg8qx5</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hash array mapped trie</title>
    <ns>0</ns>
    <id>14514583</id>
    <revision>
      <id>618215867</id>
      <parentid>615384393</parentid>
      <timestamp>2014-07-24T03:37:38Z</timestamp>
      <contributor>
        <ip>73.53.70.60</ip>
      </contributor>
      <comment>/* Implementations */ Add Javascript implementation</comment>
      <text xml:space="preserve" bytes="4578">A '''hash array mapped trie'''&lt;ref name=&quot;bagwell&quot; /&gt; ('''HAMT''') is an implementation of an [[associative array]] that combines the characteristics of a [[hash table]] and an array mapped [[trie]].&lt;ref name=&quot;bagwell&quot;&gt;{{cite report
|title=Ideal Hash Trees
|author=Phil Bagwell
|publisher=Infoscience Department, [[École Polytechnique Fédérale de Lausanne]]
|url=http://infoscience.epfl.ch/record/64398/files/idealhashtrees.pdf
|year=2000
}}&lt;/ref&gt;
It is a refined version of the more general notion of a [[Hash tree (persistent data structure)|hash tree]].

== Operation ==
A HAMT is an array mapped trie where the keys are first hashed in order to ensure an even distribution of keys and a constant key length.

In a typical implementation of HAMT's array mapped trie, each node contains a table with some fixed number N of slots with each slot containing either a nil pointer or a pointer to another node. N is commonly 32.  As allocating space for N pointers for each node would be expensive, each node instead contains a bitmap which is N bits long where each bit indicates the presence of a non-nil pointer. This is followed by an array of pointers equal in length to the number of ones in the bitmap, (its [[Hamming weight]]).

== Advantages of HAMTs ==
The hash array mapped trie achieves almost hash table-like speed while using memory much more economically.  Also, a hash table may have to be periodically resized, an expensive operation, whereas HAMTs grow dynamically.  Generally, HAMT performance is improved by a larger root table with some multiple of N slots; some HAMT variants allow the root to grow lazily&lt;ref name=&quot;bagwell&quot; /&gt; with negligible impact on performance.

== Problems with HAMTs ==
Implementation of a HAMT involves the use of the [[Hamming weight|population count]] function, which counts the number of ones in the binary representation of a number.  This operation is available in [[Hamming weight#Processor_support|many instruction set architectures]], but it is [[Hamming weight#Language_support|available in only some high-level languages]]. Although population count can be implemented in software in [[Big-O notation|O(1)]] time using a [[Hamming weight#Efficient_implementation|series of shift and add instructions]], doing so may perform the operation an order of magnitude slower.

This is of course not so much a problem with HAMTs but rather an indication that HAMT performance can be improved still further where hardware support is available.

== Implementations ==
The programming languages [[Clojure]]&lt;ref&gt;[https://github.com/clojure/clojure/blob/master/src/jvm/clojure/lang/PersistentHashMap.java Java source file of Clojure's hash map type.]&lt;/ref&gt; and [[Scala (programming language)|Scala]] use a [[Persistent data structure|persistent]] variant of hash array mapped tries for their native hash map type. The [[Haskell (programming language)|Haskell]] library [http://hackage.haskell.org/package/unordered-containers unordered-containers] uses the same to implement persistent map and set data types.&lt;ref&gt;Johan Tibell, A. [http://blog.johantibell.com/2012/03/announcing-unordered-containers-02.html Announcing unordered-containers 0.2]&lt;/ref&gt; A Javascript HAMT library &lt;ref&gt;[https://github.com/mattbierner/hamt Javascript HAMT library and source]&lt;/ref&gt; based on the Clojure implementation is also available.  The [[Rubinius]]&lt;ref&gt;[https://github.com/rubinius/rubinius/blob/master/kernel/common/hash_hamt.rb Ruby source file of Rubinius's HAMT]&lt;/ref&gt; implementation of [[Ruby (programming language)|Ruby]] includes a HAMT, mostly written in Ruby but with 3&lt;ref&gt;[https://github.com/rubinius/rubinius/blob/master/vm/builtin/system.cpp#L1647-L1676]&lt;/ref&gt; primitives. There is a C++ HAMT&lt;ref&gt;[https://github.com/chaelim/HAMT Hash Array Mapped Trie (C++ Templates)]&lt;/ref&gt; implementation that optionally uses [[Popcnt|POPCNT]] CPU instruction.

The concurrent lock-free version&lt;ref&gt;Prokopec, A. [https://github.com/axel22/Ctries Implementation of Concurrent Hash Tries on GitHub]&lt;/ref&gt; of the hash trie called [[Ctrie]] is a mutable thread-safe implementation which ensures progress. The data-structure has been proven to be correct&lt;ref&gt;Prokopec, A. et al. (2011) [http://infoscience.epfl.ch/record/166908/files/ctries-techreport.pdf Cache-Aware Lock-Free Concurrent Hash Tries]. Technical Report, 2011.&lt;/ref&gt; - Ctrie operations have been shown to have the atomicity, linearizability and lock-freedom properties.

== References ==
{{Reflist}}

{{DEFAULTSORT:Hash Array Mapped Trie}}
[[Category:Associative arrays]]
[[Category:Hashing]]</text>
      <sha1>3hjcdlok0lhki8k5ais3m9gsxsok596</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Bloom filter</title>
    <ns>0</ns>
    <id>602211</id>
    <revision>
      <id>626605064</id>
      <parentid>621644103</parentid>
      <timestamp>2014-09-22T11:24:09Z</timestamp>
      <contributor>
        <username>Rhubbarb</username>
        <id>5267554</id>
      </contributor>
      <comment>add see also Count–min sketch</comment>
      <text xml:space="preserve" bytes="56536">{{Distinguish2|[[Bloom (shader effect)|Bloom shader effect]]}}
{{More footnotes|date=November 2009}}
{{Probabilistic}}

A '''Bloom filter''' is a space-efficient [[probabilistic]] [[data structure]], conceived by [[Burton Howard Bloom]] in 1970, that is used to test whether an [[element (mathematics)|element]] is a member of a [[set (computer science)|set]]. [[Type I and type II errors|False positive]] matches are possible, but [[Type I and type II errors|false negatives]] are not, thus a Bloom filter has a 100% [[precision and recall|recall]] rate.  In other words, a query returns either &quot;possibly in set&quot; or &quot;definitely not in set&quot;. Elements can be added to the set, but not removed (though this can be addressed with a &quot;counting&quot; filter). The more elements that are added to the set, the larger the probability of false positives.

Bloom proposed the technique for applications where the amount of source data would require an impracticably large hash area in memory if &quot;conventional&quot; error-free hashing techniques were applied. He gave the example of a [[hyphenation algorithm]] for a dictionary of 500,000 words, out of which 90% follow simple hyphenation rules, but the remaining 10% require expensive disk accesses to retrieve specific hyphenation patterns. With sufficient core memory, an error-free hash could be used to eliminate all unnecessary disk accesses; on the other hand, with limited core memory, Bloom's technique uses a smaller hash area but still eliminates most unnecessary accesses. For example, a hash area only 15% of the size needed by an ideal error-free hash still eliminates 85% of the disk accesses ({{harvtxt |Bloom |1970}}).

More generally, fewer than 10 bits per element are required for a 1% false positive probability, independent of the size or number of elements in the set ({{harvtxt |Bonomi|Mitzenmacher|Panigrahy|Singh|2006}}).

==Algorithm description==
[[File:Bloom filter.svg|thumb|360px|An example of a Bloom filter, representing the set { {{mvar|x}}, {{mvar|y}}, {{mvar|z}} }. The colored arrows show the positions in the bit array that each set element is mapped to. The element {{mvar|w}} is not in the set { {{mvar|x}}, {{mvar|y}}, {{mvar|z}} }, because it hashes to one bit-array position containing 0. For this figure, {{mvar|m}}&amp;nbsp;= 18 and {{mvar|k}}&amp;nbsp;= 3.]]

An ''empty Bloom filter'' is a [[bit array]] of {{mvar|m}} bits, all set to 0. There must also be {{mvar|k}} different [[hash function]]s defined, each of which [[map (mathematics)|maps]] or hashes some set element to one of the {{mvar|m}} array positions with a uniform random distribution.

To ''add'' an element, feed it to each of the {{mvar|k}} hash functions to get {{mvar|k}} array positions. Set the bits at all these positions to 1.

To ''query'' for an element (test whether it is in the set), feed it to each of the {{mvar|k}} hash functions to get {{mvar|k}} array positions. If any of the bits at these positions are 0, the element is definitely not in the set &amp;ndash; if it were, then all the bits would have been set to 1 when it was inserted. If all are 1, then either the element is in the set, or the bits have by chance been set to 1 during the insertion of other elements, resulting in a [[false positive]]. In a simple Bloom filter, there is no way to distinguish between the two cases, but more advanced techniques can address this problem.

The requirement of designing {{mvar|k}} different independent hash functions can be prohibitive for large {{mvar|k}}. For a good [[hash function]] with a wide output, there should be little if any correlation between different bit-fields of such a hash, so this type of hash can be used to generate multiple &quot;different&quot; hash functions by slicing its output into multiple bit fields.  Alternatively, one can pass {{mvar|k}} different initial values (such as 0, 1, ..., {{mvar|k}}&amp;nbsp;&amp;minus;&amp;nbsp;1) to a hash function that takes an initial value; or add (or append) these values to the key. For larger {{mvar|m}} and/or {{mvar|k}}, independence among the hash functions can be relaxed with negligible increase in false positive rate ({{harvtxt|Dillinger|Manolios|2004a}}, {{harvtxt|Kirsch|Mitzenmacher|2006}}).  Specifically, {{harvtxt|Dillinger|Manolios|2004b}} show the effectiveness of deriving the {{mvar|k}} indices using [[enhanced double hashing]] or [[triple hashing]], variants of [[double hashing]] that are effectively simple random number generators seeded with the two or three hash values.

Removing an element from this simple Bloom filter is impossible because false negatives are not permitted. An element maps to {{mvar|k}} bits, and although setting any one of those {{mvar|k}} bits to zero suffices to remove the element, it also results in removing any other elements that happen to map onto that bit. Since there is no way of determining whether any other elements have been added that affect the bits for an element to be removed, clearing any of the bits would introduce the possibility for false negatives.

One-time removal of an element from a Bloom filter can be simulated by having a second Bloom filter that contains items that have been removed.  However, false positives in the second filter become false negatives in the composite filter, which may be undesirable.  In this approach re-adding a previously removed item is not possible, as one would have to remove it from the &quot;removed&quot; filter.

It is often the case that all the keys are available but are expensive to enumerate (for example, requiring many disk reads).  When the false positive rate gets too high, the filter can be regenerated; this should be a relatively rare event.

==Space and time advantages==
[[File:Bloom filter speed.svg|thumb|360px|Bloom filter used to speed up answers in a key-value storage system. Values are stored on a disk which has slow access times. Bloom filter decisions are much faster. However some unnecessary disk accesses are made when the filter reports a positive (in order to weed out the false positives). Overall answer speed is better with the Bloom filter than without the Bloom filter. Use of a Bloom filter for this purpose, however, does increase memory usage.]]
While risking false positives, Bloom filters have a strong space advantage over other data structures for representing sets, such as [[self-balancing binary search tree]]s, [[trie]]s, [[hash table]]s, or simple [[Array data structure|arrays]] or [[linked list]]s of the entries. Most of these require storing at least the data items themselves, which can require anywhere from a small number of bits, for small integers, to an arbitrary number of bits, such as for strings ([[trie]]s&lt;!--Yes, tries, NOT trees--&gt; are an exception, since they can share storage between elements with equal prefixes). Linked structures incur an additional linear space overhead for pointers. A Bloom filter with 1% error and an optimal value of ''k'', in contrast, requires only about 9.6 bits per element &amp;mdash; regardless of the size of the elements. This advantage comes partly from its compactness, inherited from arrays, and partly from its probabilistic nature. The 1% false-positive rate can be reduced by a factor of ten by adding only about 4.8 bits per element.

However, if the number of potential values is small and many of them can be in the set, the Bloom filter is easily surpassed by the deterministic [[bit array]], which requires only one bit for each potential element. Note also that hash tables gain a space and time advantage if they begin ignoring collisions and store only whether each bucket contains an entry; in this case, they have effectively become Bloom filters with ''k'' = 1.&lt;ref&gt;{{harvtxt|Mitzenmacher|Upfal|2005}}.&lt;/ref&gt;

Bloom filters also have the unusual property that the time needed either to add items or to check whether an item is in the set is a fixed constant, O(''k''), completely independent of the number of items already in the set. No other constant-space set data structure has this property, but the average access time of sparse [[hash table]]s can make them faster in practice than some Bloom filters.  In a hardware implementation, however, the Bloom filter shines because its ''k'' lookups are independent and can be parallelized.

To understand its space efficiency, it is instructive to compare the general Bloom filter with its special case when ''k'' = 1.  If ''k'' = 1, then in order to keep the false positive rate sufficiently low, a small fraction of bits should be set, which means the array must be very large and contain long runs of zeros.  The [[information content]] of the array relative to its size is low.  The generalized Bloom filter (''k'' greater than 1) allows many more bits to be set while still maintaining a low false positive rate; if the parameters (''k'' and ''m'') are chosen well, about half of the bits will be set,&lt;ref&gt;{{harvtxt|Blustein|El-Maazawi|2002}}, pp. 21–22&lt;/ref&gt; and these will be apparently random, minimizing redundancy and maximizing information content.

==Probability of false positives==
[[File:Bloom filter fp probability.svg|thumb|360px|The false positive probability &lt;math&gt;p&lt;/math&gt; as a function of number of elements &lt;math&gt;n&lt;/math&gt; in the filter and the filter size &lt;math&gt;m&lt;/math&gt;. An optimal number of hash functions &lt;math&gt;k= (m/n) \ln 2&lt;/math&gt; has been assumed.]]

Assume that a [[hash function]] selects each array position with equal probability. If ''m'' is the number of bits in the array, the probability that a certain bit is not set to 1 by a certain hash function during the insertion of an element is

:&lt;math&gt;1-\frac{1}{m}.&lt;/math&gt;

If ''k'' is the number of hash functions, the probability that the bit is not set to 1 by any of the hash functions is

:&lt;math&gt;\left(1-\frac{1}{m}\right)^k.&lt;/math&gt;

If we have inserted ''n'' elements, the probability that a certain bit is still 0 is

:&lt;math&gt;\left(1-\frac{1}{m}\right)^{kn};&lt;/math&gt;

the probability that it is 1 is therefore

:&lt;math&gt;1-\left(1-\frac{1}{m}\right)^{kn}.&lt;/math&gt;

Now test membership of an element that is not in the set. Each of the ''k'' array positions computed by the hash functions is 1 with a probability as above. The probability of all of them being 1, which would cause the [[algorithm]] to erroneously claim that the element is in the set, is often given as

:&lt;math&gt;\left(1-\left[1-\frac{1}{m}\right]^{kn}\right)^k \approx \left( 1-e^{-kn/m} \right)^k.&lt;/math&gt;

This is not strictly correct as it assumes independence for the probabilities of each bit being set. However, assuming it is a close approximation we have that the probability of false positives decreases as ''m'' (the number of bits in the array) increases,  and increases as ''n'' (the number of inserted elements) increases.

An alternative analysis arriving at the same approximation without the assumption of independence is given by Mitzenmacher and Upfal.&lt;ref&gt;{{harvtxt|Mitzenmacher|Upfal|2005}}, pp. 109–111, 308.&lt;/ref&gt; After all ''n'' items have been added to the Bloom filter, let ''q'' be the fraction of the ''m'' bits that are set to 0. (That is, the number of bits still set to 0 is ''qm''.) Then, when testing membership of an element not in the set, for the array position given by any of the ''k'' hash functions, the probability that the bit is found set to 1 is &lt;math&gt;1-q&lt;/math&gt;. So the probability that all ''k'' hash functions find their bit set to 1 is &lt;math&gt;(1 - q)^k&lt;/math&gt;. Further, the expected value of ''q'' is the probability that a given array position is left untouched by each of the ''k'' hash functions for each of the ''n'' items, which is (as above)
: &lt;math&gt;E[q] = \left(1 - \frac{1}{m}\right)^{kn}&lt;/math&gt;.
It is possible to prove, without the independence assumption, that ''q'' is very strongly concentrated around its expected value. In particular, from the [[Azuma–Hoeffding inequality]], they prove that&lt;ref&gt;{{harvtxt|Mitzenmacher|Upfal|2005}}, p. 308.&lt;/ref&gt;
: &lt;math&gt; \Pr(\left|q - E[q]\right| \ge \frac{\lambda}{m}) \le 2\exp(-2\lambda^2/m) &lt;/math&gt;
Because of this, we can say that the exact probability of false positives is
: &lt;math&gt; \sum_{t} \Pr(q = t) (1 - t)^k \approx (1 - E[q])^k = \left(1-\left[1-\frac{1}{m}\right]^{kn}\right)^k \approx \left( 1-e^{-kn/m} \right)^k&lt;/math&gt;
as before.

===Optimal number of hash functions===
For a given ''m'' and ''n'', the value of ''k'' (the number of hash functions) that minimizes the probability is

:&lt;math&gt;k = \frac{m}{n} \ln 2,&lt;/math&gt;

which gives

:&lt;math&gt;2^{-k} \approx {0.6185}^{m/n}.&lt;/math&gt;

The required number of bits ''m'', given ''n'' (the number of inserted elements) and a desired false positive probability ''p'' (and assuming the optimal value of ''k'' is used) can be computed by substituting the optimal value of ''k'' in the probability expression above:
:&lt;math&gt;p = \left( 1-e^{-(m/n\ln 2) n/m} \right)^{(m/n\ln 2)}&lt;/math&gt;
which can be simplified to:
:&lt;math&gt;\ln p = -\frac{m}{n} \left(\ln 2\right)^2.&lt;/math&gt;
This results in:
:&lt;math&gt;m=-\frac{n\ln p}{(\ln 2)^2}.&lt;/math&gt;

This means that for a given false positive probability ''p'', the length of a Bloom filter ''m'' is proportionate to the number of elements being filtered ''n''.&lt;ref&gt;{{harvtxt|Starobinski|Trachtenberg|Agarwal|2003}}&lt;/ref&gt; While the above formula is asymptotic (i.e. applicable as ''m'',''n'' → ∞), the agreement with finite values of ''m'',''n'' is also quite good; the false positive probability for a finite bloom filter with ''m'' bits, ''n'' elements, and ''k'' hash functions is at most

:&lt;math&gt;\left( 1-e^{-k(n+0.5)/(m-1)} \right)^k.&lt;/math&gt;

So we can use the asymptotic formula if we pay a penalty for at most half an extra element and at most one fewer bit.&lt;ref&gt;{{harvtxt|Goel|Gupta|2010}}.&lt;/ref&gt;

==Approximating the number of items in a Bloom filter==

{{harvtxt|Swamidass|Baldi|2007}} showed that the number of items in a Bloom filter can be approximated with the following formula,

:&lt;math&gt; X^* = - \tfrac{ N \ln \left[ 1 - \tfrac{X}{N} \right] } { k} &lt;/math&gt;

where &lt;var&gt;&lt;math&gt;X^*&lt;/math&gt;&lt;/var&gt; is an estimate of the number of items in the filter, &lt;var&gt;N&lt;/var&gt; is length of the filter, &lt;var&gt;k&lt;/var&gt; is the number of hash functions per item, and &lt;var&gt;X&lt;/var&gt; is the number of bits set to one.

==The union and intersection of sets==

Bloom filters are a way of compactly representing a set of items. It is common to try to compute the size of the intersection or union between two sets. Bloom filters can be used to approximate the size of the intersection and union of two sets. {{harvtxt|Swamidass|Baldi|2007}} showed that for two bloom filters of length &lt;math&gt;N&lt;/math&gt;, their counts, respectively can be estimated as

:&lt;math&gt; A^* = -N \ln \left[ 1 - A / N \right] / k&lt;/math&gt;

and

:&lt;math&gt; B^* = -N \ln \left[ 1 - B / N \right]/k&lt;/math&gt;.

The size of their union can be estimated as

:&lt;math&gt; A^*\cup B^* = -N \ln \left[ 1 - (A \cup B) / N \right]/k&lt;/math&gt;,

where &lt;math&gt;A \cup B&lt;/math&gt; is the number of bits set to one in either of the two bloom filters. And the intersection can be estimated as

:&lt;math&gt; A^*\cap B^* = A^* + B^* - A^*\cup B^*&lt;/math&gt;,

Using the three formulas together.

==Interesting properties==
*Unlike a standard [[hash table]], a Bloom filter of a fixed size can represent a set with an arbitrary large number of elements; adding an element never fails due to the data structure &quot;filling up.&quot; However, the false positive rate increases steadily as elements are added until all bits in the filter are set to 1, at which point ''all'' queries yield a positive result.

*[[Union (set theory)|Union]] and [[intersection (set theory)|intersection]] of Bloom filters with the same size and set of hash functions can be implemented with [[bitwise operation|bitwise]] OR and AND operations, respectively. The union operation on Bloom filters is lossless in the sense that the resulting Bloom filter is the same as the Bloom filter created from scratch using the union of the two sets. The intersect operation satisfies a weaker property: the false positive probability in the resulting Bloom filter is at most the false-positive probability in one of the constituent Bloom filters, but may be larger than the false positive probability in the Bloom filter created from scratch using the intersection of the two sets.

* Some kinds of [[superimposed code]] can be seen as a Bloom filter implemented with physical [[edge-notched card]]s. An example is [[Zatocoding]], invented by [[Calvin Mooers]] in 1947, in which the set of categories associated with a piece of information is represented by notches on a card, with a random pattern of four notches for each category.

==Examples==
* Google [[BigTable]] and [[Apache Cassandra]] use Bloom filters to reduce the disk lookups for non-existent rows or columns. Avoiding costly disk lookups considerably increases the performance of a database query operation.&lt;ref&gt;{{harv|Chang|Dean|Ghemawat|Hsieh|2006}}.&lt;/ref&gt;
* The [[Google Chrome]] web browser used to use a Bloom filter to identify malicious URLs. Any URL was first checked against a local Bloom filter, and only if the Bloom filter returned a positive result was a full check of the URL performed (and the user warned, if that too returned a positive result).&lt;ref&gt;{{cite web|last=Yakunin |first=Alex |url=http://blog.alexyakunin.com/2010/03/nice-bloom-filter-application.html |title=Alex Yakunin's blog: Nice Bloom filter application |publisher=Blog.alexyakunin.com |date=2010-03-25 |accessdate=2014-05-31}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://chromiumcodereview.appspot.com/10896048/ |title=Issue 10896048: Transition safe browsing from bloom filter to prefix set. - Code Review |publisher=Chromiumcodereview.appspot.com |date= |accessdate=2014-07-03}}&lt;/ref&gt;
* The [[Squid (software)|Squid]] [[World Wide Web|Web]] Proxy [[web cache|Cache]] uses Bloom filters for [http://wiki.squid-cache.org/SquidFaq/CacheDigests cache digests].&lt;ref name=&quot;Wessels172&quot;&gt;{{Citation| last=Wessels | first=Duane |date=January 2004 | chapter=10.7 Cache Digests | title=Squid: The Definitive Guide | edition=1st | publisher=O'Reilly Media | isbn=0-596-00162-2 | page=172 | quote=Cache Digests are based on a technique first published by Pei Cao, called Summary Cache. The fundamental idea is to use a Bloom filter to represent the cache contents.}}&lt;/ref&gt;
* [[Bitcoin]] uses Bloom filters to speed up wallet synchronization.&lt;ref&gt;[http://sourceforge.net/projects/bitcoin/files/Bitcoin/bitcoin-0.8.0/ Bitcoin 0.8.0]&lt;/ref&gt;&lt;ref&gt;[https://bitcoinfoundation.org/blog/?p=16 Core Development Status Report #1]&lt;/ref&gt;
* The [[Venti]] archival storage system uses Bloom filters to detect previously stored data.&lt;ref&gt;{{cite web|url=http://plan9.bell-labs.com/magic/man2html/8/venti |title=Plan 9 /sys/man/8/venti |publisher=Plan9.bell-labs.com |date= |accessdate=2014-05-31}}&lt;/ref&gt;
* The [[SPIN model checker]] uses Bloom filters to track the reachable state space for large verification problems.&lt;ref&gt;http://spinroot.com/&lt;/ref&gt;
* The [[Cascading (software)|Cascading]] analytics framework uses Bloom filters to speed up asymmetric joins, where one of the joined data sets is significantly larger than the other (often called Bloom join&lt;ref&gt;{{harvtxt|Mullin|1990}}&lt;/ref&gt; in the database literature).&lt;ref&gt;{{cite web|url=http://blog.liveramp.com/2013/04/03/bloomjoin-bloomfilter-cogroup/ |title=BloomJoin: BloomFilter + CoGroup &amp;#124; LiveRamp Blog |publisher=Blog.liveramp.com |date= |accessdate=2014-05-31}}&lt;/ref&gt;

==Alternatives==

Classic Bloom filters use &lt;math&gt;1.44\log_2(1/\epsilon)&lt;/math&gt; bits of space per inserted key, where &lt;math&gt;\epsilon&lt;/math&gt; is the false positive rate of the Bloom filter. However, the space that is strictly necessary for any data structure playing the same role as a Bloom filter is only &lt;math&gt;\log_2(1/\epsilon)&lt;/math&gt; per key {{harv|Pagh|Pagh|Rao|2005}}. Hence Bloom filters use 44% more space than a hypothetical equivalent optimal data structure. The number of hash functions used to achieve a given false positive rate &lt;math&gt;\epsilon&lt;/math&gt; is proportional to &lt;math&gt;log(1/\epsilon)&lt;/math&gt; which is not optimal as it has been proved that an optimal data structure would need only a constant number of hash functions independent of the false positive rate.

{{harvtxt|Stern|Dill|1996}} describe a probabilistic structure based on [[hash table]]s, [[hash compaction]], which {{harvtxt|Dillinger|Manolios|2004b}} identify as significantly more accurate than a Bloom filter when each is configured optimally.  Dillinger and Manolios, however, point out that the reasonable accuracy of any given Bloom filter over a wide range of numbers of additions makes it attractive for probabilistic enumeration of state spaces of unknown size.  Hash compaction is, therefore, attractive when the number of additions can be predicted accurately; however, despite being very fast in software, hash compaction is poorly suited for hardware because of worst-case linear access time.

{{harvtxt|Putze|Sanders|Singler|2007}} have studied some variants of Bloom filters that are either faster or use less space than classic Bloom filters. The basic idea of the fast variant is to locate the k hash values associated with each key into one or two blocks having the same size as processor's memory cache blocks (usually 64 bytes). This will presumably improve performance by reducing the number of potential memory [[cache misses]]. The proposed variants have however the drawback of using about 32% more space than classic Bloom filters.

The space efficient variant relies on using a single hash function that generates for each key a value in the range &lt;math&gt;\left[0,n/\varepsilon\right]&lt;/math&gt; where &lt;math&gt;\epsilon&lt;/math&gt; is the requested false positive rate. The sequence of values is then sorted and compressed using [[Golomb coding]] (or some other compression technique) to occupy a space close to &lt;math&gt;n\log_2(1/\epsilon)&lt;/math&gt; bits. To query the Bloom filter for a given key, it will suffice to check if its corresponding value is stored in the Bloom filter. Decompressing the whole Bloom filter for each query would make this variant totally unusable. To overcome this problem the sequence of  values is divided into small blocks of equal size that are compressed separately. At query time only half a block will need to be decompressed on average. Because of decompression overhead, this variant may be slower than classic Bloom filters but this may be compensated by the fact that a single hash function need to be computed.

Another alternative to classic Bloom filter is the one based on space efficient variants of [[cuckoo hashing]]. In this case once the hash table is constructed, the keys stored in the hash table are replaced with short signatures of the keys. Those signatures are strings of bits computed using a hash function applied on the keys.

==Extensions and applications==

===Counting filters===
Counting filters provide a way to implement a ''delete'' operation on a Bloom filter without recreating the filter afresh. In a counting filter the array positions (buckets) are extended from being a single bit to being an n-bit counter. In fact, regular Bloom filters can be considered as counting filters with a bucket size of one bit. Counting filters were introduced by {{harvtxt|Fan|Cao|Almeida|Broder|1998}}.

The insert operation is extended to ''increment'' the value of the buckets and the lookup operation checks that each of the required buckets is non-zero. The delete operation, obviously, then consists of decrementing the value of each of the respective buckets.

[[Arithmetic overflow]] of the buckets is a problem and the buckets should be sufficiently large to make this case rare. If it does occur then the increment and decrement operations must leave the bucket set to the maximum possible value in order to retain the properties of a Bloom filter.

The size of counters is usually 3 or 4 bits. Hence counting Bloom filters use 3 to 4 times more space than static Bloom filters. In theory, an optimal data structure equivalent to a counting Bloom filter should not use more space than a static Bloom filter.

Another issue with counting filters is limited scalability. Because the counting Bloom filter table cannot be expanded, the maximal number of keys to be stored simultaneously in the filter must be known in advance. Once the designed capacity of the table is exceeded, the false positive rate will grow rapidly as more keys are inserted.

{{harvtxt|Bonomi|Mitzenmacher|Panigrahy|Singh|2006}} introduced a data structure based on d-left hashing that is functionally equivalent  but uses approximately half as much space as counting Bloom filters. The scalability issue does not occur in this data structure. Once the designed capacity is exceeded, the keys could be reinserted in a new hash table of double size.

The space efficient variant by {{harvtxt|Putze|Sanders|Singler|2007}} could also be used to implement counting filters by supporting insertions and deletions.

{{harvtxt|Rottenstreich|Kanizo|Keslassy|2012}} introduced a new general method based on variable increments that significantly improves the false positive probability of counting Bloom filters and their variants, while still supporting deletions. Unlike counting Bloom filters, at each element insertion, the hashed counters are incremented by a hashed variable increment instead of a unit increment. To query an element, the exact values of the counters are considered and not just their positiveness. If a sum represented by a counter value cannot be composed of the corresponding variable increment for the queried element, a negative answer can be returned to the query.

===Data synchronization===
Bloom filters can be used for approximate [[data synchronization]] as in {{harvtxt|Byers|Considine|Mitzenmacher|Rost|2004}}.  Counting Bloom filters can be used to approximate the number of differences between two sets and this approach is described in {{harvtxt|Agarwal|Trachtenberg|2006}}.

===Bloomier filters===
{{harvtxt|Chazelle|Kilian|Rubinfeld|Tal|2004}} designed a generalization of Bloom filters that could associate a value with each element that had been inserted, implementing an [[associative array]]. Like Bloom filters, these structures achieve a small space overhead by accepting a small probability of false positives. In the case of &quot;Bloomier filters&quot;, a ''false positive'' is defined as returning a result when the key is not in the map. The map will never return the wrong value for a key that ''is'' in the map.
&lt;!-- too wordy for an alternative to the article topic. commented out in case someone wants to move it to a new article.
The simplest Bloomier filter is near-optimal and fairly simple to describe. Suppose initially that the only possible values are 0 and 1. We create a pair of Bloom filters ''A''&lt;sub&gt;0&lt;/sub&gt; and ''B''&lt;sub&gt;0&lt;/sub&gt; which contain, respectively, all keys mapping to 0 and all keys mapping to 1. Then, to determine which value a given key maps to, we look it up in both filters. If it is in neither, then the key is not in the map. If the key is in ''A''&lt;sub&gt;0&lt;/sub&gt; but not ''B''&lt;sub&gt;0&lt;/sub&gt;, then it does not map to 1, and has a high probability of mapping to 0. Conversely, if the key is in ''B''&lt;sub&gt;0&lt;/sub&gt; but not ''A''&lt;sub&gt;0&lt;/sub&gt;, then it does not map to 0 and has a high probability of mapping to 1.

A problem arises, however, when ''both'' filters claim to contain the key. We never insert a key into both, so one or both of the filters is lying (producing a false positive), but we don't know which. To determine this, we have another, smaller pair of filters ''A''&lt;sub&gt;1&lt;/sub&gt; and ''B''&lt;sub&gt;1&lt;/sub&gt;. ''A''&lt;sub&gt;1&lt;/sub&gt; contains keys that map to 0 and which are false positives in ''B''&lt;sub&gt;0&lt;/sub&gt;; ''B''&lt;sub&gt;1&lt;/sub&gt; contains keys that map to 1 and which are false positives in ''A''&lt;sub&gt;0&lt;/sub&gt;. But whenever ''A''&lt;sub&gt;0&lt;/sub&gt; and ''B''&lt;sub&gt;0&lt;/sub&gt; both produce positives, at most one of these cases must occur, and so we simply have to determine which if any of the two filters ''A''&lt;sub&gt;1&lt;/sub&gt; and ''B''&lt;sub&gt;1&lt;/sub&gt; contains the key, another instance of our original problem.

It may so happen again that both filters produce a positive; we apply the same idea recursively to solve this problem. Because each pair of filters only contains keys that are in the map ''and'' produced false positives on all previous filter pairs, the number of keys is extremely likely to quickly drop to a very small quantity that can be easily stored in an ordinary deterministic map, such as a pair of small arrays with linear search. Moreover, the average total search time is low, because almost all queries will be resolved by the first pair, almost all remaining queries by the second pair, and so on. The total space required is in practice independent of ''n'', and is almost entirely occupied by the first filter pair.

Now that we have the structure and a search algorithm, we also need to know how to insert new key/value pairs. The program must not attempt to insert the same key with both values. If the value is 0, insert the key into ''A''&lt;sub&gt;0&lt;/sub&gt; and then test if the key is in ''B''&lt;sub&gt;0&lt;/sub&gt;. If so, this is a false positive for ''B''&lt;sub&gt;0&lt;/sub&gt;, and the key must also be inserted into ''A''&lt;sub&gt;1&lt;/sub&gt; recursively in the same manner. If we reach the last level, we simply insert it. When the value is 1, the operation is similar but with ''A'' and ''B'' reversed.

Now that we can map a key to the value 0 or 1, how does this help us map to general values? This is simple. We create a single such Bloomier filter for each bit of the result. If the values are large, we can instead map keys to hash values that can be used to retrieve the actual values. The space required for a Bloomier filter with ''n''-bit values is typically slightly more than the space for 2''n'' Bloom filters.

A very simple way to implement Bloomier filters is by means of minimal [[perfect hashing]]. A minimal perfect hash function h is first generated for the set of n keys. Then an array is filled with n pairs (signature,value) associated with each key at the positions given by function h when applied on each key. The signature of a key is a string of r bits computed by applying a hash function g of range &lt;math&gt;2^r&lt;/math&gt; on the key. The value of r is chosen such that &lt;math&gt;2^r&gt;=1/\epsilon&lt;/math&gt;, where &lt;math&gt;\epsilon&lt;/math&gt; is the requested false positive rate. To query for a given key, hash function h is first applied on the key. This will give a position into the array from which we retrieve a pair (signature,value). Then we compute the signature of the key using function g. If the computed signature is the same as retrieved signature we return the retrieved value. The probability of false positive is &lt;math&gt;1/2^r&lt;/math&gt;.

Another alternative to implement static bloomier and bloom filters based on matrix solving has been simultaneously proposed in {{harvtxt|Porat|2008}}, {{harvtxt|Dietzfelbinger|Pagh|2008}} and {{harvtxt|Charles|Chellapilla|2008}}. The space usage of this method is optimal as it needs only &lt;math&gt;\log_2(\epsilon)&lt;/math&gt; bits per key for a bloom filter. However time to generate the bloom or bloomier filter can be very high. The generation  time can be reduced to a reasonable value at the price of a small increase in space usage.

Dynamic Bloomier filters have been studied by {{harvtxt|Mortensen|Pagh|Pătraşcu|2005}}. They proved that any dynamic Bloomier filter needs at least around
&lt;math&gt;\log(l)&lt;/math&gt; bits per key where l is the length of the key. A simple dynamic version of Bloomier filters can be implemented using two dynamic data structures. Let the two data structures be noted S1 and S2. S1 will store keys with their associated data while S2 will only store signatures of keys with their associated data. Those signatures are simply hash values of keys in the range &lt;math&gt;[0,n/\varepsilon]&lt;/math&gt; where n is the maximal number of keys to be stored in the Bloomier filter and &lt;math&gt;\epsilon&lt;/math&gt; is the requested false positive rate. To insert a key in the Bloomier filter, its hash value is first computed. Then the algorithm checks if a key with the same hash value already exists in S2. If this is not the case, the hash value is inserted in S2 along with data associated with the key. If the same hash value already exists in S2 then the key is inserted into S1 along with its associated data. The deletion is symmetric: if the key already exists in S1 it will be deleted from there, otherwise the hash value associated with the key is deleted from S2. An issue with this algorithm is on how to store efficiently S1 and S2. For S1 any hash algorithm can be used. To store S2 [[golomb coding]] could be applied to compress signatures to use a space close to &lt;math&gt;\log2(1/\epsilon)&lt;/math&gt; per key. --&gt;

===Compact approximators===
{{harvtxt|Boldi|Vigna|2005}} proposed a [[lattice (order)|lattice]]-based generalization of Bloom filters. A '''compact approximator''' associates to each key an element of a lattice (the standard Bloom filters being the case of the Boolean two-element lattice). Instead of a bit array, they have an array of lattice elements. When adding a new association between a key and an element of the lattice, they compute the maximum of the current contents of the &lt;var&gt;k&lt;/var&gt; array locations associated to the key with the lattice element. When reading the value associated to a key, they compute the minimum of the values found in the &lt;var&gt;k&lt;/var&gt; locations associated to the key. The resulting value approximates from above the original value.

==={{Anchor|STABLE}}Stable Bloom filters===
{{harvtxt|Deng|Rafiei|2006}} proposed Stable Bloom filters as a variant of Bloom filters for streaming data. The idea is that since there is no way to store the entire history of a stream (which can be infinite), Stable Bloom filters continuously evict stale information to make room for more recent elements. Since stale information is evicted, the Stable Bloom filter introduces false negatives, which do not appear in traditional bloom filters. The authors show that a tight upper bound of false positive rates is guaranteed, and the method is superior to standard bloom filters in terms of false positive rates and time efficiency when a small space and an acceptable false positive rate are given.

==={{Anchor|SCALABLE}}Scalable Bloom filters===
{{harvtxt|Almeida|Baquero|Preguica|Hutchison|2007}} proposed a variant of Bloom filters that can adapt dynamically to the number of elements stored, while assuring a minimum false positive probability. The technique is based on sequences of standard bloom filters with increasing capacity and tighter false positive probabilities, so as to ensure that a maximum false positive probability can be set beforehand, regardless of the number of elements to be inserted.

==={{Anchor|LAYERED}}Layered Bloom filters===
A layered bloom filter consists of multiple bloom filter layers. Layered bloom filters allow keeping track of how many times an item was added to the bloom filter by checking how many layers contain the item. With a layered bloom filter a check operation will normally return the deepest layer number the item was found in.&lt;ref&gt;{{cite web|url=https://github.com/pmylund/go-bloom |title=pmylund/go-bloom |publisher=Github.com |date= |accessdate=2014-06-13}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=First Name Middle Name Last Name |url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=5578947&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5578947 |title=IEEE Xplore Abstract - A multi-layer bloom filter for duplicated URL detection |doi=10.1109/ICACTE.2010.5578947 |publisher=Ieeexplore.ieee.org |date=2010-08-22 |accessdate=2014-06-13}}&lt;/ref&gt;

==={{Anchor|ATTENUATED}}Attenuated Bloom filters===
[[File:AttenuatedBloomFilter.png|thumb|Attenuated Bloom Filter Example]]

An attenuated bloom filter of depth D can be viewed as an array of D normal bloom filters. In the context of service discovery in a network, each node stores regular and attenuated bloom filters locally. The regular or local bloom filter indicates which services are offered by the node itself. The attenuated filter of level i indicates which services can be found on nodes that are i-hops away from the current node. The i-th value is constructed by taking a union of local bloom filters for nodes i-hops away from the node.&lt;ref name=&quot;kgsb09&quot;&gt;{{harvtxt|Koucheryavy|Giambene|Staehle|Barcelo-Arroyo|2009}}&lt;/ref&gt;

Let's take a small network shown on the graph below as an example. Say we are searching for a service A whose id hashes to bits 0,1, and 3 (pattern 11010). Let n1 node to be the starting point. First, we check whether service A is offered by n1 by checking its local filter. Since the patterns don't match, we check the attenuated bloom filter in order to determine which node should be the next hop. We see that n2 doesn't offer service A but lies on the path to nodes that do. Hence, we move to n2 and repeat the same procedure. We quickly find that n3 offers the service, and hence the destination is located.&lt;ref&gt;{{harvtxt|Kubiatowicz|Bindel|Czerwinski|Geels|2000}}&lt;/ref&gt;

By using attenuated Bloom filters consisting of multiple layers, services at more than one hop distance can be discovered while avoiding saturation of the Bloom filter by attenuating (shifting out) bits set by sources further away.&lt;ref name=&quot;kgsb09&quot;/&gt;

===Chemical structure searching===
Bloom filters are commonly used to search large databases of chemicals (see [[chemical similarity]]). Each molecule is represented with a bloom filter (called a fingerprint in this field) which stores substructures of the molecule. Commonly, the [[Jaccard index|tanimoto]] similarity is used to quantify the similarity between molecules' bloom filters.

==See also==
*[[Count–min sketch]]
*[[Feature hashing]]
*[[MinHash]]
*[[Quotient filter]]
*[[Skip list]]

==Notes==
{{Reflist|colwidth=30em}}

==References==
{{Refbegin|colwidth=30em}}
*{{Citation
 | first1 = Y. | last1 = Koucheryavy
 | first2 = G. | last2 = Giambene
 | first3 = D. | last3 = Staehle
 | first4 = F. | last4 =Barcelo-Arroyo
 | first5 = T. | last5 = Braun
 | first6 = V. | last6 = Siris
 | title = Traffic and QoS Management in Wireless Multimedia Networks
 | journal = COST 290 Final Report
 | location = USA
 | year = 2009
 | doi = &lt;!-- XVI, 312 --&gt;
 | page = 111}}
*{{Citation
 | first1 = J. | last1 = Kubiatowicz
 | first2 = D. | last2 = Bindel
 | first3 = Y. | last3 = Czerwinski
 | first4 = S. | last4 = Geels
 | first5 = D. | last5 = Eaton
 | first6 = R. | last6 = Gummadi
 | first7 = S. | last7 = Rhea
 | first8 = H. | last8 = Weatherspoon
 | first9 = W. | last9 = Weimer
 | title = Oceanstore: An architecture for global-scale persistent storage
 | journal = ACM SIGPLAN Notices
 | location = USA
 | year = 2000
 | doi = &lt;!-- 2000, VOL 35; PART 11 --&gt;
 | url = http://ftp.csd.uwo.ca/courses/CS9843b/papers/OceanStore.pdf
 | pages = 190–201}}
*{{Citation
 | first1 = Sachin | last1 = Agarwal
 | first2 = Ari | last2 = Trachtenberg
 | title = Approximating the number of differences between remote sets
 | journal = IEEE Information Theory Workshop
 | location = Punta del Este, Uruguay
 | year = 2006
 | doi = 10.1109/ITW.2006.1633815
 | url = http://www.deutsche-telekom-laboratories.de/~agarwals/publications/itw2006.pdf
 | page = 217
 | isbn = 1-4244-0035-X}}
*{{Citation
 | doi = 10.1109/ICON.2007.4444089
 | first1 = Mahmood| last1 = Ahmadi
 | first2 = Stephan| last2 = Wong  | contribution = A Cache Architecture for Counting Bloom Filters
 | title = 15th international Conference on Networks (ICON-2007)
 | year = 2007
 | page = 218 | url = http://www.ieeexplore.ieee.org/xpls/abs_all.jsp?isnumber=4444031&amp;arnumber=4444089&amp;count=113&amp;index=57
 | isbn = 978-1-4244-1229-7}}
*{{Citation
| first1 = Paulo | last1 = Almeida
| first2 = Carlos | last2 = Baquero
| first3 = Nuno | last3 = Preguica
| first4 = David | last4 = Hutchison
| title = Scalable Bloom Filters
| journal = Information Processing Letters
| volume = 101
| issue = 6
| year = 2007
| doi = 10.1016/j.ipl.2006.10.007
| url = http://gsd.di.uminho.pt/members/cbm/ps/dbloom.pdf
| pages = 255–261}}
*{{Citation
 | first1 = John W. | last1 = Byers
 | first2 = Jeffrey | last2 = Considine
 | first3 = Michael | last3 = Mitzenmacher
 | first4 = Stanislav | last4 = Rost
 | journal = [[IEEE/ACM Transactions on Networking]]
 | title = Informed content delivery across adaptive overlay networks
 | volume = 12
 | year = 2004
 | doi = 10.1109/TNET.2004.836103
 | page = 767
 | issue = 5}}
*{{Citation
 | first = Burton H. | last = Bloom
 | title = Space/Time Trade-offs in Hash Coding with Allowable Errors 
 | url= https://dl.acm.org/citation.cfm?doid=362686.362692
 | journal = [[Communications of the ACM]]
 | volume = 13 | issue = 7 | year = 1970 | pages = 422–426
 | doi = 10.1145/362686.362692}}
*{{Citation
 | first1 = Paolo | last1 = Boldi
 | first2 = Sebastiano | last2 = Vigna
 | title = Mutable strings in Java: design, implementation and lightweight text-search algorithms
 | journal = Science of Computer Programming
 | volume = 54 | issue = 1 | year = 2005 | pages = 3–23
 | doi = 10.1016/j.scico.2004.05.003}}
*{{Citation
 | first1 = Flavio | last1 = Bonomi
 | first2 = Michael | last2 = Mitzenmacher
 | first3 = Rina | last3 = Panigrahy
 | first4 = Sushil | last4 = Singh
 | first5 = George |last5 = Varghese
 | contribution = An Improved Construction for Counting Bloom Filters
 | title = Algorithms – ESA 2006, 14th Annual European Symposium
 | year = 2006 | pages = 684–695 | doi = 10.1007/11841036_61
 | url = http://theory.stanford.edu/~rinap/papers/esa2006b.pdf
 | volume = 4168
 | series = Lecture Notes in Computer Science
 | isbn = 978-3-540-38875-3}}
*{{Citation
 | first1 = Andrei | last1 = Broder | authorlink1 = Andrei Broder
 | first2 = Michael | last2 = Mitzenmacher
 | title = Network Applications of Bloom Filters: A Survey
 | journal = Internet Mathematics
 | volume = 1 | issue = 4 | pages = 485–509 | year = 2005
 | url = http://www.eecs.harvard.edu/~michaelm/postscripts/im2005b.pdf
 | doi = 10.1080/15427951.2004.10129096}}
*{{Citation
 | first1 = Fay| last1 = Chang
 | first2 = Jeffrey| last2 = Dean
 | first3 = Sanjay | last3 = Ghemawat
 | first4 = Wilson | last4 = Hsieh
 | first5 = Deborah | last5 = Wallach
 | first6 = Mike | last6 = Burrows
 | first7 = Tushar | last7 = Chandra
 | first8 = Andrew | last8 = Fikes
 | first9 = Robert | last9 = Gruber
 | contribution = Bigtable: A Distributed Storage System for Structured Data
 | title = Seventh Symposium on Operating System Design and Implementation
 | year = 2006 | url = http://research.google.com/archive/bigtable.html| display-authors = 9
 }}
*{{Citation
 | first1 = Denis| last1 = Charles
 | first2 = Kumar| last2 = Chellapilla
 | contribution = Bloomier Filters: A second look
 | title = The Computing Research Repository (CoRR)
 | year = 2008 | arxiv = 0807.0928
}}
*{{Citation
 | first1 = Bernard | last1 = Chazelle | authorlink1 = Bernard Chazelle
 | first2 = Joe | last2 = Kilian
 | first3 = Ronitt | last3 = Rubinfeld
 | first4 = Ayellet | last4 = Tal
 | contribution = The Bloomier filter: an efficient data structure for static support lookup tables
 | title = Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms
 | year = 2004 | pages = 30–39
 | url = http://www.ee.technion.ac.il/~ayellet/Ps/nelson.pdf}}
*{{Citation
 | first1 = Saar | last1 = Cohen
 | first2 = Yossi | last2 = Matias
 | contribution = Spectral Bloom Filters
 | title = Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data
 | year = 2003 | doi = 10.1145/872757.872787 | pages = 241–252
 | url = http://www.sigmod.org/sigmod03/eproceedings/papers/r09p02.pdf
 | isbn = 158113634X
 }} {{Dead link|date=June 2010}}
*{{Citation
 | first1 = Fan | last1 = Deng
 | first2 = Davood | last2 = Rafiei
 | contribution = Approximately Detecting Duplicates for Streaming Data using Stable Bloom Filters
 | title = Proceedings of the ACM SIGMOD Conference
 | year = 2006 | pages = 25–36 | url = http://webdocs.cs.ualberta.ca/~drafiei/papers/DupDet06Sigmod.pdf}}
*{{Citation
 | first1 = Sarang | last1 = Dharmapurikar
 | first2 = Haoyu | last2 = Song
 | first3 = Jonathan | last3 = Turner
 | first4 = John | last4 = Lockwood
 | contribution = Fast packet classification using Bloom filters
 | title = Proceedings of the 2006 ACM/IEEE Symposium on Architecture for Networking and Communications Systems
 | year = 2006 | pages = 61–70 | doi = 10.1145/1185347.1185356
 | url = http://www.arl.wustl.edu/~sarang/ancs6819-dharmapurikar.pdf
 | isbn = 1595935800}}
*{{Citation
 | first1 = Martin| last1 = Dietzfelbinger
 | first2 = Rasmus| last2 = Pagh
 | contribution = Succinct Data Structures for Retrieval and Approximate Membership
 | title = The Computing Research Repository (CoRR)
 | year = 2008 | arxiv = 0803.3693
}}
*{{Citation
| first1=S. Joshua| last1 = Swamidass | first2 = Pierre | last2 = Baldi
| title=Mathematical correction for fingerprint similarity measures to improve chemical retrieval| journal=Journal of chemical information and modeling| year=2007| volume=47| number=3| pages=952–964| publisher=ACS Publications| pmid = 17444629| doi=10.1021/ci600526a}}
*{{Citation
 | first1 = Peter C. | last1 = Dillinger
 | first2 = Panagiotis | last2 = Manolios
 | contribution = Fast and Accurate Bitstate Verification for SPIN
 | title = Proceedings of the 11th International Spin Workshop on Model Checking Software
 | publisher = Springer-Verlag, Lecture Notes in Computer Science 2989
 | year = 2004a
 | url = http://www.ccs.neu.edu/home/pete/research/spin-3spin.html}}
*{{Citation
 | first1 = Peter C. | last1 = Dillinger
 | first2 = Panagiotis | last2 = Manolios
 | contribution = Bloom Filters in Probabilistic Verification
 | title = Proceedings of the 5th International Conference on Formal Methods in Computer-Aided Design
 | publisher = Springer-Verlag, Lecture Notes in Computer Science 3312
 | year = 2004b
 | url = http://www.ccs.neu.edu/home/pete/research/bloom-filters-verification.html}}
*{{Citation
 | first1 = Benoit | last1 = Donnet
 | first2 = Bruno | last2 = Baynat
 | first3 = Timur | last3 = Friedman
 | contribution = Retouched Bloom Filters: Allowing Networked Applications to Flexibly Trade Off False Positives Against False Negatives
 | title = CoNEXT 06 – 2nd Conference on Future Networking Technologies | year = 2006
 | url = http://www.adetti.iscte.pt/events/CONEXT06/Conext06_Proceedings/papers/13.html}}
*{{Citation
 | first1 = David | last1 = Eppstein | authorlink1 = David Eppstein
 | first2 = Michael T. | last2 = Goodrich | author2-link = Michael T. Goodrich
 | contribution = Space-efficient straggler identification in round-trip data streams via Newton's identities and invertible Bloom filters
 | title = [[Workshop on Algorithms and Data Structures|Algorithms and Data Structures, 10th International Workshop, WADS 2007]]
 | publisher = Springer-Verlag, Lecture Notes in Computer Science 4619
 | year = 2007 | pages = 637–648
 | arxiv = 0704.3313
}}
*{{Citation
 | first1 = Li | last1 = Fan | first2 = Pei | last2 = Cao
 | first3 = Jussara | last3 = Almeida
 | first4 = Andrei | last4 = Broder | authorlink4 = Andrei Broder
 | title = Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol
 | journal = IEEE/ACM Transactions on Networking
 | volume = 8 | issue = 3 | year = 2000 | pages = 281–293 | doi = 10.1109/90.851975}}. A preliminary version appeared at SIGCOMM '98.
*{{Citation
 | first1 = Ashish | last1 = Goel
 | first2 = Pankaj | last2 = Gupta
 | title = Small subset queries and bloom filters using ternary associative memories, with applications
 | journal = ACM Sigmetrics 2010
 | volume = 38
 | page = 143
 | year = 2010
 | doi = 10.1145/1811099.1811056
 }}
*{{Citation
 | first1 = Adam |last1 = Kirsch | first2 = Michael | last2 = Mitzenmacher
 | contribution = Less Hashing, Same Performance: Building a Better Bloom Filter
 | title = Algorithms – ESA 2006, 14th Annual European Symposium
 | publisher = Springer-Verlag, Lecture Notes in Computer Science 4168
 | year = 2006 | pages = 456–467 | doi = 10.1007/11841036
 | url = http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/esa06.pdf
 | volume = 4168
 | editor1-last = Azar
 | editor1-first = Yossi
 | editor2-last = Erlebach
 | editor2-first = Thomas
 | series = Lecture Notes in Computer Science
 | isbn = 978-3-540-38875-3}}
*{{Citation
 | first1 = Christian Worm | last1 = Mortensen
 | first2 = Rasmus | last2 = Pagh
 | first3 = Mihai | last3 = Pătraşcu | author3-link = Mihai Pătraşcu
 | contribution = On dynamic range reporting in one dimension
 | title = Proceedings of the Thirty-seventh Annual ACM Symposium on Theory of Computing
 | year = 2005 | pages = 104–111 | doi = 10.1145/1060590.1060606
 | isbn = 1581139608}}
*{{Citation
 | first1 = Anna | last1 = Pagh
 | first2 = Rasmus | last2 = Pagh
 | first3 = S. Srinivasa | last3 = Rao
 | contribution = An optimal Bloom filter replacement
 | title = Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms
 | year = 2005 | pages = 823–829
 | url = http://www.it-c.dk/people/pagh/papers/bloom.pdf}}
*{{Citation
 | first1 = Ely| last1 = Porat
 | contribution = An Optimal Bloom Filter Replacement Based on Matrix Solving
 | title = The Computing Research Repository (CoRR)
 | year = 2008 | arxiv = 0804.1845
}}
*{{Citation
 | first1 = F. | last1 = Putze
 | first2 = P. | last2 = Sanders
 | first3 = J. | last3 = Singler
 | contribution = Cache-, Hash- and Space-Efficient Bloom Filters
 | title = Experimental Algorithms, 6th International Workshop, WEA 2007
 | publisher = Springer-Verlag, Lecture Notes in Computer Science 4525
 | year = 2007 | pages = 108–121 | doi = 10.1007/978-3-540-72845-0
 | url = http://algo2.iti.uni-karlsruhe.de/singler/publications/cacheefficientbloomfilters-wea2007.pdf
 | volume = 4525
 | editor1-last = Demetrescu
 | editor1-first = Camil
 | series = Lecture Notes in Computer Science
 | isbn = 978-3-540-72844-3}}
*{{Citation
 | first1 = Simha | last1 = Sethumadhavan
 | first2 = Rajagopalan | last2 = Desikan
 | first3 = Doug | last3 = Burger
 | first4 = Charles R. | last4 = Moore
 | first5 = Stephen W. | last5 = Keckler
 | contribution = Scalable hardware memory disambiguation for high ILP processors
 | title = 36th Annual IEEE/ACM International Symposium on Microarchitecture, 2003, MICRO-36
 | year = 2003 | pages = 399–410 | doi = 10.1109/MICRO.2003.1253244
 | url = http://www.cs.utexas.edu/users/simha/publications/lsq.pdf
 | isbn = 0-7695-2043-X}}
*{{Citation
 | first1 = Kulesh | last1 = Shanmugasundaram
 | first2 = Hervé | last2 = Brönnimann
 | first3 = Nasir | last3 = Memon
 | contribution = Payload attribution via hierarchical Bloom filters
 | title = Proceedings of the 11th ACM Conference on Computer and Communications Security
 | year = 2004 | pages = 31–41 | doi = 10.1145/1030083.1030089
 | isbn = 1581139616}}
*{{Citation
 | first1 = David | last1 = Starobinski
 | first2 = Ari | last2 = Trachtenberg
 | first3 = Sachin | last3=Agarwal
 | title = Efficient PDA Synchronization
 | journal = IEEE Transactions on Mobile Computing
 | volume = 2
 | year = 2003
 | doi = 10.1109/TMC.2003.1195150
 | page = 40
 | issue = 1}}
*{{Citation
 | first1 = Ulrich | last1 = Stern
 | first2 = David L. | last2 = Dill
 | contribution = A New Scheme for Memory-Efficient Probabilistic Verification
 | title = Proceedings of Formal Description Techniques for Distributed Systems and Communication Protocols, and Protocol Specification, Testing, and Verification: IFIP TC6/WG6.1 Joint International Conference
 | publisher = Chapman &amp; Hall, IFIP Conference Proceedings
 | year = 1996
 | id = {{citeseerx|10.1.1.47.4101}}
 | pages = 333–348}}
*{{Citation
| first1 = Mohammad Hashem | last1 = Haghighat
| first2 = Mehdi | last2 = Tavakoli
| first3 = Mehdi | last3 = Kharrazi
| title = Payload Attribution via Character Dependent Multi-Bloom Filters
| journal = Transaction on Information Forensics and Security, IEEE
| volume = 99
| issue = 
5| year = 2013
| doi = 10.1109/TIFS.2013.2252341
| page = 705}}
*{{Citation
| publisher = Cambridge University Press
| last1 = Mitzenmacher
| first1 = Michael
| last2 = Upfal
| first2 = Eli
| title = Probability and computing: Randomized algorithms and probabilistic analysis
| year = 2005
| url = http://books.google.com/books?id=0bAYl6d7hvkC&amp;pg=PA110
| pages = 107–112
| isbn = 9780521835404
}}
*{{Citation
| volume = 16
| issue = 5
| pages = 558–560
| last = Mullin
| first = James K.
| title = Optimal semijoins for distributed database systems
| journal = Software Engineering, IEEE Transactions on
| year = 1990
| doi=10.1109/32.52778
}}
*{{Citation
 | first1 = Ori | last1 = Rottenstreich
 | first2 = Yossi | last2 = Kanizo
 | first3 = Isaac | last3 = Keslassy
 | contribution = The Variable-Increment Counting Bloom Filter
 | title = 31st Annual IEEE International Conference on Computer Communications, 2012, Infocom 2012
 | year = 2012 | pages = 1880–1888 | doi = 10.1109/INFCOM.2012.6195563
 | url = http://webee.technion.ac.il/people/or/publications/Infocom12_VICBF.pdf
 | isbn = 978-1-4673-0773-4}}
*{{Citation
 |first1 = James | last1 = Blustein
 |first2 = Amal | last2 = El-Maazawi
 |contribution = optimal case for general Bloom filters  
 |title = Bloom Filters — A Tutorial, Analysis, and Survey
 |year = 2002 | pages = 1–31
 |publisher = Dalhousie University Faculty of Computer Science
 |url = https://www.cs.dal.ca/research/techreports/cs-2002-10}}
{{Refend}}

==External links==
{{Commons category|Bloom filter}}
*[http://www.michaelnielsen.org/ddi/why-bloom-filters-work-the-way-they-do/ Why Bloom filters work the way they do (Michael Nielsen, 2012)]
*[https://www.cs.dal.ca/research/techreports/cs-2002-10 Bloom Filters — A Tutorial, Analysis, and Survey (Blustein &amp; El-Maazawi, 2002)] at Dalhousie University
*[http://www.cs.wisc.edu/~cao/papers/summary-cache/node8.html Table of false-positive rates for different configurations] from a [[University of Wisconsin–Madison]] website
*[http://tr.ashcan.org/2008/12/bloomers.html Interactive Processing demonstration] from ashcan.org
*[http://www.youtube.com/watch?v=947gWqwkhu0 &quot;More Optimal Bloom Filters,&quot; Ely Porat (Nov/2007) Google TechTalk video] on [[YouTube]]
*[http://www.perl.com/pub/2004/04/08/bloom_filters.html &quot;Using Bloom Filters&quot;] Detailed Bloom Filter explanation using [[Perl]]
*[http://matthias.vallentin.net/blog/2011/06/a-garden-variety-of-bloom-filters/ &quot;A Garden Variety of Bloom Filters] - Explanation and Analysis of Bloom filter variants

=== Implementations ===
{{External links|date=April 2013}}
{{div col}}
*[http://en.literateprograms.org/Bloom_filter_(C) Implementation in C] from literateprograms.org
*[https://github.com/mavam/libbf/ Implementation in C++11] on github.com
*[http://codeplex.com/bloomfilter Implementation in C#] from codeplex.com
*[http://sites.google.com/site/scalablebloomfilters/ Implementation in Erlang] from sites.google.com
*[http://hackage.haskell.org/cgi-bin/hackage-scripts/package/bloomfilter Implementation in Haskell] from haskell.org
*[https://github.com/DivineTraube/Orestes-Bloomfilter Implementation in Java] on github.com
*[http://la.ma.la/misc/js/bloomfilter/ Implementation in Javascript] from la.ma.la
*[https://github.com/wiedi/node-bloem Implementation in JS for node.js] on github.com
*[http://lemonodor.com/archives/000881.html Implementation in Lisp] from lemonodor.com
*[https://metacpan.org/module/Bloom::Filter Implementation in Perl] from [[CPAN]]
*[http://code.google.com/p/php-bloom-filter/ Implementation in PHP] from code.google.com
*[https://pypi.python.org/pypi/drs-bloom-filter/ Implementation in Python, Traditional Bloom Filter] from pypi.python.org
*[https://pypi.python.org/pypi/pybloom/1.0.2 Implementation in Python, Scalable Bloom Filter] from pypi.python.org
*[http://www.rubyinside.com/bloom-filters-a-powerful-tool-599.html Implementation in Ruby] from rubyinside.com
*[http://www.codecommit.com/blog/scala/bloom-filters-in-scala Implementation in Scala] from codecommit.com
*[http://www.kocjan.org/tclmentor/61-bloom-filters-in-tcl.html Implementation in Tcl] from kocjan.org
*[http://typescript.codeplex.com/sourcecontrol/latest#src/compiler/bloomFilter.ts Implementation in TypeScript] from codeplex.com
{{div col end}}

{{DEFAULTSORT:Bloom Filter}}
[[Category:Hashing]]
[[Category:Probabilistic data structures]]
[[Category:Lossy compression algorithms]]</text>
      <sha1>41a3tncujyz4bv1zthkatly2x2f3alp</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Consistent hashing</title>
    <ns>0</ns>
    <id>2434041</id>
    <revision>
      <id>620909389</id>
      <parentid>620909342</parentid>
      <timestamp>2014-08-12T12:40:48Z</timestamp>
      <contributor>
        <ip>2601:8:9A80:5C6:E096:2103:E7CE:1528</ip>
      </contributor>
      <comment>/* References */</comment>
      <text xml:space="preserve" bytes="8835">'''Consistent hashing''' is a special kind of [[hash function|hashing]] such that when a hash table is resized and consistent hashing is used, only &lt;math&gt;K/n&lt;/math&gt; keys need to be remapped on average, where &lt;math&gt;K&lt;/math&gt; is the number of keys, and &lt;math&gt;n&lt;/math&gt; is the number of slots.  In contrast, in most traditional [[hash table]]s, a change in the number of array slots causes nearly all keys to be remapped. 

Consistent hashing achieves the same goals as '''[[Rendezvous hashing]]''' (also called HRW Hashing). The two techniques use different algorithms, and were devised independently and contemporaneously.

==History==

Originally devised by [[David Karger|Karger]] ''et al.'' at MIT for use in distributed caching, the idea has now been expanded to other areas also.
An academic paper from 1997 introduced the term &quot;consistent hashing&quot; as a way of distributing requests among a changing population of Web servers. Each slot is then represented by a node in a distributed system. The addition (joins) and removal (leaves/failures) of nodes only requires &lt;math&gt;K/n&lt;/math&gt; items to be re-shuffled when the number of slots/nodes change.&lt;ref name=&quot;KargerEtAl1997&quot;&gt;{{cite conference
 | url = http://portal.acm.org/citation.cfm?id=258660
 | doi = 10.1145/258533.258660
 | author = Karger, D.; Lehman, E.; [[F. Thomson Leighton|Leighton, T.]]; Panigrahy, R.; Levine, M.; [[Daniel M. Lewin|Lewin, D.]]
 | booktitle = Proceedings of the Twenty-ninth Annual ACM [[Symposium on Theory of Computing]]
 | pages = 654–663
 | year = 1997
 | publisher = ACM Press New York, NY, USA
 | title = Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web
}}&lt;/ref&gt;

Consistent hashing has also been used to reduce the impact of partial system failures in large Web applications as to allow for robust caches without incurring the system wide fallout of a failure.&lt;ref name=KargerEtAl1999&gt;{{cite journal
 | url = http://www8.org/w8-papers/2a-webserver/caching/paper2.html
 | doi = 10.1016/S1389-1286(99)00055-9
 | author = Karger, D.; Sherman, A.; Berkheimer, A.; Bogstad, B.; Dhanidina, R.; Iwamoto, K.; Kim, B.; Matkins, L.; Yerushalmi, Y.
 | journal = Computer Networks
 | volume = 31
 | issue = 11
 | pages = 1203–1213
 | year = 1999
 | title = Web Caching with Consistent Hashing
}}&lt;/ref&gt;

The consistent hashing concept also applies to the design of [[distributed hash table]]s (DHTs). DHTs use consistent hashing to partition a keyspace among a distributed set of nodes, and additionally provide an overlay network that connects nodes such that the node responsible for any key can be efficiently located.

[[Rendezvous hashing]], designed at the same time as consistent hashing, achieves the same goals using the very different Highest Random Weight (HRW) algorithm. 
&lt;!-- should describe here why the &quot;consistency&quot; of consistent hashing is essential to DHTs --&gt;

==Need for consistent hashing==

While running collections of caching machines some limitations are experienced. A common way of load balancing &lt;math&gt;n&lt;/math&gt; cache machines is to put object &lt;math&gt;o&lt;/math&gt; in cache machine number &lt;math&gt;\mbox{hash}(o) \mod n&lt;/math&gt;. But this will not work if a cache machine is added or removed because &lt;math&gt;n&lt;/math&gt; changes and every object is hashed to a new location. This can be disastrous since the originating content servers are flooded with requests from the cache machines. Hence consistent hashing is needed to avoid swamping of servers.

Consistent hashing maps objects to the same cache machine, as far as possible. It means when a cache machine is added, it takes its share of objects from all the other cache machines and when it is removed, its objects are shared between the remaining machines.

The main idea behind the consistent hashing algorithm is to associate each cache with one or more hash value intervals where the interval boundaries are determined by calculating the hash of each cache identifier. (The hash function used to define the intervals does not have to be the same function used to hash the cached values. Only the range of the two functions need match.) If the cache is removed its interval is taken over by a cache with an adjacent interval. All the remaining caches are unchanged.

==Technique==

Consistent hashing is based on mapping each object to a point on the edge of a circle (or equivalently, mapping each object to a real angle).
The system maps each available machine (or other storage bucket) to many pseudo-randomly distributed points on the edge of the same circle.

To find where an object should be placed, the system finds the location of that object's key on the edge of the circle;
then walks around the circle until falling into the first bucket it encounters (or equivalently, the first available bucket with a higher angle).
The result is that each bucket contains all the resources located between its point and the previous bucket point.

If a bucket becomes unavailable (for example because the computer it resides on is not reachable), then the angles it maps to will be removed. Requests for resources that would have mapped to each of those points now map to the next highest point. Since each bucket is associated with many pseudo-randomly distributed points, the resources that were held by that bucket will now map to many different buckets. The items that mapped to the lost bucket must be redistributed among the remaining ones, but values mapping to other buckets will still do so and do not need to be moved.

A similar process occurs when a bucket is added. By adding a bucket point, we make any resources between that and the next smaller angle map to the new bucket. These resources will no longer be associated with the previous bucket, and any value previously stored there will not be found by the selection method described above.

The portion of the keys associated with each bucket can be altered by altering the number of angles that bucket maps to.

== Monotonic keys ==
If it is known that key values will always increase monotonically, [[Hash_table#Monotonic_keys|an alternative approach to consistent hashing]] is possible.

==Properties==
{{clarify section|reason=terms need to be defined|date=June 2014}}
[[David Karger]] et al. list several properties of consistent hashing that make it useful for distributed caching protocols on the Internet:&lt;ref name=&quot;KargerEtAl1997&quot; /&gt;
* &quot;spread&quot;
* &quot;load&quot;
* &quot;smoothness&quot;
* &quot;balance&quot;
* &quot;monotonicity&quot;

== Examples of use ==
Some known instances where consistent hashing is used are:
* [[Openstack]]'s Object Storage Service Swift&lt;ref&gt;http://docs.openstack.org/developer/swift/ring.html&lt;/ref&gt;
* Partitioning component of Amazon's storage system Dynamo&lt;ref name=&quot;Amazon2007&quot;&gt;{{cite journal
 | author = DeCandia, G.; Hastorun, D.; Jampani, M.; Kakulapati, G.; Lakshman, A.; Pilchin, A.; Sivasubramanian, S.; Vosshall, P.; Vogels, W.
 | journal = Proceedings of the 21st ACM Symposium on Operating Systems Principles
 | year = 2007
 | title =Dynamo: Amazon's Highly Available Key-Value Store
}}&lt;/ref&gt;
* Data partitioning in [[Apache Cassandra]]&lt;ref name=&quot;Lakshman2010b&quot;&gt;{{cite journal
 | author = Lakshman, Avinash; Malik, Prashant
 | journal = ACM SIGOPS Operating Systems Review
 | year = 2010
 | title = Cassandra: a decentralized structured storage system
}}&lt;/ref&gt;
* [[Akka (toolkit) | Akka]]'s consistent hashing router&lt;ref name=&quot;akka-routing&quot;&gt;[http://doc.akka.io/docs/akka/snapshot/scala/routing.html Akka Routing]&lt;/ref&gt;
* [[Riak]], a distributed key-value database&lt;ref name=&quot;riak-consistent-hashing&quot;&gt;{{cite web|url=http://docs.basho.com/riak/latest/theory/concepts/|title=Riak Concepts}}&lt;/ref&gt;
* [[GlusterFS]], a network-attached storage file system&lt;ref name=&quot;GlusterFS Algorithms: Distribution&quot;&gt;http://www.gluster.org/2012/03/glusterfs-algorithms-distribution/&lt;/ref&gt;
* [[Skylable]], an open-source distributed object-storage system &lt;ref name=&quot;skylable-architecture&quot;&gt;{{cite web|url=http://wiki.skylable.com/wiki/Sxarchitecture#Data_distribution|title=Skylable architecture}}
&lt;/ref&gt;

== References ==
{{Reflist}}

== External links ==
* [http://www.spiteful.com/2008/03/17/programmers-toolbox-part-3-consistent-hashing/ Understanding Consistent hashing]

* Implentations in various languages:
** [http://www.martinbroadhurst.com/Consistent-Hash-Ring.html C++]
** [http://code.google.com/p/consistent-hash/ C#]
** [https://github.com/basho/riak_core/blob/master/src/chash.erl Erlang]
** [https://github.com/stathat/consistent Go]
** [http://weblogs.java.net/blog/tomwhite/archive/2007/11/consistent_hash.html Java]
** [https://github.com/pda/flexihash  PHP]
** [http://amix.dk/blog/post/19367 Python]
** [https://github.com/afirel/consistent_hashr Ruby]

&lt;!--Interwikies--&gt;

{{DEFAULTSORT:Consistent hashing}}
&lt;!--Categories--&gt;
[[Category:Hashing]]</text>
      <sha1>r7m9lr57opdjhhhf2jyxw7ksb1hd46j</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Universal hashing</title>
    <ns>0</ns>
    <id>4024666</id>
    <revision>
      <id>620608755</id>
      <parentid>620486011</parentid>
      <timestamp>2014-08-10T08:00:31Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (10361)</comment>
      <text xml:space="preserve" bytes="25704">Using '''universal hashing''' (in a [[randomized algorithm]] or data structure) refers to selecting a [[hash function]] at random from a family of hash functions with a certain mathematical property (see definition below). This guarantees a low number of collisions in [[expected value|expectation]], even if the data is chosen by an adversary. Many universal families are known (for hashing integers, vectors, strings), and their evaluation is often very efficient. Universal hashing has numerous uses in computer science, for example in implementations of [[hash table]]s, randomized algorithms, and [[cryptography]].

== Introduction ==
{{see also|Hash function}}

Assume we want to map keys from some universe &lt;math&gt;U&lt;/math&gt; into &lt;math&gt;m&lt;/math&gt; bins (labelled &lt;math&gt;[m] = \{0, \dots, m-1\}&lt;/math&gt;). The algorithm will have to handle some data set &lt;math&gt;S \subseteq U&lt;/math&gt; of &lt;math&gt;|S|=n&lt;/math&gt; keys, which is not known in advance. Usually, the goal of hashing is to obtain a low number of collisions (keys from &lt;math&gt;S&lt;/math&gt; that land in the same bin). A deterministic hash function cannot offer any guarantee in an adversarial setting if the size of &lt;math&gt;U&lt;/math&gt; is greater than &lt;math&gt;m^2&lt;/math&gt;, since the adversary may choose &lt;math&gt;S&lt;/math&gt; to be precisely the [[Image (mathematics)|preimage]] of a bin. This means that all data keys land in the same bin, making hashing useless. Furthermore, a deterministic hash function does not allow for ''rehashing'': sometimes the input data turns out to be bad for the hash function (e.g. there are too many collisions), so one would like to change the hash function.

The solution to these problems is to pick a function randomly from a family of hash functions. A family of functions &lt;math&gt;H = \{ h : U \to [m] \}&lt;/math&gt; is called a '''universal family''' if, &lt;math&gt;\forall x, y \in U, ~ x\ne y: ~~ \Pr_{h\in H} [h(x) = h(y)] \le \frac{1}{m}&lt;/math&gt;.

In other words, any two keys of the universe collide with probability at most &lt;math&gt;1/m&lt;/math&gt; when the hash function &lt;math&gt;h&lt;/math&gt; is drawn randomly from &lt;math&gt;H&lt;/math&gt;. This is exactly the probability of collision we would expect if the hash function assigned truly random hash codes to every key. Sometimes, the definition is relaxed to allow collision probability &lt;math&gt;O(1/m)&lt;/math&gt;. This concept was introduced by Carter and Wegman&lt;ref name=CW77&gt;
{{cite journal
   | last1 = Carter | first1 = Larry 
   | last2 = Wegman | first2 = Mark N. | author2-link = Mark N. Wegman
   | title = Universal Classes of Hash Functions
   | journal = Journal of Computer and System Sciences
   | volume = 18
   | issue = 2
   | pages = 143–154
   | year = 1979
   | doi = 10.1016/0022-0000(79)90044-8
   | id = Conference version in STOC'77
}}&lt;/ref&gt; in 1977, and has found numerous applications in computer science (see, for example &lt;ref name=Miltersen&gt;
{{cite web
   | last = Miltersen
   | first = Peter Bro
   | title = Universal Hashing
   | url = http://www.daimi.au.dk/~bromille/Notes/un.pdf
   | format = PDF
   | archiveurl = http://www.webcitation.org/5hmOaVISI
   | archivedate = 24 June 2009
}}&lt;/ref&gt;). If we have an upper bound of &lt;math&gt;\epsilon&lt;1&lt;/math&gt; on the collision probability, we say that we have &lt;math&gt;\epsilon&lt;/math&gt;-almost universality.

Many, but not all, universal families have the following stronger '''uniform difference property''':
: &lt;math&gt;\forall x,y\in U, ~ x\ne y&lt;/math&gt;, when &lt;math&gt;h&lt;/math&gt; is drawn randomly from the family &lt;math&gt;H&lt;/math&gt;, the difference &lt;math&gt;h(x)-h(y) ~\bmod~ m&lt;/math&gt; is uniformly distributed in &lt;math&gt;[m]&lt;/math&gt;.

Note that the definition of universality is only concerned with whether &lt;math&gt;h(x)-h(y)=0&lt;/math&gt;, which counts collisions. The uniform difference property is stronger.

(Similarly, a universal family can be XOR universal if &lt;math&gt;\forall x,y\in U, ~ x\ne y&lt;/math&gt;, the value &lt;math&gt;h(x) \oplus h(y) ~\bmod~ m&lt;/math&gt; is uniformly distributed in &lt;math&gt;[m]&lt;/math&gt; where &lt;math&gt;\oplus&lt;/math&gt; is the bitwise exclusive or operation. This is only possible if &lt;math&gt;m&lt;/math&gt; is a power of two.)

An even stronger condition is [[Pairwise independent|pairwise independence]]: we have this property when  &lt;math&gt;\forall x,y\in U, ~ x\ne y&lt;/math&gt; we have the probability that &lt;math&gt;x,y&lt;/math&gt; will hash to any pair of hash values &lt;math&gt;z_1, z_2&lt;/math&gt; is as if they were perfectly random: &lt;math&gt;P(h(x)=z_1 \land h(y)=z_2)= 1/m^2&lt;/math&gt;. Pairwise independence is sometimes called strong universality.

Another property is uniformity. We say that a family is uniform if all hash values are equally likely: &lt;math&gt;P(h(x)=z)=1/m&lt;/math&gt; for any hash value &lt;math&gt;z&lt;/math&gt;. Universality does not imply uniformity. However, strong  universality does imply uniformity.

Given a family with the uniform distance property, one can produce a pairwise independent or strongly universal hash family by adding a uniformly distributed random constant with values in &lt;math&gt;[m]&lt;/math&gt; to the hash functions. (Similarly, if &lt;math&gt;m&lt;/math&gt; is a power of two, we can achieve pairwise independence from an XOR universal hash family by doing an exclusive or with a uniformly distributed random constant.) Since a shift by a constant is sometimes irrelevant in applications (e.g. hash tables), a careful distinction between the uniform distance property and pairwise independent is sometimes not made.&lt;ref&gt;
{{cite book
 | last1 = Motwani
 | first1 = Rajeev
 | last2 = Raghavan
 | first2 = Prabhakar
 | title = Randomized Algorithms
 | publisher = Cambridge University Press
 | year = 1995
 | isbn = 0-521-47465-5
 | page = 221
}}
&lt;/ref&gt;

For some applications (such as hash tables), it is important for the least significant bits of the hash values to be also universal. When a family is strongly universal, this is guaranteed: if &lt;math&gt;H&lt;/math&gt; is a strongly universal family with &lt;math&gt;m=2^L&lt;/math&gt;, then the family made of the functions &lt;math&gt;h \bmod{2^{L'}}&lt;/math&gt; for all &lt;math&gt;h \in H&lt;/math&gt; is also strongly universal for &lt;math&gt;L'\leq L&lt;/math&gt;. Unfortunately, the same is not true of (merely) universal families. For example the family made of the identity function &lt;math&gt;h(x)=x&lt;/math&gt; is clearly universal, but the family made of the function &lt;math&gt;h(x)=x  \bmod{2^{L'}}&lt;/math&gt; fails to be universal.

== Mathematical guarantees ==

For any fixed set &lt;math&gt;S&lt;/math&gt; of &lt;math&gt;n&lt;/math&gt; keys, using a universal family guarantees the following properties.
# For any fixed &lt;math&gt;x&lt;/math&gt; in &lt;math&gt;S&lt;/math&gt;, the expected number of keys in the bin &lt;math&gt;h(x)&lt;/math&gt; is &lt;math&gt;n/m&lt;/math&gt;. When implementing hash tables by [[Hash table#Separate chaining|chaining]], this number is proportional to the expected running time of an operation involving the key &lt;math&gt;x&lt;/math&gt; (for example a query, insertion or deletion).
# The expected number of pairs of keys &lt;math&gt;x,y&lt;/math&gt; in &lt;math&gt;S&lt;/math&gt; with &lt;math&gt;x\ne y&lt;/math&gt; that collide (&lt;math&gt;h(x) = h(y)&lt;/math&gt;) is bounded above by &lt;math&gt;n(n-1)/2m&lt;/math&gt;, which is of order &lt;math&gt;O(n^2/m)&lt;/math&gt;. When the number of bins, &lt;math&gt;m&lt;/math&gt;, is &lt;math&gt;O(n)&lt;/math&gt;, the expected number of collisions is &lt;math&gt;O(n)&lt;/math&gt;. When hashing into &lt;math&gt;n^2&lt;/math&gt; bins, there are no collisions at all with probability at least a half.
# The expected number of keys in bins with at least &lt;math&gt;t&lt;/math&gt; keys in them is bounded above by &lt;math&gt;2n/(t-2(n/m)+1)&lt;/math&gt;.&lt;ref name=BDP&gt;
{{cite journal
   | doi = 10.1007/s00453-007-9036-3
   | last1 = Baran  | first1 = Ilya
   | last2 = Demaine  | first2 = Erik D.
   | last3 = Pătraşcu | first3 = Mihai | author3-link = Mihai Pătraşcu
   | title = Subquadratic Algorithms for 3SUM
   | journal = Algorithmica
   | volume = 50
   | issue = 4
   | pages = 584–596
   | year = 2008
   | url = http://people.csail.mit.edu/mip/papers/3sum/3sum.pdf
}}&lt;/ref&gt; Thus, if the capacity of each bin is capped to three times the average size (&lt;math&gt;t = 3n/m&lt;/math&gt;), the total number of keys in overflowing bins is at most &lt;math&gt;O(m)&lt;/math&gt;. This only holds with a hash family whose collision probability is bounded above by &lt;math&gt;1/m&lt;/math&gt;. If a weaker definition is used, bounding it by &lt;math&gt;O(1/m)&lt;/math&gt;, this result is no longer true.&lt;ref name=BDP /&gt;

As the above guarantees hold for any fixed set &lt;math&gt;S&lt;/math&gt;, they hold if the data set is chosen by an adversary. However, the adversary has to make this choice before (or independent of) the algorithm's random choice of a hash function. If the adversary can observe the random choice of the algorithm, randomness serves no purpose, and the situation is the same as deterministic hashing.

The second and third guarantee are typically used in conjunction with [[Double hashing|rehashing]]. For instance, a randomized algorithm may be prepared to handle some &lt;math&gt;O(n)&lt;/math&gt; number of collisions. If it observes too many collisions, it chooses another random &lt;math&gt;h&lt;/math&gt; from the family and repeats. Universality guarantees that the number of repetitions is a [[Geometric distribution|geometric random variable]].

== Constructions ==

Since any computer data can be represented as one or more machine words, one generally needs hash functions for three types of domains: machine words (&quot;integers&quot;); fixed-length vectors of machine words; and variable-length vectors (&quot;strings&quot;).

=== Hashing integers ===

This section refers to the case of hashing integers that fit in machines words; thus, operations like multiplication, addition, division, etc. are cheap machine-level instructions. Let the universe to be hashed be &lt;math&gt;U = \{0, \dots, u-1\}&lt;/math&gt;.

The original proposal of Carter and Wegman&lt;ref name=CW77 /&gt; was to pick a prime &lt;math&gt;p \ge u&lt;/math&gt; and define

: &lt;math&gt; h_{a,b}(x) = ((ax + b)~\bmod ~ p)~\bmod ~ m&lt;/math&gt;

where &lt;math&gt;a,b&lt;/math&gt; are randomly chosen integers modulo &lt;math&gt;p&lt;/math&gt; with &lt;math&gt;a \neq 0&lt;/math&gt;. Technically, adding &lt;math&gt;b&lt;/math&gt; is not needed for universality (but it does make the hash function 2-independent).
(This is a single iteration of a [[linear congruential generator]]).

To see that &lt;math&gt;H = \{ h_{a,b} \}&lt;/math&gt; is a universal family, note that &lt;math&gt;h(x) = h(y)&lt;/math&gt; only holds when

: &lt;math&gt;ax+b \equiv ay + b + i\cdot m \pmod{p}&lt;/math&gt;

for some integer &lt;math&gt;i&lt;/math&gt; between &lt;math&gt;0&lt;/math&gt; and &lt;math&gt;p/m&lt;/math&gt;. If &lt;math&gt;x \neq y&lt;/math&gt;, their difference, &lt;math&gt;x-y&lt;/math&gt; is nonzero and has an inverse modulo &lt;math&gt;p&lt;/math&gt;. Solving for &lt;math&gt;a&lt;/math&gt;,

: &lt;math&gt;a \equiv i\cdot m \cdot (x-y)^{-1} \pmod{p}&lt;/math&gt;.

There are &lt;math&gt;p-1&lt;/math&gt; possible choices for &lt;math&gt;a&lt;/math&gt; (since &lt;math&gt;a=0&lt;/math&gt; is excluded) and, varying &lt;math&gt;i&lt;/math&gt; in the allowed range, &lt;math&gt;\lfloor p/m \rfloor&lt;/math&gt; possible values for the right hand side. Thus the collision probability is
	 
: &lt;math&gt;\lfloor p/m \rfloor / (p-1)&lt;/math&gt;

which tends to &lt;math&gt;1/m&lt;/math&gt; for large &lt;math&gt;p&lt;/math&gt; as required. This analysis also shows that &lt;math&gt;b&lt;/math&gt; does not have to be randomised in order to have universality.

Another way to see &lt;math&gt;H&lt;/math&gt; is a universal family is via the notion of [[statistical distance]]. Write the difference &lt;math&gt;h(x) - h(y)&lt;/math&gt; as

: &lt;math&gt;h(x)-h(y) \equiv (a(x-y)~ \bmod~ p) \pmod{m}&lt;/math&gt;.

Since &lt;math&gt;x - y&lt;/math&gt; is nonzero and &lt;math&gt;a&lt;/math&gt; is uniformly distributed in &lt;math&gt;\{1,\dots,p\}&lt;/math&gt;, it follows that &lt;math&gt;a(x-y)&lt;/math&gt; modulo &lt;math&gt;p&lt;/math&gt; is also uniformly distributed in &lt;math&gt;\{1,\dots,p\}&lt;/math&gt;. The distribution of &lt;math&gt;(h(x)-h(y)) ~\bmod~ m&lt;/math&gt; is thus almost uniform, up to a difference in probability of &lt;math&gt;\pm 1/p&lt;/math&gt; between the samples. As a result, the statistical distance to a uniform family is &lt;math&gt;O(m/p)&lt;/math&gt;, which becomes negligible when &lt;math&gt;p \gg m&lt;/math&gt;.

==== Avoiding modular arithmetic ====

The state of the art for hashing integers is the '''multiply-shift''' scheme described by Dietzfelbinger et al. in 1997.&lt;ref name=DHKP97&gt;
{{cite journal
  | last1 = Dietzfelbinger | first1 = Martin
  | last2 = Hagerup | first2 = Torben
  | last3 = Katajainen | first3 = Jyrki
  | last4 = Penttonen | first4 = Martti
  | title = A Reliable Randomized Algorithm for the Closest-Pair Problem
  | journal = Journal of Algorithms
  | volume = 25
  | issue = 1
  | pages = 19–51
  | doi = 10.1006/jagm.1997.0873
  | url = http://www.diku.dk/~jyrki/Paper/CP-11.4.1997.ps
  | accessdate = 10 February 2011
  | format = Postscript
  | year = 1997
}}&lt;/ref&gt; By avoiding modular arithmetic, this method is much easier to implement and also runs significantly faster in practice (usually by at least a factor of four&lt;ref&gt;
{{cite web
   | last = Thorup
   | first = Mikkel | authorlink = Mikkel Thorup
   | title = Text-book algorithms at SODA
   | url = http://mybiasedcoin.blogspot.com/2009/12/text-book-algorithms-at-soda-guest-post.html
}}&lt;/ref&gt;). The scheme assumes the number of bins is a power of two, &lt;math&gt;m=2^M&lt;/math&gt;. Let &lt;math&gt;w&lt;/math&gt; be the number of bits in a machine word. Then the hash functions are parametrised over odd positive integers &lt;math&gt;a &lt; 2^w&lt;/math&gt; (that fit in a word of &lt;math&gt;w&lt;/math&gt; bits). To evaluate &lt;math&gt;h_{a}(x)&lt;/math&gt;, multiply &lt;math&gt;x&lt;/math&gt; by &lt;math&gt;a&lt;/math&gt; modulo &lt;math&gt;2^w&lt;/math&gt; and then keep the high order &lt;math&gt;M&lt;/math&gt; bits as the hash code. In mathematical notation, this is

: &lt;math&gt;h_a(x) = (a\cdot x\,\, \bmod\, 2^w)\,\, \mathrm{div}\,\, 2^{w-M}&lt;/math&gt;

and it can be implemented in [[C (programming language)|C]]-like programming languages by

: &lt;math&gt;h_a(x) = &lt;/math&gt; &lt;code&gt;(unsigned) (a*x) &gt;&gt; (w-M)&lt;/code&gt;

This scheme does ''not'' satisfy the uniform difference property and is only ''&lt;math&gt;2/m&lt;/math&gt;-almost-universal''; for any &lt;math&gt;x\neq y&lt;/math&gt;, &lt;math&gt;\Pr\{h_a(x) = h_a(y)\} \le 2/m&lt;/math&gt;.

To understand the behavior of the hash function, 
notice that, if &lt;math&gt;ax \bmod 2^w&lt;/math&gt; and &lt;math&gt;ay\bmod 2^w&lt;/math&gt; have the same highest-order 'M' bits, then &lt;math&gt;a(x-y) \bmod 2^w&lt;/math&gt; has either all 1's or all 0's as its highest order M bits (depending on whether &lt;math&gt;ax \bmod 2^w&lt;/math&gt; or &lt;math&gt;ay \bmod 2^w&lt;/math&gt; is larger.
Assume that the least significant set bit of &lt;math&gt;x-y&lt;/math&gt; appears on position &lt;math&gt;w-c&lt;/math&gt;. Since &lt;math&gt;a&lt;/math&gt; is a random odd integer and odd integers have inverses in the [[Ring (mathematics)|ring]] &lt;math&gt;Z_{2^w}&lt;/math&gt;, it follows that &lt;math&gt;a(x-y)\bmod 2^w&lt;/math&gt; will be uniformly distributed among &lt;math&gt;w&lt;/math&gt;-bit integers with the least significant set bit on position &lt;math&gt;w-c&lt;/math&gt;.  The probability that these bits are all 0's or all 1's is therefore at most &lt;math&gt;2/2^M=2/m&lt;/math&gt;.
On the other hand, if &lt;math&gt;c &lt; M&lt;/math&gt;, then higher-order M bits of 
&lt;math&gt;a(x-y) \bmod 2^w&lt;/math&gt; contain both 0's and 1's, so 
it is certain that &lt;math&gt;h(x) \ne h(y)&lt;/math&gt;.  Finally, if &lt;math&gt;c=M&lt;/math&gt; then bit &lt;math&gt;w-M&lt;/math&gt; of 
&lt;math&gt;a(x-y) \bmod 2^w&lt;/math&gt; is 1 and &lt;math&gt;h_a(x)=h_a(y)&lt;/math&gt; if and only if bits &lt;math&gt;w-1,\ldots,w-M+1&lt;/math&gt; are also 1, which happens with probability &lt;math&gt;1/2^{M-1}=2/m&lt;/math&gt;.

This analysis is tight, as can be shown with the example &lt;math&gt;x=2^{w-M-2}&lt;/math&gt; and &lt;math&gt;y=3x&lt;/math&gt;.  To obtain a truly 'universal' hash function, one can use the multiply-add-shift scheme

: &lt;math&gt;h_{a,b}(x) = ((ax + b) \bmod 2^w)\, \mathrm{div}\, 2^{w-M}&lt;/math&gt;

which can be implemented in [[C (programming language)|C]]-like programming languages by

: &lt;math&gt;h_{a,b}(x) = &lt;/math&gt; &lt;code&gt;(unsigned) (a*x+b) &gt;&gt; (w-M)&lt;/code&gt;

where &lt;math&gt;a&lt;/math&gt; is a random odd positive integer with &lt;math&gt;a &lt; 2^w&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; is a random non-negative integer with &lt;math&gt;b &lt; 2^{w-M}&lt;/math&gt;.  With these choices of &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt;, &lt;math&gt;\Pr\{h_{a,b}(x) = h_{a,b}(y)\}\le 1/m&lt;/math&gt; for all &lt;math&gt;x\not\equiv y\pmod{2^w}&lt;/math&gt;.&lt;ref name=&quot;w03&quot;&gt;{{cite thesis
  | type = Ph.D.
  | last = Woelfel | first = Philipp
  | title = Über die Komplexität der Multiplikation in eingeschränkten Branchingprogrammmodellen
  | publisher = Universität Dortmund
  | url = http://pages.cpsc.ucalgary.ca/~woelfel/paper/diss/index.html
  | accessdate = 18 September 2012
  | format = PDF
  | year = 2003
}}&lt;/ref&gt; This differs slightly but importantly from the mistranslation in the English paper.&lt;ref name=&quot;w99&quot;&gt;{{cite conference
  | last1 = Woelfel | first1 = Philipp
  | title = Efficient Strongly Universal and Optimally Universal Hashing
  | conference = Mathematical Foundations of Computer Science 1999
  | series = LNCS
  | volume = 1672
  | pages = 262–272
  | doi = 10.1007/3-540-48340-3_24
  | url = http://www.springerlink.com/content/a10p748w7pr48682/
  | accessdate = 17 May 2011
  | format = PDF
  | year = 1999
}}&lt;/ref&gt;

=== Hashing vectors ===

This section is concerned with hashing a fixed-length vector of machine words. Interpret the input as a vector &lt;math&gt;\bar{x} = (x_0, \dots, x_{k-1})&lt;/math&gt; of &lt;math&gt;k&lt;/math&gt; machine words (integers of &lt;math&gt;w&lt;/math&gt; bits each). If &lt;math&gt;H&lt;/math&gt; is a universal family with the uniform difference property, the following family (dating back to Carter and Wegman&lt;ref name=CW77 /&gt;) also has the uniform difference property (and hence is universal):

: &lt;math&gt;h(\bar{x}) = \left( \sum_{i=0}^{k-1} h_i(x_i) \right)\,\bmod~m&lt;/math&gt;, where each &lt;math&gt;h_i\in H&lt;/math&gt; is chosen independently at random.

If &lt;math&gt;m&lt;/math&gt; is a power of two, one may replace summation by exclusive or.&lt;ref name=thorup09&gt;
{{cite conference
   | last = Thorup | first = Mikkel | authorlink = Mikkel Thorup
   | title = String hashing for linear probing
   | booktitle = Proc. 20th ACM-SIAM Symposium on Discrete Algorithms (SODA)
   | pages = 655–664
   | year = 2009
   | url = http://www.siam.org/proceedings/soda/2009/SODA09_072_thorupm.pdf
}}, section 5.3&lt;/ref&gt;

In practice, if double-precision arithmetic is available, this is instantiated with the multiply-shift hash family of.&lt;ref name=DGMP /&gt; Initialize the hash function with a vector &lt;math&gt;\bar{a} = (a_0, \dots, a_{k-1})&lt;/math&gt; of random '''odd''' integers on &lt;math&gt;2w&lt;/math&gt; bits each. Then if the number of bins is &lt;math&gt;m=2^M&lt;/math&gt; for &lt;math&gt;M\le w&lt;/math&gt;:

: &lt;math&gt;h_{\bar{a}}(\bar{x}) =  \left(\big( \sum_{i=0}^{k-1} x_i \cdot a_i \big) ~\bmod ~ 2^{2w} \right) \,\, \mathrm{div}\,\, 2^{2w-M}&lt;/math&gt;.

It is possible to halve the number of multiplications, which roughly translates to a two-fold speed-up in practice.&lt;ref name=thorup09 /&gt; Initialize the hash function with a vector &lt;math&gt;\bar{a} = (a_0, \dots, a_{k-1})&lt;/math&gt; of random '''odd''' integers on &lt;math&gt;2w&lt;/math&gt; bits each. The following hash family is universal:&lt;ref name=black&gt;
{{cite conference
   | last1 = Black | first1 = J.
  | last2 = Halevi | first2 = S.
  | last3 = Krawczyk | first3 = H.
  | last4 = Krovetz | first4 = T.
   | title = UMAC: Fast and Secure Message Authentication
   | booktitle = Advances in Cryptology (CRYPTO '99)
   | year = 1999
   | url = http://www.cs.ucdavis.edu/~rogaway/papers/umac-full.pdf
}}, Equation 1&lt;/ref&gt;
: &lt;math&gt;h_{\bar{a}}(\bar{x}) = \left(\Big( \sum_{i=0}^{\lceil k/2 \rceil} (x_{2i} + a_{2i}) \cdot (x_{2i+1} + a_{2i+1}) \Big) \bmod ~ 2^{2w} \right) \,\, \mathrm{div}\,\, 2^{2w-M}&lt;/math&gt;.

If double-precision operations are not available, one can interpret the input as a vector of half-words (&lt;math&gt;w/2&lt;/math&gt;-bit integers). The algorithm will then use &lt;math&gt;\lceil k/2 \rceil&lt;/math&gt; multiplications, where &lt;math&gt;k&lt;/math&gt; was the number of half-words in the vector. Thus, the algorithm runs at a &quot;rate&quot; of one multiplication per word of input.

The same scheme can also be used for hashing integers, by interpreting their bits as vectors of bytes. In this variant, the vector technique is known as [[tabulation hashing]] and it provides a practical alternative to multiplication-based universal hashing schemes.&lt;ref&gt;{{cite conference
 | last1 = Pătraşcu | first1 = Mihai | author1-link = Mihai Pătraşcu
 | last2 = Thorup | first2 = Mikkel | author2-link = Mikkel Thorup
 | arxiv = 1011.5200
 | title = The power of simple tabulation hashing
 | doi = 10.1145/1993636.1993638
 | pages = 1–10
 | booktitle = Proceedings of the 43rd annual ACM Symposium on Theory of Computing (STOC '11)
 | year = 2011}}&lt;/ref&gt;

Strong universality at high speed is also possible.&lt;ref name=&quot;kaser2013&quot;&gt;{{cite journal
 | last1 = Kaser | first1 = Owen 
 | last2 = Lemire | first2 = Daniel 
 | arxiv =  1202.4961
 | title = Strongly universal string hashing is fast
 | year = 2013
 | journal = Computer Journal
 | url = http://dx.doi.org/10.1093/comjnl/bxt070
 | doi = 10.1093/comjnl/bxt070
 | publisher = Oxford University Press}}&lt;/ref&gt; Initialize the hash function with a vector &lt;math&gt;\bar{a} = (a_0, \dots, a_{k})&lt;/math&gt; of random  integers on &lt;math&gt;2w&lt;/math&gt; bits. Compute

: &lt;math&gt;h_{\bar{a}}(\bar{x})^{\mathrm{strong}} = (a_0 + \sum_{i=0}^{k} a_{i+1} x_{i}   \bmod ~ 2^{2w} ) \div 2^w &lt;/math&gt;.

The result is strongly universal on &lt;math&gt;w&lt;/math&gt; bits. Experimentally, it was found to run at  0.2 CPU cycle per byte on recent Intel processors for&lt;math&gt;w = 32&lt;/math&gt;.

=== Hashing strings ===

This refers to hashing a ''variable-sized'' vector of machine words. If the length of the string can be bounded by a small number, it is best to use the vector solution from above (conceptually padding the vector with zeros up to the upper bound). The space required is the maximal length of the string, but the time to evaluate &lt;math&gt;h(s)&lt;/math&gt; is just the length of &lt;math&gt;s&lt;/math&gt;. As long as zeroes are forbidden in the string, the zero-padding can be ignored when evaluating the hash function without affecting universality&lt;ref name=thorup09 /&gt;). Note that if zeroes are allowed in the string, then it might be best to append a fictitious non-zero (e.g., 1) character to all strings prior to padding: this will ensure that universality is not affected.&lt;ref name=kaser2013 /&gt;

Now assume we want to hash &lt;math&gt;\bar{x} = (x_0,\dots, x_\ell)&lt;/math&gt;, where a good bound on &lt;math&gt;\ell&lt;/math&gt; is not known a priori. A universal family proposed by &lt;ref name=DGMP&gt;
{{cite conference
   | last1 = Dietzfelbinger | first1 = Martin
   | last2 = Gil | first2 = Joseph
   | last3 = Matias | first3 = Yossi
   | last4 = Pippenger | first4 = Nicholas
   | title = Polynomial Hash Functions Are Reliable (Extended Abstract)
   | booktitle = Proc. 19th International Colloquium on Automata, Languages and Programming (ICALP)
   | pages = 235–246
   | year = 1992
}}&lt;/ref&gt; 
treats the string &lt;math&gt;x&lt;/math&gt; as the coefficients of a polynomial modulo a large prime. If &lt;math&gt;x_i \in [u]&lt;/math&gt;, let &lt;math&gt;p \ge \max \{ u, m \}&lt;/math&gt; be a prime and define:

:&lt;math&gt;h_a(\bar{x}) = h_\mathrm{int} \left( \big(\sum_{i=0}^\ell x_i\cdot  a^i \big) \bmod ~p \right)&lt;/math&gt;, where &lt;math&gt;a \in [p]&lt;/math&gt; is uniformly random and &lt;math&gt;h_\mathrm{int}&lt;/math&gt; is chosen randomly from a universal family mapping integer domain &lt;math&gt;[p] \mapsto [m]&lt;/math&gt;.

Using properties of modular arithmetic, above can be computed without producing large numbers for large strings as follows:&lt;ref&gt;{{cite web | url = http://www.cs.huji.ac.il/course/2002/dast/slides/tirgul9.pdf | title = Hebrew University Course Slides}}&lt;/ref&gt;

&lt;source lang=&quot;C&quot;&gt;
int hash(String x, int a, int p)
	int h=x[0]
	for ( i=1 ; i &lt; x.length ; i++)
	h = ((h*a) + x[i])) mod p
	return h
&lt;/source&gt;

Consider two strings &lt;math&gt;\bar{x}, \bar{y}&lt;/math&gt; and let &lt;math&gt;\ell&lt;/math&gt; be length of the longer one; for the analysis, the shorter string is conceptually padded with zeros up to length &lt;math&gt;\ell&lt;/math&gt;. A collision before applying &lt;math&gt;h_\mathrm{int}&lt;/math&gt; implies that &lt;math&gt;a&lt;/math&gt; is a root of the polynomial with coefficients &lt;math&gt;\bar{x} - \bar{y}&lt;/math&gt;. This polynomial has at most &lt;math&gt;\ell&lt;/math&gt; roots modulo &lt;math&gt;p&lt;/math&gt;, so the collision probability is at most &lt;math&gt;\ell/p&lt;/math&gt;. The probability of collision through the random &lt;math&gt;h_\mathrm{int}&lt;/math&gt; brings the total collision probability to &lt;math&gt;\frac{1}{m} + \frac{\ell}{p}&lt;/math&gt;. Thus, if the prime &lt;math&gt;p&lt;/math&gt; is sufficiently large compared to the length of strings hashed, the family is very close to universal (in [[statistical distance]]).

To mitigate the computational penalty of modular arithmetic, two tricks are used in practice:&lt;ref name=thorup09 /&gt;
# One chooses the prime &lt;math&gt;p&lt;/math&gt; to be close to a power of two, such as a [[Mersenne prime]]. This allows arithmetic modulo &lt;math&gt;p&lt;/math&gt; to be implemented without division (using faster operations like addition and shifts). For instance, on modern architectures one can work with &lt;math&gt;p = 2^{61}-1&lt;/math&gt;, while &lt;math&gt;x_i&lt;/math&gt;'s are 32-bit values.
# One can apply vector hashing to blocks. For instance, one applies vector hashing to each 16-word block of the string, and applies string hashing to the &lt;math&gt;\lceil k/16 \rceil&lt;/math&gt; results. Since the slower string hashing is applied on a substantially smaller vector, this will essentially be as fast as vector hashing.

==See also==
* [[K-independent hashing]]
* [[Rolling hashing]]
* [[Tabulation hashing]]
* [[Min-wise independence]]
* [[Universal one-way hash function]]
* [[Low-discrepancy sequence]]
* [[Perfect hashing]]

== References ==
&lt;references /&gt;

== Further reading ==
* {{cite book
   | last = Knuth
   | first = Donald Ervin
   | authorlink = Donald Knuth
   | title = [[The Art of Computer Programming]], Vol. III: Sorting and Searching
   | edition = 3rd
   | year = 1998
   | publisher = Addison-Wesley
   | location = Reading, Mass; London
   | isbn = 0-201-89685-0
  }}

==External links==
* [http://opendatastructures.org/versions/edition-0.1e/ods-java/5_1_ChainedHashTable_Hashin.html#SECTION00811000000000000000 Open Data Structures - Section 5.1.1 - Multiplicative Hashing]

[[Category:Cryptographic hash functions]]
[[Category:Hashing]]
[[Category:Search algorithms]]
[[Category:Computational complexity theory]]</text>
      <sha1>elxlx72v1hxkovkoy9aghhks1e2b8et</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Quadratic probing</title>
    <ns>0</ns>
    <id>1852324</id>
    <revision>
      <id>621052974</id>
      <parentid>613624091</parentid>
      <timestamp>2014-08-13T12:32:33Z</timestamp>
      <contributor>
        <ip>188.123.106.105</ip>
      </contributor>
      <comment>/* Java function for key insertion */ this is C code, not Java</comment>
      <text xml:space="preserve" bytes="9154">{{multiple issues|
{{Original research|article|date=May 2008}}
{{Howto|date=April 2013}}
}}

'''Quadratic probing''' is an open addressing scheme in [[computer programming]] for resolving collisions in [[hash table]]s—when an incoming data's hash value indicates it should be stored in an already-occupied slot or bucket. Quadratic probing operates by taking the original hash index and adding successive values of an arbitrary [[quadratic polynomial]] until an open slot is found.

For a given hash value, the indices generated by [[linear probing]] are as follows:

&lt;math&gt;H + 1 , H + 2 , H + 3 , H + 4 , ... , H + k&lt;/math&gt;

This method results in [[primary clustering]], and as the cluster grows larger, the search for those items hashing within the cluster becomes less efficient.

An example sequence using quadratic probing is:

&lt;math&gt;H + 1^2 , H + 2^2 , H + 3^2 , H + 4^2 , ... , H + k^2&lt;/math&gt;

Quadratic probing can be a more efficient algorithm in a closed hash table, since it better avoids the clustering problem that can occur with linear probing, although it is not immune. It also provides good memory caching because it preserves some [[locality of reference]]; however, linear probing has greater locality and, thus, better cache performance.

Quadratic probing is used in the [[Berkeley Fast File System]] to allocate free blocks. The allocation routine chooses a new cylinder-group when the current is nearly full using quadratic probing, because of the speed it shows in finding unused cylinder-groups.

==Quadratic function==


Let h(k) be a [[hash function]] that maps an element k to an integer in [0,m-1], where m is the size of the table. Let the i&lt;sup&gt;th&lt;/sup&gt; probe position for a value k be given by the function 
:&lt;math&gt;h(k,i) = ( h(k) + c_1 i + c_2 i^2 )  \pmod{m}&lt;/math&gt;
where c&lt;sub&gt;2&lt;/sub&gt; ≠ 0. If c&lt;sub&gt;2&lt;/sub&gt; = 0, then h(k,i) degrades to a [[linear probing|linear probe]].  For a given [[hash table]], the values of c&lt;sub&gt;1&lt;/sub&gt; and c&lt;sub&gt;2&lt;/sub&gt; remain constant.

'''Examples:'''
*If &lt;math&gt;h(k,i) = (h(k) + i + i^2) \pmod{m}&lt;/math&gt;, then the probe sequence will be &lt;math&gt;h(k), h(k)+2, h(k)+6, ...&lt;/math&gt;
*For m = 2&lt;sup&gt;n&lt;/sup&gt;, a good choice for the constants are c&lt;sub&gt;1&lt;/sub&gt; = c&lt;sub&gt;2&lt;/sub&gt; = 1/2, as the values of h(k,i) for i in [0,m-1] are all distinct. This leads to a probe sequence of &lt;math&gt;h(k), h(k)+1, h(k)+3, h(k)+6, ...&lt;/math&gt; where the values increase by 1, 2, 3, ...
*For prime m &gt; 2, most choices of c&lt;sub&gt;1&lt;/sub&gt; and c&lt;sub&gt;2&lt;/sub&gt; will make h(k,i) distinct for i in [0, (m-1)/2].  Such choices include c&lt;sub&gt;1&lt;/sub&gt; = c&lt;sub&gt;2&lt;/sub&gt; = 1/2, c&lt;sub&gt;1&lt;/sub&gt; = c&lt;sub&gt;2&lt;/sub&gt; = 1, and c&lt;sub&gt;1&lt;/sub&gt; = 0, c&lt;sub&gt;2&lt;/sub&gt; = 1.  Because there are only about m/2 distinct probes for a given element, it is difficult to guarantee that insertions will succeed when the load factor is &gt; 1/2.

== Quadratic probing insertion ==

The problem, here, is to insert a key at an available key space in a given Hash Table using quadratic probing.&lt;ref&gt;{{cite book|last=Horowitz, Sahni, Anderson-Freed|title=Fundamentals of Data Structures in C|year=2011|publisher=University Press|isbn=978-81-7371-605-8}}&lt;/ref&gt;

=== Algorithm to insert key in hash table ===

&lt;code&gt;
  1. Get the key k
  2. Set counter j = 0
  3. Compute hash function h[k] = k % SIZE
  4. If hashtable[h[k]] is empty
          (4.1) Insert key k at hashtable[h[k]]
          (4.2) Stop
     Else
         (4.3) The key space at hashtable[h[k]] is occupied, so we need to find the next available key space
         (4.4) Increment j
         (4.5) Compute new hash function h[k] = ( k + j * j ) % SIZE
         (4.6) Repeat Step 4 till j is equal to the SIZE of hash table
  5. The hash table is full
  6. Stop
&lt;/code&gt;

=== C function for key insertion ===
&lt;source lang=&quot;c&quot;&gt;

int quadratic_probing_insert(int *hashtable, int key, int *empty){
    /* hashtable[] is an integer hash table; empty[] is another array which indicates whether the key space is occupied;
       If an empty key space is found, the function returns the index of the bucket where the key is inserted, otherwise it 
       returns (-1) if no empty key space is found */

    int j = 0, hk;
    hk = key  % SIZE;
    while(j &lt; SIZE) {
        if(empty[hk] == 1){
            hashtable[hk] = key;
            empty[hk] = 0;
            return (hk);
        }
        j++;
        hk = (key + j * j) % SIZE;
    }
    return (-1);
}

&lt;/source&gt;

== Quadratic probing search ==

=== Algorithm to search element in hash table ===

&lt;code&gt;
  1. Get the key k to be searched
  2. Set counter j = 0
  3. Compute hash function h[k] = k % SIZE
  4. If the key space at hashtable[h[k]] is occupied
          (4.1) Compare the element at hashtable[h[k]] with the key k.
          (4.2) If they are equal
          (4.2.1) The key is found at the bucket h[k]
          (4.2.2) Stop
     Else
          (4.3) The element might be placed at the next location given by the quadratic function
          (4.4) Increment j
          (4.5) Compute new hash function h[k] = ( k + j * j ) % SIZE
          (4.6) Repeat Step 4 till j is greater than SIZE of hash table
  5. The key was not found in the hash table
  6. Stop
&lt;/code&gt;

=== C function for key searching ===

&lt;source lang=&quot;c&quot;&gt;
int quadratic_probing_search(int *hashtable, int key, int *empty)
{
    /* If the key is found in the hash table, the function returns the index of the hashtable where the key is inserted, otherwise it 
       returns (-1) if the key is not found */ 

    int j = 0, hk;
    hk = key  % SIZE;
    while(j &lt; SIZE) 
    {
        if((empty[hk] == 0) &amp;&amp; (hashtable[hk] == key))
            return (hk);
        j++;
        hk = (key + j * j) % SIZE;
    }
    return (-1);
}
&lt;/source&gt;

==Limitations==

&lt;ref&gt;{{cite book|first=Mark Allen Weiss|title=Data Structures and Algorithm Analysis in C++|year=2009|publisher=Pearson Education|isbn=978-81-317-1474-4}}&lt;/ref&gt; For linear probing it is a bad idea to let the hash table get nearly full, because performance is degraded as the hash table gets filled.
In the case of quadratic probing, the situation is even more drastic. With the exception of the triangular number case for a power-of-two-sized hash table, there is no guarantee of finding an empty cell once the table gets more than half full, or even before the table gets half full if the table size is not prime. This is because at most half of the table can be used as alternative locations to resolve collisions.
If the hash table size is b (a prime greater than 3), it can be proven that the first &lt;math&gt; b / 2 &lt;/math&gt; alternative locations including the initial location h(k) are all distinct and unique.
Suppose, we assume two of the alternative locations to be given by &lt;math&gt; h(k) + x^2 \pmod{b} &lt;/math&gt; and &lt;math&gt; h(k) + y^2 \pmod{b} &lt;/math&gt;, where 0 ≤ x, y ≤ (b / 2).
If these two locations point to the same key space, but x ≠ y. Then the following would have to be true,
    &lt;math&gt; h(k) + x^2 = h(k) + y^2     \pmod{b} &lt;/math&gt;
    &lt;math&gt; x^2 = y^2                   \pmod{b}  &lt;/math&gt;
    &lt;math&gt; x^2 - y^2 = 0               \pmod{b}  &lt;/math&gt;
    &lt;math&gt; (x - y)(x + y) = 0          \pmod{b}   &lt;/math&gt;
As b (table size) is a prime greater than 3, either (x - y) or (x + y) has to be equal to zero.
Since x and y are unique, (x - y) cannot be zero.
Also, since 0 ≤ x, y ≤ (b / 2), (x + y) cannot be zero.

Thus, by contradiction, it can be said that the first (b / 2) alternative locations after h(k) are unique.
So an empty key space can always be found as long as at most (b / 2) locations are filled, i.e., the hash table is not more than half full.

=== Alternating sign ===
If the sign of the offset is alternated (e.g. +1, -4, +9, -16 etc.), and if the number of buckets is a prime number p congruent to 3 modulo 4 (i.e. one of 3, 7, 11, 19, 23, 31 and so on), then the first p offsets will be unique modulo p.

In other words, a permutation of 0 through p-1 is obtained, and, consequently, a free bucket will always be found as long as there exists at least one.

The insertion algorithm only receives a minor modification (but do note that SIZE has to be a suitable prime number as explained above):
&lt;code&gt;
 1. Get the key k
 2. Set counter j = 0
 3. Compute hash function h[k] = k % SIZE
 4. If hashtable[h[k]] is empty
         (4.1) Insert key k at hashtable[h[k]]
         (4.2) Stop
    Else
        (4.3) The key space at hashtable[h[k]] is occupied, so we need to find the next available key space
        (4.4) Increment j
        (4.5) Compute new hash function h[k]. If j is odd, then
              h[k] = ( k + j * j ) % SIZE, else h[k] = ( k - j * j ) % SIZE
        (4.6) Repeat Step 4 till j is equal to the SIZE of hash table
 5. The hash table is full
 6. Stop
&lt;/code&gt;

The search algorithm is modified likewise.

==See also==
* [[Hash tables]]
* [[Hash collision]]
* [[Double hashing]]
* [[Linear probing]]
* [[Hash function]]

==References==
{{Reflist}}

==External links==
*[http://research.cs.vt.edu/AVresearch/hashing/quadratic.php Tutorial/quadratic probing]

[[Category:Hashing]]
[[Category:Articles with example C code]]
[[Category:Articles with example Java code]]</text>
      <sha1>dwuaohdnsgs6zw7a2k3brtt1r9sgh1i</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tabulation hashing</title>
    <ns>0</ns>
    <id>33467978</id>
    <revision>
      <id>611583874</id>
      <parentid>593746788</parentid>
      <timestamp>2014-06-04T21:06:37Z</timestamp>
      <contributor>
        <username>Thomasda</username>
        <id>2387809</id>
      </contributor>
      <comment>The table is two dimensional. This is very important.</comment>
      <text xml:space="preserve" bytes="11498">In [[computer science]], '''tabulation hashing''' is a method for constructing [[Universal hashing|universal families of hash functions]] by combining [[lookup table|table lookup]] with [[exclusive or]] operations. It is simple and fast enough to be usable in practice, and has theoretical properties that (in contrast to some other universal hashing methods) make it usable with [[linear probing]], [[cuckoo hashing]], and the [[MinHash]] technique for estimating the size of set intersections. The first instance of tabulation hashing is [[Zobrist hashing]] (1969). It was later rediscovered by {{harvtxt|Carter|Wegman|1979}} and studied in more detail by {{harvtxt|Pătraşcu|Thorup|2011}}.

==Method==
Let ''p'' denote the number of [[bit]]s in a key to be hashed, and ''q'' denote the number of bits desired in an output hash function. Let ''r'' be a number smaller than ''p'', and let ''t'' be the smallest integer that is at least as large as ''p''/''r''.  For instance, if ''r''&amp;nbsp;=&amp;nbsp;8, then an ''r''-bit number is a [[byte]], and ''t'' is the number of bytes per key.

The key idea of tabulation hashing is to view a key as a [[Row vector|vector]] of ''t'' ''r''-bit numbers, use a [[lookup table]] filled with random values to compute a hash value for each of the ''r''-bit numbers representing a given key, and combine these values with the bitwise binary [[exclusive or]] operation. The choice of ''r'' should be made in such a way that this table is not too large; e.g., so that it fits into the computer's [[cache memory]].

The initialization phase of the algorithm creates a two-dimensional array ''T'' of dimensions 2&lt;sup&gt;''r''&lt;/sup&gt; by ''q'', and fills the array with random numbers. Once the array ''T'' is initialized, it can be used to compute the hash value ''h''(''x'') of any given key ''x''. To do so, partition ''x'' into ''r''-bit values, where ''x''&lt;sub&gt;0&lt;/sub&gt; consists of the low order ''r'' bits of ''x'', ''x''&lt;sub&gt;1&lt;/sub&gt; consists of the next ''r'' bits, etc. (E.g., again, with ''r''&amp;nbsp;=&amp;nbsp;8, ''x''&lt;sub&gt;''i''&lt;/sub&gt; is just the ''i''th byte of ''x'').
Then, use these values as indices into ''T'' and combine them with the exclusive or operation:
:''h''(''x'') = ''T''[0][''x''&lt;sub&gt;0&lt;/sub&gt;] &amp;oplus; ''T''[1][''x''&lt;sub&gt;1&lt;/sub&gt;] &amp;oplus; ''T''[2][''x''&lt;sub&gt;2&lt;/sub&gt;] &amp;oplus; ...

==Universality==
{{harvtxt|Carter|Wegman|1979}} define a randomized scheme for generating hash functions to be [[universal hashing|universal]] if, for any two keys, the probability that they [[Collision (computer science)|collide]] (that is, they are mapped to the same value as each other) is 1/''m'', where ''m'' is the number of values that the keys can take on. They defined a stronger property in the subsequent paper {{harvtxt|Wegman|Carter|1981}}: a randomized scheme for generating hash functions is [[K-independent hashing|''k''-independent'']] if, for every ''k''-tuple of keys, and each possible ''k''-tuple of values, the probability that those keys are mapped to those values is 1/''m''&lt;sup&gt;''k''&lt;/sup&gt;. 2-independent hashing schemes are automatically universal, and any universal hashing scheme can be converted into a 2-independent scheme by storing a random number ''x'' in the initialization phase of the algorithm and adding ''x'' to each hash value, so universality is essentially the same as 2-independence, but ''k''-independence for larger values of ''k'' is a stronger property, held by fewer hashing algorithms.

As {{harvtxt|Pătraşcu|Thorup|2011}} observe, tabulation hashing is 3-independent but not 4-independent.  For any single key ''x'', ''T''[''x''&lt;sub&gt;0&lt;/sub&gt;,0] is equally likely to take on any hash value, and the exclusive or of ''T''[''x''&lt;sub&gt;0&lt;/sub&gt;,0] with the remaining table values does not change this property. For any two keys ''x'' and ''y'', ''x'' is equally likely to be mapped to any hash value as before, and there is at least one position ''i'' where ''x&lt;sub&gt;i&lt;/sub&gt;''&amp;nbsp;≠&amp;nbsp;''y&lt;sub&gt;i&lt;/sub&gt;''; the table value ''T''[''y''&lt;sub&gt;''i''&lt;/sub&gt;,''i''] is used in the calculation of ''h''(''y'') but not in the calculation of ''h''(''x''), so even after the value of ''h''(''x'') has been determined, ''h''(''y'') is equally likely to be any valid hash value. Similarly, for any three keys ''x'', ''y'', and ''z'', at least one of the three keys has a position ''i'' where its value ''z''&lt;sub&gt;''i''&lt;/sub&gt; differs from the other two, so that even after the values of ''h''(''x'') and ''h''(''y'') are determined, ''h''(''z'') is equally likely to be any valid hash value.

However, this reasoning breaks down for four keys because there are sets of keys ''w'', ''x'', ''y'', and ''z'' where none of the four has a byte value that it does not share with at least one of the other keys. For instance, if the keys have two bytes each, and ''w'', ''x'', ''y'', and ''z'' are the four keys that have either zero or one as their byte values, then each byte value in each position is shared by exactly two of the four keys. For these four keys, the hash values computed by tabulation hashing will always satisfy the equation {{nowrap|1=''h''(''w'') &amp;oplus; ''h''(''x'') &amp;oplus; ''h''(''y'') &amp;oplus; ''h''(''z'') = 0}}, whereas for a 4-independent hashing scheme the same equation would only be satisfied with probability 1/''m''. Therefore, tabulation hashing is not 4-independent.

{{harvtxt|Siegel|2004}} uses the same idea of using exclusive or operations to combine random values from a table, with a more complicated algorithm based on [[expander graph]]s for transforming the key bits into table indices, to define hashing schemes that are ''k''-independent for any constant or even logarithmic value of ''k''. However, the number of table lookups needed to compute each hash value using Siegel's variation of tabulation hashing, while constant, is still too large to be practical, and the use of expanders in Siegel's technique also makes it not fully constructive.

One limitation of tabulation hashing is that it assumes that the input keys have a fixed number of bits. {{harvtxt|Lemire|2012}} has studied variations of tabulation hashing that can be applied to variable-length strings, and shown that they can be universal (2-independent) but not 3-independent.

==Application==
Because tabulation hashing is a universal hashing scheme, it can be used in any hashing-based algorithm in which universality is sufficient. For instance, in [[Hash table#Separate chaining|hash chaining]], the expected time per operation is proportional to the sum of collision probabilities, which is the same for any universal scheme as it would be for truly random hash functions, and is constant whenever the load factor of the hash table is constant. Therefore, tabulation hashing can be used to compute hash functions for hash chaining with a theoretical guarantee of constant expected time per operation.&lt;ref name=&quot;cw79&quot;&gt;{{harvtxt|Carter|Wegman|1979}}.&lt;/ref&gt;

However, universal hashing is not strong enough to guarantee the performance of some other hashing algorithms. For instance, for [[linear probing]], 5-independent hash functions are strong enough to guarantee constant time operation, but there are 4-independent hash functions that fail.&lt;ref&gt;For the sufficiency of 5-independent hashing for linear probing, see {{harvtxt|Pagh|Pagh|Ružić|2009}}. For examples of weaker hashing schemes that fail, see {{harvtxt|Pătraşcu|Thorup|2010}}.&lt;/ref&gt; Nevertheless, despite only being 3-independent, tabulation hashing provides the same constant-time guarantee for linear probing.&lt;ref name=&quot;pt11&quot;&gt;{{harvtxt|Pătraşcu|Thorup|2011}}.&lt;/ref&gt;

[[Cuckoo hashing]], another technique for implementing [[hash table]]s, guarantees constant time per lookup (regardless of the hash function). Insertions into a cuckoo hash table may fail, causing the entire table to be rebuilt, but such failures are sufficiently unlikely that the expected time per insertion (using either a truly random hash function or a hash function with logarithmic independence) is constant. With tabulation hashing, on the other hand, the best bound known on the failure probability is higher, high enough that insertions cannot be guaranteed to take constant expected time. Nevertheless, tabulation hashing is adequate to ensure the linear-expected-time construction of a cuckoo hash table for a static set of keys that does not change as the table is used.&lt;ref name=&quot;pt11&quot;/&gt;

Algorithms such as [[Karp-Rabin]] requires the efficient computation of hashing all consecutive sequences of &lt;math&gt;k&lt;/math&gt; characters. We typically use  [[rolling hash]] functions for these problems. Tabulation hashing is used to construct families of strongly [[universal hashing|universal  functions]] (for example, [[rolling hash#Cyclic_polynomial|hashing by cyclic polynomials]]).

==Notes==
{{reflist}}

==References==
*{{citation
 | last1 = Carter | first1 = J. Lawrence
 | last2 = Wegman | first2 = Mark N. | author2-link = Mark N. Wegman
 | doi = 10.1016/0022-0000(79)90044-8
 | issue = 2
 | journal = Journal of Computer and System Sciences
 | mr = 532173
 | pages = 143–154
 | title = Universal classes of hash functions
 | volume = 18
 | year = 1979}}.
*{{citation
 | last = Lemire | first = Daniel
 | arxiv = 1008.1715
 | journal = Discrete Applied Mathematics
 | volume = 160
 | pages = 604–617
 | doi = 10.1016/j.dam.2011.11.009
 | title = The universality of iterated hashing over variable-length strings
 | year = 2012}}.
*{{citation
 | last1 = Pagh | first1 = Anna
 | last2 = Pagh | first2 = Rasmus
 | last3 = Ružić | first3 = Milan
 | doi = 10.1137/070702278
 | issue = 3
 | journal = SIAM Journal on Computing
 | mr = 2538852
 | pages = 1107–1120
 | title = Linear probing with constant independence
 | volume = 39
 | year = 2009}}.
*{{citation
 | last1 = Pătraşcu | first1 = Mihai | author1-link = Mihai Pătraşcu
 | last2 = Thorup | first2 = Mikkel | author2-link = Mikkel Thorup
 | contribution = On the k-independence required by linear probing and minwise independence
 | doi = 10.1007/978-3-642-14165-2_60
 | pages = 715–726
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Automata, Languages and Programming, 37th International Colloquium, ICALP 2010, Bordeaux, France, July 6-10, 2010, Proceedings, Part I
 | url = http://people.csail.mit.edu/mip/papers/kwise-lb/kwise-lb.pdf
 | volume = 6198
 | year = 2010}}.
*{{citation
 | last1 = Pătraşcu | first1 = Mihai | author1-link = Mihai Pătraşcu
 | last2 = Thorup | first2 = Mikkel | author2-link = Mikkel Thorup
 | arxiv = 1011.5200
 | contribution = The power of simple tabulation hashing
 | doi = 10.1145/1993636.1993638
 | pages = 1–10
 | title = Proceedings of the 43rd annual ACM Symposium on Theory of Computing (STOC '11)
 | year = 2011}}.
*{{citation
 | last = Siegel | first = Alan
 | doi = 10.1137/S0097539701386216
 | issue = 3
 | journal = SIAM Journal on Computing
 | mr = 2066640
 | pages = 505–543 (electronic)
 | title = On universal classes of extremely random constant-time hash functions
 | volume = 33
 | year = 2004}}.
*{{citation
 | last1 = Wegman | first1 = Mark N. | author1-link = Mark N. Wegman
 | last2 = Carter | first2 = J. Lawrence
 | doi = 10.1016/0022-0000(81)90033-7
 | issue = 3
 | journal = Journal of Computer and System Sciences
 | mr = 633535
 | pages = 265–279
 | title = New hash functions and their use in authentication and set equality
 | volume = 22
 | year = 1981}}.

[[Category:Hashing]]
[[Category:Hash functions]]</text>
      <sha1>4seikq7vtwactc46gkn40ihcwqh6xri</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Symmetric Hash Join</title>
    <ns>0</ns>
    <id>35712115</id>
    <revision>
      <id>535029249</id>
      <parentid>494057362</parentid>
      <timestamp>2013-01-26T19:24:52Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>/* See also */fixed header names + [[WP:GENFIXES|general fixes]] using [[Project:AWB|AWB]] (8863)</comment>
      <text xml:space="preserve" bytes="758">{{Cleanup|reason=More information needed|date=May 2012}}
'''Symmetric Hash Join''' is a special type of [[hash join]] designed for [[data stream]]s.&lt;ref&gt;[http://www.inf.ed.ac.uk/teaching/courses/exc/reading/p5-golab.pdf Issues in Data Stream Management]&lt;/ref&gt;&lt;ref&gt;[http://www.cs.uwaterloo.ca/~david/cs448/A2.pdf University of Waterloo - Database Systems Implementation]&lt;/ref&gt;

==Algorithm==
* For each input create a hash table
* For each new record hash and insert into inputs hash table
** Test if input is equal to a pre defined set of other inputs
*** If so then output the records

==References==
&lt;references /&gt;

==See also==
* [[Hash Join]]
* [[Data-stream management system]]
* [[Data stream mining]]

[[Category:Hashing]]
[[Category:Join algorithms]]</text>
      <sha1>faz3ue1vvadbjwsccfziv2ddwgif1sc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Feature hashing</title>
    <ns>0</ns>
    <id>36126852</id>
    <revision>
      <id>625121210</id>
      <parentid>625121166</parentid>
      <timestamp>2014-09-11T17:59:53Z</timestamp>
      <contributor>
        <ip>97.94.70.103</ip>
      </contributor>
      <text xml:space="preserve" bytes="10346">In [[machine learning]], '''feature hashing''', also known as the '''hashing trick'''&lt;ref name=&quot;Weinberger&quot;/&gt; (by analogy to the [[kernel trick]]), is a fast and space-efficient way of vectorizing [[Feature (machine learning)|features]], i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a [[hash function]] to the features and using their hash values as indices directly, rather than looking the indices up in an [[associative array]].

==Motivating example==
In a typical [[document classification]] task, the input to the machine learning algorithm (both during learning and classification) is free text. From this, a [[bag of words]] (BOW) representation is constructed: the individual [[Type–token distinction|tokens]] are extracted and counted, and each distinct token in the training set defines a [[Feature (machine learning)|feature]] (independent variable) of each of the documents in both the training and test sets.

Machine learning algorithms, however, are typically defined in terms of numerical vectors. Therefore, the bags of words for a set of documents is regarded as a [[term-document matrix]] where each column is a single document, and each row is a single feature/word; the entry {{math|''i'', ''j''}} in such a matrix captures the frequency (or weight) of the {{mvar|j}}'th term of the ''vocabulary'' in document {{mvar|i}}. (An alternative convention swaps the rows and columns of the matrix, but this difference is immaterial.)
Typically, these vectors are extremely [[sparse matrix|sparse]].

The common approach is to construct, at learning time or prior to that, a ''dictionary'' representation of the vocabulary of the training set, and use that to map words to indices. [[Hash table]]s and [[trie]]s are common candidates for dictionary implementation. E.g., the three documents

* ''John likes to watch movies. ''
* ''Mary likes movies too.''
* ''John also likes football.''

can be converted, using the dictionary

{| class=&quot;wikitable&quot;
|-
! Term !! Index
|-
| John || 1
|-
| likes || 2
|-
| to || 3
|-
| watch || 4
|-
| movies || 5
|-
| Mary || 6
|-
| too || 7
|-
| also || 8
|-
| football || 9
|}

to the [[Transpose|transposed]] term-document matrix

:&lt;math&gt;
\begin{pmatrix}
\textrm{John} &amp; \textrm{likes} &amp; \textrm{to} &amp; \textrm{watch} &amp; \textrm{movies} &amp; \textrm{Mary} &amp; \textrm{too} &amp; \textrm{also} &amp; \textrm{football} \\
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1
\end{pmatrix}
&lt;/math&gt;

(Punctuation was removed, as is usual in document classification and clustering.)

The problem with this process is that such dictionaries take up a large amount of storage space, and [[Heaps' law|grow in size as the training set grows]].&lt;ref name=&quot;mobilenlp&quot;&gt;{{cite conference |author1=K. Ganchev |author2=M. Dredze |year=2008 |url=http://www.cs.jhu.edu/~mdredze/publications/mobile_nlp_feature_mixing.pdf |title=Small statistical models by random feature mixing |conference=Proc. ACL08 HLT Workshop on Mobile Language Processing}}&lt;/ref&gt;
Moreover, when the vocabulary is kept fixed, an adversary may try to invent new words or misspellings that are not in the stored vocabulary so as to circumvent a machine learned filter; this is why feature hashing has been tried for [[spam filtering]] at [[Yahoo! Research]].&lt;ref&gt;{{cite journal |author1=Josh Attenberg |author2=Kilian Weinberger |author3=Alex Smola |author4=Anirban Dasgupta |author5=Martin Zinkevich |title=Collaborative spam filtering with the hashing trick |journal=Virus Bulletin |year=2009}}&lt;/ref&gt;

Note that the hashing trick isn't limited to text classification and similar tasks at the document level, but can be applied to any problem that involves large (perhaps unbounded) amounts of features.

==Feature vectorization using the hashing trick==
Instead of maintaining a dictionary, a feature vectorizer that uses the hashing trick can build a vector of a pre-defined length by applying a hash function {{mvar|h}} to the features (e.g., words) in the items under consideration, then using the hash values directly as feature indices and updating the resulting vector at those indices:
&lt;source lang=&quot;pascal&quot;&gt;
 function hashing_vectorizer(features : array of string, N : integer):
     x := new vector[N]
     for f in features:
         h := hash(f)
         x[h mod N] += 1
     return x
&lt;/source&gt;
It has been suggested that a second, single-bit output hash function {{mvar|ξ}} be used to determine the sign of the update value, to counter the effect of [[Collision (computer science)|hash collision]]s.&lt;ref name=&quot;Weinberger&quot;&gt;{{cite conference |author1=Kilian Weinberger |author2=Anirban Dasgupta |author3=John Langford |author4=Alex Smola |author5=Josh Attenberg |year=2009 |url=http://alex.smola.org/papers/2009/Weinbergeretal09.pdf |title=Feature Hashing for Large Scale Multitask Learning |conference=Proc. ICML}}&lt;/ref&gt; If such a hash function is used, the algorithm becomes
&lt;source lang=&quot;pascal&quot;&gt;
 function hashing_vectorizer(features : array of string, N : integer):
     x := new vector[N]
     for f in features:
         h := hash(f)
         idx := h mod N
         if ξ(f) == 1:
             x[idx] += 1
         else:
             x[idx] -= 1
     return x
&lt;/source&gt;

The above pseudocode actually converts each sample into a vector. An optimized version would instead only generate a stream of ({{mvar|h}},{{mvar|ξ}}) pairs and let the learning and prediction algorithms consume such streams; a [[linear model]] can then be implemented as a single hash table representing the coefficient vector.

===Properties===
{| class=&quot;wikitable&quot; style=&quot;float: right; margin-left: 1.5em; margin-right: 0; margin-top: 0;&quot;
|-
! ''ξ''(''f''₁) !! ''ξ''(''f''₂) !! Final value, ''ξ''(''f''₁) + ''ξ''(''f''₂)
|-
| -1 || -1 || -2
|-
| -1 || 1 || 0
|-
| 1 || -1 || 0
|-
| 1 || 1 || 2
|}

When a second hash function ''ξ'' is used to determine the sign of a feature's value, the [[Expected value|expected]] [[mean]] of each column in the output array becomes zero because ''ξ'' causes some collisions to cancel out.&lt;ref name=&quot;Weinberger&quot;/&gt; E.g., suppose an input contains two symbolic features ''f''₁ and ''f''₂ that collide with each other, but not with any other features in the same input; then there are four possibilities which, if we make no assumptions about ''ξ'', have equal probability, as listed in the table on the right.

In this example, there is a 50% probability that the hash collision cancels out. Multiple hash functions can be used to further reduce the risk of collisions.&lt;ref name=&quot;mahout&quot;&gt;{{cite book
|last1 = Owen
|first1 = Sean
|last2 = Anil
|first2 = Robin
|last3 = Dunning
|first3 = Ted
|last4 = Friedman
|first4 = Ellen
|title=Mahout in Action
|pages=261–265
|publisher = Manning
|year = 2012
}}&lt;/ref&gt;

Furthermore, if ''φ'' is the transformation implemented by a hashing trick with a sign hash ''ξ'' (i.e. ''φ''(''x'') is the feature vector produced for a sample ''x''), then [[inner product]]s in the hashed space are unbiased:

:&lt;math&gt; \mathbb{E}[\langle \varphi(x), \varphi(x') \rangle] = \langle x, x' \rangle&lt;/math&gt;

where the expectation is taken over the hashing function ''φ''.&lt;ref name=&quot;Weinberger&quot;/&gt; It can be verified that&lt;math&gt;\langle \varphi(x), \varphi(x') \rangle&lt;/math&gt; is a [[Positive-definite matrix|positive semi-definite]] [[Kernel trick|kernel]].&lt;ref name=&quot;Weinberger&quot;/&gt;&lt;ref&gt;{{cite conference|last=Shi|first=Q.|author2=Petterson J. |author3=Dror G. |author4=Langford J. |author5=Smola A. |author6=Strehl A. |author7=Vishwanathan V. |title=Hash Kernels|conference=AISTATS|year=2009}}&lt;/ref&gt;

===Extensions and variations===
Recent work extends the hashing trick to supervised mappings from words to indices,&lt;ref&gt;{{cite conference|last=Bai|first=B.|author2=Weston J. |author3=Grangier D. |author4=Collobert R. |author5=Sadamasa K. |author6=Qi Y. |author7=Chapelle O. |author8=Weinberger K. |title=Supervised semantic indexing|conference=CIKM|year=2009|pages=187–196|url=http://www.cse.wustl.edu/~kilian/papers/ssi-cikm.pdf}}&lt;/ref&gt;
which are explicitly learned to avoid collisions of important terms.

===Applications and practical performance===
Ganchev and Dredze showed that in text classification applications with random hash functions and several tens of thousands of columns in the output vectors, feature hashing need not have an adverse effect on classification performance, even without the signed hash function.&lt;ref name=&quot;mobilenlp&quot;/&gt;
Weinberger et al. applied their variant of hashing to the problem of [[spam filter]]ing, formulating this as a [[multi-task learning]] problem where the input features are pairs (user, feature) so that a single parameter vector captured per-user spam filters as well as a global filter for several hundred thousand users, and found that the accuracy of the filter went up.&lt;ref name=&quot;Weinberger&quot;/&gt;

==Implementations==
Implementations of the hashing trick are present in:

* [[Apache Mahout]]&lt;ref name=&quot;mahout&quot; /&gt;
* [[Gensim]]&lt;ref&gt;{{cite web|url=http://radimrehurek.com/gensim/corpora/hashdictionary.html |title=gensim: corpora.hashdictionary – Construct word&lt;-&gt;id mappings |publisher=Radimrehurek.com |accessdate=2014-02-13}}&lt;/ref&gt;
* [[scikit-learn]]&lt;ref&gt;{{cite web|url=http://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing |title=4.1. Feature extraction — scikit-learn 0.14 documentation |publisher=Scikit-learn.org |accessdate=2014-02-13}}&lt;/ref&gt;
* sofia-ml&lt;ref&gt;{{cite web|url=https://code.google.com/p/sofia-ml/ |title=sofia-ml - Suite of Fast Incremental Algorithms for Machine Learning. Includes methods for learning classification and ranking models, using Pegasos SVM, SGD-SVM, ROMMA, Passive-Aggressive Perceptron, Perceptron with Margins, and Logistic Regression |publisher=Code.google.com |accessdate=2014-02-13}}&lt;/ref&gt;
* [[Vowpal Wabbit]]

==See also==
* [[Bloom filter]]
* [[Count–min sketch]]
* [[Locality-sensitive hashing]]
* [[MinHash]]

==References==
{{Reflist|30em}}

==External links==
* [http://hunch.net/~jl/projects/hash_reps/index.html Hashing Representations for Machine Learning] on John Langford's website
* [http://metaoptimize.com/qa/questions/6943/what-is-the-hashing-trick What is the &quot;hashing trick&quot;? - MetaOptimize Q+A]

[[Category:Hashing]]
[[Category:Machine learning]]</text>
      <sha1>d74v1dlc7bkmo7z7cbgiwu9jk0wnokt</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Quotient filter</title>
    <ns>0</ns>
    <id>36476171</id>
    <revision>
      <id>618193020</id>
      <parentid>605195136</parentid>
      <timestamp>2014-07-23T23:47:47Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* Application */Task 5: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <text xml:space="preserve" bytes="17461">{{Probabilistic}}

A '''quotient filter''', introduced by Bender, Farach-Colton, Johnson, Kuszmaul, Medjedovic, Montes, Shetty, Spillane, and Zadok in 2011,&lt;ref name=&quot;Bender&quot;&gt;{{cite conference|last1=Bender|first1=Michael A.|last2=Farach-Colton|first2=Martin|last3=Johnson|first3=Rob|last4=Kuszmaul|first4=Bradley C.|last5=Medjedovic|first5=Dzejla|last6=Montes|first6=Pablo|last7=Shetty|first7=Pradeep|last8=Spillane|first8=Richard P.|last9=Zadok|first9=Erez|displayauthors=9|title=Don't thrash: how to cache your hash on flash|booktitle=Proceedings of the 3rd USENIX conference on Hot topics in storage and file systems (HotStorage'11)|date=June 2011|url=http://static.usenix.org/events/hotstorage11/tech/final_files/Bender.pdf|accessdate=21 July 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last1=Bender|first1=Michael A.|last2=Farach-Colton|first2=Martin|last3=Johnson|first3=Rob|last4=Kraner|first4=Russell|last5=Kuszmaul|first5=Bradley C.|last6=Medjedovic|first6=Dzejla|last7=Montes|first7=Pablo|last8=Shetty|first8=Pradeep|last9=Spillane|first9=Richard P.|last10=Zadok|first10=Erez|title=Don't thrash: how to cache your hash on flash|journal=The Proceedings of the VLDB Endowment (PVLDB)|volume=5|number=11|year=2012|pages=1627–1637|url=http://vldb.org/pvldb/vol5/p1627_michaelabender_vldb2012.pdf}}&lt;/ref&gt; is a kind of [[approximate membership query]] (AMQ).
An AMQ is a space-efficient [[probabilistic]] [[data structure]] used to test whether an [[element (mathematics)|element]] is a member of a [[set (computer science)|set]]. A query will elicit a reply specifying either that the element is definitely not in the set or that the element is probably in the set. The former result is definitive; ''i.e.'', the test does not generate [[Type I and type II errors|false negative]]s.  But with the latter result there is some probability, ε, of the test returning &quot;element is in the set&quot; when in fact the element is not present in the set (''i.e.'', a [[Type I and type II errors|false positive]]). There is a tradeoff between ε, the false positive rate, and storage size; increasing the filter's storage size reduces ε.  Other AMQ operations include &quot;insert&quot; and &quot;optionally delete&quot;. The more elements that are added to the set, the larger the probability of false positives.

[[File:Bloom filter speed.svg|thumb|360px| An approximate member query (AMQ) filter used to speed up answers in a key-value storage system. Key-value pairs are stored on a disk which has slow access times.  AMQ filter decisions are much faster. However some unnecessary disk accesses are made when the filter reports a positive (in order to weed out the false positives). Overall answer speed is better with the AMQ filter than without it. Use of an AMQ filter for this purpose, however, does increase memory usage.]]

Each AMQ is associated with a more space-consuming set, such as a [[B-tree]], and its contents are reflective of the associated set. As elements&amp;nbsp;– key/value pairs&amp;nbsp;– are added to the set, their keys are also added to the AMQ.  However the AMQ stores only a few bits per key, whereas the set stores the entire key, which can be of arbitrary size; therefore, an AMQ can often be memory-resident while the associated set is stored in slower secondary storage. Thus this association can dramatically improve the performance of membership tests, because a test that results in &quot;absent&quot; can be resolved by the AMQ without necessitating any I/Os to access the set itself.

A quotient filter has the usual AMQ operations of insert and query.  In addition it can also be merged and re-sized without having to re-hash the original keys (thereby avoiding the need to access those keys from secondary storage).  This property benefits certain kinds of [[log-structured merge-tree]]s.

==Algorithm description==
The quotient filter is a ''compact'' [[hash table]].  Cleary&lt;ref name=Cleary&gt;{{cite journal|last=Cleary|first=J.G.|title=Compact hash tables using bidirectional linear probing|journal=IEEE T. Comput.|year=1984|volume=33|issue=9|pages=828–834|doi=10.1109/TC.1984.1676499}}&lt;/ref&gt; defines a compact hash table as one in which the table entries contain only a portion of the key plus some additional meta-data bits.  These bits are used to deal with the case when distinct keys happen to hash to the same table entry. By way of contrast, other types of hash tables that deal with such collisions by linking to overflow areas are not compact because the overhead due to linkage can exceed the storage used to store the key.&lt;ref name=&quot;Cleary&quot;/&gt; In a quotient filter a [[hash function]] generates a ''p''-bit fingerprint. The ''r'' least significant bits is called the remainder while the ''q'' = ''p'' - ''r'' most significant bits is called the quotient, hence the name ''quotienting'' (coined by [[Donald Knuth|Knuth]].&lt;ref&gt;{{cite book|last=Knuth|first=Donald|title=The Art of Computer Programming:Searching and Sorting, volume 3|year=1973|publisher=Addison Wesley|location=Section 6.4, exercise 13}}&lt;/ref&gt;)
The hash table has 2&lt;sup&gt;q&lt;/sup&gt; slots.

For some key ''d'' which hashes to the fingerprint ''d&lt;sub&gt;H&lt;/sub&gt;'', let its quotient be ''d&lt;sub&gt;Q&lt;/sub&gt;'' and the remainder be ''d&lt;sub&gt;R&lt;/sub&gt;''.
QF will try to store the remainder in slot d&lt;sub&gt;Q&lt;/sub&gt;, which is known as the ''canonical slot''.
However the canonical slot might already be occupied because multiple keys can hash to the same fingerprint—a ''hard collision''—or because even when the keys' fingerprints are distinct they can have the same quotient—a ''soft collision''. If the canonical slot is occupied then the remainder is stored in some slot to the right.

As described below, the insertion algorithm ensures that all fingerprints having the same quotient are stored in contiguous slots. Such a set of fingerprints is defined as a ''run''.&lt;ref name=&quot;Bender&quot;/&gt;  Note that a run's first fingerprint might not occupy its canonical slot if the run has been forced right by some run to the left.

However a run whose first fingerprint occupies its canonical slot indicates the start of a ''cluster''.&lt;ref name=&quot;Bender&quot;/&gt; The initial run and all subsequent runs comprise the cluster, which terminates at an unoccupied slot or the start of another cluster.

The three additional bits are used to reconstruct a slot's fingerprint.  They have the following function:
* '''is_occupied''' is set when a slot is the canonical slot for some key stored (somewhere) in the ﬁlter (but not necessarily in this slot).
* '''is_continuation''' is set when a slot is occupied but not by the first remainder in a run.
* '''is_shifted''' is set when the remainder in a slot is not in its canonical slot.

The various combinations have the following meaning:
 is_occupied
   is_continuation
     is_shifted
 0 0 0 : Empty Slot
 0 0 1 : Slot is holding start of run that has been shifted from its canonical slot.
 0 1 0 : not used.
 0 1 1 : Slot is holding continuation of run that has been shifted from its canonical slot.
 1 0 0 : Slot is holding start of run that is in its canonical slot.
 1 0 1 : Slot is holding start of run that has been shifted from its canonical slot.&lt;br&gt;        Also the run for which this is the canonical slot exists but is shifted right.
 1 1 0 : not used.
 1 1 1 : Slot is holding continuation of run that has been shifted from its canonical slot.&lt;br&gt;        Also the run for which this is the canonical slot exists but is shifted right.

===Lookup===
We can test if a quotient filter contains some key, d, as follows.&lt;ref name=&quot;Bender&quot;/&gt;

We hash the key to produce its fingerprint, d&lt;sub&gt;H&lt;/sub&gt;, which we then partition into its high-order q bits, d&lt;sub&gt;Q&lt;/sub&gt;, which comprise its quotient, and its low-order r bits, d&lt;sub&gt;R&lt;/sub&gt;, which comprise its remainder.  Slot d&lt;sub&gt;Q&lt;/sub&gt; is the key's canonical slot. That slot is empty if its three meta-data bits are false.  In that case the filter does not contain the key.

If the canonical slot is occupied then we must locate the quotient's run.  The set of slots that hold remainders belonging to the same quotient are stored contiguously and these comprise the quotient's run. The first slot in the run might be the canonical slot but it is also possible that the entire run has been shifted to the right by the encroachment from the left of another run.

To locate the quotient's run we must first locate the start of the cluster.  The cluster consists of a contiguous set of runs.  Starting with the quotient's canonical slot we can scan left to locate the start of the cluster, then scan right to locate the quotient's run.

We scan left looking for a slot with ''is_shifted'' is false.  This indicates the start of the cluster.  Then we scan right keeping a running count of the number of runs we must skip over.  Each slot to the left of the canonical slot having ''is_occupied'' '''set''' indicates another run to be skipped, so we increment the running count.  Each slot having ''is_continuation'' '''clear''' indicates the start of another run, thus the end of the previous run, so we decrement the running count.  When the running count reaches zero, we are scanning the quotient's run.  We can compare the remainder in each slot in the run with d&lt;sub&gt;R&lt;/sub&gt;.  If found, we report that the key is (probably) in the filter otherwise we report that the key is definitely not in the filter.

===Lookup example===
[[File:Quotient Filter States.svg|thumb|540px|An example quotient filter showing in order the insertion of elements ''b'', ''e'', ''f'', ''c'', ''d'' and ''a'']]
Take, for example, looking up element ''e''. See state 3 in the figure.  We would compute ''hash(e)'', partition it into its remainder, e&lt;sub&gt;R&lt;/sub&gt; and its quotient e&lt;sub&gt;Q&lt;/sub&gt;, which is 4.  Scanning left from slot 4 we encounter three ''is_occupied'' slots, at indexes 4, 2 and 1, indicating e&lt;sub&gt;Q&lt;/sub&gt;'s run is the 3rd run in the cluster.  The scan stops at slot 1, which we detect as the cluster's start because it is not empty and not shifted.  Now we must scan right to the 3rd run. The start of a run is indicated by ''is_continuation'' being false. The 1st run is found at index 1, the 2nd at 4 and the 3rd at 5.  We compare the remainder held in each slot in the run that starts at index 5.  There is only one slot in that run but, in our example, its remainder equals e&lt;sub&gt;R&lt;/sub&gt;, indicating that ''e'' is indeed a member of the filter, with probability 1 - ε.

===Insertion===
Insertion follows a path similar to lookup until we ascertain that the key is definitely not in the filter.&lt;ref name=&quot;Bender&quot;/&gt; At that point we insert the remainder in a slot in the current run, a slot chosen to keep the run in sorted order.  We shift forward the remainders in any slots in the cluster at or after the chosen slot and update the slot bits.

* Shifting a slot's remainder does not affect the slot's ''is_occupied'' bit because it pertains to the slot, not the remainder contained in the slot.

* If we insert a remainder at the start of an existing run, the previous remainder is shifted and becomes a continuation slot, so we set its ''is_continuation'' bit.

* We set the ''is_shifted'' bit of any remainder that we shift.

===Insertion example===
The figure shows a quotient filter proceeding through a series of states as elements are added.  In state 1 three elements have been added.  The slot each one occupies forms a one-slot run which is also a distinct cluster.

In state 2 elements ''c'' and ''d'' have been added.  Element ''c'' has a quotient of 1, the same as ''b''.  We assume b&lt;sub&gt;R&lt;/sub&gt; &lt; c&lt;sub&gt;R&lt;/sub&gt; so c&lt;sub&gt;R&lt;/sub&gt; is shifted into slot 2, and is marked as both a ''continuation'' and ''shifted''.  Element ''d'' has a quotient of 2.  Since its canonical slot is in use, it is shifted into slot 3, and is marked as ''shifted''.  In addition its canonical slot is marked as ''occupied''.  The runs for quotients 1 and 2 now comprise a cluster.

In state 3 element ''a'' has been added.  Its quotient is 1.  We assume a&lt;sub&gt;R&lt;/sub&gt; &lt; b&lt;sub&gt;R&lt;/sub&gt; so the remainders in slots 1 through 4 must be shifted.  Slot 2 receives b&lt;sub&gt;R&lt;/sub&gt; and is marked as a ''continuation'' and ''shifted''.  Slot 5 receives e&lt;sub&gt;R&lt;/sub&gt; and is marked as ''shifted''.  The runs for quotients 1, 2 and 4 now comprise a cluster, and the presence of those three runs in the cluster is indicated by having slots 1, 2 and 4 being marked as ''occupied''.

==Cost/performance==

===Cluster length===

Bender&lt;ref name=&quot;Bender&quot;/&gt; argues that clusters are small. This is important because lookups and inserts require locating the start and length of an entire cluster. If the hash function generates uniformly distributed fingerprints then the length of most runs is ''O''(1) and it is highly likely that ''all'' runs have length ''O''(log ''m'') where ''m'' is the number of slots in the table.&lt;ref name=&quot;Bender&quot;/&gt;

===Probability of false positives===
Bender&lt;ref name=&quot;Bender&quot;/&gt; calculates the probability of a false positive (i.e. when the hash of two keys results in the same fingerprint) in terms of the hash table's remainder size and load factor. Recall that a ''p'' bit fingerprint is partitioned into a ''q'' bit quotient, which determines the table size of ''m'' = 2&lt;sup&gt;q&lt;/sup&gt; slots, and a ''r'' bit remainder. The load factor  &lt;math&gt;\alpha&lt;/math&gt; is the proportion of occupied slots ''n'' to total slots ''m'':  &lt;math&gt;\alpha = n/m&lt;/math&gt;.  Then, for a good hash function, &lt;math&gt; 1-e^{\alpha/2^r} \leq 2^{-r}&lt;/math&gt; is approximately the probability of a hard collision.

===Space/performance===
A quotient filter requires 10–25% more space than a comparable Bloom filter but is faster because each access requires evaluating only a single hash function.&lt;ref name=&quot;Spillane&quot;/&gt;

==Application==

Quotient filters are AMQs and, as such, provide many of the same benefits as [[Bloom filter]]s. A large database, such as Webtable&lt;ref&gt;{{cite journal|last=Chang|first=Fay|author2=et al. |title=Bigtable: A Distributed Storage System for Structured Data|journal=OSDI '06: Proceedings of the 7th USENIX Symposium on Operating Systems Design and Implementation|year=2006|pages=15–15|url=http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/bigtable-osdi06.pdf|accessdate=21 July 2012}}&lt;/ref&gt; may be composed of smaller sub-tables each of which has an associated filter.  Each query is distributed concurrently to all sub-tables.  If a sub-table does not contain the requested element, its filter can quickly complete the request without incurring any I/O.

Quotient filters offer two benefits in some applications.

#Two quotient filters can be efficiently merged without affecting their false positive rates.  This is not possible with Bloom filters.
#A few duplicates can be tolerated efficiently and can be deleted.

The space used by quotient filters is comparable to that of Bloom filters.  However quotient filters can be efficiently merged within memory without having to re-insert the original keys.

This is particularly important in some log structured storage systems that use the [[log-structured merge-tree]] or LSM-tree.&lt;ref&gt;{{cite journal|last=O'Neil|first=Patrick|author2=et al. |title=The log-structured merge-tree (LSM-tree)|journal=Acta Informatica|year=1996|volume=33|issue=4|pages=351–385|doi=10.1007/s002360050048}}&lt;/ref&gt; The LSM-tree is actually a collection of trees but which is treated as a single key-value store.  One variation of the LSM-Tree is the [[Sorted Array Merge Tree]] or SAMT.&lt;ref name=&quot;Spillane&quot;&gt;{{cite journal|last=Spillane|first=Richard|title=Efficient, Scalable, and Versatile Application and System Transaction Management for Direct Storage Layers|date=May 2012|url=http://www.fsl.cs.sunysb.edu/~rick/richard_spillane.pdf|accessdate=21 July 2012}}&lt;/ref&gt;  In this variation, a SAMT's component trees are called [[Wanna-B-tree]]s. Each Wanna-''B''-tree has an associated quotient filter.  A query on the SAMT is directed at only select Wanna-''B''-trees as evidenced by their quotient filters.

The storage system in its normal operation compacts the SAMT's Wanna-''B''-trees, merging smaller Wanna-''B''-trees into larger ones and merging their quotient filters.  An essential property of quotient filters is that they can be efficiently merged without having to re-insert the original keys.  Given that for large data sets the Wanna-''B''-trees may not be in memory, accessing them to retrieve the original keys would incur many I/Os.

By construction the values in a quotient filter are stored in sorted order.  Each run is associated with a specific quotient value, which provides the most significant portion of the fingerprint, the runs are stored in order and each slot in the run provides the least significant portion of the fingerprint.

So, by working from left to right, one can reconstruct all the fingerprints and the resulting list of integers will be in sorted order. Merging two quotient filters is then a simple matter of converting each quotient filter into such a list, merging the two lists and using it to populate a new larger quotient filter. Similarly, we can halve or double the size of a quotient filter without rehashing the keys since the fingerprints can be recomputed using just the quotients and remainders.&lt;ref name=&quot;Bender&quot;/&gt;

==See also==
*[[MinHash]]
*[[Bloom filter]]

==Notes==
{{reflist|30em}}

{{DEFAULTSORT:Quotient Filter}}
[[Category:Hashing]]
[[Category:Probabilistic data structures]]</text>
      <sha1>2dd2hqwxvmaotafeklqn4la9pw3lhcn</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Lazy deletion</title>
    <ns>0</ns>
    <id>13999239</id>
    <revision>
      <id>575676733</id>
      <parentid>557888034</parentid>
      <timestamp>2013-10-04T03:38:03Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>{{algorithm-stub}}</comment>
      <text xml:space="preserve" bytes="1360">In [[computer science]], '''lazy deletion''' refers to a method of deleting elements from a [[hash table]] that uses [[open addressing]]. In this method, deletions are done by marking an element as deleted, rather than erasing it entirely. Deleted locations are treated as empty when inserting and as occupied during a search.

The problem with this scheme is that as the number of delete/insert operations increases the cost of a successful search increases. To improve this, when an element is searched and found in the table, the element is relocated to the first location marked for deletion that was probed during the search. Instead of finding an element to relocate when the deletion occurs, the relocation occurs lazily during the next search.&lt;ref&gt;
{{Citation  | year=1995 |
first1=Pedro | last1=[[Pedro Celis|Celis]] |
first2=John | last2=Franco |
title=The Analysis of Hashing with Lazy Deletions|
id= Technical Report CS-86-14 |
publisher=Computer Science Department, Indiana University |
url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.39.9637}}&lt;/ref&gt;&lt;ref&gt;{{Citation|doi=10.1016/0020-0255(92)90022-Z|title=The analysis of hashing with lazy deletions|year=1992|last1=Celis|first1=Pedro|last2=Franco|first2=John|journal=Information Sciences|volume=62|pages=13}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Hashing]]


{{algorithm-stub}}</text>
      <sha1>kja3lezm87rvb91vfxqemfny05rr2ky</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Collision (computer science)</title>
    <ns>0</ns>
    <id>45344</id>
    <revision>
      <id>607443888</id>
      <parentid>599986752</parentid>
      <timestamp>2014-05-07T07:01:10Z</timestamp>
      <contributor>
        <username>Alessandro57</username>
        <id>941995</id>
      </contributor>
      <comment>Vandalism removed</comment>
      <text xml:space="preserve" bytes="2788">{{multiple issues|
{{one source|date=May 2013}}
{{ref improve|date=May 2013}}
{{self-published|date=May 2013}}
}}
:''Not to be confused with [[Collision domain|wireless packet collision]] or [[Hash_table#Collision_resolution|hash table]] collisions.''

In [[computer science]], a '''collision''' or '''clash''' is a situation that occurs when two distinct pieces of [[data]] have the same [[hash function|hash value]], [[checksum]], [[fingerprint (computing)|fingerprint]], or [[cryptographic hash function|cryptographic digest]].&lt;ref name=&quot;Hash_collisions&quot;&gt;{{cite web 
| accessdate = 2011-03-24
| author = Jered Floyd
| date = 2008-07-18
| location = http://permabit.wordpress.com/
| publisher = Permabits and Petabytes
| title = What do Hash Collisions Really Mean?
| quote = For the long explanation on cryptographic hashes and hash collisions, I wrote a column a bit back for SNW Online, “What you need to know about cryptographic hashes and enterprise storage”. The short version is that deduplicating systems that use cryptographic hashes use those hashes to generate shorter “fingerprints” to uniquely identify each piece of data, and determine if that data already exists in the system. The trouble is, by a mathematical rule called the “pigeonhole principle”, you can’t uniquely map any possible files or file chunk to a shorter fingerprint. Statistically, there are multiple possible files that have the same hash.
| url = http://permabit.wordpress.com/2008/07/18/what-do-hash-collisions-really-mean/}}&lt;/ref&gt;

Collisions are unavoidable whenever members of a very large set (such as all possible person names, or all possible [[file (computer)|computer files]]) are mapped to a relatively short bit string. This is merely an instance of the [[pigeonhole principle]].&lt;ref name=&quot;Hash_collisions&quot;/&gt;

The impact of collisions depends on the application. When hash functions and fingerprints are used to identify similar data, such as [[homology (biology)|homologous]] [[DNA]] sequences or similar [[audio file]]s, the functions are designed so as to ''maximize'' the probability of collision between distinct but similar data. [[Checksums]], on the other hand, are designed to minimize the probability of collisions between similar inputs, without regard for collisions between very different inputs.{{citation needed|date=April 2011}}

==References==
{{reflist}}

==See also==
*[[Birthday attack]]
*[[Collision attack]] (cryptographic hash functions)
*[[Hash table#Collision resolution|Collision resolution]] (hash tables)
*[[Perfect hash function]]: a hash function that is free of collisions by design

&lt;!--Interwikies--&gt;

&lt;!--Categories--&gt;
[[Category:Cryptographic hash functions]]
[[Category:Cryptographic attacks]]
[[Category:Hashing]]

{{computer-science-stub}}</text>
      <sha1>afraud8cqdmnat7prf5tvpr1ct1ksab</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Java hashCode()</title>
    <ns>0</ns>
    <id>13469202</id>
    <revision>
      <id>606476703</id>
      <parentid>606476648</parentid>
      <timestamp>2014-04-30T13:57:45Z</timestamp>
      <contributor>
        <ip>124.124.203.94</ip>
      </contributor>
      <comment>/* hashCode() in general */</comment>
      <text xml:space="preserve" bytes="5054">{{DISPLAYTITLE:Java &lt;tt&gt;hashCode()&lt;/tt&gt;}}
In the [[Java (programming language)|Java]] [[programming language]], every [[class (computer science)|class]] implicitly or explicitly provides a '''&lt;code&gt;hashCode()&lt;/code&gt;''' [[method (computer science)|method]], which digests the data stored in an instance of the class into a single hash value (a 32-[[bit]] [[integer (computer science)|signed integer]]).  This hash is used by other code when storing or manipulating the instance – the values are intended to be evenly distributed for varied inputs in order to use in [[hash table#Choosing a good hash function|clustering]].  This property is important to the performance of [[hash table]]s and other [[data structure]]s that store objects in groups (&quot;buckets&quot;) based on their computed hash values. Technically, in Java, &lt;tt&gt;hashCode()&lt;/tt&gt; by default is a native method, meaning, it has the modifier 'native', as it is implemented directly in the native code in the JVM.

==&lt;tt&gt;hashCode()&lt;/tt&gt; in general==
All the classes inherit a basic hash scheme from the fundamental base class &lt;tt&gt;java.lang.Object&lt;/tt&gt;, but instead many override this to provide a hash function that better handles their specific data.  Classes which provide their own implementation must override the object method &lt;tt&gt;public int hashCode()&lt;/tt&gt;.  

The general [[design by contract|contract]] for overridden implementations of this method is that they behave in a way consistent with the same object's &lt;tt&gt;equals()&lt;/tt&gt; method: that a given object must consistently report the same hash value (unless it is changed so that the new version is no longer considered &quot;equal&quot; to the old), and that two objects which &lt;tt&gt;equals()&lt;/tt&gt; says are equal ''must'' report the same hash value.  There's no requirement that hash values be consistent between different Java implementations, or even between different execution runs of the same program, and while two ''unequal'' objects having different hashes is very desirable, this is not mandatory (that is, the hash function implemented need not be a [[perfect hash function|perfect hash]]).&lt;ref name=&quot;oracle_objectdoc&quot;&gt;[http://download.oracle.com/javase/1.5.0/docs/api/java/lang/Object.html#hashCode%28%29 java.lang.Object.hashCode() documentation], Java SE 1.5.0 documentation, Oracle Inc.&lt;/ref&gt;

For example, the class &lt;tt&gt;Employee&lt;/tt&gt; might implement its hash function by composing the hashes of its members:
&lt;source lang=&quot;java&quot;&gt;
public class Employee {
    int        employeeId;
    String     name;
    Department dept;

    // other methods would be in here 

    @Override
    public int hashCode() {
        int hash = 1;
        hash = hash * 17 + employeeId;
        hash = hash * 31 + name.hashCode();
        hash = hash * 13 + (dept == null ? 0 : dept.hashCode());
        return hash;
    }
}
&lt;/source&gt;

==The &lt;tt&gt;java.lang.String&lt;/tt&gt; hash function==
In an attempt to provide a fast implementation, early versions of the Java &lt;tt&gt;String&lt;/tt&gt; class provided a &lt;tt&gt;hashCode()&lt;/tt&gt; implementation that considered at most 16 characters picked from the string. For some common data this worked very poorly, delivering unacceptably clustered results and consequently slow hashtable performance.&lt;ref name=&quot;Bloch&quot;&gt;Bloch&lt;/ref&gt;

From Java 1.2, &lt;tt&gt;java.lang.String&lt;/tt&gt; class implements its &lt;tt&gt;hashCode()&lt;/tt&gt; using a product sum algorithm over the entire text of the string.&lt;ref name=&quot;Bloch&quot; /&gt; An instance &lt;code&gt;s&lt;/code&gt; of the &lt;code&gt;java.lang.String&lt;/code&gt; class, for example, would have a hash code &lt;math&gt;h(s)&lt;/math&gt; defined by

:&lt;math&gt;h(s)=\sum_{i=0}^{n-1}s[i] \cdot 31^{n-1-i}&lt;/math&gt;

where terms are summed using Java 32-bit &lt;code&gt;int&lt;/code&gt; addition, &lt;math&gt;s[i]&lt;/math&gt; denotes the &lt;math&gt;i&lt;/math&gt;th character of the string, and &lt;math&gt;n&lt;/math&gt; is the length of &lt;code&gt;s&lt;/code&gt;.&lt;ref name=&quot;javadoc&quot;&gt;[http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/String.html#hashCode%28%29 java.lang.String.hashCode() documentation], Java SE 1.5.0 documentation, Oracle Inc.&lt;/ref&gt;
&lt;ref name=&quot;williams&quot;&gt;[http://web.archive.org/web/20130703081745/http://www.cogs.susx.ac.uk/courses/dats/notes/html/node114.html Choice of hash function -&gt; The String hash function&quot;], Data Structures course notes (2006), Peter M Williams, [[University of Sussex]]  School of Information &lt;/ref&gt;

==References==
*&quot;Always override hashCode when you override equals&quot; in {{Citation
 | last =Bloch
 | first =Joshua
 | author-link =Joshua Bloch
 | year =2008
 | title =Effective Java
 | edition =2
 | publisher =Addison-Wesley
 | isbn =978-0-321-35668-0
}}

&lt;references /&gt;

==External links==
* [http://www.ibm.com/developerworks/java/library/j-jtp05273.html &quot;Java theory and practice: Hashing it out&quot;], Brian Goetz, IBM Developer Works article, 27 May 2003 
* [http://www.javamex.com/tutorials/collections/hash_function_technical.shtml &quot;How the String hash function works (and implications for other hash functions)&quot;], Neil Coffey, Javamex

[[Category:Java programming language]]
[[Category:Hashing]]
[[Category:Hash functions]]
[[Category:Checksum algorithms]]</text>
      <sha1>gv5tz3o1w85jb30xialwd3lkjuy9rf0</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hash calendar</title>
    <ns>0</ns>
    <id>38063451</id>
    <revision>
      <id>619249763</id>
      <parentid>616554016</parentid>
      <timestamp>2014-07-31T06:46:30Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>Replace unicode entity nbsp for character [NBSP] (or space)  per [[WP:NBSP]] + other fixes, replaced: →   (2) using [[Project:AWB|AWB]] (10331)</comment>
      <text xml:space="preserve" bytes="4973">A '''hash calendar''' is a data structure that is used to measure the passage of time by adding hash values to an append-only database with one hash value per elapsed second. It can be thought of special kind of [[Merkle tree|Merkle or hash tree]], with the property that at any given moment, the tree contains a leaf node for each second since 1970‑01‑01 00:00:00 UTC.

[[File:A hash tree with 8 leaf nodes and a hash calendar after 7 seconds.jpg|center|800px|A hash tree with 8 leaf nodes and a hash calendar after 7 seconds]]

A hash tree with 8 leaf nodes and a hash calendar after 7 seconds.

[[File:Hash calendar after 31 seconds.jpg|center|800px|Hash calendar after 31 seconds]]

A hash calendar after 31 seconds consists of 5 disjoint hash trees.

The leaves are numbered left to right starting from zero and new leaves are always added to the right. By periodically publishing the root of the hash-tree is it possible to use a hash calendar as the basis of a [[Linked Timestamping|hash-linking based digital timestamping scheme]].

== History ==

The hash calendar construct was invented by Estonian cryptographers [[Ahto Buldas]] and Mart Saarepera based on their research on the security properties of [[cryptographic hash function]]s and hash-linking based digital timestamping.&lt;ref&gt;[http://www.google.com/patents/US8312528 System and method for generating a digital certificate patent 8,312,528]&lt;/ref&gt; Their design goal was to remove the need for a trusted third party i.e. that the time of the timestamp should be verifiable independently from the issuer of the timestamp.&lt;ref&gt;http://www.guardtime.com/resources/video-library/educational-series-on-hash-functions&lt;/ref&gt;

== Construction of a hash calendar ==

There are different algorithms that can be used to build a hash calendar and extract a relevant [[Hash chain#Binary hash chains|hash chain]] per second. The easiest is to imagine the calendar being built in two phases. In the first phase, the leaves are collected into complete binary trees, starting from left, and making each tree as large as possible.

[[File:Sparse hash calendar with 11 10 = 1011 2 leaves.jpg|Sparse hash calendar with 11 10 = 1011 2 leaves]]

Sparse hash calendar with 11&lt;sub&gt;10&lt;/sub&gt; = 1011&lt;sub&gt;2&lt;/sub&gt; leaves

In the second phase, the multiple unconnected trees are turned into a single tree by merging the roots of the initial trees, but this time starting from the right and adding new parent nodes as needed (red nodes).

[[File:Compact hash calendar with 11 10 = 1011 2 leaves.jpg|Compact hash calendar with 11 10 = 1011 2 leaves]]

Compact hash calendar with 11&lt;sub&gt;10&lt;/sub&gt; = 1011&lt;sub&gt;2&lt;/sub&gt; leaves.

The hash chains can then be extracted as from any hash tree. Since the hash calendar is built in a deterministic manner, the shape of the tree for any moment can be reconstructed knowing just the number of leaf nodes in the tree at that moment, which is one more than the number of seconds from 1970‑01‑01 00:00:00 UTC to that moment. Therefore, given the time when the calendar tree was created and a hash chain extracted from it, the time value corresponding to each leaf node can be computed.

== Distributed hash calendar  ==

The Distributed hash calendar is  a distributed network of hash calendar nodes. In order to ensure a high availability service it is possible to have multiple calendars in different physical locations all of which communicate with each other to ensure that each calendar contains identical hash values. Ensuring that the calendars remain in agreement is a form of [[Byzantine generals|Byzantine fault tolerance]]

To the right  a 5 node calendar cluster is shown where each node communicates with every other node in the cluster and there is no single point of failure. Although each node has a clock the clock is not used for setting the time directly but as a metronome to ensure that the nodes “beat” at the same time.

== Applications ==
A five node hash calendar cluster is a component of Keyless Signature Infrastructure (KSI), each leaf in the hash calendar being the aggregate hash value of a globally distributed hash tree.

== See also ==
* [[Cryptographic hash functions]] 
* [[Linked Timestamping]]
* [[Hash list]]
* [[Hash table]]
* [[Merkle tree]] 
* [[Provably secure cryptographic hash function]]

== References ==

&lt;references /&gt;

== External links ==
* {{US patent|4309569|Merkle tree patent 4,309,569}} – Explains both the hash tree structure and the use of it to handle many one-time signatures.
* [http://www.rsasecurity.com/rsalabs/node.asp?id=2003 Efficient Use of Merkle Trees] – [[RSA Security|RSA labs]] explanation of the original purpose of Merkle trees: To handle many Lamport one-time signatures.
* http://csrc.nist.gov/groups/ST/toolkit/secure_hashing.html

{{Cryptography navbox}}
{{CS-Trees}}

[[Category:Error detection and correction]]
[[Category:Cryptographic hash functions]]
[[Category:Hashing]]
[[Category:Trees (data structures)]]</text>
      <sha1>fsyati246i63prbbid34s62uudqbz1u</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Dirhash</title>
    <ns>0</ns>
    <id>19461027</id>
    <revision>
      <id>557450226</id>
      <parentid>536900460</parentid>
      <timestamp>2013-05-30T02:50:55Z</timestamp>
      <contributor>
        <username>Khazar2</username>
        <id>16460235</id>
      </contributor>
      <minor/>
      <comment>clean up, [[WP:AWB/T|typos fixed]]: Janurary → January using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="1016">'''Dirhash''' is a feature of [[FreeBSD]] that improves the speed of finding files in a directory.  Rather than finding a file in a directory using a [[linear search]] algorithm, FreeBSD uses a [[hash table]].&lt;ref&gt;[http://www.maths.tcd.ie/~dwmalone/p/bsdcon01.pdf Recent Filesystem Optimisations in FreeBSD, by Ian Dowse and David Malone]&lt;/ref&gt;&lt;ref&gt;[http://static.usenix.org/events/usenix02/tech/freenix/dowse.html Recent Filesystem Optimisations in FreeBSD, by Ian Dowse and David Malone, 2002 FREENIX Track Technical Program]&lt;/ref&gt; The feature is backwards-compatible because the hash table is built in memory when the directory is accessed, and it does not affect the on-disk format of the filesystem, in contrast to systems such as [[Htree]]. Dirhash was implemented by Ian Dowse early in 2001 and imported into FreeBSD in July 2001. It was subsequently imported into [[OpenBSD]] in December 2003 and [[NetBSD]] in January 2005.

== References ==
&lt;references/&gt;

{{FreeBSD}}

[[Category:Hashing]]


{{compu-stub}}</text>
      <sha1>dx4yltou4wz37a9qkdu4wsen1j8061f</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Locality-sensitive hashing</title>
    <ns>0</ns>
    <id>11634012</id>
    <revision>
      <id>625138862</id>
      <parentid>625138686</parentid>
      <timestamp>2014-09-11T20:16:48Z</timestamp>
      <contributor>
        <ip>173.228.123.196</ip>
      </contributor>
      <comment>/* Applications */</comment>
      <text xml:space="preserve" bytes="19175">'''Locality-sensitive hashing''' ('''LSH''') is a method of performing probabilistic [[dimension reduction]] of high-dimensional data. The basic idea is to [[Hash Function|hash]] the input items so that similar items are mapped to the same buckets with high probability (the number of buckets being much smaller than the universe of possible input items). The hashing used in LSH is different from conventional hash functions, such as those used in [[cryptography]], as in the LSH case the goal is to maximize probability of &quot;collision&quot; of similar items rather than avoid collisions.
&lt;ref name=MOMD&gt;{{cite web
| author = A. Rajaraman and J. Ullman
| url = http://infolab.stanford.edu/~ullman/mmds.html
| title=Mining of Massive Datasets, Ch. 3.
| year = 2010
}}&lt;/ref&gt;
Note how locality-sensitive hashing, in many ways, mirrors [[Cluster analysis|data clustering]] and [[Nearest neighbor search]].

==Definition==

An ''LSH family''
&lt;ref name=MOMD /&gt;
&lt;ref name=GIM1999&gt;{{cite journal
| author1 = Gionis, A.
| author2 = [[Piotr Indyk|Indyk, P.]] | author3 = [[Rajeev Motwani|Motwani, R.]]
| year = 1999
| title = Similarity Search in High Dimensions via Hashing
| url = http://people.csail.mit.edu/indyk/vldb99.ps ,
| journal = Proceedings of the 25th Very Large Database (VLDB) Conference
}}&lt;/ref&gt; 
&lt;ref name=IndykMotwani98&gt;{{cite journal
 | author1 = [[Piotr Indyk|Indyk, Piotr]].
 | author2 = [[Rajeev Motwani|Motwani, Rajeev]].
 | year = 1998
 | title = Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality.
 | url = http://people.csail.mit.edu/indyk/nndraft.ps ,
 | journal = Proceedings of 30th Symposium on Theory of Computing
}}&lt;/ref&gt;
&lt;math&gt;\mathcal F&lt;/math&gt; is defined for a [[metric space]] &lt;math&gt;\mathcal M =(M, d)&lt;/math&gt;, a threshold &lt;math&gt;R&gt;0&lt;/math&gt; and an approximation factor &lt;math&gt;c&gt;1&lt;/math&gt;.  This family &lt;math&gt;\mathcal F&lt;/math&gt; is a family of functions &lt;math&gt;h:{\mathcal M}\to S&lt;/math&gt; which map elements from the [[metric space]] to a bucket &lt;math&gt;s \in S&lt;/math&gt;.  The LSH family satisfies the following conditions for any two points &lt;math&gt;p, q \in {\mathcal M}&lt;/math&gt;, using a function &lt;math&gt;h \in \mathcal F&lt;/math&gt; which is chosen uniformly at random:
* if &lt;math&gt;d(p,q) \le R&lt;/math&gt;, then &lt;math&gt;h(p)=h(q)&lt;/math&gt; (i.e.,&lt;math&gt;p&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt; collide) with probability at least &lt;math&gt;P_1&lt;/math&gt;,
* if &lt;math&gt;d(p,q) \ge cR&lt;/math&gt;, then &lt;math&gt;h(p)=h(q)&lt;/math&gt; with probability at most &lt;math&gt;P_2&lt;/math&gt;.

A family is interesting when &lt;math&gt;P_1&gt;P_2&lt;/math&gt;.  Such a family &lt;math&gt;\mathcal F&lt;/math&gt; is called ''&lt;math&gt;(R,cR,P_1,P_2)&lt;/math&gt;-sensitive''.

Alternatively&lt;ref name=Charikar2002&gt;{{cite journal
 | author = Charikar, Moses S..
 | year = 2002
 | title = Similarity Estimation Techniques from Rounding Algorithms
 | journal = Proceedings of the 34th Annual ACM Symposium on Theory of Computing 2002
 | pages = (ACM 1–58113–495–9/02/0005)…
 | url = http://portal.acm.org/citation.cfm?id=509965
 | accessdate = 2007-12-21
 | doi = 10.1145/509907.509965
}}&lt;/ref&gt; it is defined with respect to a universe of items &lt;math&gt;U&lt;/math&gt; that have a [[String metric|similarity]] function &lt;math&gt;\phi : U \times U \to [0,1]&lt;/math&gt;. An LSH scheme is a family of [[hash function]]s &lt;math&gt;H&lt;/math&gt; coupled with a probability distribution &lt;math&gt;D&lt;/math&gt; over the functions such that a function &lt;math&gt;h \in H&lt;/math&gt; chosen according to &lt;math&gt;D&lt;/math&gt; satisfies the property that &lt;math&gt;Pr_{h \in H} [h(a) = h(b)] = \phi(a,b)&lt;/math&gt; for any &lt;math&gt;a,b \in U&lt;/math&gt;.

===Amplification===

Given a &lt;math&gt;(d_1, d_2, p_1, p_2)&lt;/math&gt;-sensitive family &lt;math&gt;\mathcal F&lt;/math&gt;, we can construct new families &lt;math&gt;\mathcal G&lt;/math&gt; by either the AND-construction or OR-construction of &lt;math&gt;\mathcal F&lt;/math&gt;.&lt;ref name=MOMD /&gt;

To create an AND-construction, we define a new family &lt;math&gt;\mathcal G&lt;/math&gt; of hash functions &lt;math&gt;g&lt;/math&gt;, where each function &lt;math&gt;g&lt;/math&gt; is constructed from &lt;math&gt;k&lt;/math&gt; random functions &lt;math&gt;h_1, ..., h_k&lt;/math&gt; from &lt;math&gt;\mathcal F&lt;/math&gt;.  We then say that for a hash function &lt;math&gt;g \in \mathcal G&lt;/math&gt;, &lt;math&gt;g(x) = g(y)&lt;/math&gt; if and only if all &lt;math&gt;h_i(x) = h_i(y)&lt;/math&gt; for &lt;math&gt;i = 1, 2, ..., k&lt;/math&gt;.  Since the members of &lt;math&gt;\mathcal F&lt;/math&gt; are independently chosen for any &lt;math&gt;g \in \mathcal G&lt;/math&gt;, &lt;math&gt;\mathcal G&lt;/math&gt; is a &lt;math&gt;(d_1, d_2, p_{1}^r, p_{2}^r)&lt;/math&gt;-sensitive family.

To create an OR-construction, we define a new family &lt;math&gt;\mathcal G&lt;/math&gt; of hash functions &lt;math&gt;g&lt;/math&gt;, where each function &lt;math&gt;g&lt;/math&gt; is constructed from &lt;math&gt;k&lt;/math&gt; random functions &lt;math&gt;h_1, ..., h_k&lt;/math&gt; from &lt;math&gt;\mathcal F&lt;/math&gt;.  We then say that for a hash function &lt;math&gt;g \in \mathcal G&lt;/math&gt;, &lt;math&gt;g(x) = g(y)&lt;/math&gt; if and only if &lt;math&gt;h_i(x) = h_i(y)&lt;/math&gt; for one or more values of &lt;math&gt;i&lt;/math&gt;.  Since the members of &lt;math&gt;\mathcal F&lt;/math&gt; are independently chosen for any &lt;math&gt;g \in \mathcal G&lt;/math&gt;, &lt;math&gt;\mathcal G&lt;/math&gt; is a &lt;math&gt;(d_1, d_2, 1- (1 - p_1)^r, 1 - (1 - p_2)^r)&lt;/math&gt;-sensitive family.

==Applications==

LSH has been applied to several problem domains including{{citation needed|date=August 2011}}
*[[Near-duplicate detection]]&lt;ref&gt;
{{citation
 | last1 = Gurmeet Singh | first1 = Manku
 | last2 = Jain | first2 =  Arvind
 | last2 = Das Sarma | first2 =  Anish
 | title = Detecting near-duplicates for web crawling
 | journal = Proceedings of the 16th international conference on World Wide Web. ACM,
 | year = 2007}}.&lt;/ref&gt;&lt;ref&gt;
{{citation
 | author = Das, Abhinandan S., et al.
 | title = Google news personalization: scalable online collaborative filtering
 | journal = Proceedings of the 16th international conference on World Wide Web. ACM,
 | year = 2007|doi=10.1145/1242572.1242610}}.&lt;/ref&gt;
*[[Hierarchical clustering]]&lt;ref&gt;
{{citation
 | author = Koga, Hisashi, Tetsuo Ishibashi, and Toshinori Watanabe
 | title = Fast agglomerative hierarchical clustering algorithm using Locality-Sensitive Hashing
 | journal = Knowledge and Information Systems 12.1: 25-53,
 | year = 2007}}.&lt;/ref&gt;
*[[Genome-wide association study]]&lt;ref&gt;
{{citation
 | author = Brinza, Dumitru, et al.
 | title = RAPID detection of gene–gene interactions in genome-wide association studies
 | journal = Bioinformatics 26.22 (2010): 2856-2862.}}
&lt;/ref&gt;
*[[Image similarity identification]]
**[[VisualRank]]
*[[Gene expression similarity identification]]{{citation needed|date=October 2013}}
*[[Audio similarity identification]]
*[[Nearest neighbor search]]
*[[Audio fingerprint]]&lt;ref&gt;
{{citation
| title = dejavu - Audio fingerprinting and recognition in Python
| url = https://github.com/worldveil/dejavu}}
&lt;/ref&gt;

==Methods==

===Bit sampling for Hamming distance===

One of the easiest ways to construct an LSH family is by bit sampling.&lt;ref name=IndykMotwani98 /&gt; This approach works for the [[Hamming distance]] over d-dimensional vectors &lt;math&gt;\{0,1\}^d&lt;/math&gt;. Here, the family &lt;math&gt;\mathcal F&lt;/math&gt; of hash functions is simply the family of all the projections of points on one of the &lt;math&gt;d&lt;/math&gt; coordinates, i.e., &lt;math&gt;{\mathcal F}=\{h:\{0,1\}^d\to \{0,1\}\mid h(x)=x_i,i =1 ... d\}&lt;/math&gt;, where &lt;math&gt;x_i&lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;th coordinate of &lt;math&gt;x&lt;/math&gt;. A random function &lt;math&gt;h&lt;/math&gt; from &lt;math&gt;{\mathcal F}&lt;/math&gt; simply selects a random bit from  the input point. This family has the following parameters: &lt;math&gt;P_1=1-R/d&lt;/math&gt;, &lt;math&gt;P_2=1-cR/d&lt;/math&gt;.

===Min-wise independent permutations===

{{main|MinHash}}

Suppose &lt;math&gt;U&lt;/math&gt;  is composed of subsets of some ground set of enumerable items &lt;math&gt;S&lt;/math&gt; and the similarity function of interest is the [[Jaccard index]] &lt;math&gt;J&lt;/math&gt;. If &lt;math&gt;\pi&lt;/math&gt; is a permutation on the indices of &lt;math&gt;S&lt;/math&gt;, for &lt;math&gt;A \subseteq S&lt;/math&gt; let &lt;math&gt;h(A) = \min_{a \in A} \{ \pi(a) \}&lt;/math&gt;. Each possible choice of &lt;math&gt;\pi&lt;/math&gt; defines a single hash function &lt;math&gt;h&lt;/math&gt; mapping input sets to integers.

Define the function family &lt;math&gt;H&lt;/math&gt; to be the set of all such functions and let &lt;math&gt;D&lt;/math&gt; be the uniform distribution. Given two sets &lt;math&gt;A,B \subseteq S&lt;/math&gt; the event that &lt;math&gt;h(A) = h(B)&lt;/math&gt; corresponds exactly to the event that the minimizer of &lt;math&gt;\pi&lt;/math&gt; lies inside &lt;math&gt;A \bigcap B&lt;/math&gt;. As &lt;math&gt;h&lt;/math&gt; was chosen uniformly at random, &lt;math&gt;Pr[h(A) = h(B)] = J(A,B)\,&lt;/math&gt; and &lt;math&gt;(H,D)\,&lt;/math&gt; define an LSH scheme for the Jaccard index.

Because the symmetric group on n elements has size n!, choosing a truly random permutation from the full symmetric group is infeasible for even moderately sized n. Because of this fact, there has been significant work on finding a family of permutations that is &quot;min-wise independent&quot; - a permutation family for which each element of the domain has equal probability of being the minimum under a randomly chosen &lt;math&gt;\pi&lt;/math&gt;. It has been established that a min-wise independent family of permutations is at least of size &lt;math&gt;lcm(1, 2, ..., n) \ge e^{n-o(n)}&lt;/math&gt;.&lt;ref name=Broder1998&gt;{{cite journal
 | author = Broder, A.Z.
 |author2= Charikar, M.|author3= Frieze, A.M.|author4= Mitzenmacher, M.
 | year = 1998
 | title = Min-wise independent permutations
 | journal = Proceedings of the thirtieth annual ACM symposium on Theory of computing
 | pages = 327–336
 | url = http://www.cs.princeton.edu/~moses/papers/minwise.ps 
 | accessdate = 2007-11-14
 | doi = 10.1145/276698.276781
}}&lt;/ref&gt; and that this boundary is tight&lt;ref&gt;
{{cite journal
  | title=An optimal construction of exactly min-wise independent permutations
  | author1=Takei, Y. | author2 = Itoh, T. | author3 = Shinozaki, T.
  | journal=Technical Report COMP98-62, IEICE, 1998
}}
&lt;/ref&gt;

Because min-wise independent families are too big for practical applications, two variant notions of min-wise independence are introduced: restricted min-wise independent permutations families, and approximate min-wise independent families.
Restricted min-wise independence is the min-wise independence property restricted to certain sets of cardinality at most k.&lt;ref name=Matousek2002&gt;{{cite journal
 | author = [[Jiří Matoušek (mathematician)|Matoušek]], J.
 |author2=Stojakovic, M.
 | year = 2002
 | title = On Restricted Min-Wise Independence of Permutations
 | journal = Preprint
 | url = http://citeseer.ist.psu.edu/689217.html 
 | accessdate = 2007-11-14
}}&lt;/ref&gt;
Approximate min-wise independence differs from the property by at most a fixed &lt;math&gt;\epsilon&lt;/math&gt;.&lt;ref name=Saks2000&gt;{{cite journal
 | author = Saks, M.
 |author2= Srinivasan, A.|author3= Zhou, S.|author4= Zuckerman, D.
 | year = 2000
 | title = Low discrepancy sets yield approximate min-wise independent permutation families
 | journal = Information Processing Letters
 | volume = 73
 | issue = 1-2
 | pages = 29–32
 | url = http://citeseer.ist.psu.edu/saks99low.html 
 | accessdate = 2007-11-14
 | doi = 10.1016/S0020-0190(99)00163-5
}}&lt;/ref&gt;

===Nilsimsa Hash===

{{main|Nilsimsa Hash}}

'''Nilsimsa''' is an [[Anti-spam techniques|anti-spam]] focused locality-sensitive hashing algorithm.&lt;ref&gt;{{cite web|authors=Damiani et. al|title=An Open Digest-based Technique for Spam Detection|year=2004|url=http://spdp.di.unimi.it/papers/pdcs04.pdf|accessdate=2013-09-01}}&lt;/ref&gt; The goal of Nilsimsa is to generate a hash digest of an email message such that the digests of two similar messages are similar to each other.  Nilsimsa satisfies three requirements outlined by the paper's authors:

# The digest identifying each message should not vary significantly for changes that can be produced automatically.
# The encoding must be robust against intentional attacks.
# The encoding should support an extremely low risk of false positives.

===Random projection===

The random projection method of LSH&lt;ref name=Charikar2002 /&gt; (termed arccos by Andoni and Indyk &lt;ref name=Andoni2008&gt;{{cite journal 
| author1 = Alexandr Andoni 
| author2 = [[Piotr Indyk|Indyk, P.]]
| year = 2008
| title = Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions
| journal = Communications of the ACM
| volume = 51
| number = 1
| pages = 117–122.
| doi=10.1145/1327452.1327494
}}&lt;/ref&gt;) is designed to approximate the [[cosine distance]] between vectors. The basic idea of this technique is to choose a random [[hyperplane]] (defined by a normal unit vector &lt;math&gt;r&lt;/math&gt;) at the outset and use the hyperplane to hash input vectors.

Given an input vector &lt;math&gt;v&lt;/math&gt; and a hyperplane defined by &lt;math&gt;r&lt;/math&gt;, we let &lt;math&gt;h(v) = sgn(v \cdot r)&lt;/math&gt;. That is,  &lt;math&gt;h(v) = \pm 1&lt;/math&gt; depending on which side of the hyperplane &lt;math&gt;v&lt;/math&gt; lies.

Each possible choice of &lt;math&gt;r&lt;/math&gt; defines a single function. Let &lt;math&gt;H&lt;/math&gt; be the set of all such functions and let &lt;math&gt;D&lt;/math&gt; be the uniform distribution once again. It is not difficult to prove that, for two vectors &lt;math&gt;u,v&lt;/math&gt;, &lt;math&gt;Pr[h(u) = h(v)] = 1 - \frac{\theta(u,v)}{\pi}&lt;/math&gt;, where &lt;math&gt;\theta(u,v)&lt;/math&gt; is the angle between &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt;. &lt;math&gt;1 - \frac{\theta(u,v)}{\pi}&lt;/math&gt; is closely related to &lt;math&gt;\cos(\theta(u,v))&lt;/math&gt;.

In this instance hashing produces only a single bit. Two vectors' bits match with probability proportional to the cosine of the angle between them.

===Stable distributions===

The hash function
&lt;ref name=DIIM04&gt;{{cite journal
 | author1 = Datar, M.
 | author2 =  Immorlica, N. | author3 = [[Piotr Indyk|Indyk, P.]] | author4 = Mirrokni, V.S.
 | year=2004
 | title = Locality-Sensitive Hashing Scheme Based on p-Stable Distributions
 | url = http://theory.csail.mit.edu/~mirrokni/pstable.ps 
 | journal = Proceedings of the Symposium on Computational Geometry
}}&lt;/ref&gt; &lt;math&gt;h_{\mathbf{a},b} (\boldsymbol{\upsilon}) : 
\mathcal{R}^d
\to \mathcal{N} &lt;/math&gt; maps a ''d'' dimensional vector
&lt;math&gt;\boldsymbol{\upsilon}&lt;/math&gt; onto a set of integers. Each hash function
in the family is indexed by a choice of random &lt;math&gt;\mathbf{a}&lt;/math&gt; and
&lt;math&gt;b&lt;/math&gt; where &lt;math&gt;\mathbf{a}&lt;/math&gt; is a ''d'' dimensional 
vector with
entries chosen independently from a [[stable distribution]] and 
&lt;math&gt;b&lt;/math&gt; is
a real number chosen uniformly from the range [0,r]. For a fixed
&lt;math&gt;\mathbf{a},b&lt;/math&gt; the hash function &lt;math&gt;h_{\mathbf{a},b}&lt;/math&gt; is
given by &lt;math&gt;h_{\mathbf{a},b} (\boldsymbol{\upsilon}) = \left \lfloor
\frac{\mathbf{a}\cdot \boldsymbol{\upsilon}+b}{r} \right \rfloor &lt;/math&gt;.

Other construction methods for hash functions have been proposed to better fit the data. 
&lt;ref name=PJA10&gt;{{cite journal
 | author1 = Pauleve, L. | author2 =  Jegou, H. | author3 = Amsaleg, L.
 | year=2010
 | title = Locality sensitive hashing: A comparison of hash function types and querying mechanisms
 | url = http://hal.inria.fr/inria-00567191/en/ 
 | journal = Pattern recognition Letters
}}&lt;/ref&gt;
In particular k-means hash functions are better in practice than projection-based hash functions, but without any theoretical guarantee.

==LSH algorithm for nearest neighbor search==

One of the main applications of LSH is to provide a method for efficient approximate [[nearest neighbor search]] algorithms.  Consider an LSH family &lt;math&gt;\mathcal F&lt;/math&gt;.  The algorithm has two main parameters: the width parameter &lt;math&gt;k&lt;/math&gt; and the number of hash tables &lt;math&gt;L&lt;/math&gt;.

In the first step, we define a new family &lt;math&gt;\mathcal G&lt;/math&gt; of hash functions &lt;math&gt;g&lt;/math&gt;, where each function &lt;math&gt;g&lt;/math&gt; is obtained by concatenating &lt;math&gt;k&lt;/math&gt; functions &lt;math&gt;h_1, ..., h_k&lt;/math&gt; from &lt;math&gt;\mathcal F&lt;/math&gt;, i.e., &lt;math&gt;g(p) = [h_1(p), ..., h_k(p)]&lt;/math&gt;.  In other words, a random hash function &lt;math&gt;g&lt;/math&gt; is obtained by concatenating &lt;math&gt;k&lt;/math&gt; randomly chosen hash functions from &lt;math&gt;\mathcal F&lt;/math&gt;.  The algorithm then constructs &lt;math&gt;L&lt;/math&gt; hash tables, each corresponding to a different randomly chosen hash function &lt;math&gt;g&lt;/math&gt;.

In the preprocessing step we hash all &lt;math&gt;n&lt;/math&gt; points from the data set &lt;math&gt;S&lt;/math&gt; into each of the &lt;math&gt;L&lt;/math&gt; hash tables.  Given that the resulting hash tables have only &lt;math&gt;n&lt;/math&gt; non-zero entries, one can reduce the amount of memory used per each hash table to &lt;math&gt;O(n)&lt;/math&gt; using standard [[hash functions]].

Given a query point &lt;math&gt;q&lt;/math&gt;, the algorithm iterates over the &lt;math&gt;L&lt;/math&gt; hash functions &lt;math&gt;g&lt;/math&gt;.  For each &lt;math&gt;g&lt;/math&gt; considered, it retrieves the data points that are hashed into the same bucket as &lt;math&gt;q&lt;/math&gt;.  The process is stopped as soon as a point within distance &lt;math&gt;cR&lt;/math&gt; from &lt;math&gt;q&lt;/math&gt; is found.

Given the parameters &lt;math&gt;k&lt;/math&gt; and &lt;math&gt;L&lt;/math&gt;, the algorithm has the following performance guarantees:
* preprocessing time: &lt;math&gt;O(nLkt)&lt;/math&gt;, where &lt;math&gt;t&lt;/math&gt; is the time to evaluate a function &lt;math&gt;h \in \mathcal F&lt;/math&gt; on an input point &lt;math&gt;p&lt;/math&gt;;
* space: &lt;math&gt;O(nL)&lt;/math&gt;, plus the space for storing data points;
* query time: &lt;math&gt;O(L(kt+dnP_2^k))&lt;/math&gt;;
* the algorithm succeeds in finding a point within distance &lt;math&gt;cR&lt;/math&gt; from &lt;math&gt;q&lt;/math&gt; (if there exists a point within distance &lt;math&gt;R&lt;/math&gt;) with probability at least &lt;math&gt;1 - ( 1 - P_1^k ) ^ L&lt;/math&gt;;

For a fixed approximation ratio &lt;math&gt;c=1+\epsilon&lt;/math&gt; and probabilities &lt;math&gt;P_1&lt;/math&gt; and &lt;math&gt;P_2&lt;/math&gt;, one can set &lt;math&gt;k={\log n \over \log 1/P_2}&lt;/math&gt; and &lt;math&gt;L = n^{\rho}&lt;/math&gt;, where &lt;math&gt;\rho={\log P_1\over \log P_2}&lt;/math&gt;. Then one obtains the following performance guarantees:
* preprocessing time: &lt;math&gt;O(n^{1+\rho}kt)&lt;/math&gt;;
* space: &lt;math&gt;O(n^{1+\rho})&lt;/math&gt;, plus the space for storing data points;
* query time: &lt;math&gt;O(n^{\rho}(kt+d))&lt;/math&gt;;

==See also==
*[[Curse of dimensionality]]
*[[Feature hashing]]
*[[Fourier-related transforms]]
*[[Multilinear subspace learning]]
*[[Principal component analysis]]
*[[Singular value decomposition]]
*[[Wavelet compression]]
*[[Rolling hash]]
*[[Bloom Filter]]

==References==

{{reflist}}

==Further reading==

*Samet, H. (2006) ''Foundations of Multidimensional and Metric Data Structures''. Morgan Kaufmann. ISBN 0-12-369446-9

==External links==
* [http://web.mit.edu/andoni/www/LSH/index.html Alex Andoni's LSH homepage]
* [http://lshkit.sourceforge.net/ LSHKIT: A C++ Locality Sensitive Hashing Library]
* [https://github.com/simonemainardi/LSHash A Python Locality Sensitive Hashing library that optionally supports persistence via redis]
* [http://www.vision.caltech.edu/malaa/software/research/image-search/ Caltech Large Scale Image Search Toolbox]: a Matlab toolbox implementing several LSH hash functions, in addition to Kd-Trees, Hierarchical K-Means, and Inverted File search algorithms.
* [https://github.com/salviati/slash Slash: A C++ LSH library, implementing Spherical LSH by Terasawa, K., Tanaka, Y]
* [https://github.com/RSIA-LIESMARS-WHU/LSHBOX LSHBOX: An Open Source C++ Toolbox of Locality-Sensitive Hashing for Large Scale Image Retrieval, Also Support Python and MATLAB.]

{{DEFAULTSORT:Locality Sensitive Hashing}}
[[Category:Search algorithms]]
[[Category:Classification algorithms]]
[[Category:Dimension reduction]]
[[Category:Hashing]]
[[Category:Probabilistic data structures]]</text>
      <sha1>jj35rus7zpt2ve271zavyof9d8tf1ww</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>MinHash</title>
    <ns>0</ns>
    <id>30632997</id>
    <revision>
      <id>617702594</id>
      <parentid>607165561</parentid>
      <timestamp>2014-07-20T12:51:26Z</timestamp>
      <contributor>
        <username>Linuxjava</username>
        <id>19672902</id>
      </contributor>
      <minor/>
      <comment>/* See also */</comment>
      <text xml:space="preserve" bytes="17032">In [[computer science]], '''MinHash''' (or the '''min-wise independent permutations''' [[locality sensitive hashing]] scheme) is a technique for quickly estimating how [[Similarity measure|similar]] two sets are. The scheme was invented by {{harvs|first=Andrei|last=Broder|authorlink=Andrei Broder|year=1997|txt}},&lt;ref name=&quot;b97&quot;/&gt; and initially used in the [[AltaVista]] search engine to detect duplicate web pages and eliminate them from search results.&lt;ref name=&quot;bcfm&quot;&gt;{{citation
 | last1 = Broder | first1 = Andrei Z. | author1-link = Andrei Broder
 | last2 = Charikar | first2 = Moses
 | last3 = Frieze | first3 = Alan M. | author3-link = Alan M. Frieze
 | last4 = Mitzenmacher | first4 = Michael | author4-link = Michael Mitzenmacher
 | contribution = Min-wise independent permutations
 | doi = 10.1145/276698.276781
 | location = New York, NY, USA
 | pages = 327–336
 | publisher = [[Association for Computing Machinery]]
 | title = [[Symposium on Theory of Computing|Proc. 30th ACM Symposium on Theory of Computing (STOC '98)]]
 | year = 1998}}.&lt;/ref&gt;
It has also been applied in large-scale [[Cluster analysis|clustering]] problems, such as [[document clustering|clustering documents]] by the similarity of their sets of words.&lt;ref name=&quot;b97&quot;&gt;{{citation
 | last = Broder | first = Andrei Z. | author-link = Andrei Broder
 | contribution = On the resemblance and containment of documents
 | doi = 10.1109/SEQUEN.1997.666900
 | url = http://gatekeeper.dec.com/ftp/pub/dec/SRC/publications/broder/positano-final-wpnums.pdf
 | pages = 21–29
 | publisher = [[IEEE]]
 | title = Compression and Complexity of Sequences: Proceedings, Positano, Amalfitan Coast, Salerno, Italy, June 11-13, 1997
 | year = 1997}}.&lt;/ref&gt;

==Jaccard similarity and minimum hash values==
The [[Jaccard index|Jaccard similarity coefficient]] of two sets {{math|''A''}} and {{math|''B''}} is defined to be
:&lt;math&gt; J(A,B) = {{|A \cap B|}\over{|A \cup B|}}.&lt;/math&gt;
It is a number between 0 and 1; it is 0 when the two sets are disjoint, 1 when they are equal, and strictly between 0 and 1 otherwise. It is a commonly used indicator of the similarity between two sets: two sets are more similar when their Jaccard index is closer to 1, and more dissimilar when their Jaccard index is closer to 0.

Let ''h'' be a [[hash function]] that maps the members of {{math|''A''}} and {{math|''B''}} to distinct integers, and for any set ''S'' define {{math|''h''&lt;sub&gt;min&lt;/sub&gt;(''S'')}} to be the member {{math|''x''}} of {{math|''S''}} with the minimum value of {{math|''h''(''x'')}}. Then {{math|1=''h''&lt;sub&gt;min&lt;/sub&gt;(''A'') = ''h''&lt;sub&gt;min&lt;/sub&gt;(''B'')}} exactly when the minimum hash value of the union {{math|''A'' &amp;cup; ''B''}} lies in the intersection {{math|''A'' &amp;cap; ''B''}}.
Therefore,
:{{math|1=Pr[''h''&lt;sub&gt;min&lt;/sub&gt;(''A'') = ''h''&lt;sub&gt;min&lt;/sub&gt;(''B'')] = ''J''(''A'',''B'').}}
In other words, if {{math|''r''}} is a random variable that is one when {{math|1=''h''&lt;sub&gt;min&lt;/sub&gt;(''A'') = ''h''&lt;sub&gt;min&lt;/sub&gt;(''B'')}} and zero otherwise, then {{math|''r''}} is an [[Bias of an estimator|unbiased estimator]] of {{math|''J''(''A'',''B'')}}, although it has too high a [[variance]] to be useful on its own. The idea of the MinHash scheme is to reduce the variance by averaging together several variables constructed in the same way.

==Algorithm==

===Variant with many hash functions===
The simplest version of the minhash scheme uses {{math|''k''}} different hash functions, where {{math|''k''}} is a fixed integer parameter, and represents each set {{math|''S''}} by the {{math|''k''}} values of {{math|''h''&lt;sub&gt;min&lt;/sub&gt;(''S'')}} for these {{math|''k''}} functions.

To estimate {{math|''J''(''A'',''B'')}} using this version of the scheme, let {{math|''y''}} be the number of hash functions for which {{math|1=''h''&lt;sub&gt;min&lt;/sub&gt;(''A'') = ''h''&lt;sub&gt;min&lt;/sub&gt;(''B'')}}, and use {{math|''y''/''k''}} as the estimate. This estimate is the average of {{math|''k''}} different 0-1 random variables, each of which is one when {{math|1=''h''&lt;sub&gt;min&lt;/sub&gt;(''A'') = ''h''&lt;sub&gt;min&lt;/sub&gt;(''B'')}} and zero otherwise, and each of which is an unbiased estimator of  {{math|''J''(''A'',''B'')}}. Therefore, their average is also an unbiased estimator, and by standard [[Chernoff bound]]s for sums of 0-1 random variables, its expected error is {{math|O(1/&amp;radic;''k'')}}.&lt;ref&gt;{{citation|first=Sergey|last=Vassilvitskii|year=2011|title=COMS 6998-12: Dealing with Massive Data (lecture notes, Columbia university)|url=http://www.cs.columbia.edu/~coms699812/lecture1.pdf}}.&lt;/ref&gt;

Therefore, for any constant {{math|&amp;epsilon; &gt; 0}} there is a constant {{math|1=''k'' = O(1/&amp;epsilon;&lt;sup&gt;2&lt;/sup&gt;)}} such that the expected error of the estimate is at most&amp;nbsp;{{math|&amp;epsilon;}}.  For example, 400 hashes would be required to estimate {{math|''J''(''A'',''B'')}} with an expected error less than or equal to .05.

===Variant with a single hash function===
It may be computationally expensive to compute multiple hash functions, but a related version of MinHash scheme avoids this penalty by using only a single hash function and uses it to select multiple values from each set rather than selecting only a single minimum value per hash function. Let {{math|''h''}} be a hash function, and let {{math|''k''}} be a fixed integer. If {{math|''S''}} is any set of {{math|''k''}} or more values in the domain of {{math|''h''}},
define {{math|''h''&lt;sub&gt;(''k'')&lt;/sub&gt;(''S'')}} to be the subset of the {{math|''k''}} members of {{math|''S''}} that have the smallest values of {{math|''h''}}. This subset {{math|''h''&lt;sub&gt;(''k'')&lt;/sub&gt;(''S'')}} is used as a ''signature'' for the set {{math|''S''}}, and the similarity of any two sets is estimated by comparing their signatures.

Specifically, let ''A'' and ''B'' be any two sets.
Then {{math|1=''X'' = ''h''&lt;sub&gt;(''k'')&lt;/sub&gt;(''h''&lt;sub&gt;(''k'')&lt;/sub&gt;(''A'') &amp;cup; ''h''&lt;sub&gt;(''k'')&lt;/sub&gt;(''B'')) = ''h''&lt;sub&gt;(''k'')&lt;/sub&gt;(''A'' &amp;cup; ''B'')}} is a set of ''k'' elements of {{math|''A'' &amp;cup; ''B''}}, and if ''h'' is a random function then any subset of ''k'' elements is equally likely to be chosen; that is, {{math|''X''}} is a [[simple random sample]] of {{math|''A'' &amp;cup; ''B''}}. The subset {{math|1=''Y'' = ''X'' &amp;cap; ''h''&lt;sub&gt;(''k'')&lt;/sub&gt;(''A'') &amp;cap; ''h''&lt;sub&gt;(''k'')&lt;/sub&gt;(''B'')}} is the set of members of {{math|''X''}} that belong to the intersection {{math|''A'' &amp;cap; ''B''}}. Therefore, |{{math|''Y''}}|/{{math|''k''}} is an unbiased estimator of {{math|''J''(''A'',''B'')}}. The difference between this estimator and the estimator produced by multiple hash functions is that {{math|''X''}} always has exactly {{math|''k''}} members, whereas the multiple hash functions may lead to a smaller number of sampled elements due to the possibility that two different hash functions may have the same minima. However, when {{math|''k''}} is small relative to the sizes of the sets, this difference is negligible.

By standard [[Chernoff bound]]s for sampling without replacement, this estimator has expected error {{math|O(1/&amp;radic;''k'')}}, matching the performance of the multiple-hash-function scheme.

===Time analysis===
The estimator {{math|&amp;#124;''Y''&amp;#124;/''k''}} can be computed in time {{math|O(''k'')}} from the two signatures of the given sets, in either variant of the scheme. Therefore, when {{math|&amp;epsilon;}} and {{math|''k''}} are constants, the time to compute the estimated similarity from the signatures is also constant. The signature of each set can be computed in [[linear time]] on the size of the set, so when many pairwise similarities need to be estimated this method can lead to a substantial savings in running time compared to doing a full comparison of the members of each set.  Specifically, for set size {{math|''n''}} the many hash variant takes {{math|O(''n'' ''k'')}} time.  The single hash variant is generally faster, requiring {{math|O(''n'' log ''k'')}} time to maintain the sorted list of minima.{{citation needed|date=October 2012}}

==Min-wise independent permutations==
In order to implement the MinHash scheme as described above, one needs the hash function {{math|''h''}} to define a random [[permutation]] on {{math|''n''}} elements, where {{math|''n''}} is the total number of distinct elements in the union of all of the sets to be compared.
But because there are {{math|''n''!}} different permutations, it would require {{math|&amp;Omega;(''n'' log ''n'')}} bits just to specify a truly random permutation, an infeasibly large number for even moderate values of {{math|''n''}}. Because of this fact, by analogy to the theory of [[universal hashing]], there has been significant work on finding a family of permutations that is &quot;min-wise independent&quot;, meaning that for any subset of the domain, any element is equally likely to be the minimum. It has been established that a min-wise independent family of permutations must include at least
:&lt;math&gt;lcm(1, 2, ..., n) \ge e^{n-o(n)}&lt;/math&gt;
different permutations, and therefore that it needs {{math|&amp;Omega;(''n'')}} bits to specify a single permutation, still infeasibly large.&lt;ref name=&quot;bcfm&quot;/&gt;

Because of this impracticality, two variant notions of min-wise independence have been introduced: restricted min-wise independent permutations families, and approximate min-wise independent families.
Restricted min-wise independence is the min-wise independence property restricted to certain sets of cardinality at most {{math|''k''}}.&lt;ref&gt;{{citation
 | last1 = Matoušek | first1 = Jiří | author1-link = Jiří Matoušek (mathematician)
 | last2 = Stojaković | first2 = Miloš
 | doi = 10.1002/rsa.10101
 | issue = 4
 | journal = Random Structures and Algorithms
 | pages = 397–408
 | title = On restricted min-wise independence of permutations
 | volume = 23
 | year = 2003}}.&lt;/ref&gt;
Approximate min-wise independence has at most a fixed probability {{math|&amp;epsilon;}} of varying from full independence.&lt;ref&gt;{{citation
 | last1 = Saks | first1 = M. | author1-link = Michael Saks (mathematician)
 | last2 = Srinivasan | first2 = A.
 | last3 = Zhou | first3 = S.
 | last4 = Zuckerman | first4 = D.
 | doi = 10.1016/S0020-0190(99)00163-5
 | issue = 1–2
 | journal = [[Information Processing Letters]]
 | pages = 29–32
 | title = Low discrepancy sets yield approximate min-wise independent permutation families
 | volume = 73
 | year = 2000}}.&lt;/ref&gt;

==Applications==
The original applications for MinHash involved clustering and eliminating near-duplicates among web documents, represented as sets of the words occurring in those documents.&lt;ref name=&quot;b97&quot;/&gt;&lt;ref name=&quot;bcfm&quot;/&gt; Similar techniques have also been used for clustering and near-duplicate elimination for other types of data, such as images: in the case of image data, an image can be represented as a set of smaller subimages cropped from it, or as sets of more complex image feature descriptions.&lt;ref&gt;{{citation
 | last1 = Chum | first1 = Ondřej
 | last2 = Philbin | first2 = James
 | last3 = Isard | first3 = Michael
 | last4 = Zisserman | first4 = Andrew
 | contribution = Scalable near identical image and shot detection
 | doi = 10.1145/1282280.1282359
 | title = Proceedings of the 6th ACM International Conference on Image and Cideo Retrieval (CIVR'07)
 | year = 2007}}; {{citation
 | last1 = Chum | first1 = Ondřej
 | last2 = Philbin | first2 = James
 | last3 = Zisserman | first3 = Andrew
 | contribution = Near duplicate image detection: min-hash and tf-idf weighting
 | page = 4
 | title = Proceedings of the British Machine Vision Conference
 | url = http://www.bmva.org/bmvc/2008/papers/119.pdf
 | volume = 3
 | year = 2008}}.&lt;/ref&gt;

In [[data mining]], {{harvtxt|Cohen|Datar|Fujiwara|Gionis|2001}} use MinHash as a tool for [[association rule learning]]. Given a database in which each entry has multiple attributes (viewed as a 0-1 matrix with a row per database entry and a column per attribute) they use MinHash-based approximations to the Jaccard index to identify candidate pairs of attributes that frequently co-occur, and then compute the exact value of the index for only those pairs to determine the ones whose frequencies of co-occurrence are below a given strict threshold.&lt;ref&gt;{{citation
 | last1 = Cohen | first1 = E.
 | last2 = Datar | first2 = M.
 | last3 = Fujiwara | first3 = S.
 | last4 = Gionis | first4 = A.
 | last5 = Indyk | first5 = P. | author5-link = Piotr Indyk
 | last6 = Motwani | first6 = R. | author6-link = Rajeev Motwani
 | last7 = Ullman | first7 = J. D. | author7-link = Jeffrey Ullman
 | last8 = Yang | first8 = C.
 | doi = 10.1109/69.908981
 | issue = 1
 | journal = IEEE Transactions on Knowledge and Data Engineering
 | pages = 64–78
 | title = Finding interesting associations without support pruning
 | volume = 13
 | year = 2001}}.&lt;/ref&gt;

==Other uses==
The MinHash scheme may be seen as an instance of [[locality sensitive hashing]], a collection of techniques for using hash functions to map large sets of objects down to smaller hash values in such a way that, when two objects have a small distance from each other, their hash values are likely to be the same. In this instance, the signature of a set may be seen as its hash value. Other locality sensitive hashing techniques exist for [[Hamming distance]] between sets and [[cosine distance]] between [[Euclidean vector|vector]]s; locality sensitive hashing has important applications in [[nearest neighbor search]] algorithms.&lt;ref&gt;{{citation
 | last1 = Andoni | first1 = Alexandr
 | last2 = Indyk | first2 = Piotr | author2-link = Piotr Indyk
 | doi = 10.1145/1327452.1327494
 | issue = 1
 | journal = [[Communications of the ACM]]
 | pages = 117–122
 | title = Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions
 | volume = 51
 | year = 2008}}.&lt;/ref&gt; For large distributed systems, and in particular [[MapReduce]], there exist modified versions of MinHash to help compute similarities with no dependence on the point dimension.&lt;ref&gt;
{{citation
 | last1 = Zadeh | first1 = Reza
 | last2 = Goel | first2 = Ashish
 | title = Dimension Independent Similarity Computation
 | arxiv = 1206.2082
 | year = 2012}}.&lt;/ref&gt;

==Evaluation and benchmarks==
A large scale evaluation has been conducted by [[Google]] in 2006 &lt;ref&gt;
{{citation
 | last1 = Henzinger | first1 = Monika
 | title = Finding near-duplicate web pages: a large-scale evaluation of algorithms
 | journal = Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM
 | url = http://infoscience.epfl.ch/record/99373/files/Henzinger06.pdf
 | year = 2006}}.&lt;/ref&gt;  to compare the performance of Minhash and [[Simhash]]&lt;ref&gt;{{citation
 | author = Charikar, Moses S.
 | title = Similarity estimation techniques from rounding algorithms
 | journal = Proceedings of the thiry-fourth annual ACM symposium on Theory of computing. ACM
 | year = 2002}}.&lt;/ref&gt; algorithms. In 2007 Google reported using Simhash for duplicate detection for web crawling&lt;ref&gt;
{{citation
 | last1 = Gurmeet Singh | first1 = Manku
 | last2 = Jain | first2 =  Arvind
 | last2 = Das Sarma | first2 =  Anish
 | title = Detecting near-duplicates for web crawling
 | journal = Proceedings of the 16th international conference on World Wide Web. ACM,
 | year = 2007}}.&lt;/ref&gt; and using Minhash and [[Locality-sensitive hashing|LSH]] for [[Google News]] personalization.&lt;ref&gt;
{{citation
 | author = Das, Abhinandan S., et al.
 | title = Google news personalization: scalable online collaborative filtering
 | journal = Proceedings of the 16th international conference on World Wide Web. ACM,
 | year = 2007}}.&lt;/ref&gt;

==See also==
*[[Approximate string matching]]
*[[Rolling hash]]
*[[w-shingling]]
*[[Tabulation hashing]]
*[[Bloom filter]]
*[[Count-Min sketch]]
*[[Set cover problem]]
*[[Levenshtein distance]]
*[[String metric]]

==External links==
*  [http://infolab.stanford.edu/~ullman/mmds.html  Mining of Massive Datasets, Ch. 3. Finding similar Items ]
* [http://moultano.wordpress.com/article/simple-simhashing-3kbzhsxyg4467-6/ Simple Simhashing]
* [http://blogs.msdn.com/b/spt/archive/2008/06/10/set-similarity-and-min-hash.aspx Set Similarity &amp; MinHash - C# implementation]
*[http://blogs.msdn.com/b/spt/archive/2008/06/11/locality-sensitive-hashing-lsh-and-min-hash.aspx Minhash with LSH for all-pair search (C# implementation)]
* [http://mymagnadata.wordpress.com/2011/01/04/minhash-java-implementation/ MinHash – Java implementation]
*[https://code.google.com/p/google-all-pairs-similarity-search/ All pairs similarity search (Google Research)]
*[http://reference.wolfram.com/mathematica/guide/DistanceAndSimilarityMeasures.html Distance and Similarity Measures(Wolfram Alpha)]
*[https://code.google.com/p/py-nilsimsa/source/browse/trunk/nilsimsa/__init__.py Nilsimsa hash (Python implementation) ]
*[http://matpalm.com/resemblance/simhash/ Simhash]

==References==
{{reflist}}

[[Category:Hash functions]]
[[Category:Clustering criteria]]
[[Category:Hashing]]
[[Category:Probabilistic data structures]]</text>
      <sha1>o6u7sygjhjg65p36tqbllsei00yzj5j</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Rabin–Karp algorithm</title>
    <ns>0</ns>
    <id>684698</id>
    <revision>
      <id>608756349</id>
      <parentid>608495077</parentid>
      <timestamp>2014-05-15T23:27:15Z</timestamp>
      <contributor>
        <username>Glrx</username>
        <id>2289521</id>
      </contributor>
      <comment>/* See also */ Rolling hash is in body</comment>
      <text xml:space="preserve" bytes="10704">In [[computer science]], the '''Rabin–Karp algorithm''' or '''Karp–Rabin algorithm''' is a [[string searching algorithm]] created by {{harvs|first1=Richard M.|last1=Karp|author1-link=Richard M. Karp|first2=Michael O.|last2=Rabin|author2-link=Michael O. Rabin|year=1987|txt}} that uses [[Hash function|hashing]] to find any one of a set of pattern strings in a text. For text of length ''n'' and ''p'' patterns of combined length ''m'', its average and best case running time is [[Big-O notation|O]](''n''+''m'') in space O(''p''), but its worst-case time is O(''nm''). In contrast, the [[Aho–Corasick string matching algorithm]] has asymptotic worst-time complexity O(''n''+''m'') in space O(''m'').

A practical application of the algorithm is detecting [[plagiarism]]. Given source material, the algorithm can rapidly search through a paper for instances of sentences from the source material, ignoring details such as case and punctuation. Because of the abundance of the sought strings, single-string searching algorithms are impractical.

== Shifting substrings search and competing algorithms ==
A brute-force substring search algorithm checks all possible positions:

&lt;syntaxhighlight lang=&quot;php&quot; line&gt;
function NaiveSearch(string s[1..n], string sub[1..m])
   for i from 1 to n-m+1
      for j from 1 to m
         if s[i+j-1] ≠ sub[j]
            jump to next iteration of outer loop
      return i
   return not found
&lt;/syntaxhighlight&gt;

This algorithm works well in many practical cases, but can exhibit relatively long running times on certain examples, such as searching for a string of 10,000 &quot;a&quot;s followed by a &quot;b&quot; in a string of 10 million &quot;a&quot;s, in which case it exhibits its worst-case [[Big-O notation|O]](''mn'') time.

The [[Knuth–Morris–Pratt algorithm]] reduces this to [[Big-O notation|O]](''n'') time using precomputation to examine each text character only once; the [[Boyer–Moore string search algorithm|Boyer–Moore algorithm]] skips forward not by 1 character, but by as many as possible for the search to succeed, effectively decreasing the number of times we iterate through the outer loop, so that the number of characters examined can be as small as ''n/m'' in the best case.  The Rabin–Karp algorithm focuses instead on speeding up lines 3-6.

== Use of hashing for shifting substring search ==
Rather than pursuing more sophisticated skipping, the Rabin–Karp algorithm seeks to speed up the testing of equality of the pattern to the substrings in the text by using a [[hash function]]. A hash function is a function which converts every string into a numeric value, called its ''hash value''; for example, we might have hash(&quot;hello&quot;)=5. The algorithm exploits the fact that if two strings are equal, their hash values are also equal. Thus, it would seem all we have to do is compute the hash value of the substring we're searching for, and then look for a substring with the same hash value.

However, there are two problems with this. First, because there are so many different strings, to keep the hash values small we have to assign some strings the same number. This means that if the hash values match, the strings might not match; we have to verify that they do, which can take a long time for long substrings. Luckily, a good hash function promises us that on most reasonable inputs, this won't happen too often, which keeps the average search time good.

The algorithm is as shown:

&lt;syntaxhighlight lang=&quot;php&quot; line&gt;
function RabinKarp(string s[1..n], string sub[1..m])
hsub := hash(sub[1..m]);  hs := hash(s[1..m])
for i from 1 to n-m+1
  if hs = hsub
    if s[i..i+m-1] = sub
      return i
  hs := hash(s[i+1..i+m])
return not found
&lt;/syntaxhighlight&gt;

Lines 2, 5, and 7 each require [[Big-O notation|O]](m) time. However, line 2 is only executed once, and line 5 is only executed if the hash values match, which is unlikely to happen more than a few times. Line 4 is executed ''n'' times, but only requires constant time. So the only problem is line 7.

If we naively recompute the hash value for the substring &lt;code&gt;s[i+1..i+m]&lt;/code&gt;, this would require [[Big-O notation|O]](''m'') time, and since this is done on each loop, the algorithm would require [[Big-O notation|O]](mn) time, the same as the most naive algorithms. The trick to solving this is to note that the variable &lt;code&gt;hs&lt;/code&gt; already contains the hash value of &lt;code&gt;s[i..i+m-1]&lt;/code&gt;. If we can use this to compute the next hash value in constant time, then our problem will be solved.

We do this using what is called a [[rolling hash]]. A rolling hash is a hash function specially designed to enable this operation. One simple example is adding up the values of each character in the substring. Then, we can use this formula to compute the next hash value in constant time:
&lt;pre&gt;
s[i+1..i+m] = s[i..i+m-1] - s[i] + s[i+m]
&lt;/pre&gt;
This simple function works, but will result in statement 5 being executed more often than other more sophisticated rolling hash functions such as those discussed in the next section.

Notice that if we're very unlucky, or have a very bad hash function such as a constant function, line 5 might very well be executed ''n'' times, on every iteration of the loop. Because it requires [[Big-O notation|O]](m) time, the whole algorithm then takes a worst-case [[Big-O notation|O]](mn) time.

== Hash function used ==
The key to the Rabin–Karp algorithm's performance is the efficient computation of [[hash value]]s of the successive substrings of the text. One popular and effective rolling hash function treats every substring as a number in some base, the base being usually a large [[prime number|prime]]. For example, if the substring is &quot;hi&quot; and the base is 101, the hash value would be 104 &amp;times; 101&lt;sup&gt;1&lt;/sup&gt; + 105 &amp;times; 101&lt;sup&gt;0&lt;/sup&gt; = 10609 ([[ASCII]] of 'h' is 104 and of 'i' is 105).

Technically, this algorithm is only similar to the true number in a non-decimal system representation, since for example we could have the &quot;base&quot; less than one of the &quot;digits&quot;. See [[hash function]] for a much more detailed discussion. The essential benefit achieved by such representation is that it is possible to compute the hash value of the next substring from the previous one by doing only a constant number of operations, independent of the substrings' lengths.

For example, if we have text &quot;abracadabra&quot; and we are searching for a pattern of length 3, we can compute the hash of &quot;bra&quot; from the hash for &quot;abr&quot; (the previous substring) by subtracting the number added for the first 'a' of &quot;abr&quot;, i.e.  97 &amp;times; 101&lt;sup&gt;2&lt;/sup&gt; (97 is ASCII for 'a' and 101 is the base we are using), multiplying by the base and adding for the last a of &quot;bra&quot;, i.e. 97 &amp;times; 101&lt;sup&gt;0&lt;/sup&gt; = 97. If the substrings in question are long, this algorithm achieves great savings compared with many other hashing schemes.

Theoretically, there exist other algorithms that could provide convenient recomputation, e.g. multiplying together ASCII values of all characters so that shifting substring would only entail dividing by the first character and multiplying by the last. The limitation, however, is the limited size of the integer [[data type]] and the necessity of using [[modular arithmetic]] to scale down the hash results, (see [[hash function]] article). Meanwhile, naive hash functions do not produce large numbers quickly, but, just like adding ASCII values, are likely to cause many [[hash collision]]s and hence slow down the algorithm. Hence the described hash function is typically the preferred one in the Rabin–Karp algorithm.

== Multiple pattern search ==
The Rabin–Karp algorithm is inferior for single pattern searching to [[Knuth–Morris–Pratt algorithm]], [[Boyer–Moore string search algorithm]] and other faster single pattern [[string searching algorithm]]s because of its slow worst case behavior. However, it is an algorithm of choice for [[String searching algorithm#Algorithms using finite set of patterns|multiple pattern search]].

That is, if we want to find any of a large number, say ''k'', fixed length patterns in a text, we can create a simple variant of the Rabin–Karp algorithm that uses a [[Bloom filter]] or a [[set data structure]] to check whether the hash of a given string belongs to a set of hash values of patterns we are looking for:

&lt;syntaxhighlight lang=&quot;php&quot; line&gt;
function RabinKarpSet(string s[1..n], set of string subs, m):
    set hsubs := emptySet
    foreach sub in subs
        insert hash(sub[1..m]) into hsubs
    hs := hash(s[1..m])
    for i from 1 to n-m+1
        if hs ∈ hsubs and s[i..i+m-1] ∈ subs
            return i
        hs := hash(s[i+1..i+m])
    return not found
&lt;/syntaxhighlight&gt;

We assume all the substrings have a fixed length ''m''.

A naïve way to search for ''k'' patterns is to repeat a
single-pattern search taking [[Big-O notation|O]](''n'') time, totalling in [[Big-O notation|O]](''n'' ''k'') time. In contrast, the variant algorithm above can find all ''k'' patterns in [[Big-O notation|O]](''n''+''k'') time in expectation, because a hash table checks whether a substring hash equals any of the pattern hashes in [[Big-O notation|O]](1) time.

==References==

{{Reflist}}
* {{Cite journal |last1=Karp|first1= Richard M. | authorlink=Richard Karp | last2=Rabin|first2=Michael O.|author2-link=Michael O. Rabin | title=Efficient randomized pattern-matching algorithms |date=March 1987 |journal=IBM Journal of Research and Development  |volume=31 |issue=2|pages=249–260|doi=10.1147/rd.312.0249 |url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.9502&amp;rep=rep1&amp;type=pdf|accessdate=2013-09-24|ref=harv}}
* {{Cite book |author=Cormen, Thomas H. |authorlink=Thomas H. Cormen |coauthors=[[Charles E. Leiserson|Leiserson, Charles E.]]; [[Ronald L. Rivest|Rivest, Ronald L.]]; [[Clifford Stein|Stein, Clifford]] |editor= |others= |title=[[Introduction to Algorithms]] |origyear=1990 |edition=2nd |date=2001-09-01 |publisher=MIT Press |location=[[Cambridge, Massachusetts]] |isbn=978-0-262-03293-3 |pages=911–916 |chapter=The Rabin–Karp algorithm}}
* {{cite book|author1=K. Selçuk Candan|author2=Maria Luisa Sapino|title=Data Management for Multimedia Retrieval|url=http://books.google.com/books?id=Uk9tyXgQME8C&amp;pg=PA205|year=2010|publisher=Cambridge University Press|isbn=978-0-521-88739-7|pages=205–206}} (for the Bloom filter extension)

==External links==
*[http://courses.csail.mit.edu/6.006/spring11/rec/rec06.pdf MIT 6.006: Introduction to Algorithms 2011- Lecture Notes - Rabin–Karp Algorithm/Rolling Hash]

{{DEFAULTSORT:Rabin-Karp String Search Algorithm}}
[[Category:String matching algorithms]]
[[Category:Hashing]]</text>
      <sha1>exkax1kcv9fzycui27tseyt59c24ia5</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hash tree (persistent data structure)</title>
    <ns>0</ns>
    <id>39393649</id>
    <revision>
      <id>604716589</id>
      <parentid>578857060</parentid>
      <timestamp>2014-04-18T10:06:34Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>merges not supported on talk page</comment>
      <text xml:space="preserve" bytes="1021">In computer science, a '''hash tree''' (or '''hash [[trie]]''') is a [[persistent data structure]] that can be used to implement [[Set (abstract data type)|sets]] and [[Associative array|maps]], intended to replace [[hash table]]s in [[purely functional]] programming. In its basic form, a hash tree stores the [[Hash function|hashes]] of its keys, regarded as strings of bits, in a trie, with the actual keys and (optional) values stored at the trie's &quot;final&quot; nodes.&lt;ref name=&quot;bagwell&quot;&gt;{{cite report
|title=Ideal Hash Trees
|author=Phil Bagwell
|publisher=Infoscience Department, [[École Polytechnique Fédérale de Lausanne]]
|url=http://infoscience.epfl.ch/record/64398/files/idealhashtrees.pdf
|year=2000
}}&lt;/ref&gt;

[[Hash array mapped trie]]s and [[Ctrie]]s are refined versions of this data structure, using particular type of trie implementations.&lt;ref name=&quot;bagwell&quot;/&gt;

==References==
{{reflist}}

{{CS-Trees}}
{{Data structures}}

[[Category:Functional data structures]]
[[Category:Hashing]]


{{compu-prog-stub}}</text>
      <sha1>0fkpppw5tua6uzaiu0ql30kwxheqlxy</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Static Hashing</title>
    <ns>0</ns>
    <id>39255377</id>
    <revision>
      <id>617691796</id>
      <parentid>617689983</parentid>
      <timestamp>2014-07-20T10:30:17Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>/* References */clean up using [[Project:AWB|AWB]] (10316)</comment>
      <text xml:space="preserve" bytes="3126">{{multiple issues|
{{Underlinked|date=December 2013}}
{{Orphan|date=July 2013}}
}}

'''Static Hashing''' is another form of the hashing problem which allows users to perform lookups on a finalized dictionary set (all objects in the dictionary are final and not changing).

== Usage &lt;ref name=&quot;Roche2013&quot;&gt;{{cite book | author = [[Daniel Roche]] | year = 2013 | title = SI486D: Randomness in Computing, Hashing Unit | publisher = United States Naval Academy, Computer Science Department | url = http://www.usna.edu/Users/cs/roche/courses/s13si486d/u04/#static-and-perfect-hashing}}&lt;/ref&gt; ==

===Application===
Since static hashing requires that the [[database]], its objects and reference remain the same its applications are limited. Databases which contain information which changes rarely are also eligible as it would only require a full rehash of the entire database on rare occasion. Examples of this include sets of words and definitions of specific languages, sets of significant data for an organization's personnel, etc.

===Perfect Hashing===
Perfect hashing is a model of hashing in which any set of n elements can be stored in a [[hash table]] of equal size and can have lookups done in constant time. It was specifically discovered and discussed by Fredman, Komlos and Szemeredi (1984) and has therefore been nicknamed &quot;FKS Hashing&quot;.&lt;ref name=&quot;FKS1984&quot;&gt;{{cite book | author = Michael Fredman, Janos Komlos, Endre Szemeredi | year = 1984 | title = Storing a Sparse Table with O(1) Worst Case Access Time | publisher = Journal of the ACM (Volume 31, Issue 3) | url = http://dl.acm.org/citation.cfm?id=1884}}&lt;/ref&gt;

==FKS Hashing==

FKS Hashing makes use of a hash table with two levels in which the top level contains n buckets which each contain their own hash table. FKS hashing requires that if collisions occur they must do so only on the top level.

===Implementation===
The top level contains a randomly created hash function, h(x), which fits within the constraints of a Carter and Wegman hash function - seen in [[Universal hashing]]. Having done so the top level shall contain n buckets labeled k&lt;sub&gt;1&lt;/sub&gt;, k&lt;sub&gt;2&lt;/sub&gt;, k&lt;sub&gt;2&lt;/sub&gt;, ..., k&lt;sub&gt;n&lt;/sub&gt;. Following this pattern, all of the buckets hold a hash table of size s&lt;sub&gt;i&lt;/sub&gt; and a respective hash function, h&lt;sub&gt;i&lt;/sub&gt;(x). The hash function will be decided by setting s&lt;sub&gt;i&lt;/sub&gt; to k&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; and randomly going through functions until there are no collisions. This can be done in constant time.

===Performance===
Because there are &quot;n choose 2&quot; pairs of elements, of which have a probability of collision equal to 1/n, FKS hashing can expect to have strictly less than n/2 collisions. Based on this fact and that each h(x) was selected so that the number of collisions would be at most n/2, the size of each table on the lower level will be no greater than 2n.

== References ==
&lt;!--- See [[Wikipedia:Footnotes]] on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

&lt;!--- Categories ---&gt;
[[Category:Articles created via the Article Wizard]]
[[Category:Hashing]]</text>
      <sha1>t2rl06y6czw7sphhtw440f8qxnmvgaw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Count–min sketch</title>
    <ns>0</ns>
    <id>33410671</id>
    <revision>
      <id>616514352</id>
      <parentid>616513547</parentid>
      <timestamp>2014-07-11T12:05:02Z</timestamp>
      <contributor>
        <ip>130.60.6.54</ip>
      </contributor>
      <text xml:space="preserve" bytes="3348">The '''Count–min sketch''' (or '''CM sketch''') is a [[Randomized algorithm|probabilistic]] [[Complexity class|sub-linear space]] [[streaming algorithm]] which can be used to summarize a data stream in many different ways. The algorithm was invented in 2003 by [[Graham Cormode]] and [[S. Muthu Muthukrishnan]].&lt;ref&gt;{{cite journal|last=Cormode|first=Graham|author2=S. Muthukrishnan|title=An Improved Data Stream Summary: The Count-Min Sketch and its Applications|journal=J. Algorithms|year=2004|volume=55|pages=29–38|accessdate=14 October 2011}}&lt;/ref&gt;

Count–min sketches are somewhat similar to [[Bloom filter]]s; the main distinction is that Bloom filters represent sets, while CM sketches represent [[multiset]]s and [[Frequency (statistics)|frequency tables]]. Spectral Bloom filters with multi-set policy, are conceptually isomorphic to the Count-Min Sketch.&lt;ref&gt;{{cite web |url=http://matthias.vallentin.net/blog/2011/06/a-garden-variety-of-bloom-filters/ |title=A Garden Variety of Bloom Filters |last1=Vallentin |first1=Matthias  |last2= |first2= |date=11 July 2014 |website= |publisher= |accessdate=}}&lt;/ref&gt;

== Algorithm ==

=== Setup ===
The data structure is parameterized by the constants &lt;math&gt;w&lt;/math&gt; and &lt;math&gt;d&lt;/math&gt; which determine the time and space needs and the probability of error of the queries. The algorithm needs a [[Array_data_structure#Two-dimensional_arrays|two dimensional array]], called here ''count'', with &lt;math&gt;w&lt;/math&gt; columns and &lt;math&gt;d&lt;/math&gt; rows. A series of &lt;math&gt;d&lt;/math&gt; hash functions must be randomly drawn from a [[Pairwise independence|pairwise independent]] [[hash function]] family, each associated with a row in the array.

For later convenience we assign &lt;math&gt;w = \lceil e/\epsilon \rceil&lt;/math&gt; and &lt;math&gt; d = \lceil \ln{1/\delta} \rceil&lt;/math&gt;, where the error in answering a query is within a factor of &lt;math&gt;\epsilon&lt;/math&gt; with probability &lt;math&gt;\delta&lt;/math&gt;.

=== Update ===
When a new value &lt;math&gt;a&lt;/math&gt; arrives we update as follows: &lt;math&gt;\forall j : 1 \leq j \leq d&lt;/math&gt;, &lt;math&gt; count[j,h_j(a)] \leftarrow count[j,h_j(a)] + 1 &lt;/math&gt;. That is, for each row we take the corresponding hash function, apply it to the newly received value and add one to the column corresponding to the hash value.

=== Query ===
The array can then be used to estimate any of several different statistics at any point. If we want to estimate, for instance, the number of times &lt;math&gt; a_i&lt;/math&gt; for a specific value &lt;math&gt;i&lt;/math&gt; appeared so far in the stream we would compute &lt;math&gt;\hat a_i=\min_j count[j,h_j(i)]&lt;/math&gt; (this assumes all added values are positive). This estimate has the guarantee that &lt;math&gt;\hat a_i \leq a_i + \epsilon |a| &lt;/math&gt; with probability &lt;math&gt;1-\delta&lt;/math&gt;.

Small modifications to the data structure can be used to sketch other different stream statistics.

== External links ==
* [http://www.corelab.ece.ntua.gr/courses/ds.grad/count-min.ppt Powerpoint presentation on the algorithm]
* [https://sites.google.com/site/countminsketch/home/faq Count–min FAQ]
* [http://www.cs.rutgers.edu/~muthu/countmin.c C code by Cormode]

==See also==
* [[Bloom filter]]
* [[Feature hashing]]
* [[Locality-sensitive hashing]]
* [[MinHash]]

== References ==
&lt;references /&gt;

{{DEFAULTSORT:Count-min sketch}}
[[Category:Hashing]]
[[Category:Probabilistic data structures]]</text>
      <sha1>5ao3vxp1d6klc0gng96xcu7hrl5615t</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hash-based message authentication code</title>
    <ns>0</ns>
    <id>17112</id>
    <revision>
      <id>621106612</id>
      <parentid>619575086</parentid>
      <timestamp>2014-08-13T19:53:50Z</timestamp>
      <contributor>
        <username>Seattleandrew</username>
        <id>13581450</id>
      </contributor>
      <minor/>
      <comment>Aligned HMAC results to more clearly show the difference in result lengths</comment>
      <text xml:space="preserve" bytes="14645">[[File:SHAhmac.svg|thumb|right|400px|SHA-1 HMAC Generation.]]In [[cryptography]], a '''keyed-hash message authentication code''' ('''HMAC''') is a specific construction for calculating a [[message authentication code]] (MAC) involving a [[cryptographic hash function]] in combination with a secret [[cryptographic key]].  As with any MAC, it may be used to simultaneously verify both the ''[[data integrity]]'' and the ''[[authentication]]'' of a [[cleartext|message]].  Any cryptographic hash function, such as [[MD5]] or [[SHA-1]], may be used in the calculation of an HMAC; the resulting MAC algorithm is termed HMAC-MD5 or HMAC-SHA1 accordingly. The cryptographic strength of the HMAC depends upon the [[cryptographic strength]] of the underlying hash function, the size of its hash output, and on the size and quality of the key.

An iterative hash function breaks up a message into blocks of a fixed size and iterates over them with a [[One-way compression function|compression function]]. For example, MD5 and SHA-1 operate on 512-bit blocks. The size of the output of HMAC is the same as that of the underlying hash function (128 or 160 bits in the case of MD5 or SHA-1, respectively), although it can be truncated if desired.

The definition and analysis of the HMAC construction was first published in 1996 by [[Mihir Bellare]], Ran Canetti, and Hugo Krawczyk,&lt;ref name=BCK96/&gt; who also wrote RFC 2104.  This paper also defined a variant called NMAC that is rarely, if ever, used. [[Federal Information Processing Standard|FIPS]] PUB 198 generalizes and standardizes the use of HMACs. HMAC-SHA1 and HMAC-MD5 are used within the [[IPsec]] and [[Transport Layer Security|TLS]] protocols.

==Definition (from RFC 2104)==

&lt;math&gt;
\textit{HMAC}(K, m) = H \left( (K \oplus opad) | H((K \oplus ipad) | m)\right)
&lt;/math&gt;

where
:''H'' is a cryptographic hash function,
:''K'' is a secret key [[Padding (cryptography)|padded]] to the right with extra zeros to the input block size of the hash function, or the hash of the original key if it's longer than that block size,
:''m'' is the message to be authenticated,
:| denotes [[concatenation]],
:⊕ denotes [[exclusive or]] (XOR),
:''opad'' is the outer padding (0x5c5c5c…5c5c, one-block-long [[hexadecimal]] constant),
:and {{Sic|''ipad''|hide=yes}} is the inner padding (0x363636…3636, one-block-long [[hexadecimal]] constant).

==Implementation==
The following [[pseudocode]] demonstrates how HMAC may be implemented.  Blocksize is 64 (bytes) when using one of the following hash functions: SHA-1, MD5, RIPEMD-128/160.&lt;ref&gt;[http://tools.ietf.org/html/rfc2104 RFC 2104], section 2, &quot;Definition of HMAC&quot;, page 3.&lt;/ref&gt;

 '''function''' hmac (key, message)
     '''if''' (length(key) &gt; blocksize) '''then'''
         key = hash(key) &lt;span style=&quot;color: green;&quot;&gt;''// keys longer than blocksize are shortened''&lt;/span&gt;
     '''end if'''
     '''if''' (length(key) &lt; blocksize) '''then'''
         key = key ∥ [0x00 * (blocksize - length(key))] &lt;span style=&quot;color:green;&quot;&gt;''// keys shorter than blocksize are zero-padded (where ''∥'' is concatenation)''&lt;/span&gt;
     '''end if'''
    
     o_key_pad = [0x5c * blocksize] ⊕ key &lt;span style=&quot;color: green;&quot;&gt;''// Where blocksize is that of the underlying hash function''&lt;/span&gt;
     i_key_pad = [0x36 * blocksize] ⊕ key &lt;span style=&quot;color: green;&quot;&gt;''// Where ⊕ is exclusive or (XOR)''&lt;/span&gt;
    
     '''return''' hash(o_key_pad ∥ hash(i_key_pad ∥ message)) &lt;span style=&quot;color: green;&quot;&gt;''// Where ''∥'' is concatenation''&lt;/span&gt;
 '''end function'''

The following is a [[Python (programming language)|Python]] implementation of HMAC-MD5:
&lt;source lang=&quot;python&quot;&gt;
#!/usr/bin/env python

from hashlib import md5

trans_5C = bytearray((x ^ 0x5c) for x in range(256))
trans_36 = bytearray((x ^ 0x36) for x in range(256))
blocksize = md5().block_size # 64

def hmac_md5(key, msg):
    if len(key) &gt; blocksize:
        key = md5(key).digest()
    key = key + bytearray(blocksize - len(key))
    o_key_pad = key.translate(trans_5C)
    i_key_pad = key.translate(trans_36)
    return md5(o_key_pad + md5(i_key_pad + msg).digest())

if __name__ == &quot;__main__&quot;:
    # This is one example from the appendix of RFC 2104    
    h = hmac_md5(b&quot;Jefe&quot;, b&quot;what do ya want for nothing?&quot;)
    print(h.hexdigest()) # 750c783e6ab0b503eaa86e310a5db738
&lt;/source&gt;

Python includes a hmac module,&lt;ref&gt;{{Citation|url=https://docs.python.org/dev/library/hmac|title=hmac — Keyed-Hashing for Message Authentication|publisher=[[Python Software Foundation]]|accessdate=7 May 2014}}&lt;/ref&gt; so the function above can be replaced by a shorter version.
&lt;source lang=&quot;python&quot;&gt;
def hmac_md5(key, msg):
    return hmac.HMAC(key, msg, md5)
&lt;/source&gt;

== Example usage ==
A business that suffers from attackers that place fraudulent [[Internet]] orders may insist that all its customers deposit a secret [[Symmetric-key algorithm|symmetric key]] with them. Along with an order, a customer must supply the order's HMAC digest, computed using the customer's key. The business, knowing the customer's key, can then verify that the order originated from the stated customer and has not been tampered with.

==Design principles==

The design of the HMAC specification was motivated by the existence of attacks on more trivial mechanisms for combining a key with a hash function.  For example, one might assume the same security that HMAC provides could be achieved with MAC = '''H'''(''key'' ∥ ''message'').  However, this method suffers from a serious flaw: with most hash functions, it is easy to append data to the message without knowing the key and obtain another valid MAC (&quot;[[Length extension attack|length-extension attack]]&quot;).  The alternative, appending the key using MAC = '''H'''(''message'' ∥ ''key''), suffers from the problem that an attacker who can find a collision in the (unkeyed) hash function has a collision in the MAC (as two messages m1 and m2 yielding the same hash will provide the same start condition to the hash function before the appended key is hashed, hence the final hash will be the same).   Using MAC = '''H'''(''key'' ∥ ''message'' ∥ ''key'') is better, however various security papers have suggested vulnerabilities with this approach, even when two different keys are used.&lt;ref name=BCK96&gt;{{Cite web |title=Keying Hash Functions for Message Authentication |first1=Mihir |last1=Bellare |authorlink1=Mihir Bellare |first2=Ran |last2=Canetti |first3=Hugo |last3=Krawczyk |url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.134.8430 |year=1996 |postscript=.}}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title=MDx-MAC and Building Fast MACs from Hash Functions |year=1995 |first1=Bart |last1=Preneel |authorlink1=Bart Preneel |first2=Paul C. |last2=van Oorschot |authorlink2=Paul van Oorschot |url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.3855 |accessdate=28 August 2009 |postscript=.}}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title=On the Security of Two MAC Algorithms |year=1995 |first1=Bart |last1=Preneel |authorlink1=Bart Preneel |first2=Paul C. |last2=van Oorschot |authorlink2=Paul van Oorschot |url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.8908 |accessdate=28 August 2009 |postscript=.}}&lt;/ref&gt;

No known extensions attacks have been found against the current HMAC specification which is defined as '''H'''(''key'' ∥ '''H'''(''key'' ∥ ''message'')) because the outer application of the hash function masks the intermediate result of the internal hash.  The values of ''ipad'' and ''opad'' are not critical to the security of the algorithm, but were defined in such a way to have a large [[Hamming distance]] from each other and so the inner and outer keys will have fewer bits in common. The security reduction of HMAC does require them to be different in at least one bit.

The [[Keccak]] hash function, that was selected by [[NIST]] as the [[SHA-3]] competition winner, doesn't need this nested approach and can be used to generate a MAC by simply prepending the key to the message.&lt;ref&gt;{{cite web
| url=http://keccak.noekeon.org
| title=Strengths of Keccak - Design and security
| quote=''Unlike SHA-1 and SHA-2, Keccak does not have the length-extension weakness, hence does not need the HMAC nested construction. Instead, MAC computation can be performed by simply prepending the message with the key.''
| author=Keccak team
| accessdate=30 January 2013}}
&lt;/ref&gt;

== Security ==
The cryptographic strength of the HMAC depends upon the size of the secret key that is used.  The most common attack against HMACs is brute force to uncover the secret key.  HMACs are substantially less affected by collisions than their underlying hashing algorithms alone.&lt;ref&gt;{{cite web
| url=http://www.schneier.com/blog/archives/2005/02/sha1_broken.html
| title=SHA-1 Broken
| quote=''although it doesn't affect applications such as HMAC where collisions aren't important''
| author=Bruce Schneier
| date=August 2005
| accessdate=9 January 2009}}
&lt;/ref&gt;
&lt;ref&gt;{{cite web
| url=http://www.ietf.org/rfc/rfc2104.txt
| title=RFC 2104
| quote=''The strongest attack known against HMAC is based on the frequency of collisions for the hash function H (&quot;birthday attack&quot;) [PV,BCK2], and is totally impractical for minimally reasonable hash functions.''
| author=IETF
| date=February 1997
| accessdate=3 December 2009}}
&lt;/ref&gt;
&lt;ref&gt;{{cite conference
| first=Mihir
| last=Bellare
| title=New Proofs for NMAC and HMAC: Security without Collision-Resistance
| booktitle=Advances in Cryptology – Crypto 2006 Proceedings
| url=http://cseweb.ucsd.edu/~mihir/papers/hmac-new.html
| quote=''This paper proves that HMAC is a [[Pseudo-random function|PRF]] under the sole assumption that the compression function is a PRF. This recovers a proof based guarantee since no known attacks compromise the pseudorandomness of the compression function, and it also helps explain the resistance-to-attack that HMAC has shown even when implemented with hash functions whose (weak) collision resistance is compromised.''
| editor-last=Dwork
| editor-first=Cynthia
| series=Lecture Notes in Computer Science 4117
| year=2006
| publisher=Springer-Verlag
| date=June 2006
| accessdate=25 May 2010}}
&lt;/ref&gt; Therefore, HMAC-MD5 does not suffer from the same weaknesses that have been found in MD5.

In 2006, [[Jongsung Kim]], [[Alex Biryukov]], [[Bart Preneel]], and [[Seokhie Hong]] showed how to distinguish HMAC with reduced versions of MD5 and SHA-1 or full versions of [[HAVAL]], [[MD4]], and [[SHA-1#SHA-0|SHA-0]] from a [[random function]] or HMAC with a random function. Differential distinguishers allow an attacker to devise a forgery attack on HMAC. Furthermore, differential and rectangle distinguishers can lead to [[preimage attack|second-preimage attacks]]. HMAC with the full version of MD4 can be [[forgery (Cryptography)|forged]] with this knowledge. These attacks do not contradict the security proof of HMAC, but provide insight into HMAC based on existing cryptographic hash functions.
&lt;ref&gt;
{{cite journal
| last = Jongsung
| first = Kim
| coauthors = Biryukov, Alex; Preneel, Bart; Hong, Seokhie
| year = 2006
| title = On the Security of HMAC and NMAC Based on HAVAL, MD4, MD5, SHA-0 and SHA-1
| url=http://eprint.iacr.org/2006/187.pdf
| postscript = .}}&lt;/ref&gt;

In improperly-secured systems a [[timing attack]] can be performed to find out a HMAC digit by digit.&lt;ref&gt;Briefly mentioned at the end of this session [http://events.ccc.de/congress/2011/Fahrplan/events/4640.en.html Sebastian Schinzel:Time is on my Side - Exploiting Timing Side Channel Vulnerabilities on the Web] 28th Chaos Communication Congress, 2011.&lt;/ref&gt;

== Examples of HMAC (MD5, SHA1, SHA256&lt;!-- and SHA512--&gt;) ==
Here are some empty HMAC values:
 HMAC_MD5(&quot;&quot;, &quot;&quot;)    = 0x74e6f7298a9c2d168935f58c001bad88
 HMAC_SHA1(&quot;&quot;, &quot;&quot;)   = 0xfbdb1d1b18aa6c08324b7d64b71fb76370690e1d
 HMAC_SHA256(&quot;&quot;, &quot;&quot;) = 0xb613679a0814d9ec772f95d778c35fc5ff1697c493715653c6c712144292c5ad
&lt;!--HMAC_SHA512(&quot;&quot;, &quot;&quot;) = 0xb936cee86c9f87aa5d3c6f2e84cb5a4239a5fe50480a6ec66b70ab5b1f4ac6730c6c515421b327ec1d69402e53dfb49ad7381eb067b338fd7b0cb22247225d47 --&gt;

Here are some non-empty HMAC values, assuming 8-bit [[ASCII]] or [[UTF-8]] encoding:
 HMAC_MD5(&quot;key&quot;, &quot;The quick brown fox jumps over the lazy dog&quot;)    = 0x80070713463e7749b90c2dc24911e275
 HMAC_SHA1(&quot;key&quot;, &quot;The quick brown fox jumps over the lazy dog&quot;)   = 0xde7c9b85b8b78aa6bc8a7a36f70a90701c9db4d9
 HMAC_SHA256(&quot;key&quot;, &quot;The quick brown fox jumps over the lazy dog&quot;) = 0xf7bc83f430538424b13298e6aa6fb143ef4d59a14946175997479dbc2d1a3cd8
&lt;!-- HMAC_SHA512(&quot;key&quot;, &quot;The quick brown fox jumps over the lazy dog&quot;) = 0xb42af09057bac1e2d41708e48a902e09b5ff7f12ab428a4fe86653c73dd248fb82f948a549f7b791a5b41915ee4d1ec3935357e4e2317250d0372afa2ebeeb3a --&gt;

==References==
{{reflist|30em}}
;Notes
{{refbegin}}
* Mihir Bellare, Ran Canetti and Hugo Krawczyk, Keying Hash Functions for Message Authentication, [[CRYPTO]] 1996, pp1&amp;ndash;15 [http://www-cse.ucsd.edu/users/mihir/papers/hmac.html#kmd5-paper (PS or PDF)].
* Mihir Bellare, Ran Canetti and Hugo Krawczyk, Message authentication using hash functions: The HMAC construction, ''CryptoBytes'' 2(1), Spring 1996 [http://www-cse.ucsd.edu/users/mihir/papers/hmac.html#hmac-cryptobytes (PS or PDF)].
{{refend}}

== External links ==
* [http://www.ietf.org/rfc/rfc2104.txt RFC2104]
* [http://quickhash.com/hmac Online HMAC Calculator for dozens of underlying hashing algorithms]
* [http://www.freeformatter.com/hmac-generator.html Online HMAC Generator / Tester Tool]
* [http://csrc.nist.gov/publications/fips/fips198-1/FIPS-198-1_final.pdf FIPS PUB 198-1, ''The Keyed-Hash Message Authentication Code (HMAC)'']
* [http://php.net/manual/en/function.hash-hmac.php PHP HMAC implementation]
* [https://docs.python.org/2/library/hmac.html Python HMAC implementation]
* [http://cpan.uwinnipeg.ca/htdocs/Digest-HMAC/Digest/HMAC.pm.html Perl HMAC implementation]
* [http://ruby-hmac.rubyforge.org/ Ruby HMAC implementation]
* [http://www.ouah.org/ogay/hmac/ C HMAC implementation]
* [http://www.cryptopp.com C++ HMAC implementation (part of Crypto++)] 
* [http://docs.oracle.com/javase/1.5.0/docs/guide/security/jce/JCERefGuide.html#HmacEx Java implementation]
* [http://pajhome.org.uk/crypt/md5/instructions.html JavaScript MD5 and SHA HMAC implementation]
* [http://caligatio.github.com/jsSHA/ JavaScript SHA-only HMAC implementation]
* [http://msdn.microsoft.com/en-us/library/system.security.cryptography.hmac.aspx .NET's System.Security.Cryptography.HMAC]

{{Cryptography navbox | hash}}

{{Use dmy dates|date=July 2012}}

{{DEFAULTSORT:Hmac}}
[[Category:Message authentication codes]]
[[Category:Hashing]]</text>
      <sha1>lhc01y5uss62n1e78y40j01tmdwjz3u</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Most frequent k characters</title>
    <ns>0</ns>
    <id>42232886</id>
    <revision>
      <id>619897754</id>
      <parentid>615922871</parentid>
      <timestamp>2014-08-05T01:14:53Z</timestamp>
      <contributor>
        <ip>2401:FA00:9:1:1800:E719:97:5AE2</ip>
      </contributor>
      <comment>/* Success on text mining */</comment>
      <text xml:space="preserve" bytes="11954">{{multiple issues|
{{Third-party|date=March 2014}}
{{Notability|date=March 2014}}
}}

In [[information theory]], '''MostFreqKDistance''' is a [[string metric]] technique for quickly estimating how [[Similarity measure|similar]] two [[Order theory|ordered sets]] or [[String (computer science)|strings]] are. The scheme was invented by {{harvs|first=Sadi Evren|last=SEKER|authorlink=Sadi Evren SEKER|year=2014|txt}},&lt;ref name=&quot;mfkc&quot;/&gt; and initially used in [[text mining]] applications like [[author recognition]].&lt;ref name=&quot;mfkc&quot;&gt;{{citation
 | last1 = SEKER | first1 = Sadi E. | author1-link = Sadi Evren SEKER
 | last2 = Altun | first2 = Oguz
 | last3 = Ayan | first3 = Ugur
 | last4 = Mert | first4 = Cihan
 | contribution = A Novel String Distance Function based on Most Frequent K Characters
 | volume = 4
 | issue = 2
 | pages = 177–183
 | publisher = [[International Association of Computer Science and Information Technology Press (IACSIT Press)]]
 | title = [[International Journal of Machine Learning and Computing (IJMLC)]]
 | url = http://arxiv.org/abs/1401.6596
 | year = 2014}}.&lt;/ref&gt;
Method is originally based on a hashing function MaxFreqKChars &lt;ref name=&quot;hashfunc&quot;&gt;{{citation
 | last1 = Seker | first1 = Sadi E. | author1-link = Sadi Evren SEKER
 | last2 = Mert | first2 = Cihan
 | contribution = A Novel Feature Hashing For Text Mining
 | url = http://journal.ibsu.edu.ge/index.php/jtst/article/view/428
 | pages = 37–41
 | publisher = [[International Black Sea University]]
 | title = Journal of Technical Science and Technologies
 | ISSN = 2298-0032
 | volume = 2
 | issue = 1
 | year = 2013}}.&lt;/ref&gt; classical [[author recognition]] problem and idea first came out while studying on [[data stream mining]].&lt;ref name=&quot;author&quot;&gt;{{citation
 | last1 = Seker | first1 = Sadi E. | author1-link = Sadi Evren SEKER
 | last2 = Al-Naami | first2 = Khaled
 | last3 = Khan | first3 = Latifur
 | contribution = Author attribution on streaming data
 | doi = 10.1109/IRI.2013.6642511
 | url = http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6642511
 | pages = 497–503
 | publisher = [[IEEE]]
 | title = Information Reuse and Integration (IRI), 2013 IEEE 14th International Conference on, San Francisco, USA, Aug 14-16, 2013
 | year = 2013}}.&lt;/ref&gt;


==Definition==
Method has two steps.
* [[Hash function|Hash]] input strings str1 and str2 separately using MostFreqKHashing and output hstr1 and hstr2 respectively
* Calculate string distance (or string similarity coefficient) of two hash outputs, hstr1 and hstr2 and output an integer value

===Most frequent K hashing===
The first step of algorithm is calculating the hashing based on the most frequent k characters. The hashing algorithm has below steps:
&lt;syntaxhighlight lang=&quot;Java&quot;&gt;
String function MostFreqKHashing (String inputString, int K)
    def string outputString
    for each distinct character
        count occurrence of each character
    for i := 0 to K
        char c = next most freq ith character  (if two chars have same frequency than get the first occurrence in inputString)
        int count = number of occurrence of the character
        append to outputString, c and count
    end for
    return outputString
&lt;/syntaxhighlight&gt;

Above function, simply gets an input string and an integer K value and outputs the most frequent K characters from the input string. The only condition during the creation of output string is adding the first occurring character first, if the frequencies of two characters are equal. Similar to the most of [[hashing function]]s, ''Most Frequent K Hashing'' is also a [[one way function]].

===Most frequent K distance===
The second step of algorithm works on two outputs from two different input strings and outputs the similarity coefficient (or distance metric).
&lt;syntaxhighlight lang=&quot;Java&quot;&gt;
int function MostFreqKSimilarity (String inputStr1, String inputStr2, int limit)
    def int similarity
    for each c = next character from inputStr1
        lookup c in inputStr2
        if c is null
             continue
             similarity += frequency of c in inputStr1
    return limit-similarity
&lt;/syntaxhighlight&gt;
Above function, simply gets two input strings, previously outputted from the &lt;code&gt;MostFreqKHashing&lt;/code&gt; function. From the most frequent k hashing function, the characters and their frequencies are returned. So, the similarity function calculates the similarity based on characters and their frequencies by checking if the same character appears on both strings. The limit is usually taken to be 10 and in the end the function returns the result of the subtraction of the sum of similarities from limit.

In some implementations, the distance metric is required instead of similarity coefficient. In order to convert the output of above similarity coefficient to distance metric, the output can be subtracted from any constant value (like the maximum possible output value). For the case, it is also possible to implement a [[wrapper function]] over above two functions.

===String distance wrapper function===
In order to calculate the distance between two strings, below function can be implemented
&lt;syntaxhighlight lang=&quot;Java&quot;&gt;
int function MostFreqKSDF (String inputStr1, String inputStr2, int K, int maxDistance)
    return maxDistance - MostFreqKSimilarity(MostFreqKHashing(inputStr1, K), MostFreqKHashing(inputStr2, K))
&lt;/syntaxhighlight&gt;

Any call to above string distance function will supply two input strings and a maximum distance value. The function will calculate the similarity and subtract that value from the maximum possible distance. It can be considered as a simple [[additive inverse]] of similarity.

==Examples==
Let's consider maximum 2 frequent hashing over two strings ‘research’ and ‘seeking’.
MostFreqKHashing('research', 2) = r2e2
because we have 2 'r' and 2 'e' characters with the highest frequency and we return in the order they appear in the string.
MostFreqKHashing('seeking', 2) = e2s1
Again we have character 'e' with highest frequency and rest of the characters have same frequency of 1, so we return the first character of equal frequencies, which is 's'.
Finally we make the comparison:
MostFreqKSimilarity('r2e2', 'e2s1') = 2
We simply compared the outputs and only character occurring in both input is character 'e' and the occurrence in both input is 2.
Instead running the sample step by step as above, we can simply run by using the string distance wrapper function as below:
MostFreqKSDF('research', 'seeking', 2) = 2

Below table holds some sample runs between example inputs for K=2:
{|class=&quot;wikitable&quot;
|-
! Inputs
! Hash Outputs
! SDF Output (max from 10)
|-
|'night'
'nacht'
|n1i1
n1a1
|9
|-
|'my'
'a'
|m1y1
a1NULL0
|10
|-
|‘research’
‘research’	
|r2e2
r2e2	
|6
|-
|‘aaaaabbbb’
‘ababababa’	
|a5b4
a5b4	
|1
|-
|‘significant’
‘capabilities’	
|i3n2
i3a2	
|7
|}

Method is also suitable for bioinformatics to compare the genetic strings like in [[fasta]] format.

Str1 = LCLYTHIGRNIYYGSYLYSETWNTGIMLLLITMATAFMGYVLPWGQMSFWGATVITNLFSAIPYIGTNLV

Str2 = EWIWGGFSVDKATLNRFFAFHFILPFTMVALAGVHLTFLHETGSNNPLGLTSDSDKIPFHPYYTIKDFLG

MostFreqKHashing(str1, 2) = L9T8

MostFreqKHashing(str2, 2) = F9L8

MostFreqKSDF(str1, str2, 2, 100) = 83

==Algorithm complexity and comparison==
The motivation behind algorithm is calculating the similarity between two input strings. So, the hashing function should be able to reduce the size of input and at the same time keep the characteristics of the input. Other hashing algorithms like [[MD5]] or [[SHA-1]], the output is completely unrelated with the input and those hashing algorithms are not suitable for string similarity check.

On the other hand string similarity functions like [[Levenshtein distance]] have the algorithm complexity problem.

Also algorithms like [[Hamming distance]], [[Jaccard coefficient]] or [[Tanimoto coefficient]] have relatively low algorithm complexity but the success rate in [[text mining]] studies are also low.

===Time complexity===
The calculation of time complexity of 'most frequent k char string similarity' is quite simple. In order to get the maximum frequent K characters from a string, the first step is sorting the string in a lexiconical manner. After this sort, the input with highest occurrence can be achieved with a simple pass in linear time complexity. Since major classical sorting algorithms are working in O(nlogn) complexity like [[merge sort]] or [[quick sort]], we can sort the first string in O(nlogn) and second string on O(mlogm) times. The total complexity would be O(nlog n ) + O (m log m) which is O(n log n) as the upper bound [[worst case analysis]].

===Comparison===
Below table compares the complexity of algorithms:
{|class=&quot;wikitable&quot;
|-
! Algorithm
! Time Complexity
|-
| [[Levenshtein distance]]
| O(nm) = O(n^2)
|-
| [[Jaccard index]]
| O(n+m) = O(n)
|-
| MostFreqKSDF
| O(nlogn+mlogm) = O(n log n)
|}

For the above table, n is the length of first string and m is the length of second string.

==Success on text mining==
The success of string similarity algorithms are compared on a study. The study is based on IMDB62 dataset which is holding 1000 comment entries in [[Internet Movie Database]] from each 62 people. The data set is challenged for three string similarity functions and the success rates are as below:

{|class=&quot;wikitable&quot;
|-
! Algorithm
! Running Time
! Error (RMSE)
! Error (RAE)
|-
|[[Levenshtein distance]]
|3647286.54 sec
|29
|0.47
|-
|[[Jaccard index]]
|228647.22 sec
|45
|0.68
|-
|MostFreqKSDF
|2712323.51 sec
|32
|0.49
|}

The running times for [[author recognition]] are in seconds and the error rates are [[root mean square error]] (RMSE) and [[relative absolute error]] (RAE).

Above table shows, the 'most frequent k similarity' is better than [[Levenshtein distance]] by time and [[Jaccard index]] by success rate.

For the time performance and the success rates, the bitwise similarity functions like [[Dice's coefficient|Sørensen–Dice index]], [[Tversky index]] or [[Hamming Distance]] are all in the same category with similar success rates and running times. There are obviously slight differences but the idea behind bitwise operation, looses the string operations like deletion or addition. For example a single bit addition to the front of one of the input strings would yield a catastrophic result on the similarity for bitwise operators while Levenshtein distance is successfully catching.

Unfortunately, [[big data]] studies requires a faster algorithm with still acceptable success. Here the 'max frequent k characters' is an easy and simple algorithm (as in [[Occams razor]]), which is straight forward to implement.

==See also==
&lt;div class= style=&quot;-moz-column-count:2; column-count:2;&quot;&gt;
* [[agrep]]
* [[Approximate string matching]]
* [[Bitap algorithm]]
* [[Damerau–Levenshtein distance]]
* [[diff]]
* [[MinHash]]
* [[Dynamic time warping]]
* [[Euclidean distance]]
* [[Fuzzy string searching]]
* [[Hamming weight]]
* [[Hirschberg's algorithm]]
* [[Homology (biology)#Sequence homology|Homology of sequences in genetics]]
* [[Hunt–McIlroy algorithm]]
* [[Jaccard index]]
* [[Jaro–Winkler distance]]
* [[Levenshtein distance]]
* [[Longest common subsequence problem]]
* [[Lucene]] (an open source search engine that implements edit distance)
* [[Manhattan distance]]
* [[Metric space]]
* [[Needleman–Wunsch algorithm]]
* [[Optimal matching]] algorithm
* [[Sequence alignment]]
* Similarity space on [[Numerical taxonomy]]
* [[Smith–Waterman algorithm]]
* [[Sørensen similarity index]]
* [[String distance metric]]
* [[String similarity function]]
* [[Wagner-Fischer algorithm]]
* [[Locality-sensitive hashing]]
&lt;/div&gt;

==References==
{{reflist}}

[[Category:String similarity measures]]
[[Category:Dynamic programming]]
[[Category:Articles with example pseudocode]]
[[Category:Quantitative linguistics]]
[[Category:Hash functions]]
[[Category:Hashing]]</text>
      <sha1>qb6jicq13i122bawxozefbph5bo9z48</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Ctrie</title>
    <ns>0</ns>
    <id>33676538</id>
    <revision>
      <id>624808564</id>
      <parentid>622596484</parentid>
      <timestamp>2014-09-09T14:09:00Z</timestamp>
      <contributor>
        <username>Palmcluster</username>
        <id>109793</id>
      </contributor>
      <comment>/* Implementations */ expand reference for Prolog implementation</comment>
      <text xml:space="preserve" bytes="10640">{{distinguish|C-trie}}
A '''concurrent hash-trie''' or '''Ctrie'''&lt;ref name=&quot;techreport&quot;&gt;Prokopec, A. et al. (2011) [http://infoscience.epfl.ch/record/166908/files/ctries-techreport.pdf Cache-Aware Lock-Free Concurrent Hash Tries]. Technical Report, 2011.&lt;/ref&gt;&lt;ref name=&quot;snapshots&quot;&gt;Prokopec, A., Bronson N., Bagwell P., Odersky M. (2011) [http://lamp.epfl.ch/~prokopec/ctries-snapshot.pdf Concurrent Tries with Efficient Non-Blocking Snapshots]&lt;/ref&gt; is a concurrent [[thread-safe]] [[lock-free]] implementation of a [[hash array mapped trie]]. It is used to implement the concurrent map abstraction. It has particularly scalable concurrent insert and remove operations and is memory-efficient.&lt;ref&gt;Prokopec, A. et al. (2011) [http://lamp.epfl.ch/~prokopec/lcpc_ctries.pdf Lock-Free Resizeable Concurrent Tries]. The 24th International Workshop on Languages and Compilers for Parallel Computing, 2011.&lt;/ref&gt; It is the first known concurrent data-structure that supports [[O(1)]], atomic, [[lock-free]] snapshots.&lt;ref name=&quot;snapshots&quot;/&gt;&lt;ref name=&quot;github-ctries&quot;&gt;Prokopec, A. [https://github.com/axel22/Ctries JVM implementation on GitHub]&lt;/ref&gt;

== Operation ==

The Ctrie data structure is a non-blocking concurrent [[hash array mapped trie]] based on single-word compare-and-swap instructions in a shared-memory system. It supports concurrent lookup, insert and remove operations. Just like the [[hash array mapped trie]], it uses the entire 32-bit space for hash values thus having low risk of hashcode collisions. Each node may branch to up to 32 sub tries. To conserve memory, each node contains a 32 bits bitmap where each bit indicates the presence of a branch followed by an array of length equal to the [[Hamming weight]] of the bitmap.

Keys are inserted by doing an atomic compare-and-swap operation on the node which needs to be modified. To ensure that updates are done independently and in a proper order, a special indirection node (an I-node) is inserted between each regular node and its subtries.

[[File:Ctrie-insert.png|500px|Ctrie insert operation]]

The figure above illustrates the Ctrie insert operation. Trie A is empty - an atomic CAS instruction is used to swap the old node C1 with the new version of C1 which has the new key ''k1''. If the CAS is not successful, the operation is restarted. If the CAS is successful, we obtain the trie B. This procedure is repeated when a new key ''k2'' is added (trie C). If two hashcodes of the keys in the Ctrie collide as is the case with ''k2'' and ''k3'', the Ctrie must be extended with at least one more level - trie D has a new indirection node I2 with a new node C2 which holds both colliding keys. Further CAS instructions are done on the contents of the indirection nodes I1 and I2 - such CAS instructions can be done independently of each other, thus enabling concurrent updates with less contention.

The Ctrie is defined by the pointer to the root indirection node (or a root I-node). The following types of nodes are defined for the Ctrie:

  structure INode {
    main: CNode
  }
  
  structure CNode {
    bmp: integer
    array: Branch[2^W]
  }
  
  Branch: INode | SNode
  
  structure SNode {
    k: KeyType
    v: ValueType
  }

A C-node is a branching node. It typically contains up to 32 branches, so ''W'' above is 5. Each branch may either be a key-value pair (represented with an S-node) or another I-node. To avoid wasting 32 entries in the branching array when some branches may be empty, an integer bitmap is used to denote which bits are full and which are empty. The helper method ''flagpos'' is used to inspect the relevant hashcode bits for a given level and extract the value of the bit in the bitmap to see if its set or not - denoting whether there is a branch at that position or not. If there is a bit, it also computes its position in the branch array. The formula used to do this is:
  
  bit = bmp &amp; (1 &lt;&lt; ((hashcode &gt;&gt; level) &amp; 0x1F))
  pos = bitcount((bit - 1) &amp; bmp)

Note that the operations treat only the I-nodes as mutable nodes - all other nodes are never changed after being created and added to the Ctrie.

Below is an illustration of the pseudocode of the insert operation:

  def insert(k, v)
    r = READ(root)
    if iinsert(r, k, v, 0, null) = RESTART insert(k, v)

  def iinsert(i, k, v, lev, parent)
    cn = READ(i.main)
    flag, pos = flagpos(k.hc, lev, cn.bmp)
    if cn.bmp &amp; flag = 0 {
      ncn = cn.inserted(pos, flag, SNode(k, v))
      if CAS(i.main, cn, ncn) return OK
      else return RESTART
    }
    cn.array(pos) match {
      case sin: INode =&gt; {
        return iinsert(sin, k, v, lev + W, i)
      case sn: SNode =&gt;
        if sn.k ≠ k {
          nsn = SNode(k, v)
          nin = INode(CNode(sn, nsn, lev + W))
          ncn = cn.updated(pos, nin)
          if CAS(i.main, cn, ncn) return OK
          else return RESTART
        } else {
          ncn = cn.updated(pos, SNode(k, v))
          if CAS(i.main, cn, ncn) return OK
          else return RESTART
        }
    }

The ''inserted'' and ''updated'' methods on nodes return new versions of the C-node with a value inserted or updated at the specified position, respectively. Note that the insert operation above is tail-recursive, so it can be rewritten as a [[while loop]]. Other operations are described in more detail in the original paper on Ctries.&lt;ref name=&quot;techreport&quot;/&gt;&lt;ref&gt;http://axel22.github.io/resources/docs/lcpc-ctries.ppt&lt;/ref&gt;

The data-structure has been proven to be correct&lt;ref name=&quot;techreport&quot;/&gt; - Ctrie operations have been shown to have the atomicity, linearizability and lock-freedom properties. The lookup operation can be modified to guarantee [[wait-free]]dom.

== Advantages of Ctries ==

Ctries have been shown to be comparable in performance with concurrent [[skip lists]],&lt;ref name=&quot;snapshots&quot;/&gt;&lt;ref name=&quot;github-ctries&quot;/&gt; concurrent [[hash tables]] and similar data structures in terms of the lookup operation, being slightly slower than hash tables and faster than skip lists due to the lower level of indirections. However, they are far more scalable than most concurrent hash tables where the insertions are concerned.&lt;ref name=&quot;techreport&quot;/&gt; Most concurrent hash tables are bad at conserving memory - when the keys are removed from the hash table, the underlying array is not shrunk. Ctries have the property that the allocated memory is always a function of only the current number of keys in the data-structure.&lt;ref name=&quot;techreport&quot;/&gt;

Ctries have logarithmic complexity bounds of the basic operations, albeit with a low constant factor due to the high branching level (usually 32).

Ctries support a lock-free, linearizable, constant-time snapshot operation,&lt;ref name=&quot;snapshots&quot;/&gt; based on the insight obtained from [[persistent data structures]]. This is a breakthrough in concurrent data-structure design, since existing concurrent data-structures do not support snapshots. The snapshot operation allows implementing lock-free, linearizable iterator, size and clear operations - existing concurrent data-structures have implementations which either use global locks or are correct only given that there are no concurrent modifications to the data-structure. In particular, Ctries have an O(1) iterator creation operation, O(1) clear operation, O(1) duplicate operation and an [[amortized]] O(logn) size retrieval operation.

== Problems with Ctries ==

Most concurrent data structures require dynamic memory allocation, and [[lock-free]] concurrent data structures rely on garbage collection on most platforms. The current implementation&lt;ref name=&quot;github-ctries&quot;/&gt; of the Ctrie is written for the JVM, where garbage collection is provided by the platform itself. While it's possible to keep a concurrent memory pool for the nodes shared by all instances of Ctries in an application or use reference counting to properly deallocate nodes, the only implementation so far to deal with manual memory management of nodes used in Ctries is the common-lisp implementation [http://github.com/danlentz/cl-ctrie cl-ctrie], which implements several stop-and-copy and mark-and-sweep garbage collection techniques for persistent, memory-mapped storage. [[Hazard pointer]]s are another possible solution for a correct manual management of removed nodes. Such a technique may be viable for managed environments as well, since it could lower the pressure on the GC.

== Implementations ==

A Ctrie implementation&lt;ref name=&quot;github-ctries&quot;/&gt; for Scala 2.9.x is available on GitHub. It is a mutable thread-safe implementation which ensures progress and supports lock-free, linearizable, O(1) snapshots.

A data-structure similar to Ctries has been used in ScalaSTM,&lt;ref&gt;N. Bronson [https://github.com/nbronson/scala-stm ScalaSTM]&lt;/ref&gt; a [[software transactional memory]] library for the JVM.

The [[Scala (programming language)|Scala]] standard library includes a Ctries implementation since February 2012.&lt;ref&gt;[https://github.com/scala/scala/blob/master/src/library/scala/collection/concurrent/TrieMap.scala TrieMap.scala]&lt;/ref&gt;

Haskell implementation is available as a package&lt;ref&gt;[http://hackage.haskell.org/package/ctrie Haskell ctrie package]&lt;/ref&gt; and on GitHub.&lt;ref&gt;[https://github.com/mcschroeder/ctrie GitHub repo for Haskell Ctrie]&lt;/ref&gt;

A standalone Java implementation is available on GitHub.&lt;ref&gt;[https://github.com/romix/java-concurrent-hash-trie-map GitHub repo for Java Ctrie]&lt;/ref&gt;

CL-CTRIE is the Common Lisp implementation is available on GitHub.&lt;ref&gt;[https://github.com/danlentz/cl-ctrie GitHub repo for Common Lisp Ctrie]&lt;/ref&gt;

An insert-only Ctrie variant has been used for tabling in Prolog programs.&lt;ref&gt;Miguel Areias and Ricardo Rocha, [http://www.dcc.fc.up.pt/~ricroc/homepage/publications/2014-HLPP.html A Lock-Free Hash Trie Design for Concurrent Tabled Logic Programs]&lt;/ref&gt;

==History==

Ctries were first described in 2011 by [[Aleksandar Prokopec]].&lt;ref name=&quot;techreport&quot;/&gt; To quote the author:

''Ctrie is a non-blocking concurrent shared-memory hash trie based on single-word compare-and-swap instructions. Insert, lookup and remove operations modifying different parts of the hash trie can be run independent of each other and do not contend. Remove operations ensure that the unneeded memory is freed and that the trie is kept compact.''

In 2012, a revised version of the Ctrie data structure was published,&lt;ref name=&quot;snapshots&quot;/&gt; simplifying the data structure and introducing an optional constant-time, lock-free, atomic snapshot operation.

== References ==
{{reflist}}

{{DEFAULTSORT:Hash Array Mapped Trie}}
[[Category:Associative arrays]]
[[Category:Hashing|Ctrie]]</text>
      <sha1>kclkne8lmxt8gq6kfz2c58e1674rqq4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>XOR linked list</title>
    <ns>0</ns>
    <id>291683</id>
    <revision>
      <id>604964294</id>
      <parentid>604964209</parentid>
      <timestamp>2014-04-20T02:32:57Z</timestamp>
      <contributor>
        <username>Spasemunki</username>
        <id>262779</id>
      </contributor>
      <comment>/* Subtraction linked list */  Remove C-specific implementation notes</comment>
      <text xml:space="preserve" bytes="7592">{{Refimprove|date=October 2009}}

An '''XOR linked list''' is a [[data structure]] used in [[computer programming]]. It takes advantage of the [[Bitwise operation#XOR|bitwise XOR]] operation to decrease storage requirements for [[doubly linked list]]s. 

== Description ==
An ordinary doubly linked list stores addresses of the previous and next list items in each list node, requiring two address fields:

  ...  A       B         C         D         E  ...
           –&gt;  next –&gt;  next  –&gt;  next  –&gt;
           &lt;–  prev &lt;–  prev  &lt;–  prev  &lt;–

An XOR linked list compresses the same information into ''one'' address field by storing the bitwise XOR (here denoted by ⊕) of the address for ''previous'' and the address for ''next'' in one field:

  ...  A        B         C         D         E  ...
          &lt;–&gt;  A⊕C  &lt;-&gt;  B⊕D  &lt;-&gt;  C⊕E  &lt;-&gt;

More formally: 
   link(B) = addr(A)⊕addr(C), link(C) = addr(B)⊕addr(D), ...

When you traverse the list from left to right: supposing you are at C, you can take the address of the previous item, B, and XOR it with the value in the link field (B⊕D). You will then have the address for D and you can continue traversing the list. The same pattern applies in the other direction.
     i.e.  addr(D) = link(C) ⊕ addr(B)
     where
           link(C) = addr(B)⊕addr(D)
      so  
           addr(D) = addr(B)⊕addr(D) ⊕ addr(B)           
       
           addr(D) = addr(B)⊕addr(B) ⊕ addr(D) 
     since 
            X⊕X = 0                 
            =&gt; addr(D) = 0 ⊕ addr(D)
     since
            X⊕0 = x
            =&gt; addr(D) = addr(D)
     The XOR operation cancels addr(B) appearing twice in the equation and all we are left with is the addr(D).

To start traversing the list in either direction from some point, you need the address of two consecutive items, not just one. If the addresses of the two consecutive items are reversed, you will end up traversing the list in the opposite direction.

===Theory of Operation===
The key is the first operation, and the properties of XOR: 
*X⊕X=0 
*X⊕0=X 
*X⊕Y=Y⊕X
*(X⊕Y)⊕Z=X⊕(Y⊕Z)

The R2 register always contains the XOR of the address of current item C with the address of the predecessor item P: C⊕P. The Link fields in the records contain the XOR of the left and right successor addresses, say L⊕R. XOR of R2 (C⊕P) with the current link field (L⊕R) yields C⊕P⊕L⊕R. 
* If the predecessor was L, the P(=L) and L ''cancel out''  leaving C⊕R. 
* If the predecessor had been R, the P(=R) and R  cancel, leaving C⊕L. 

In each case, the result is the XOR of the current address with the next address. XOR of this with the current address in R1 leaves the next address. R2 is left with the requisite XOR pair of the (now) current address and the predecessor.

==Features==
* Given only one list item, one cannot immediately obtain the addresses of the other elements of the list. 
* Two XOR operations suffice to do the traversal from one item to the next, the same instructions sufficing in both cases. Consider a list with items &lt;code&gt;{…B C D…}&lt;/code&gt; and with R1 and R2 being [[Processor register|registers]] containing, respectively, the address of the current (say C) list item and a work register containing the XOR of the current address with the previous address (say C⊕D).  Cast as [[System/360]] instructions:

 X  R2,Link    R2 &lt;- C⊕D ⊕ B⊕D (i.e. B⊕C, &quot;Link&quot; being the link field
                                   in the current record, containing B⊕D)
 XR R1,R2      R1 &lt;- C ⊕ B⊕C    (i.e. B, voilà: the next record)

* End of list is signified by imagining a list item at address zero placed adjacent to an end point, as in &lt;code&gt;{0 A B C…}&lt;/code&gt;. The link field at A would be 0⊕B. An additional instruction is needed in the above sequence after the two XOR operations to detect a zero result in developing the address of the current item,
* A list end point can be made reflective by making the link pointer be zero. A zero pointer is a ''mirror''. (The XOR of the left and right neighbor addresses, being the same, is zero.)

== Drawbacks ==

* General-purpose debugging tools cannot follow the XOR chain, making debugging more difficult; &lt;ref&gt;http://www.iecc.com/gclist/GC-faq.html#GC,%20C,%20and%20C++&lt;/ref&gt;
* The price for the decrease in memory usage is an increase in code complexity, making maintenance more expensive;
* Most [[garbage collection (computer science)|garbage collection]] schemes do not work with data structures that do not contain literal [[pointer (computer programming)|pointer]]s;
* XOR of pointers is not defined in some contexts (e.g., the [[C (programming language)|C]] language), although many languages provide some kind of [[type conversion]] between pointers and integers;
* The pointers will be unreadable if one isn't traversing the list &amp;mdash; for example, if the pointer to a list item was contained in another data structure;
* While traversing the list you need to remember the address of the previously accessed node in order to calculate the next node's address.
* XOR linked lists do not provide some of the important advantages of doubly-linked lists, such as the ability to delete a node from the list knowing only its address or the ability to insert a new node before or after an existing node when knowing only the address of the existing node.

Computer systems have increasingly cheap and plentiful memory, and storage overhead is not generally an overriding issue outside specialized [[embedded system]]s. Where it is still desirable to reduce the overhead of a linked list, [[unrolled linked list|unrolling]] provides a more practical approach (as well as other advantages, such as increasing cache performance and speeding [[random access]]).

==Variations==
The underlying principle of the XOR linked list can be applied to any reversible binary operation. Replacing XOR by addition or subtraction gives slightly different, but largely equivalent, formulations:

===Addition linked list===

  ...  A        B         C         D         E  ...
          &lt;–&gt;  A+C  &lt;-&gt;  B+D  &lt;-&gt;  C+E  &lt;-&gt;

This kind of list has exactly the same properties as the XOR linked list, except that a zero link field is not a &quot;mirror&quot;. The address of the next node in the list is given by subtracting the previous node's address from the current node's link field.

===Subtraction linked list===

  ...  A        B         C         D         E  ...
          &lt;–&gt;  C-A  &lt;-&gt;  D-B  &lt;-&gt;  E-C  &lt;-&gt;

This kind of list differs from the &quot;traditional&quot; XOR linked list in that the instruction sequences needed to traverse the list forwards is different from the sequence needed to traverse the list in reverse. The address of the next node, going forwards, is given by ''adding'' the link field to the previous node's address; the address of the preceding node is given by ''subtracting'' the link field from the next node's address.

The subtraction linked list is also special in that the entire list can be relocated in memory without needing any patching of pointer values, since adding a constant offset to each address in the list will not require any changes to the values stored in the link fields. (See also [[Serialization]].) This is an advantage over both XOR linked lists and traditional linked lists.

==See also==
*[[XOR swap algorithm]]

==References==
&lt;references/&gt;

==External links==
* [http://blog.wsensors.com/?p=177 Example implementation in C++.]

{{Data structures}}

[[Category:Binary arithmetic]]
[[Category:Linked lists]]</text>
      <sha1>sw5i677fmcn2w0xcyyr164xuxjw7tl2</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Sentinel node</title>
    <ns>0</ns>
    <id>3123459</id>
    <revision>
      <id>626602831</id>
      <parentid>593360853</parentid>
      <timestamp>2014-09-22T11:00:00Z</timestamp>
      <contributor>
        <ip>92.239.123.184</ip>
      </contributor>
      <comment>Removed pointless cast</comment>
      <text xml:space="preserve" bytes="2056">{{about|the computer programming construct|the body part|Sentinel lymph node}}
{{distinguish|sentinel value}}
A '''sentinel node''' is a specifically designated [[node (computer science)|node]] used with [[linked list]]s and [[Tree (data structure)|trees]] as a traversal path terminator. A sentinel node does not hold or reference any data managed by the data structure. Sentinels are used as an alternative over using [[null pointer|null]] as the path terminator in order to get one or more of the following benefits:
# Increased speed of operations
# Reduced algorithmic complexity and code size
# Increased data structure [[Robustness (computer science)|robustness]] (arguably)

== Example ==
Below shows an initialization of a sentinel node in a [[binary tree]] implemented in the [[C (programming language)|C programming language]]:&lt;ref&gt;http://www.eternallyconfuzzled.com/tuts/datastructures/jsw_tut_andersson.aspx&lt;/ref&gt;

&lt;source lang=&quot;c&quot;&gt;
struct jsw_node {
   int data;
   int level;
   struct jsw_node *link[2];                  // link[0] to next_left node; link[1] to next_right node
};

struct jsw_node *nil;                         // pointer to the sentinel node

int jsw_init ( void )
{
   nil = malloc ( sizeof *nil );              // allocating memory for the sentinel node
   if ( nil == NULL )                         // check for malloc() failure
      return 0;                               // malloc() failed to allocate memory

   nil-&gt;level = 0;
   nil-&gt;link[0] = nil-&gt;link[1] = nil;         // link both right and left nodes to self (empty tree)

   return 1;                                  // successful initialization
}
&lt;/source&gt;

As nodes that would normally link to NULL now link to &quot;nil&quot; (including nil itself), it removes the need for an expensive branch operation to check for NULL. NULL itself is known as a ''[[sentinel value]]'', a different approach to the same problem.

==References==
&lt;references/&gt;

[[Category:Programming idioms]]
[[Category:Linked lists]]
[[Category:Trees (data structures)]]


{{Compu-prog-stub}}</text>
      <sha1>cw0dlf00xccd8shnotm4lmr4v50srb0</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Sentinel value</title>
    <ns>0</ns>
    <id>3364948</id>
    <revision>
      <id>623167594</id>
      <parentid>623136984</parentid>
      <timestamp>2014-08-28T12:04:10Z</timestamp>
      <contributor>
        <username>Nbarth</username>
        <id>570614</id>
      </contributor>
      <minor/>
      <comment>/* top */ fix link</comment>
      <text xml:space="preserve" bytes="6690">{{distinguish|sentinel node}}
In [[computer programming]], a '''sentinel value''' (also referred to as a '''flag value''', '''trip value''', '''rogue value''', '''signal value''', or '''dummy data''')&lt;ref&gt;
{{cite book
 | last = Knuth
 | first = Donald
 | authorlink = Donald Knuth
 | title = The Art of Computer Programming, Volume 1: Fundamental Algorithms (second edition)
 | publisher = [[Addison-Wesley]]
 | year = 1973
 | pages = 213&amp;ndash;214, also p. 631
 | isbn = 0-201-03809-9 }}&lt;/ref&gt; is a special [[value (computer science)|value]] whose presence guarantees termination of an algorithm that processes [[data structure|structured]] (especially [[sequential access|sequential]]) [[data (computing)|data]], typically a [[Control flow|loop]] or recursive algorithm.

The sentinel value is a form of [[in-band]] data that makes it possible to detect the end of the data when no [[out-of-band data]] (such as an explicit size indication) is provided. The value should be selected in such a way that it is guaranteed to be distinct from all legal data values, since otherwise the presence of such values would prematurely signal the end of the data (the [[semipredicate problem]]). A sentinel value is sometimes known as an &quot;[[Elephant in Cairo]]&quot;, due to a joke where this is used as a physical sentinel. In safe languages, most uses of sentinel values could be replaced with [[option type]]s, which enforce explicit handling of the exceptional case.

==Examples==
Some examples of common sentinel values and their uses:
* [[Null character]] for indicating the end of a [[null-terminated string]]
* [[Null pointer]] for indicating the end of a [[linked list]] or a [[Tree (data structure)|tree]].
* A negative integer for indicating the end of a sequence of non-negative integers
* [[End-of-file]], a non-character value returned by certain input routines to signal that no further characters are available from a file
* [[High Values]], a key value of hexadecimal 0xFF used in business programming

==Variants==
A related practice, used in slightly different circumstances, is to place some specific value at the end of the data, in order to avoid the need for an explicit test for termination in some processing loop, because the value will trigger termination by the tests already present for other reasons. Unlike the above uses, this is not how the data is naturally stored or processed, but is instead an optimization, compared to the straightforward algorithm that checks for termination. This is typically used in searching.&lt;ref&gt;McConnell, Steve. &quot;Code Complete&quot; Edition 2 Pg. 621 ISBN 0-7356-1967-0&lt;/ref&gt;

For instance, when searching for a particular value in an unsorted [[List (abstract data type)|list]], every element will be compared against this value, with the loop terminating when equality is found; however to deal with the case that the value should be absent, one must also test after each step for having completed the search unsuccessfully. By appending the value searched for to the end of the list, an unsuccessful search is no longer possible, and no explicit termination test is required in the [[inner loop]]; afterwards one must still decide whether a true match was found, but this test needs to be performed only once rather than at each iteration.&lt;ref&gt;
{{cite book
 | last = Knuth
 | first = Donald
 | authorlink = Donald Knuth
 | title = The Art of Computer Programming, Volume 3: Sorting and searching
 | publisher = [[Addison-Wesley]]
 | year = 1973
 | pages = 395
 | isbn = 0-201-03803-X }}&lt;/ref&gt;
Knuth calls the value so placed at the end of the data a '''dummy value''' rather than a sentinel.

===Examples===

====Array====

For example, if searching for a value in an array in C, a straightforward implementation is as follows; note the use of a negative number (invalid index) to solve the semipredicate problem of returning &quot;no result&quot;:
&lt;source lang=C&gt;
// Returns index of value, -1 for no result
int find(int* a, int l, int v)
{
  for (i = 0; i &lt; l; i++)
    if (a[i] == v)
      return i;
  return -1; // -1 means &quot;no result&quot;
}
&lt;/source&gt;
However, this does two tests at each iteration of the loop: whether the value has been found, and then whether the end of the array has been reached. This latter test is what is avoided by using a sentinel value. Assuming the array can be extended by one element (without memory allocation or cleanup; this is more realistic for a linked list, as below), this can be rewritten as:
&lt;source lang=C&gt;
int find(int* a, int l, int v)
{
  a[l] = v; // add sentinel value
  for (i = 0; ; i++)
    if (a[i] == v) {
      if (i == l) // sentinel value, not real result
        return -1;
      return i;
    }
}
&lt;/source&gt;
In this case each loop iteration only has a single test (for the value), and is guaranteed to terminate, due to the sentinel value. On termination, there is a single check if the sentinel value has been hit, which replaces a test for each iteration.

In this case the loop can more simply be written as a while loop:
&lt;source lang=C&gt;
int find(int* a, int l, int v)
{
  a[l] = v;
  i = 0;
  while (a[i] != v)
    i++;
  if (i == l) // sentinel value, not real result
    return -1;
  return i;
}
&lt;/source&gt;

====Linked list====

For searching in a linked list, the following is the straightforward algorithm, starting at a given head node; note the use of NULL to solve the semipredicate problem:
&lt;source lang=C&gt;
typedef struct Node {
  Node* next;
  int value;
}

// Returns pointer to node with value, NULL for no result
int find(Node* n, int v)
{
  for (; n-&gt;next != NULL; n = n-&gt;next)
    if (n-&gt;value == v)
      return n;
  return NULL;
}
&lt;/source&gt;
However, if the last node is known, the inner loop can be optimized by firstly adding (and lastly removing) a sentinel node after the last node:
&lt;source lang=C&gt;
typedef struct List {
  Node* firstElement;
  Node* lastElement;
}

int find(List l, int v)
{
  // Add sentinel node
  Node sentinelNode;
  sentinelNode.value = v;
  l.lastElement-&gt;next = sentinelNode;

  // main loop
  n = l.firstElement;
  while (n-&gt;value != v)
    n = n-&gt;next;
  
  // termination
  l.lastElement-&gt;next = NULL; // clean up
  if (n == &amp;sentinelNode) // sentinel value, not real result
    return NULL;
  return n;
}
&lt;/source&gt;
Note that this relies on memory addresses providing a unique identity to detect the sentinel node; this commonly holds in implementation.

==Notes==
{{Notelist}}

== See also ==
* [[Sentinel node]]
* [[Semipredicate problem]]
* [[Elephant in Cairo]]
* [[Magic number (programming)]]
* [[Magic string]]
* [[Time formatting and storage bugs]]

==References==
{{reflist}}

[[Category:Linked lists]]
[[Category:Trees (data structures)]]</text>
      <sha1>4ek25unua20iawjik603m92znj44ek4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Unrolled linked list</title>
    <ns>0</ns>
    <id>1035267</id>
    <revision>
      <id>611073705</id>
      <parentid>600731511</parentid>
      <timestamp>2014-06-01T13:04:11Z</timestamp>
      <contributor>
        <username>EmausBot</username>
        <id>11292982</id>
      </contributor>
      <minor/>
      <comment>Bot: Migrating 3 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:Q4388313]]</comment>
      <text xml:space="preserve" bytes="5755">[[File:Unrolled linked lists (1-8).PNG|thumb|180px|right|'''Unrolled linked list'''&lt;br /&gt;On this model, maximum number of elements is 4 for each node.]]

In computer programming, an '''unrolled linked list''' is a variation on the [[linked list]] which stores multiple elements in each node. It can dramatically increase [[CPU cache|cache]] performance, while  decreasing the memory overhead associated with storing list metadata such as [[reference]]s. It is related to the [[B-tree]].

==Overview==
A typical unrolled linked list node looks like this:

  '''[[record (computer science)|record]]''' node {
      ''node'' next       ''// reference to next node in list''
      ''int'' numElements ''// number of elements in this node, up to maxElements''
      ''array'' elements  ''// an array of numElements elements,''
                      ''//   with space allocated for maxElements elements''
  }

Each node holds up to a certain maximum number of elements, typically just large enough so that the node fills a single [[cache line]] or a small multiple thereof. A position in the list is indicated by both a reference to the node and a position in the elements array. It is also possible to include a ''previous'' pointer for an unrolled [[doubly linked list]].

To insert a new element, we simply find the node the element should be in and insert the element into the &lt;code&gt;elements&lt;/code&gt; array, incrementing &lt;code&gt;numElements&lt;/code&gt;. If the array is already full, we first insert a new node either preceding or following the current one and move half of the elements in the current node into it.

To remove an element, we simply find the node it is in and delete it from the &lt;code&gt;elements&lt;/code&gt; array, decrementing &lt;code&gt;numElements&lt;/code&gt;. If this reduces the node to less than half-full, then we move elements from the next node to fill it back up above half. If this leaves the next node less than half full, then we move all its remaining elements into the current node, then bypass and delete it.

==Performance==
One of the primary benefits of unrolled linked lists is decreased storage requirements. All nodes (except at most one) are at least half-full. If many random inserts and deletes are done, the average node will be about three-quarters full, and if inserts and deletes are only done at the beginning and end, almost all nodes will be full. Assume that:
* ''m'' = &lt;code&gt;maxElements&lt;/code&gt;, the maximum number of elements in each &lt;code&gt;elements&lt;/code&gt; array;
* ''v'' = the overhead per node for references and element counts;
* ''s'' = the size of a single element.
Then, the space used for ''n'' elements varies between &lt;math&gt;(v/m + s)n&lt;/math&gt; and &lt;math&gt;(2v/m + s)n&lt;/math&gt;. For comparison, ordinary linked lists require &lt;math&gt;(v + s)n&lt;/math&gt; space, although ''v'' may be smaller, and [[array data structure|array]]s, one of the most compact data structures, require &lt;math&gt;sn&lt;/math&gt; space. Unrolled linked lists effectively spread the overhead ''v'' over a number of elements of the list. Thus, we see the most significant space gain when overhead is large, &lt;code&gt;maxElements&lt;/code&gt; is large, or elements are small.

If the elements are particularly small, such as bits, the overhead can be as much as 64 times larger than the data on many machines. Moreover, many popular memory allocators will keep a small amount of metadata for each node allocated, increasing the effective overhead ''v''. Both of these make unrolled linked lists more attractive.

Because unrolled linked list nodes each store a count next to the ''next'' field, retrieving the ''k''th element of an unrolled linked list (indexing) can be done in ''n''/''m'' + 1 cache misses, up to a factor of ''m'' better than ordinary linked lists. Additionally, if the size of each element is small compared to the cache line size, the list can be traversed in order with fewer cache misses than ordinary linked lists. In either case, operation time still increases linearly with the size of the list.

== See also ==
* [[CDR coding]], another technique for decreasing overhead and improving cache locality in linked lists similar to unrolled linked lists.
* the [[skip list]], a similar variation on the linked list, offers fast lookup and hurts the advantages of linked lists (quick insert/deletion) less than an unrolled linked list
* the [[B-tree]] and [[T-tree]], data structures that are similar to unrolled linked lists in the sense that each of them could be viewed as an &quot;unrolled binary tree&quot;
* [[XOR linked list]], a doubly linked list that uses one XORed pointer per node instead of two ordinary pointers.
* [[Hashed array tree]], where pointers to the chunks of data are held in a higher-level, separate array.

==References==
{{reflist|30em}}

* {{Citation
 | last = Shao
 | first = Z
 | author-link = Z. Shao
 | last2 = Reppy
 | first2 = J. H.
 | author2-link = J. H. Reppy
 | last3 = Appel
 | first3 = A.W.
 | author3-link = A.W. Appel
 | title = Unrolling lists
 | journal = Conference record of the 1994 ACM Conference on Lisp and Functional Programming
 | pages =  185–191
 | year = 1994
 | url = http://dl.acm.org/ft_gateway.cfm?id=182453&amp;type=pdf&amp;CFID=80599027&amp;CFTOKEN=68993242
 | ref = harv
 | doi = 10.1145/182409.182453
 | isbn = 0897916433}}

== External links ==
* [http://en.literateprograms.org/Unrolled_linked_list_%28C_Plus_Plus%29 Implementation written in C++]
* [https://github.com/badgerman/quicklist Implementation written in C]
* [https://github.com/megatherion/Unrolled-linked-list Another implementation written in Java]
* [http://opendatastructures.org/ods-java/3_3_SEList_Space_Efficient_.html Open Data Structures&amp;mdash;Section 3.3&amp;mdash;SEList: A Space-Efficient Linked List]
{{Data structures}}




[[Category:Linked lists]]</text>
      <sha1>42zo758esi9mjisphq9kojd9sdjln5m</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Dancing Links</title>
    <ns>0</ns>
    <id>2736402</id>
    <revision>
      <id>582500284</id>
      <parentid>567664219</parentid>
      <timestamp>2013-11-20T09:38:11Z</timestamp>
      <contributor>
        <ip>188.39.27.254</ip>
      </contributor>
      <comment>Corrected spelling of &quot;Millennial&quot;</comment>
      <text xml:space="preserve" bytes="6698">[[File:Dancing links Quantumino puzzle.ogv|thumb|The Dancing Links algorithm solving a [[polycube]] puzzle]]
In [[computer science]], '''Dancing Links''', also known as '''DLX''', is the technique suggested by [[Donald Knuth]] to efficiently implement his [[Algorithm X]].&lt;ref&gt;{{cite journal
  | author = [[Donald Knuth|Knuth, Donald]]
  | title = Dancing links
  | version = P159
  | year = 2000
  | volume = 187
  | journal = Millennial Perspectives in Computer Science
  | arxiv = cs/0011047
  | url = http://www-cs-faculty.stanford.edu/~uno/papers/dancing-color.ps.gz
  | accessdate = 2006-07-11 }}&lt;/ref&gt; Algorithm X is a [[recursion (computer science)|recursive]], [[nondeterministic algorithm|nondeterministic]], [[depth-first]], [[backtracking]] [[algorithm]] that finds all solutions to the [[exact cover]] problem. Some of the better-known exact cover problems include [[tessellation|tiling]], the [[Eight queens puzzle|''n'' queens problem]], and [[Sudoku]].

The name ''Dancing Links'' comes from the way the algorithm works, as iterations of the algorithm cause the links to &quot;dance&quot; with partner links so as to resemble an &quot;exquisitely choreographed dance.&quot; Knuth credits Hiroshi Hitotsumatsu and Kōhei Noshita with having invented the idea in 1979,&lt;ref&gt;{{Cite journal
 | last1 = Hitotumatu | first1 = Hirosi
 | last2 = Noshita | first2 = Kohei
 | doi = 10.1016/0020-0190(79)90016-4
 | issue = 4
 | journal = Information Processing Letters
 | pages = 174–175
 | title = A Technique for Implementing Backtrack Algorithms and its Application
 | volume = 8
 | year = 1979}}&lt;/ref&gt; but it is his paper which has popularized it.

==Implementation==
As the remainder of this article discusses the details of an implementation technique for Algorithm X, the reader is strongly encouraged to read the [[Algorithm X]] article first.

===Main ideas===
The idea of DLX is based on the observation that in a circular [[doubly linked list]] of nodes,

 x.left.right ← x.right;
 x.right.left ← x.left;

will remove node ''x'' from the list, while

 x.left.right ← x;
 x.right.left ← x;

will restore ''x'''s position in the list, assuming that x.right and x.left have been left unmodified. This works regardless of the number of elements in the list, even if that number is 1.

[[Donald Knuth|Knuth]] observed that a naive implementation of his Algorithm X would spend an inordinate amount of time searching for 1's. When selecting a column, the entire matrix had to be searched for 1's. When selecting a row, an entire column had to be searched for 1's. After selecting a row, that row and a number of columns had to be searched for 1's. To improve this search time from [[Big O notation|complexity]] O(n) to O(1), Knuth implemented a [[sparse matrix]] where only 1's are stored.

At all times, each node in the matrix will point to the adjacent nodes to the left and right (1's in the same row), above and below (1's in the same column), and the header for its column (described below).  Each row and column in the matrix will consist of a circular doubly linked list of nodes.

===Header===
Each column will have a special node known as the &quot;column header,&quot; which will be included in the column list, and will form a special row (&quot;control row&quot;) consisting of all the columns which still exist in the matrix.

Finally, each column header may optionally track the number of nodes in its column, so that locating a column with the lowest number of nodes is of [[Big O notation|complexity]] O(''n'') rather than O(''n''&amp;times;''m'') where ''n'' is the number of columns and ''m'' is the number of rows.  Selecting a column with a low node count is a heuristic which improves performance in some cases, but is not essential to the algorithm.

===Exploring===
In Algorithm X, rows and columns are regularly eliminated from and restored to the matrix. Eliminations are determined by selecting a column and a row in that column. If a selected column doesn't have any rows, the current matrix is unsolvable and must be backtracked. When an elimination occurs, all columns for which the selected row contains a 1 are removed, along with all rows (including the selected row) that contain a 1 in any of the removed columns. The columns are removed because they have been filled, and the rows are removed because they conflict with the selected row. To remove a single column, first remove the selected column's header. Next, for each row where the selected column contains a 1, traverse the row and remove it from other columns (this makes those rows inaccessible and is how conflicts are prevented). Repeat this column removal for each column where the selected row contains a 1. This order ensures that any removed node is removed exactly once and in a predictable order, so it can be backtracked appropriately. If the resulting matrix has no columns, then they have all been filled and the selected rows form the solution.

===Backtracking===
To backtrack, the above process must be reversed using the second algorithm stated above. One requirement of using that algorithm is that backtracking must be done as an exact reversal of eliminations. Knuth's paper gives a clear picture of these relationships and how the node removal and reinsertion works, and provides a slight relaxation of this limitation.

===Optional constraints===
It is also possible to solve one-cover problems in which a particular constraint is optional, but can be satisfied no more than once. Dancing Links accommodates these with primary columns which must be filled and secondary columns which are optional. This alters the algorithm's solution test from a matrix having no columns to a matrix having no primary columns, but doesn't require any further changes. Knuth discusses optional constraints as applied to the [[Eight queens puzzle|''n'' queens problem]]. The chessboard diagonals represent optional constraints, as some diagonals may not be occupied. If a diagonal is occupied, it can be occupied only once.

==References==
{{reflist}}

==External links==
* A [http://hadoop.apache.org/common/docs/r0.20.2/api/org/apache/hadoop/examples/dancing/package-summary.html distributed Dancing Links] implementation as a [[Hadoop]] [[MapReduce]] example
* [http://cheeso.members.winisp.net/srcview.aspx?dir=Sudoku&amp;file=ExactCover.cs C# implementation of an Exact Cover solver] - uses Algorithm X and the Dancing Links trick.
* [http://www.nuget.org/packages/DlxLib DlxLib NuGet package] - a C# class library that implements DLX

&lt;!--  this is not really a German version of this article --&gt;

{{Donald Knuth navbox}}

[[Category:Search algorithms]]
[[Category:Linked lists]]
[[Category:Donald Knuth]]
[[Category:Sudoku]]</text>
      <sha1>153qc0a26k9ow47kagadxzqhcr1qo9a</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Doubly linked list</title>
    <ns>0</ns>
    <id>4731859</id>
    <revision>
      <id>615636944</id>
      <parentid>612718899</parentid>
      <timestamp>2014-07-05T00:24:04Z</timestamp>
      <contributor>
        <username>Tyriar</username>
        <id>9451169</id>
      </contributor>
      <minor/>
      <comment>/* Open doubly-linked lists */ Fix indentation</comment>
      <text xml:space="preserve" bytes="10681">{{refimprove|date=January 2014}}

In [[computer science]], a '''doubly-linked list''' is a [[linked data structure]] that consists of a set of sequentially linked [[record (computer science)|record]]s called [[node (computer science)|nodes]]. Each node contains two [[field (computer science)|field]]s, called ''links'', that are [[reference (computer science)|reference]]s to the previous and to the next node in the sequence of nodes.  The beginning and ending nodes' '''previous''' and '''next''' links, respectively, point to some kind of terminator, typically a [[sentinel node]] or [[null pointer|null]], to facilitate traversal of the list. If there is only one sentinel node, then the list is circularly linked via the sentinel node. It can be conceptualized as two [[linked list|singly linked list]]s formed from the same data items, but in opposite sequential orders.

[[File:Doubly-linked-list.svg|frame|none|alt=A doubly-linked list whose nodes contain three fields: an integer value, the link to the next node, and the link to the previous node.|A doubly-linked list whose nodes contain three fields: an integer value, the link to the next node, and the link to the previous node.]]&lt;!--THIS IMAGE SHOULD BE MODIFIED TO USE THE STANDARD NOTATION FOR NULL POINTERS. (Actually, the concept of &quot;null pointer&quot; should be left out and be replaced by &quot;terminator&quot; because sentinel links are quite commonly used instead of null pointers -tony).--&gt;

The two node links allow traversal of the list in either direction. While adding or removing a node in a doubly-linked list requires changing more links than the same operations on a singly linked list, the operations are simpler and potentially more efficient (for nodes other than first nodes) because there is no need to keep track of the previous node during traversal or no need to traverse the list to find the previous node, so that its link can be modified.

==Nomenclature and implementation==
The first and last nodes of a doubly-linked list are immediately accessible (i.e., accessible without traversal, and usually called ''head'' and ''tail'') and therefore allow traversal of the list from the beginning or end of the list, respectively: e.g., traversing the list from beginning to end, or from end to beginning, in a search of the list for a node with specific data value. Any node of a doubly-linked list, once obtained, can be used to begin a new traversal of the list, in either direction (towards beginning or end), from the given node.

The link fields of a doubly-linked list node are often called '''next''' and '''previous''' or '''forward''' and '''backward'''. The references stored in the link fields are usually implemented as [[pointer (computer programming)|pointer]]s, but (as in any linked data structure) they may also be address offsets or indices into an [[Array data structure|array]] where the nodes live.

==Basic algorithms==

===Open doubly-linked lists===

 '''record''' ''DoublyLinkedNode'' {
     prev ''// A reference to the previous node''
     next ''// A reference to the next node''
     data ''// Data or a reference to data''
  }

 '''record''' ''DoublyLinkedList'' {
      ''DoublyLinkedNode'' firstNode   ''// points to first node of list''
      ''DoublyLinkedNode'' lastNode    ''// points to last node of list''
 }

====Traversing the list====

Traversal of a doubly-linked list can be in either direction. In fact, the direction of traversal can change many times, if desired. '''Traversal''' is often called '''[[iteration]]''', but that choice of terminology is unfortunate, for '''iteration''' has well-defined semantics (e.g., in mathematics) which are not analogous to '''traversal'''.

''Forwards''
 node  := list.firstNode
  '''while''' node ≠ '''null'''
      &lt;do something with node.data&gt;
      node  := node.next

''Backwards''
 node  := list.lastNode
  '''while''' node ≠ '''null'''
      &lt;do something with node.data&gt;
      node  := node.prev

====Inserting a node====

These symmetric functions insert a node either after or before a given node:

 '''function''' insertAfter(''List'' list, ''Node'' node, ''Node'' newNode)
      newNode.prev  := node
      newNode.next  := node.next
      '''if''' node.next == '''null'''
          list.lastNode  := newNode
      '''else'''
          node.next.prev  := newNode
      node.next  := newNode

 '''function''' insertBefore(''List'' list, ''Node'' node, ''Node'' newNode)
      newNode.prev  := node.prev
      newNode.next  := node
      '''if''' node.prev == '''null'''
          list.firstNode  := newNode
      '''else'''
          node.prev.next  := newNode
      node.prev  := newNode

We also need a function to insert a node at the beginning of a possibly empty list:

 '''function''' insertBeginning(''List'' list, ''Node'' newNode)
      '''if''' list.firstNode == '''null'''
          list.firstNode  := newNode
          list.lastNode   := newNode
          newNode.prev  := null
          newNode.next  := null
      '''else'''
          insertBefore(list, list.firstNode, newNode)

A symmetric function inserts at the end:

 '''function''' insertEnd(''List'' list, ''Node'' newNode)
      '''if''' list.lastNode == '''null'''
          insertBeginning(list, newNode)
      '''else'''
          insertAfter(list, list.lastNode, newNode)

====Removing a node====
Removal of a node is easier than insertion, but requires special handling if the node to be removed is the ''firstNode'' or ''lastNode'':

 '''function''' remove(''List'' list, ''Node'' node)
    '''if''' node.prev == '''null'''
        list.firstNode  := node.next
    '''else'''
        node.prev.next  := node.next
    '''if''' node.next == '''null'''
        list.lastNode  := node.prev
    '''else'''
        node.next.prev  := node.prev
    '''destroy''' node

One subtle consequence of the above procedure is that deleting the last node of a list sets both ''firstNode'' and ''lastNode'' to ''null'', and so it handles removing the last node from a one-element list correctly. Notice that we also don't need separate &quot;removeBefore&quot; or &quot;removeAfter&quot; methods, because in a doubly-linked list we can just use &quot;remove(node.prev)&quot; or &quot;remove(node.next)&quot; where these are valid. This also assumes that the node being removed is guaranteed to exist. If the node does not exist in this list, then some error handling would be required.

===Circular doubly-linked lists===

====Traversing the list====
Assuming that ''someNode'' is some node in a non-empty list, this code traverses through that list starting with ''someNode'' (any node will do):

''Forwards''
 node  := someNode
  '''do'''
      do something with node.value
      node  := node.next
  '''while''' node ≠ someNode

''Backwards''
 node  := someNode
  '''do'''
      do something with node.value
      node  := node.prev
  '''while''' node ≠ someNode

Notice the postponing of the test to the end of the loop. This is important for the case where the list contains only the single node ''someNode''.

====Inserting a node====
This simple function inserts a node into a doubly-linked circularly linked list after a given element:

 '''function''' insertAfter(''Node'' node, ''Node'' newNode)
      newNode.next  := node.next
      newNode.prev  := node
      node.next.prev  := newNode
      node.next       := newNode

To do an &quot;insertBefore&quot;, we can simply &quot;insertAfter(node.prev, newNode)&quot;.

Inserting an element in a possibly empty list requires a special function:

 '''function''' insertEnd(''List'' list, ''Node'' node)
      '''if''' list.lastNode == '''null'''
          node.prev := node
          node.next := node
      '''else'''
          insertAfter(list.lastNode, node)
      list.lastNode := node

To insert at the beginning we simply &quot;insertAfter(list.lastNode, node)&quot;.

Finally, removing a node must deal with the case where the list empties:

 '''function''' remove(''List'' list, ''Node'' node)
      '''if''' node.next == node
          list.lastNode := ''null''
      '''else'''
          node.next.prev := node.prev
          node.prev.next := node.next
          '''if''' node == list.lastNode
              list.lastNode := node.prev;
      '''destroy''' node

&lt;!--PLEASE FIX THIS:
====Deleting a node====
As in doubly-linked lists, &quot;removeAfter&quot; and &quot;removeBefore&quot; can be implemented with &quot;remove(list, node.prev)&quot; and &quot;remove(list, node.next)&quot;.

example:

  node 'tom' next pointing to node 'jerry'...till the last point at node 'fix'..
  data in node 'fix' previous to node 'tom'...looping instruction..
--&gt;

== Advanced concepts ==

=== Asymmetric doubly-linked list ===

An asymmetric doubly-linked list is somewhere between the singly-linked list and the regular doubly-linked list. It shares some features with the singly linked list (single-direction traversal) and others from the doubly-linked list (ease of modification)

It is a list where each node's ''previous'' link points not to the previous node, but to the link to itself. While this makes little difference between nodes (it just points to an offset within the previous node), it changes the head of the list: It allows the first node to modify the ''firstNode'' link easily.&lt;ref&gt;http://www.codeofhonor.com/blog/avoiding-game-crashes-related-to-linked-lists&lt;/ref&gt;&lt;ref&gt;https://github.com/webcoyote/coho/blob/master/Base/List.h&lt;/ref&gt;

As long as a node is in a list, its ''previous'' link is never null.

==== Inserting a node ====
To insert a node before another, we change the link that pointed to the old node, using the ''prev'' link; then set the new node's ''next'' link to point to the old node, and change that node's ''prev'' link accordingly.

 '''function''' insertBefore(''Node'' node, ''Node'' newNode)
      '''if''' node.prev == '''null'''
           '''error''' &quot;The node is not in a list&quot;
      newNode.prev  := node.prev
      atAddress(newNode.prev)  := newNode
      newNode.next  := node
      node.prev = addressOf(newNode.next)

 '''function''' insertAfter(''Node'' node, ''Node'' newNode)
      newNode.next  := node.next
      '''if''' newNode.next != '''null'''
          newNode.next.prev = addressOf(newNode.next)
      node.next  := newNode
      newNode.prev  := addressOf(node.next)

==== Deleting a node ====
To remove a node, we simply modify the link pointed by ''prev'', regardless of whether the node was the first one of the list.

 '''function''' remove(''Node'' node)
      atAddress(node.prev)  := node.next
      '''if''' node.next != '''null'''
          node.next.prev = node.prev
      '''destroy''' node

==See also==
* [[XOR linked list]]
* [[SLIP (programming language)]]

==References==
{{Reflist}}

[[Category:Linked lists]]</text>
      <sha1>p211pmm1e0dzf4su3hm8bj8tl7fd6g3</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Association list</title>
    <ns>0</ns>
    <id>394077</id>
    <revision>
      <id>547284302</id>
      <parentid>492108771</parentid>
      <timestamp>2013-03-27T16:02:43Z</timestamp>
      <contributor>
        <ip>1.127.85.83</ip>
      </contributor>
      <comment>add data structure infobox</comment>
      <text xml:space="preserve" bytes="2473">{{Infobox data structure
|name=Association list
|type=[[associative array]]
|invented_by=
|invented_year=
|space_avg=O(''n'')
|space_worst=O(''n'')
|search_avg=O(''n'')
|search_worst=O(''n'')
|insert_avg=O(1)
|insert_worst=O(1)
|delete_avg=O(''n'')
|delete_worst=O(''n'')
}}

In [[computer programming]] and particularly in [[Lisp (programming language)|Lisp]], an '''association list''', often referred to as an '''alist''', is a [[linked list]] in which each list element (or [[node (computer science)|node]]) comprises a key and a value.  The association list is said to ''associate'' the value with the key.  In order to find the value associated with a given key, each element of the list is searched in turn, starting at the head, until the key is found.  Duplicate keys that appear later in the list are ignored.  It is a simple way of implementing an [[associative array]].

The disadvantage of association lists is that the time to search is [[Big O notation|O(n)]], where n is the length of the list.  And unless the list is regularly pruned to remove elements with duplicate keys multiple values associated with the same key will increase the size of the list, and thus the time to search, without providing any compensatory advantage.  One advantage is that a new element can be added to the list at its head, which can be done in constant time.  For quite small values of n it is more efficient in terms of time and space than more sophisticated strategies such as [[hash table]]s and [[Binary search tree|trees]].

In the early development of Lisp, association lists were used to resolve references to [[Free variables and bound variables|free variables]] in procedures.&lt;ref name=&quot;mccarthy_lisp_1.5&quot;&gt;{{cite book | url = http://www.softwarepreservation.org/projects/LISP/book/LISP%201.5%20Programmers%20Manual.pdf | title = LISP 1.5 Programmer's Manual | publisher = [[MIT Press]] | first1 = John | last1 = McCarthy | first2 = Paul W. | last2 = Abrahams | first3 = Daniel J. | last3 = Edwards | first4 = Timothy P. | last4 = Hart | first5 = Michael I. | last5 = Levin | isbn = 0-262-13011-4 | year = 1985}}&lt;/ref&gt;

Many programming languages, including Lisp, [[Scheme (programming language)|Scheme]], [[OCaml]], and [[Haskell (programming language)|Haskell]] have functions for handling association lists in their standard library.

==References==
&lt;references/&gt;

{{Data structures}}

[[Category:Linked lists]]
[[Category:Associative arrays]]


{{Comp-sci-stub}}</text>
      <sha1>6h6ntayh9xeg2oxxo8mqt2ptrykvsmy</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Difference list</title>
    <ns>0</ns>
    <id>11436072</id>
    <revision>
      <id>555567343</id>
      <parentid>555567142</parentid>
      <timestamp>2013-05-17T20:57:39Z</timestamp>
      <contributor>
        <username>Fvillanustre</username>
        <id>14141205</id>
      </contributor>
      <comment>No longer an orphan article. Added incoming link from the data structures wiki topic</comment>
      <text xml:space="preserve" bytes="2107">In [[computer science]], the term '''difference list''' may refer to one of two [[data structure]]s for representing [[list (computing)|list]]s. One of these data structures contains two lists, and represents the difference of those two lists. The second data structure is a [[functional programming|functional]] representation of a list with an efficient [[concatenation]] operation. In the second approach, difference lists are implemented as single-argument [[function (programming)|function]]s, which take a list as [[argument (programming)|argument]] and prepend to that list. As a consequence, concatenation of difference lists of the second type is implemented essentially as [[function composition]], which is [[constant time|O(1)]]. However, of course the list still has to be constructed eventually (assuming all of its elements are needed), which is plainly at least O(n).

== Difference lists as functions ==

A difference list of the second sort represents lists as a function ''f'', which when given a list ''x'', returns the list that ''f'' represents, prepended to ''x''. It is typically used in functional programming languages such as [[Haskell programming language|Haskell]], although it could be used in imperative languages as well. Whether this kind of difference list is more efficient than another list representations depends on usage patterns. If an algorithm builds a list by concatenating smaller lists, which are themselves built by concatenating still smaller lists, then use of difference lists can improve performance by effectively &quot;flattening&quot; the list building computations.

Examples of use are in the ''ShowS'' type in the Prelude of Haskell, and in Donald Bruce Stewart's [http://hackage.haskell.org/cgi-bin/hackage-scripts/package/dlist difference list library for Haskell].


== External links ==
* [http://homepages.inf.ed.ac.uk/pbrna/prologbook/node180.html Open Lists and Difference Lists] in [[Prolog]]
* [http://www.haskell.org/haskellwiki/Difference_list Difference Lists] in [[Haskell (programming language)]]

{{datastructure-stub}}

[[Category:Linked lists]]</text>
      <sha1>1b1m03r2ibvibvocbkbuwbqg17rksjn</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Skip list</title>
    <ns>0</ns>
    <id>336155</id>
    <revision>
      <id>625732813</id>
      <parentid>625204393</parentid>
      <timestamp>2014-09-15T22:51:58Z</timestamp>
      <contributor>
        <ip>2A01:E35:2EBD:C510:5246:5DFF:FE57:C524</ip>
      </contributor>
      <comment>/* Usages */</comment>
      <text xml:space="preserve" bytes="16011">{{Technical|reason=I am a very intelligent native speaker of the English language, and the lead section of this article is completely incomprehensible to me. Highly technical language may be appropriate in later sections of an article on a highly technical subject, but the lead section of an article on ANY subject should be comprehensible to a person who is not an expert in the field, without having to follow links to other articles that may be no more comprehensible than it is. This one fails completely.|date=October 2011}}

{{Infobox data structure
|name=Skip List
|type=List
|invented_by=[[William Pugh|W. Pugh]]
|invented_year=1989
|
|space_avg=O(n)
|space_worst=O(n log n)&lt;ref name=&quot;cs.uwaterloo&quot;&gt;http://www.cs.uwaterloo.ca/research/tr/1993/28/root2side.pdf&lt;/ref&gt;
|search_avg=O(log n)
|search_worst=O(n)&lt;ref name=&quot;cs.uwaterloo&quot; /&gt;
|insert_avg=O(log n)
|insert_worst=O(n)
|delete_avg=O(log n)
|delete_worst=O(n)
}}
{{Probabilistic}}
In [[computer science]], a '''skip list''' is a [[data structure]] that allows fast search within an [[ordered sequence]] of elements. Fast search is made possible by maintaining a [[linked list|linked]] hierarchy of subsequences, each skipping over fewer elements. Searching starts in the sparsest subsequence until two consecutive elements have been found, one smaller and one larger than the element searched for. Via the linked hierarchy these two elements link to elements of the next sparsest subsequence where searching is continued until finally we are searching in the full sequence. The elements that are skipped over may be chosen probabilistically.&lt;ref name=&quot;pugh&quot;&gt;{{cite doi|10.1145/78973.78977}}&lt;/ref&gt;&lt;ref&gt;[http://www.ic.unicamp.br/~celio/peer2peer/skip-net-graph/deterministic-skip-lists-munro.pdf Deterministic skip lists]&lt;/ref&gt;

[[Image:Skip list.svg|center]]

== Description ==

A skip list is built in layers.  The bottom layer is an ordinary ordered [[linked list]].  Each higher layer acts as an &quot;express lane&quot; for the lists below, where an element in layer ''i'' appears in layer ''i''+1 with some fixed probability ''p'' (two commonly used values for ''p'' are 1/2 or 1/4).  On average, each element appears in 1/(1-''p'') lists, and the tallest element (usually a special head element at the front of the skip list) in &lt;math&gt;\log_{1/p} n\,&lt;/math&gt; lists.

A search for a target element begins at the head element in the top list, and proceeds horizontally until the current element is greater than or equal to the target. If the current element is equal to the target, it has been found. If the current element is greater than the target, or the search reaches the end of the linked list, the procedure is repeated after returning to the previous element and dropping down vertically to the next lower list. The expected number of steps in each linked list is at most 1/''p'', which can be seen by tracing the search path backwards from the target until reaching an element that appears in the next higher list or reaching the beginning of the current list.  Therefore, the total ''expected'' cost of a search is &lt;math&gt;(\log_{1/p} n)/p,\,&lt;/math&gt; which is &lt;math&gt;\mathcal{O}(\log n)\,&lt;/math&gt; when ''p'' is a constant.  By choosing different values of ''p'', it is possible to trade search costs against storage costs.

=== Implementation details ===
[[File:Skip list add element-en.gif|thumb|Skip list add element-en|500px|Inserting elements to skip list]]
The elements used for a skip list can contain more than one pointer since they can participate in more than one list.

Insertions and deletions are implemented much like the corresponding linked-list operations, except that &quot;tall&quot; elements must be inserted into or deleted from more than one linked list.

&lt;math&gt;\mathcal{O}(n)&lt;/math&gt; operations, which force us to visit every node in ascending order (such as printing the entire list), provide the opportunity to perform a behind-the-scenes derandomization of the level structure of the skip-list in an optimal way, bringing the skip list to &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; search time. (Choose the level of the i'th finite node to be 1 plus the number of times we can repeatedly divide i by 2 before it becomes odd.  Also, i=0 for the negative infinity header as we have the usual special case of choosing the highest possible level for negative and/or positive infinite nodes.) However this also allows someone to know where all of the higher-than-level 1 nodes are and delete them.

Alternatively, we could make the level structure quasi-random in the following way:

 make all nodes level 1
 j ← 1
 '''while''' the number of nodes at level j &gt; 1 '''do'''
   '''for''' each i'th node at level j '''do'''
     '''if''' i is odd 
       '''if''' i is not the last node at level j
         randomly choose whether to promote it to level j+1
       '''else'''
         do not promote
       '''end if'''
     '''else if''' i is even and node i-1 was not promoted
       promote it to level j+1
     '''end if'''
   '''repeat'''
   j ← j + 1
 '''repeat'''

Like the derandomized version, quasi-randomization is only done when there is some other reason to be running a &lt;math&gt;\mathcal{O}(n)&lt;/math&gt; operation (which visits every node).

The advantage of this quasi-randomness is that it doesn't give away nearly as much level-structure related information to an [[Adversary (online algorithm)|adversarial user]] as the de-randomized one.  This is desirable because an adversarial user who is able to tell which nodes are not at the lowest level can pessimize performance by simply deleting higher-level nodes. The search performance is still guaranteed to be logarithmic.

It would be tempting to make the following &quot;optimization&quot;:  In the part which says &quot;Next, for each i'th...&quot;, forget about doing a coin-flip for each even-odd pair.  Just flip a coin once to decide whether to promote only the even ones or only the odd ones.  Instead of &lt;math&gt;\mathcal{O}(n \log n)&lt;/math&gt; coin flips, there would only be &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; of them.  Unfortunately, this gives the adversarial user a 50/50 chance of being correct upon guessing that all of the even numbered nodes (among the ones at level 1 or higher) are higher than level one.  This is despite the property that he has a very low probability of guessing that a particular node is at level ''N'' for some integer ''N''.

A skip list does not provide the same absolute worst-case performance guarantees as more traditional [[balanced tree]] data structures, because it is always possible (though with very low probability) that the coin-flips used to build the skip list will produce a badly balanced structure.  However, they work well in practice, and the randomized balancing scheme has been argued to be easier to implement than the deterministic balancing schemes used in balanced binary search trees.  Skip lists are also useful in [[parallel computing]], where insertions can be done in different parts of the skip list in parallel without any global rebalancing of the data structure. Such parallelism can be especially advantageous for resource discovery in an ad-hoc [[Wireless network]] because a randomized skip list can be made robust to the loss of any single node.&lt;ref&gt;{{cite paper | last=Shah | first=Gauri Ph.D. |author2=James Aspnes  | title=Distributed Data Structures for Peer-to-Peer Systems | date=December 2003 | url=http://www.cs.yale.edu/homes/shah/pubs/thesis.pdf | format=PDF | accessdate=2008-09-23}}&lt;/ref&gt;

There has been some evidence that skip lists have worse real-world performance and space requirements than [[B tree]]s due to [[memory locality]] and other issues.&lt;ref&gt;http://resnet.uoregon.edu/~gurney_j/jmpc/skiplist.html&lt;/ref&gt;

=== Indexable skiplist ===

As described above, a skiplist is capable of fast &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; insertion and removal of values from a sorted sequence, but it has only slow &lt;math&gt;\mathcal{O}(n)&lt;/math&gt; lookups of values at a given position in the sequence (i.e. return the 500th value); however, with a minor modification the speed of [[random access]] indexed lookups can be improved to  &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt;.

For every link, also store the width of the link.  The width is defined as the number of bottom layer links being traversed by each of the higher layer &quot;express lane&quot; links.

For example, here are the widths of the links in the example at the top of the page:

    1                               10
  o---&gt; o---------------------------------------------------------&gt; o    Top level
    1           3              2                    5
  o---&gt; o---------------&gt; o---------&gt; o---------------------------&gt; o    Level 3
    1        2        1        2                    5
  o---&gt; o---------&gt; o---&gt; o---------&gt; o---------------------------&gt; o    Level 2
    1     1     1     1     1     1     1     1     1     1     1 
  o---&gt; o---&gt; o---&gt; o---&gt; o---&gt; o---&gt; o---&gt; o---&gt; o---&gt; o---&gt; o---&gt; o    Bottom level
                                         ''' '''
 Head  1st   2nd   3rd   4th   5th   6th   7th   8th   9th   10th  NIL
       Node  Node  Node  Node  Node  Node  Node  Node  Node  Node

Notice that the width of a higher level link is the sum of the component links below it (i.e. the width 10 link spans the links of widths 3, 2 and 5 immediately below it).  Consequently, the sum of all widths is the same on every level (10 + 1 = 1 + 3 + 2 + 5 = 1 + 2 + 1 + 2 + 5).

To index the skiplist and find the i'th value, traverse the skiplist while counting down the widths of each traversed link.  Descend a level whenever the upcoming width would be too large.

For example, to find the node in the fifth position (Node 5), traverse a link of width 1 at the top level.  Now four more steps are needed but the next width on this level is ten which is too large, so drop one level.  Traverse one link of width 3.  Since another step of width 2 would be too far, drop down to the bottom level.  Now traverse the final link of width 1 to reach the target running total of 5 (1+3+1). 
  
  '''function''' lookupByPositionIndex(i)
      node ← head
      i ← i + 1                           ''# don't count the head as a step''
      '''for''' level '''from''' top '''to''' bottom '''do'''
           '''while''' i ≥ node.width[level] '''do''' ''# if next step is not too far''
               i ← i - node.width[level]  ''# subtract the current width''
               node ← node.next[level]    ''# traverse forward at the current level''
           '''repeat'''
      '''repeat'''
      '''return''' node.value
  '''end function'''

This method of implementing indexing is detailed in [http://cg.scs.carleton.ca/~morin/teaching/5408/refs/p90b.pdf Section 3.4 Linear List Operations in &quot;A skip list cookbook&quot; by William Pugh].

==History==

Skip lists were first described in 1989 by [[William Pugh]].&lt;ref&gt;[[William Pugh]] (April 1989). &quot;Concurrent Maintenance of Skip Lists&quot;, Tech. Report CS-TR-2222, Dept. of Computer Science, U. Maryland.&lt;/ref&gt;

To quote the author:

:''Skip lists are a probabilistic data structure that seem likely to supplant balanced trees as the implementation method of choice for many applications. Skip list algorithms have the same asymptotic expected time bounds as balanced trees and are simpler, faster and use less space.''

==Usages==
List of applications and frameworks that use skip lists:
*[[Cyrus IMAP server]] offers a &quot;skiplist&quot; backend DB implementation ([http://git.cyrusimap.org/cyrus-imapd/tree/lib/cyrusdb_skiplist.c source file])
*[[Lucene]] uses skip lists to search delta-encoded posting lists in logarithmic time.{{Citation needed|date=June 2014}}
*[http://qt-project.org/doc/qt-4.8/qmap.html#details QMap] (up to Qt 4) template class of [[Qt (framework)|Qt]] that provides a dictionary.
*[[Redis]], an ANSI-C open-source persistent key/value store for Posix systems, uses skip lists in its implementation of ordered sets.&lt;ref&gt;{{cite web | title=Redis ordered set implementation | url=https://github.com/antirez/redis/blob/unstable/src/t_zset.c}}&lt;/ref&gt;
*[https://github.com/shuttler/nessDB nessDB], a very fast key-value embedded Database Storage Engine (Using log-structured-merge (LSM) trees), uses skip lists for its memtable.
*[http://www.dekorte.com/projects/opensource/skipdb/ skipdb] is an open-source database format using ordered key/value pairs.
* [http://download.oracle.com/javase/6/docs/api/java/util/concurrent/ConcurrentSkipListSet.html ConcurrentSkipListSet] and [http://download.oracle.com/javase/6/docs/api/java/util/concurrent/ConcurrentSkipListMap.html ConcurrentSkipListMap] in the Java 1.6 API.
*[https://code.google.com/p/leveldb/ leveldb], a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values
* [http://code.activestate.com/recipes/576930/ Skip lists are used for efficient statistical computations] of [[Moving average#Moving median|running medians]] (also known as moving medians).
Skip lists are also used in distributed applications (where the nodes represent physical computers, and pointers represent network connections) and for implementing highly scalable concurrent [[priority queue|priority queues]] with less lock contention,&lt;ref&gt;{{cite doi|10.1109/IPDPS.2000.845994}}&lt;/ref&gt; or even without locking,&lt;ref&gt;{{cite doi|10.1109/IPDPS.2003.1213189}}&lt;/ref&gt;&lt;ref&gt;{{cite doi|10.1145/1011767.1011776}}&lt;/ref&gt;&lt;ref&gt;{{cite doi|10.1109/ISPA.2008.90}}&lt;/ref&gt; as well lockless concurrent dictionaries.&lt;ref&gt;{{cite doi|10.1145/967900.968188}}&lt;/ref&gt; There are also several US patents for using skip lists to implement (lockless) priority queues and concurrent dictionaries.[https://www.google.com/patents/US7937378]

==See also==
*[[Bloom filter]]
*[[Skip graph]]
* Skip trees, an alternative data structure to Skip lists in a concurrent approach: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.514
* Skip tree graphs, a distributed version of skip trees: http://www0.cs.ucl.ac.uk/staff/a.gonzalezbeltran/pubs/icc2007.pdf, http://www0.cs.ucl.ac.uk/staff/a.gonzalezbeltran/pubs/AGB-comcom08.pdf

==References==
&lt;references/&gt;

==External links==
*[http://nist.gov/dads/HTML/skiplist.html &quot;Skip list&quot; entry] in the [[Dictionary of Algorithms and Data Structures]]
*[http://msdn.microsoft.com/en-us/library/ms379573(VS.80).aspx#datastructures20_4_topic4 Skip Lists: A Linked List with Self-Balancing BST-Like Properties] on [[MSDN]] in C# 2.0
*[http://dekorte.com/projects/opensource/skipdb/ SkipDB, a BerkeleyDB-style database implemented using skip lists.]
*[http://videolectures.net/mit6046jf05_demaine_lec12/ Skip Lists lecture (MIT OpenCourseWare: Introduction to Algorithms) ]
*[http://opendatastructures.org/versions/edition-0.1e/ods-java/4_Skiplists.html Open Data Structures - Chapter 4 - Skiplists]

;Demo applets
*[http://people.ksp.sk/~kuko/bak/index.html Skip List Applet] by Kubo Kovac
*[http://iamwww.unibe.ch/~wenger/DA/SkipList/ Thomas Wenger's demo applet on skiplists]
;Implementations
*[http://codingplayground.blogspot.com/2009/01/generic-skip-list-skiplist.html A generic Skip List in C++] by Antonio Gulli
*[https://metacpan.org/module/Algorithm::SkipList Algorithm::SkipList, implementation in Perl on CPAN]
*[http://infohost.nmt.edu/tcc/help/lang/python/examples/pyskip/ John Shipman's implementation in Python]
*[http://code.activestate.com/recipes/576930/ Raymond Hettinger's implementation in Python]
*[http://love2d.org/wiki/Skip_list A Lua port of John Shipman's Python version]
*[https://gist.github.com/dmx2010/5426422 Java Implementation with index based access]
*[http://java.sun.com/javase/6/docs/api/java/util/concurrent/ConcurrentSkipListSet.html ConcurrentSkipListSet documentation for Java 6] (and [http://www.docjar.com/html/api/java/util/concurrent/ConcurrentSkipListSet.java.html sourcecode])

{{Data structures}}

{{DEFAULTSORT:Skip List}}
[[Category:1989 introductions]]
[[Category:Linked lists]]
[[Category:Probabilistic data structures]]

[[de:Liste (Datenstruktur)#Skip-Liste]]</text>
      <sha1>ew3jub64ssamq4trykrs6yfogs7korp</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Self-organizing list</title>
    <ns>0</ns>
    <id>6213252</id>
    <revision>
      <id>624941647</id>
      <parentid>621486839</parentid>
      <timestamp>2014-09-10T13:18:49Z</timestamp>
      <contributor>
        <ip>173.240.226.254</ip>
      </contributor>
      <comment>/* History */ Fixed typo, Fixed grammar</comment>
      <text xml:space="preserve" bytes="15169">A '''self-organizing list''' is a [[list (computing)|list]] that reorders its elements based on some [[self-organizing heuristic]] to improve [[average]] [[access time]].
The aim of a self-organizing list is to improve efficiency of linear search by moving more frequently accessed items towards the head of the list. A self-organizing list achieves near constant time for element access in the best case. A self-organizing list uses a reorganizing algorithm to adapt to various query distributions at runtime.

==History==
The concept of self-organizing lists has its roots in the idea of activity organization &lt;ref&gt;{{Citation
|last = Becker
|first = J.
|last = Hayes
|first = R.M.
|title = Information Storage and Retrieval: Tools, Elements, Theories
|publisher = Wiley
|place = New York
|year = 1963
}}&lt;/ref&gt;
of records in files stored on disks or tapes.    
One frequently cited discussion of self-organizing files and lists is
Knuth
.&lt;ref&gt;{{Citation
 | last = Knuth
 | first = Donald
 | author-link = Donald Knuth
 | series = [[The Art of Computer Programming]]
 | title = Sorting and Searching
 | place =
 | publisher = Addison-Wesley
 | year = 1998
 | volume = Volume 3
 | edition = Second
 | page = 402
 | pages =
 | url =
 | doi =
 | isbn = 0-201-89685-0 }}&lt;/ref&gt;
John McCabe has the first algorithmic complexity analyses of the Move-to-Front (MTF) strategy where an item
is moved to the front of the list after it is accessed.
.&lt;ref&gt;{{Citation
 | last = McCabe
 | first = John
 | title = On Serial Files with Relocatable Records
 | place =
 | journal = Operations Research
 | volume = 13
 | number = 4
 | publisher = INFORMS
 | year = 1965
 | pages = 609–618
 | url =
 | doi =10.1287/opre.13.4.609
 | isbn =  }}&lt;/ref&gt;
He analyses the average time needed for randomly ordered list to get in optimal order.
The optimal ordering of a list is the one in which items are ordered in the list by
the probability with which they will be needed, with the most accessed item first.
The optimal ordering may not be known in advance, and may also change over time.
McCabe introduced the transposition strategy in which an accessed item is exchanged with the
item in front of it in the list. He made the conjecture that transposition
worked at least as well in the average case as MTF in approaching the optimal ordering of records in the limit.
This conjecture was later proved by Rivest.&lt;ref&gt;{{Citation
 | last = Rivest
 | first = Ronald
 | title = On self-organizing sequential search heuristics
 | place =
 | journal = Communications of the ACM
 | volume = 19
 | number = 2
 | publisher = 
 | year = 1976
 | pages = 63–67
 | url =
 | doi =10.1145/359997.360000
 | isbn =  }}&lt;/ref&gt;
McCabe also noted that with either the transposition or MTF heuristic, the optimal ordering of records would
be approached even if the heuristic was only applied every Nth access, and that a value of N might be 
chosen that would reflect the relative cost of relocating records with the value of approaching the optimal ordering
more quickly.
Further improvements were made, and algorithms suggested by researchers including: Rivest, Tenenbaum and Nemes, Knuth, and 
Bentley and McGeoch (e.g. Worst-case analyses for self-organizing sequential search heuristics).

==Introduction==
The simplest implementation of a self-organizing list is as a [[linked list]] and thus while being efficient in random node inserting and memory allocation, suffers from inefficient accesses to random nodes. A self-organizing list reduces the inefficiency by dynamically rearranging the nodes in the list based on access frequency.

===Inefficiency of linked list traversals===

If a particular node is to be searched for in the list, each node in the list must be sequentially compared till the desired node is reached. In a linked list, retrieving the nth element is an O(n) operation. This is highly inefficient when compared to an array for example, where accessing the n&lt;sup&gt;th&lt;/sup&gt; element is an O(1) operation.

===Efficiency of self-organizing lists===

A self organizing list rearranges the nodes keeping the most frequently accessed ones at the head of the list. Generally, in a particular query, the chances of accessing a node which has been accessed many times before are higher than the chances of accessing a node which historically has not been so frequently accessed. As a result, keeping the commonly accessed nodes at the head of the list results in reducing the number of comparisons required in an average case to reach the desired node. This leads to better efficiency and generally reduced query times.

==Implementation of a self-organizing list==
The implementation and methods of a self-organizing list are identical to the those for a standard [[linked list]]. The linked list and the self-organizing list differ only in terms of the organization of the nodes; the interface remains the same.

==Analysis of Running Times for Access/ Search in a List==

===Average Case===

It can be shown that in the average case, the time required to a search on a self-organizing list of size n is
:&lt;math&gt;Tavg = 1 * p(1) + 2 * p(2) + 3 * p(3) + . . . + n * p(n).&lt;/math&gt;
where p(i) is the probability of accessing the ith element in the list, thus also called the access probability.
If the access probability of each element is the same (i.e. p(1) = p(2) = p(3) = ... = p(n) = 1/n) then the ordering of the elements is irrelevant and the average time complexity is given by
:&lt;math&gt;T(n) = 1/n + 2/n + 3/n + ... + n/n = (1 + 2 + 3 + ... + n)/n = (n+1)/2&lt;/math&gt;
and T(n) does not depend on the individual access probabilities of the elements in the list in this case.
However in the case of searches on lists with non uniform record access probabilities (i.e. those lists in which the probability of accessing one element is different from another), the average time complexity can be reduced drastically by proper positioning of the elements contained in the list.&lt;br/&gt;
This is done by pairing smaller i with larger access probabilities so as to reduce the overall average time complexity.
&lt;br/&gt;
This may be demonstrated as follows:&lt;br/&gt;
Given List: A(0.1), B(0.1), C(0.3), D(0.1), E(0.4)&lt;br/&gt;
Without rearranging, average search time required is:
:&lt;math&gt;T(n) = 1*0.1 + 2*0.1 + 3*0.3 + 4*0.1 + 5*0.4 = 3.6&lt;/math&gt;
Now suppose the nodes are rearranged so that those nodes with highest probability of access are placed closest to the front so that the rearranged list is now:&lt;br/&gt;
E(0.4), C(0.3), D(0.1), A(0.1), B(0.1)&lt;br/&gt;
Here, average search time is:&lt;br/&gt;
:&lt;math&gt;T(n) = 1*0.4 + 2*0.3 + 3*0.1 + 4*0.1 + 5*0.1 = 2.2&lt;/math&gt;
Thus the average time required for searching in an organized list is (in this case) around 40% less than the time required to search a randomly arranged list.
&lt;br/&gt;
This is the concept of the self-organized list in that the average speed of data retrieval is increased by rearranging the nodes according to access frequency.

===Worst Case===

In the worst case, the element to be located is at the very end of the list be it a normal list or a self-organized one and thus n comparisons must be made to reach it. Therefore the worst case running time of a linear search on the list is O(n) independent of the type of list used.
Note that the expression for the average search time in the previous section is a probabilistic one. Keeping the commonly accessed elements at the head of the list simply reduces the probability of the worst case occurring but does not eliminate it completely. Even in a self-organizing list, if a lowest access probability element (obviously located at the end of the list) is to be accessed, the entire list must be traversed completely to retrieve it. This is the worst case search.

===Best Case===

In the best case, the node to be searched is one which has been commonly accessed and has thus been identified by the list and kept at the head. This will result in a near constant time operation. In big-oh notation, in the best case, accessing an element is an O(1) operation.

==Techniques for Rearranging Nodes==
While ordering the elements in the list, the access probabilities of the elements are not generally known in advance. This has led to the development of various heuristics to approximate optimal behavior. The basic heuristics used to reorder the elements in the list are:

===Move to Front Method (MTF)===

This technique moves the element which is accessed to the head of the list. This has the advantage of being easily implemented and requiring no extra memory. This heuristic also adapts quickly to rapid changes in the query distribution. On the other hand, this method may prioritize infrequently accessed nodes-for example, if an uncommon node is accessed even once, it is moved to the head of the list and given maximum priority even if it is not going to be accessed frequently in the future. These 'over rewarded' nodes destroy the optimal ordering of the list and lead to slower access times for commonly accessed elements. Another disadvantage is that this method may become too flexible leading to access patterns that change too rapidly. This means that due to the very short memories of access patterns even an optimal arrangement of the list can be disturbed immediately by accessing an infrequent node in the list.

&lt;div class=&quot;center&quot;&gt;[[File:MTF Algorithm.png|330px|Move To Front Algorithm]]&lt;br/&gt;&lt;small&gt;If the 5th node is selected, it is moved to the front&lt;/small&gt;&lt;/div&gt;

     At the t-th item selection:
          '''if''' item i is selected:
                  move item i to head of the list

===Count Method===

In this technique, the number of times each node was searched for is counted i.e. every node keeps a separate counter variable which is incremented every time it is called. The nodes are then rearranged according to decreasing count. Thus, the nodes of highest count i.e. most frequently accessed are kept at the head of the list. The primary advantage of this technique is that it generally is more realistic in representing the actual access pattern. However, there is an added memory requirement, that of maintaining a counter variable for each node in the list. Also, this technique does not adapt quickly to rapid changes in the access patterns. For example: if the count of the head element say A is 100 and for any node after it say B is 40, then even if B becomes the new most commonly accessed element, it must still be accessed at least (100 - 40 = 60) times before it can become the head element and thus make the list ordering optimal.
&lt;br/&gt;
&lt;div class=&quot;center&quot;&gt;[[File:CountAlgorithm.png|330px|Count Algorithm]]
&lt;small&gt;If the 5th node in the list is searched for twice, it will be swapped with the 4th&lt;/small&gt;
&lt;/div&gt;

     '''init:''' count(i) = 0 for each item i
     At t-th item selection:
        '''if''' item i is searched:
            count(i) = count(i) + 1
            rearrange items based on count

===Transpose Method===

This technique involves swapping an accessed node with its predecessor. Therefore, if any node is accessed, it is swapped with the node in front unless it is the head node, thereby increasing its priority. This algorithm is again easy to implement and space efficient and is more likely to keep frequently accessed nodes at the front of the list. However, the transpose method is more cautious. i.e. it will take many accesses to move the element to the head of the list. This method also does not allow for rapid response to changes in the query distributions on the nodes in the list.&lt;br/&gt;
&lt;br/&gt;
&lt;div class=&quot;center&quot;&gt;[[File:Transpose Algorithm.png|330px|Transpose Algorithm]]
&lt;small&gt;If the 5th node in the list is selected, it will be swapped with the 4th&lt;/small&gt;
&lt;/div&gt;
&lt;div&gt;

     At the t-th item selection:
          '''if''' item i is selected:
              '''if''' i is not the head of list:
                      swap item i with item (i - 1)

&lt;/div&gt;

===Other Methods===

Research has been focused on fusing the above algorithms to achieve better efficiency.&lt;ref&gt;http://www.springerlink.com/content/978-3-540-34597-8/#section=508698&amp;page=1&amp;locus=3 Lists on Lists: A Framework for Self Organizing-Lists in Environments  with Locality of Reference&lt;/ref&gt; Bitner's Algorithm uses MTF initially and then uses transpose method for finer rearrangements. Some algorithms are randomized and try to prevent the over-rewarding of infrequently accessed nodes that may occur in the MTF algorithm. Other techniques involve reorganizing the nodes based on the above algorithms after every n accesses on the list as a whole or after n accesses in a row on a particular node and so on. Some algorithms rearrange the nodes which are accessed based on their proximity to the head node, for example: Swap-With-Parent or Move-To-Parent algorithms.
Another class of algorithms are used when the search pattern exhibits a property called locality of reference whereby in a given interval of time, only a smaller subset of the list is probabilistically most likely to be accessed. This is also referred to as dependent access where the probability of the access of a particular element depends on the probability of access of its neighboring elements. Such models are common in real world applications such as database or file systems and memory management and caching. A common framework for algorithms dealing with such dependent environments is to rearrange the list not only based on the record accessed but also on the records near it. This effectively involves reorganizing a sublist of the list to which the record belongs.

==Applications of self-organizing lists==
Language translators like compilers and interpreters use self-organizing lists to maintain [[symbol table]]s during compilation or interpretation of program source code. Currently research is underway to incorporate the self-organizing list data structure in [[embedded system]]s to reduce bus transition activity which leads to power dissipation in those circuits. These lists are also used in [[artificial intelligence]] and [[neural networks]] as well as self-adjusting programs. The algorithms used in self-organizing lists are also used as [[caching algorithm]]s as in the case of LFU algorithm.

== References ==
{{reflist}}
*{{Citation|last=Vlajic|first=N|title=Self-Organizing Lists|url=http://www.cse.yorku.ca/course_archive/2003-04/F/2011/2011A/DatStr_071_SOLists.pdf|format=pdf|year=2003}}
*{{Citation|title=Self Organization|url=http://courses.cs.vt.edu/~cs2604/spring04/Notes/C16.SelfOrganizingLists.pdf|format=pdf|year=2004}}
* NIST [http://www.nist.gov/dads/HTML/selfOrganizingList.html DADS entry]
* A Drozdek, Data Structures and Algorithms in Java Third edition
* {{Citation|last=Amer|first=Abdelrehman|coauthors=B. John Oommen|title=Lists on Lists: A Framework for Self-organizing Lists in Environments with Locality of Reference|url=http://www.springerlink.com/content/978-3-540-34597-8/#section=508698&amp;page=1&amp;locus=3 Lists on Lists: A Framework for Self Organizing-Lists in Environments  with Locality of Reference|format=pdf|year=2006}}

{{Data structures}}

{{DEFAULTSORT:Linked List}}
[[Category:Linked lists|S]]
[[Category:Articles with example C code]]</text>
      <sha1>i2zt47iqih0odp0gmd6nk4vtqwvvays</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Leftist tree</title>
    <ns>0</ns>
    <id>2754256</id>
    <revision>
      <id>616540737</id>
      <parentid>605951728</parentid>
      <timestamp>2014-07-11T15:57:41Z</timestamp>
      <contributor>
        <ip>69.58.248.1</ip>
      </contributor>
      <comment>/* Merging height biased leftist trees */</comment>
      <text xml:space="preserve" bytes="6214">In [[computer science]], a '''leftist tree''' or '''leftist heap''' is a [[priority queue]] implemented with a variant of a [[binary heap]]. Every node has an ''s-value'' which is the distance to the nearest [[leaf node|leaf]]. In contrast to a ''binary heap'', a leftist tree attempts to be very unbalanced. In addition to the [[heap (data structure)|heap]] property, leftist trees are maintained so the right descendant of each node has the lower s-value.

The leftist tree was invented by [[Clark Allan Crane]]. The name comes from the fact that the left subtree is usually taller than the right subtree.

When inserting a new node into a tree, a new one-node tree is created and merged into the existing tree. To delete a minimum item, we remove the root and the left and right sub-trees are then merged. Both these operations take O(log ''n'') time. For insertions, this is slower than binomial heaps which support insertion in [[amortized]] constant time, O(1) and O(log ''n'') worst-case.

Leftist trees are advantageous because of their ability to merge quickly, compared to binary heaps which take Θ(''n''). In almost all cases, the merging of [[skew heap]]s has better performance. However merging leftist heaps has worst-case O(log ''n'') complexity while merging skew heaps has only amortized O(log ''n'') complexity.

==Bias==
The usual leftist tree is a ''height-biased'' leftist tree. However, other biases can exist, such as in the ''weight-biased'' leftist tree.

== S-value ==
[[image:Leftist-trees-S-value.svg|thumb|right|S-values of a leftist tree]]
The s-value of a node is the distance from that node to the nearest [[leaf node|leaf]] of the [http://mathworld.wolfram.com/ExtendedBinaryTree.html extended binary representation of the tree]. The extended representation (not shown) fills out the tree so that each node has 2 children (adding a total of 5 leaves here). The minimum distance to these leaves are marked in the diagram. Thus s-value of 4 is 2, since the closest leaf is that of 8 --if 8 were extended. The s-value of 5 is 1 since its extended representation would have one leaf itself.

== Merging height biased leftist trees ==
Merging two nodes together depends on whether the tree is a min or max height biased leftist tree.  For a min height biased leftist tree, set the higher valued node as the right child of the lower valued node.  If the lower valued node already has a right child, then merge the higher valued node with the sub-tree rooted by the right child of the lower valued node.  

After merging, the s-value of the lower valued node must be updated (see above section, s-value).  Now check if the lower valued node has a left child.  If it does not, then move the right child to the left.  If it does have a left child, then the child with the highest s-value should go on the left.

=== Java code for merging a min height biased leftist tree ===

&lt;source lang=&quot;java&quot;&gt;
public Node merge(Node x, Node y) {
  if(x == null)
    return y;
  if(y == null) 
    return x;

  // if this was a max height biased leftist tree, then the 
  // next line would be: if(x.element &lt; y.element)
  if(x.element.compareTo(y.element) &gt; 0) {  
    // x.element &gt; y.element
    Node temp = x;
    x = y;
    y = temp;
  }

  x.rightChild = merge(x.rightChild, y);

  if(x.leftChild == null) {
    // left child doesn't exist, so move right child to the left side
    x.leftChild = x.rightChild;
    x.rightChild = null;

  } else {
    // left child does exist, so compare s-values
    if(x.leftChild.s &lt; x.rightChild.s) {
      Node temp = x.leftChild;
      x.leftChild = x.rightChild;
      x.rightChild = temp;
    }
    // since we know the right child has the lower s-value, we can just
    // add one to its s-value
    x.s = x.rightChild.s + 1;
  }
  return x;
}
&lt;/source&gt;

== Initializing a height biased leftist tree ==
[[Image:Min-height-biased-leftist-tree-initialization-part1.png|thumb|left|Initializing a min HBLT - Part 1]]
Initializing a height biased leftist tree is primarily done in one of two ways.  The first is to merge each node one at a time into one HBLT.  This process is inefficient and takes O(''nlogn'') time.  The other approach is to use a queue to store each node and resulting tree.  The first two items in the queue are removed, merged, and placed back into the queue.  This can initialize a HBLT in O(''n'') time.  This approach is detailed in the three diagrams supplied.  A min height biased leftist tree is shown.

To initialize a min HBLT, place each element to be added to the tree into a queue.  In the example (see Part 1 to the left), the set of numbers [4, 8, 10, 9, 1, 3, 5, 6, 11] are initialized.  Each line of the diagram represents another cycle of the algorithm, depicting the contents of the queue.  The first five steps are easy to follow.  Notice that the freshly created HBLT is added to the end of the queue.  In the fifth step, the first occurrence of an s-value greater than 1 occurs.  The sixth step shows two trees merged with each other, with predictable results.

&lt;br style=&quot;clear:both;&quot;/&gt;
[[Image:Min-height-biased-leftist-tree-initialization-part2.png|thumb|left|Initializing a min HBLT - Part 2]]
In part 2 a slightly more complex merge happens.  The tree with the lower value (tree x) has a right child, so merge must be called again on the subtree rooted by tree x's right child and the other tree.  After the merge with the subtree, the resulting tree is put back into tree x.  The s-value of the right child (s=2) is now greater than the s-value of the left child (s=1), so they must be swapped.  The s-value of the root node 4 is also now 2.

&lt;br style=&quot;clear:both;&quot;/&gt;
[[Image:Min-height-biased-leftist-tree-initialization-part3.png|thumb|left|Initializing a min HBLT - Part 3]] 
Part 3 is the most complex.  Here, we recursively call merge twice (each time with the right child 's subtree that is not grayed out).  This uses the same process described for part 2.

&lt;br style=&quot;clear:both;&quot;/&gt;

== External links ==
*[http://www.cise.ufl.edu/~sahni/cop5536/powerpoint/lec11.ppt Leftist Trees], [[Sartaj Sahni]]

[[Category:Trees (data structures)]]
[[Category:Heaps (data structures)]]
[[Category:Priority queues]]</text>
      <sha1>ki37a2jeate4cccou5lxorulem3n4li</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Pagoda (data structure)</title>
    <ns>0</ns>
    <id>14104194</id>
    <revision>
      <id>549470812</id>
      <parentid>547189515</parentid>
      <timestamp>2013-04-09T07:41:50Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (9075)</comment>
      <text xml:space="preserve" bytes="965">In [[computer science]], a '''pagoda''' is a [[priority queue]] implemented with a variant of a [[binary tree]]. The root points to its children, as in a binary tree. Every other node points back to its parent and down to its leftmost (if it is a right child) or rightmost (if it is a left child) descendant leaf. The basic operation is merge or meld, which maintains the [[heap property]]. An element is inserted by merging it as a singleton. The root is removed by merging its right and left children. Merging is bottom-up, merging the leftmost edge of one with the rightmost edge of the other.

== References ==
* J. Francon, G. Viennot, and J. Vuillemin, Description and analysis of an efficient priority queue representation, Proc. 19th Annual Symp. on Foundations of Computer Science. IEEE, 1978, pages 1–7.
* R. Nix, An Evaluation of Pagodas, Res. Rep. 164, Dept. of Computer Science, Yale Univ. 1988?
* {{DADS|pagoda|pagoda}}

[[Category:Priority queues]]</text>
      <sha1>i3dxvncnwtbymaqcwynqrk79v4n5mmx</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Min-max heap</title>
    <ns>0</ns>
    <id>30317554</id>
    <revision>
      <id>609092369</id>
      <parentid>609092330</parentid>
      <timestamp>2014-05-18T13:28:53Z</timestamp>
      <contributor>
        <ip>59.16.131.72</ip>
      </contributor>
      <comment>/* Deletion of min element */</comment>
      <text xml:space="preserve" bytes="5058">{{Multiple issues|
{{cleanup-rewrite|date=April 2012}}
{{inappropriate person|date=April 2012}}
{{tone|date=April 2012}}
{{howto|date=April 2012}}
{{original research|date=April 2012}}
{{no footnotes|date=April 2012}}
{{refimprove|date=April 2012}}
}}
In [[computer science]], a '''min-max heap''' is a [[double-ended priority queue]] implemented as a modified version of a [[binary heap]]. Like a binary heap, a min-max heap is represented as a complete binary tree. Unlike a binary heap, though, the nodes in this tree do not obey the min-heap property; rather they obey the min-max heap property. Each node at an even level in the tree is less than all of its descendants, while each node at an odd level in the tree is greater than all of its descendants.

Like binary heaps, min-max heaps support O(lg n) insertion and deletion, can be built in time O(n), and are often represented implicitly in an array. Operations like findmin() and findmax() take constant time.

== Min-Max Heap ==

=== Introduction ===
*A min-max heap is a complete binary tree containing alternating min and max levels.
[[File:Min-max heap.jpg|thumb|300px|Example of Min-max heap]]
*If it is not empty, each element has a data member called ''key''. The root is always present at min level. Let x be any node in a min-max heap. If x is on a min (max) level then the element in x has the minimum (maximum) key from among all elements in the subtree with root x. A node on a min (max) level is called a min (max) node.

=== Operations on Min-Max ===
*Inserting an element with an arbitrary key
*Deletion of largest key
*Deletion of smallest key

====  Inserting an element with an arbitrary key ====
To add an element to a Min-Max Heap perform following operations:

Insert the required key into given Min-Max Heap.

Compare this key with its parent. If it is found to be smaller (greater) compared to its parent, then it is surely smaller (greater) than all other keys present at nodes at max(min) level that are on the path from the present position of key to the root of heap. Now, just check for 
nodes on Min(Max) levels.

If the key added is in correct order then stop otherwise swap that key with its parent.

*Here is one example for inserting an element to a Min-Max Heap.
Say we have the following min-max heap and want to install a new node with value 6.

::[[File:Min-max heap.jpg|400px|Example of Min-max heap]]

Initially, element 6 is inserted at the position indicated by ''j''. Element 6 is less than its parent element. Hence it is smaller than all max levels and we only need to check the min levels. Thusly, element 6 gets moved to the root position of the heap and the former root, element 8, gets moved down one step.




If we want to insert a new node with value ''81'' in the given heap, we advance similarly. Initially the node is inserted at the position ''j''. Since element 81 is larger than its parent element and the parent element is at min level, it is larger than all elements that are on min levels. Now we only need to check the nodes on max levels.

====  Deletion of min element ====

To delete min element from a Min-Max Heap perform following operations:

The smallest element is the root element.

#Remove the root node and the node which is at the end of heap. Let it be x.
#Reinsert key of x into the min-max heap

Reinsertion may have 2 cases -

If root has no children. Then x can be inserted into the root.

Suppose root has at least one child. Find minimum value ( Let this is be node m).m is in one of the children or grandchildren of the root. Then following condition must be considered:
#x.key &lt;= h[m].key. x must be inserted into the root.
#x.key &gt; h[m].key and m is child of the root.since m is in max level, it has no descendants.So the element h[m] is moved to the root and x is inserted into node m.
#x.key &gt; h[m].key and m is grandchild of the root.So the element h[m] is moved to the root.Let p be parent of m. if x.key &gt; h[p].key then h[p] and x are interchanged.

Program - to delete the element with minimum key
&lt;source lang=&quot;c&quot;&gt;
element del_min(element heap[], int *s){  // *s: capacity of the heap

int i, last, m, parent;
element temp, x;
if(!(*s)){
      heap[0].key = INT_MAX;
      return(heap[0]);
}
heap[0] = heap[1];
x = heap[(*s)--];

/* find place to insert x */

for(i = 1, last = (*s) / 2; i &lt;= last;){
      m = min_child_grandchild(i, *s);
      if(x.key &lt;= heap[m].key) // case 1
            break;
      heap[i] = heap[m]; // case 2 or 3
      if( m &lt;= 2 * i + 1){ // case 2
            i = m;
            break;
      }
      /* case 3 */
      parent = m/2;
      if(x.key &gt; heap[parent].key)
            SWAP(heap[parent], x, temp);
      i = m;
}
heap[i] = x;
return heap[0];
}
&lt;/source&gt;

== References ==
* M. D. Atkinson, J.R. Sack, N. Santoro, and T. Strothotte, Communications of the ACM, October, 1986,  [http://www.cs.otago.ac.nz/staffpriv/mike/Papers/MinMaxHeaps/MinMaxHeaps.pdf Min-Max Heaps and Generalized Priority Queues].

[[Category:Priority queues]]
[[Category:Heaps (data structures)]]</text>
      <sha1>nzc56d4kjsv48bczl95go2blbxgjyts</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Skew binomial heap</title>
    <ns>0</ns>
    <id>30537813</id>
    <revision>
      <id>606564223</id>
      <parentid>496610574</parentid>
      <timestamp>2014-05-01T01:50:27Z</timestamp>
      <contributor>
        <ip>68.183.178.187</ip>
      </contributor>
      <text xml:space="preserve" bytes="559">In [[computer science]], a '''skew binomial heap''' (or '''skew binomial queue''') is a variant of the [[binomial heap]] that supports worst-case O(1) insertion, rather than the O(log n) worst-case insertion from the original binomial heap.  Just as [[binomial heap]]s are based on the [[binary number system]], skew binary heaps are based on the [[skew binary number system]].&lt;ref&gt;Okasaki, Chris. ''Purely Functional Data Structures.''&lt;/ref&gt;

== References ==
{{reflist}}

[[Category:Priority queues]]
[[Category:Heaps (data structures)]]

{{algorithm-stub}}</text>
      <sha1>kwvhemjp8lr3ozahqjz8lmnrwgexssb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>AF-heap</title>
    <ns>0</ns>
    <id>1857196</id>
    <revision>
      <id>534992430</id>
      <parentid>492491393</parentid>
      <timestamp>2013-01-26T15:13:02Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>fixed header names + [[WP:GENFIXES|general fixes]] using [[Project:AWB|AWB]] (8863)</comment>
      <text xml:space="preserve" bytes="1178">In [[computer science]], the '''AF-heap''' is a type of [[priority queue]] for integer data, an extension of the [[fusion tree]] using an [[atomic heap]] proposed by [[M. L. Fredman]] and [[D. E. Willard]].&lt;ref name=Willard&gt;M. L. Fredman and D. E. Willard.  Trans-dichotomous algorithms for minimum spanning trees and shortest paths.  Journal of Computer and System Sciences 48, 533-551 (1994)&lt;/ref&gt;

Using an AF-heap, it is possible to perform {{mvar|m}} insert or decrease-key operations and {{mvar|n}} delete-min operations on machine-integer keys in time {{math|''O''(''m'' + ''n'' log ''n'' / log log ''n'')}}. This allows [[Dijkstra's algorithm]] to be performed in the same {{math|''O''(''m'' + ''n'' log ''n'' / log log ''n'')}} time bound on graphs with {{mvar|n}} edges and {{mvar|m}} vertices, and leads to a [[linear time]] algorithm for [[minimum spanning tree]]s, with the assumption for both problems that the edge weights of the input graph are machine integers in the [[transdichotomous model]].

==See also==
* [[Fusion tree]]

==References==
&lt;references/&gt;

[[Category:Heaps (data structures)]]
[[Category:Priority queues]]


{{algorithm-stub}}
{{Combin-stub}}</text>
      <sha1>62xczf44uby7ngf84onwvjyo3j6qjii</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Double-ended priority queue</title>
    <ns>0</ns>
    <id>29244037</id>
    <revision>
      <id>626400488</id>
      <parentid>609084512</parentid>
      <timestamp>2014-09-20T22:48:31Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>also double-ended heap; copyedit</comment>
      <text xml:space="preserve" bytes="10779">{{Distinguish|Double-ended queue}}
In [[computer science]], a '''double-ended priority queue (DEPQ)'''&lt;ref name = &quot;Sahni&quot;&gt;[http://www.cise.ufl.edu/~sahni/dsaaj/enrich/c13/double.htm Data Structures, Algorithms, &amp; Applications in Java: Double-Ended Priority Queues], [[Sartaj Sahni]], 1999.&lt;/ref&gt; or '''double-ended heap'''&lt;ref name=&quot;brass&quot;&gt;{{cite book |first=Peter |last=Brass |title=Advanced Data Structures |publisher=Cambridge University Press |year=2008 |isbn=9780521880374 |page=211}}&lt;/ref&gt; is an [[abstract data type]] similar to a [[priority queue]] or [[heap (data structure)|heap]], but allows for efficient removal of both the maximum and minimum, according to some ordering on the ''keys'' (items) stored in the structure. Every element in a DEPQ has a priority or value. In a DEPQ, it is possible to remove the elements in both ascending as well as descending order.&lt;ref name=rubyforge&gt;{{cite web|title=Depq - Double-Ended Priority Queue|url=http://depq.rubyforge.org/}}&lt;/ref&gt;

== Operations ==
A double-ended priority queue features the follow operations:
;isEmpty(): Checks if DEPQ is empty and returns true if empty.
;size(): Returns the total number of elements present in the DEPQ.
;getMin(): Returns the element having least priority.
;getMax(): Returns the element having highest priority.
;put(''x''): Inserts the element ''x'' in the DEPQ.
;removeMin(): Removes an element with minimum priority and returns this element.
;removeMax(): Removes an element with maximum priority and returns this element.

If an operation is to be performed on two elements having the same priority, then the element inserted first is chosen. Also, the priority of any element can be changed once it has been inserted in the DEPQ.&lt;ref&gt;{{cite web|title=depq|url=http://rubygems.org/gems/depq}}&lt;/ref&gt;

== Implementation ==
Double-ended priority queues can be built from [[balanced binary search tree]]s (where the minimum and maximum elements are the leftmost and rightmost leaves, respectively), or using specialized data structures like [[min-max heap]] and [[pairing heap]].

Generic methods of arriving at double-ended priority queues from normal priority queues are:&lt;ref&gt;Fundamentals of Data Structures in C++ - Ellis Horowitz, [[Sartaj Sahni]] and Dinesh Mehta&lt;/ref&gt;

===Dual structure method===
[[File:Dual heap.jpg|thumb|upright=1.5|A dual structure with 14,12,4,10,8 as the members of DEPQ.&lt;ref name = &quot;Sahni&quot;/&gt;]]
In this method two different priority queues for min and max are maintained. The same elements in both the PQs are shown with the help of correspondence pointers.&lt;br&gt;
Here, the minimum and maximum elements are values contained in the root nodes of min heap and max heap respectively.
*'''Removing the min element''': Perform removemin() on the min heap and remove(''node value'') on the max heap, where ''node value'' is the value in the corresponding node in the max heap.
*'''Removing the max element''': Perform removemax() on the max heap and remove(''node value'') on the min heap, where ''node value'' is the value in the corresponding node in the min heap.
{{clear}}

===Total correspondence===
[[File:Total correspondence heap.jpg|thumb|upright=1.5|A total correspondence heap for the elements 3, 4, 5, 5, 6, 6, 7, 8, 9, 10, 11 with element 11 as buffer.&lt;ref name = &quot;Sahni&quot;/&gt;]]
Half the elements are in the min PQ and the other half in the max PQ. Each element in the min PQ has a one to one correspondence with an element in max PQ. If the number of elements in the DEPQ is odd, one of the elements is retained in a buffer.&lt;ref name = &quot;Sahni&quot;/&gt; Priority of every element in the min PQ will be less than or equal to the corresponding element in the max PQ.&lt;br&gt;
{{clear}}

===Leaf correspondence===
[[File:Leaf correspondence.jpg|thumb|upright=1.5|A leaf correspondence heap for the same elements as above.&lt;ref name = &quot;Sahni&quot;/&gt;]]
In this method only the leaf elements of the min and max PQ form corresponding one to one pairs. It is not necessary for non-leaf elements to be in a one to one correspondence pair.&lt;ref name = &quot;Sahni&quot;/&gt;&lt;br&gt;
{{clear}}

===Interval heaps===
[[File:Interval heap depq.jpg|thumb|upright=1.5|Implementing a DEPQ using interval heap.]]
Apart from the above mentioned correspondence methods, DEPQ's can be obtained efficiently using interval heaps.&lt;ref name=&quot;interval_heap&quot;&gt;http://www.mhhe.com/engcs/compsci/sahni/enrich/c9/interval.pdf&lt;/ref&gt; An interval heap is like an embedded [[min-max heap]] in which each node contains two elements. It is a complete binary tree in which:&lt;ref name=&quot;interval_heap&quot;/&gt;
* The left element is less than or equal to the right element.
* Both the elements define a closed interval.
* Interval represented by any node except the root is a sub-interval of the parent node.
* Elements on the left hand side define a [[min heap]].
* Elements on the right hand side define a [[max heap]].

Depending on the number of elements, two cases are possible&lt;ref name=&quot;interval_heap&quot;/&gt; - 
# '''Even number of elements:''' In this case, each node contains two elements say ''p'' and ''q'', with ''p''&amp;nbsp;≤&amp;nbsp;''q''. Every node is then represented by the interval [''p'',&amp;nbsp;''q''].
# '''Odd number of elements:''' In this case, each node except the last contains two elements represented by the interval [''p'',&amp;nbsp;''q''] whereas the last node will contain a single element and is represented by the interval [''p'',&amp;nbsp;''p''].

====Inserting an element====
Depending on the number of elements already present in the interval heap, following cases are possible:

* '''Odd number of elements:''' If the number of elements in the interval heap is odd, the new element is firstly inserted in the last node. Then, it is successively compared with the previous node elements and tested to satisfy the criteria essential for an interval heap as stated above. In case if the element does not satisfy any of the criteria, it is moved from the last node to the root until all the conditions are satisfied.&lt;ref name=&quot;interval_heap&quot;/&gt;
* '''Even number of elements:''' If the number of elements is even, then for the insertion of a new element an additional node is created. If the element falls to the left of the parent interval, it is considered to be in the min heap and if the element falls to the right of the parent interval, it is considered in the [[max heap]]. Further, it is compared successively and moved from the last node to the root until all the conditions for interval heap are satisfied. If the element lies within the interval of the parent node itself, the process is stopped then and there itself and moving of elements does not take place.&lt;ref name=&quot;interval_heap&quot;/&gt;

The time required for inserting an element depends on the number of movements required to meet all the conditions and is [[O notation|O]](log&amp;nbsp;''n'').

====Deleting an element====
*'''Min element:''' In an interval heap, the minimum element is the element on the left hand side of the root node. This element is removed and returned. To fill in the vacancy created on the left hand side of the root node, an element from the last node is removed and reinserted into the root node. This element is then compared successively with all the left hand elements of the descending nodes and the process stops when all the conditions for an interval heap are satisfied.In case if the left hand side element in the node becomes greater than the right side element at any stage, the two elements are swapped&lt;ref name=&quot;interval_heap&quot;/&gt; and then further comparisons are done. Finally, the root node will again contain the minimum element on the left hand side.

*'''Max element:''' In an interval heap, the maximum element is the element on the right hand side of the root node. This element is removed and returned. To fill in the vacancy created on the right hand side of the root node, an element from the last node is removed and reinserted into the root node. Further comparisons are carried out on a similar basis as discussed above. Finally, the root node will again contain the max element on the right hand side.

Thus, with interval heaps, both the minimum and maximum elements can be removed efficiently traversing from root to leaf. Thus, a DEPQ can be obtained&lt;ref name=&quot;interval_heap&quot;/&gt; from an interval heap where the elements of the interval heap are the priorities of elements in the DEPQ.

==Time Complexity ==
===Interval Heaps===

When DEPQ's are implemented using Interval heaps consisting of ''n'' elements, the time complexities for the various functions are formulated in the table below&lt;ref name = &quot;Sahni&quot;/&gt;

{| class=&quot;wikitable sortable&quot;
|-
! Operation !! Time Complexity
|-
| init( )|| O(n)
|-
| isEmpty( ) || O(1)
|-
| getmin( ) || O(1)
|-
| getmax( ) || O(1)
|-
| size( ) || O(1)
|-
| insert(x) || O(log ''n'') 
|-
| removeMin( ) || O(log ''n'') 
|-
| removeMax( ) || O(log ''n'') 
|}

===Pairing heaps===
When DEPQ's are implemented using heaps or pairing heaps consisting of ''n'' elements, the time complexities for the various functions are formulated in the table below.&lt;ref name = &quot;Sahni&quot;/&gt; For pairing heaps, it is an [[amortized complexity]].

{| class=&quot;wikitable sortable&quot;
|-
! Operation !! Time Complexity
|-
| isEmpty( ) || O(1)
|-
| getmin( ) || O(1)
|-
| getmax( ) || O(1)
|-
| insert(x) || O(log ''n'')
|-
| removeMax( ) || O(log ''n'')
|-
| removeMin( ) || O(log ''n'')
|}

== Applications ==
===External sorting===
One example application of the double-ended priority queue is [[external sorting]]. In an external sort, there are more elements than can be held in the computer's memory. The elements to be sorted are initially on a disk and the sorted sequence is to be left on the disk. The external [[quick sort]] is implemented using the DEPQ as follows:

# Read in as many elements as will fit into an internal DEPQ. The elements in the DEPQ will eventually be the middle group (pivot) of elements.
# Read in the remaining elements. If the next element is ≤ the smallest element in the DEPQ, output this next element as part of the left group. If the next element is ≥ the largest element in the DEPQ, output this next element as part of the right group. Otherwise, remove either the max or min element from the DEPQ (the choice may be made randomly or alternately); if the max element is removed, output it as part of the right group; otherwise, output the removed element as part of the left group; insert the newly input element into the DEPQ.
# Output the elements in the DEPQ, in sorted order, as the middle group.
# Sort the left and right groups recursively.

== See also ==
* [[Queue (abstract data type)]]
* [[Priority queue]]
* [[Double-ended queue]]

== References ==
{{reflist}}

{{Data structures}}

[[Category:Abstract data types]]
[[Category:Priority queues]]</text>
      <sha1>hwvct07hxsmyuiktwkgh0ba0zq2blvk</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Kinetic priority queue</title>
    <ns>0</ns>
    <id>35846544</id>
    <revision>
      <id>607153317</id>
      <parentid>577781771</parentid>
      <timestamp>2014-05-05T11:18:29Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>/* Applications */[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (10093)</comment>
      <text xml:space="preserve" bytes="6027">A '''Kinetic Priority Queue''' is an [[abstract data type|abstract]] [[kinetic data structure]]. It is a variant of a [[priority queue]] designed to maintain the maximum (or minimum) priority element (key-value pair) when the priority of every element is changing as a continuous function of time. Kinetic priority queues have been used as components of several kinetic data structures, as well as to solve some important non-kinetic problems such as the k-set problem and the connected red blue segments intersection problem.

== Implementations ==
The operations supported are:
* {{math|'''create-queue'''(&lt;var&gt;q&lt;/var&gt;)}}: create an empty kinetic priority queue {{math|&lt;var&gt;q&lt;/var&gt;}}
* {{math|'''find-max'''(&lt;var&gt;q, t&lt;/var&gt;)}} (or '''find-min'''): - return the {{math|max}} (or {{math|min}} for a {{math|min-queue}}) value stored in the queue {{math|&lt;var&gt;q&lt;/var&gt;}} at the current virtual time {{math|&lt;var&gt;t&lt;/var&gt;}}.
* {{math|'''insert'''(&lt;var&gt;X&lt;/var&gt;, f&lt;sub&gt;&lt;var&gt;X&lt;/var&gt;&lt;/sub&gt;, &lt;var&gt;t&lt;var&gt;)}}: - insert a key {{math|&lt;var&gt;X&lt;/var&gt;}} into the kinetic queue at the current virtual time{{math|&lt;var&gt;t&lt;/var&gt;}}, whose value changes as a continuous function {{math|f&lt;sub&gt;&lt;var&gt;X&lt;/sub&gt;&lt;/var&gt;(&lt;var&gt;t&lt;/var&gt;)}} of time {{math|&lt;var&gt;t&lt;/var&gt;}}.
* {{math|'''delete'''(&lt;var&gt;X&lt;/var&gt;, &lt;var&gt;t&lt;/var&gt;)}} - delete a key {{math||&lt;var&gt;X&lt;/var&gt;}} at the current virtual time {{math|&lt;var&gt;t&lt;/var&gt;}}.

There are several variants of kinetic priority queues, which support the same basic operations but have different performance guarantees. Some of the most common implementations are [[kinetic heap]]s which are simple to implement but don't have tight theoretical performance bounds, and their randomized variants - [[kinetic heater]]s and [[kinetic hanger]]s - which are easier to analyze. There is also a heap-like structure based on the [[dynamic convex hull]] data structure&lt;ref name=&quot;dch&quot;/&gt; which achieves better performance for affine motion of the priorities, but doesn't support curved trajectories. The [[kinetic tournament]] is another commonly used implementation. It achieves, deterministically, the same performance bounds as the heater or hanger, however it is less local and responsive than the heap-based data-structures.
{| class=&quot;wikitable&quot; border=&quot;1&quot;
|+ Time complexities of kinetic priority queue implementations &lt;ref name=&quot;hanger&quot;/&gt;
!  Trajectory of element priorities !! Kinetic heap !! Kinetic hanger, heater &amp; tournament !! Dynamic convex hull 
|-
| Lines || &lt;math&gt;O(n\log^2 n)&lt;/math&gt; || &lt;math&gt;O(n\log^2 n)&lt;/math&gt; || &lt;math&gt;O(n\log n)&lt;/math&gt;
|-
| Line segments || &lt;math&gt;O(m\sqrt{n}\log^{\frac{3}{2}}n)&lt;/math&gt; || &lt;math&gt;O(m \alpha(n)\log^2 n)&lt;/math&gt; || &lt;math&gt;O(m\log n \log \log n)&lt;/math&gt;
|-
| {{math|&amp;delta;}}-intersecting curves|| &lt;math&gt;O(n^2\log n)&lt;/math&gt; || &lt;math&gt;O(\lambda_\delta(n)\log n)&lt;/math&gt; || n/a
|}

Here, &lt;math&gt;\alpha(x)&lt;/math&gt; denotes the [[Ackermann function#Inverse|inverse Ackermann function]].&lt;math&gt;\delta&lt;/math&gt;-intersecting curves refer to curves where each pair has at most &lt;math&gt;\delta&lt;/math&gt; intersections, and &lt;math&gt;\lambda_\delta(n)&lt;/math&gt; refers to a term in the [[Davenport-Schinzel sequence]], which gives the maximum size of the upper envelope of &lt;math&gt;n&lt;/math&gt;  &lt;math&gt;\delta-&lt;/math&gt;intersecting curves. &lt;math&gt;n&lt;/math&gt; is the largest number of elements in the queue at any given time, while &lt;math&gt;m&lt;/math&gt; refers to the total number of elements that are ever in the queue.

== Applications ==
Kinetic priority queues are used as part of other kinetic data structures/algorithms such as [[kinetic closest pair]], [[kinetic max-cut]]&lt;ref name=&quot;max-cut&quot;/&gt; or [[kinetic clustering]].&lt;ref name=&quot;clustering&quot;/&gt;

They can also be used to solve problems such as [[broadcast scheduling]]&lt;ref name=&quot;tarjan&quot;/&gt; or the connected red blue segments intersection problem.&lt;ref name=&quot;red-blue&quot;/&gt;

== References ==
{{Reflist|refs=
&lt;ref name=&quot;tarjan&quot;&gt;
{{cite conference| url= http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.12.2739 | title = Faster kinetic heaps and their use in broadcast scheduling | publisher=ACM | accessdate=May 17, 2012| author = K. H., Tarjan, R. and T. K. | booktitle=Proc. 12th ACM-SIAM Symposium on Discrete Algorithms|pages=836–844| year=2001}}
&lt;/ref&gt;
&lt;ref name=&quot;hanger&quot;&gt;
{{cite web | url=http://www.uniriotec.br/~fonseca/hanger.pdf | title=Kinetic hanger|publisher=Information Processing Letters | accessdate=May 17, 2012 |author=da Fonseca, Guilherme D. and de Figueiredo, Celina M. H. and Carvalho, Paulo C. P. | pages=151–157}}
&lt;/ref&gt;
&lt;ref name=&quot;red-blue&quot;&gt;
{{cite book| url=http://dx.doi.org/10.1007/3-540-61680-2_64 | author=Basch, Julien; Guibas, Leonidas; Ramkumar, G. | title=Reporting red-blue intersections between two sets of connected line segments | booktitle=Algorithms — ESA '96| year=1996| publisher=Springer Berlin / Heidelberg|isbn=978-3-540-61680-1}} 
&lt;/ref&gt;
&lt;ref name=&quot;max-cut&quot;&gt;
{{cite conference | url=http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/32977.pdf|title=Efficient kinetic data structures for MaxCut | accessdate=May 17, 2012 | author=Czumaj, Arthur; Frahling, Gereon; Sohler, Christian | year=2007 | conference=Canadian Conference on Computational Geometry}}
&lt;/ref&gt;

&lt;ref name=&quot;clustering&quot;&gt;
{{cite conference | url=http://dl.acm.org/citation.cfm?id=1014129| title= Clustering moving objects| author = Li, Yifan; Han, Jiawei; Yang, Jiong| publisher=ACM| conference=SIGKDD|booktitle=Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining|pages=617–622}} 
&lt;/ref&gt;
&lt;ref name=&quot;dch&quot;&gt;
{{cite conference| url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1181985&amp;tag=1| author=Brodal, G.S.; Jacob, R.|pages=617–626| conference=FCS|year=2002|booktitle=Proc. The 43rd Annual IEEE Symposium on Foundations of Computer Science| title=Dynamic planar convex hull}} 
&lt;/ref&gt;
}}

&lt;!--- Categories ---&gt;

[[Category:Articles created via the Article Wizard]]
[[Category:Kinetic data structures]]
[[Category:Abstract data types]]
[[Category:Priority queues]]</text>
      <sha1>i5021d0oc2h4v1yfio9lsqensxdyi45</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Priority queue</title>
    <ns>0</ns>
    <id>24485</id>
    <revision>
      <id>626653602</id>
      <parentid>624723609</parentid>
      <timestamp>2014-09-22T17:59:03Z</timestamp>
      <contributor>
        <ip>204.28.140.7</ip>
      </contributor>
      <comment>/* Naive implementations */ typo</comment>
      <text xml:space="preserve" bytes="19017">{{more footnotes|date=October 2013}}
In [[computer science]]/data structures, a '''priority queue''' is an [[abstract data type]] which is like a regular [[queue (abstract data type)|queue]] or [[stack (abstract data type)|stack]] data structure, but where additionally each element has a &quot;priority&quot; associated with it. In a priority queue, an element with high priority is served before an element with low priority. If two elements have the same priority, they are served according to their order in the queue.

While priority queues are often implemented with [[Heap (data structure)|heaps]], they are conceptually distinct from heaps. A priority queue is an abstract concept like &quot;a list&quot; or &quot;a map&quot;; just as a list can be implemented with a [[linked list]] or an [[Array data structure|array]], a priority queue can be implemented with a heap or a variety of other methods such as an unordered array.

== Operations ==

A priority queue must at least support the following operations:

* ''insert_with_priority'': add an [[element (mathematics)|element]] to the [[Queue (abstract data type)|queue]] with an associated priority.
* ''pull_highest_priority_element'': remove the element from the queue that has the ''highest priority'', and return it.
*: This is also known as &quot;''pop_element(Off)''&quot;, &quot;''get_maximum_element''&quot; or &quot;''get_front(most)_element''&quot;.
*: Some conventions reverse the order of priorities, considering lower values to be higher priority, so this may also be known as &quot;''get_minimum_element''&quot;, and is often referred to as &quot;''get-min''&quot; in the literature.
*: This may instead be specified as separate &quot;''peek_at_highest_priority_element''&quot; and &quot;''delete_element''&quot; functions, which can be combined to produce &quot;''pull_highest_priority_element''&quot;.

In addition, ''[[Peek (data type operation)|peek]]'' (in this context often called ''find-max'' or ''find-min''), which returns the highest-priority element but does not modify the queue, is very frequently implemented, and nearly always executes in [[Big O notation|''O''(1)]] time. This operation and its ''O''(1) performance is crucial to many applications of priority queues.

More advanced implementations may support more complicated operations, such as ''pull_lowest_priority_element'', inspecting the first few highest- or lowest-priority elements, clearing the queue, clearing subsets of the queue, performing a batch insert, merging two or more queues into one, incrementing priority of any element, etc.

== Similarity to queues ==

One can imagine a priority queue as a modified [[queue (abstract data type)|queue]], but when one would get the next element off the queue, the highest-priority element is retrieved first.

* ''stack'' &amp;ndash; elements are pulled in [[LIFO (computing)|last-in first-out]]-order (e.g., a stack of papers)
* ''queue'' &amp;ndash; elements are pulled in [[first-in first-out]]-order (e.g., a line in a cafeteria)

Stacks and queues may be modeled as particular kinds of priority queues. In a stack, the priority of each inserted element is monotonically increasing; thus, the last element inserted is always the first retrieved. In a queue, the priority of each inserted element is monotonically decreasing; thus, the first element inserted is always the first retrieved.

== Implementation ==

=== Native implementations ===

There are a variety of simple, usually inefficient, ways to implement a priority queue. They provide an analogy to help one understand what a priority queue is. For instance, one can keep all the elements in an unsorted list. Whenever the highest-priority element is requested, search through all elements for the one with the highest priority. (In [[big O notation|big ''O'' notation]]: ''O''(1) insertion time, ''O''(''n'') pull time due to search.)

=== Usual implementation ===

To improve performance, priority queues typically use a [[Heap (data structure)|heap]] as their backbone, giving ''O''(log ''n'') performance for inserts and removals, and ''O''(''n'') to build initially. Alternatively, when a [[self-balancing binary search tree]] is used, insertion and removal also take ''O''(log ''n'') time, although building trees from existing sequences of elements takes ''O''(''n'' log ''n'') time; this is typical where one might already have access to these data structures, such as with third-party or standard libraries.

Note that from a computational-complexity standpoint, priority queues are congruent to sorting algorithms. See [[priority queue#Equivalence_of_priority_queues_and_sorting_algorithms|the next section]] for how efficient sorting algorithms can create efficient priority queues.

There are several specialized [[heap (data structure)|heap]] [[data structures]] that either supply additional operations or outperform these approaches. The [[binary heap]] uses ''O''(log ''n'') time for both operations, but also allows queries of the element of highest priority without removing it in constant time. [[Binomial heap]]s add several more operations, but require ''O''(log ''n'') time for requests. [[Fibonacci heap]]s can insert elements, query the highest priority element, and increase an element's priority in [[amortized analysis|amortized]] constant time,&lt;ref name=&quot;CLRS&quot;&gt;[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. [[MIT Press]] and McGraw-Hill, 2001. ISBN 0-262-03293-7. Chapter 20: Fibonacci Heaps, pp.476&amp;ndash;497. Third edition p518.&lt;/ref&gt; though deletions are still ''O''(log ''n''). [[Brodal queue]]s can do this in worst-case constant time however Brodal calls them &quot;quite complicated&quot; and &quot;[not] applicable in practice.&quot;.

While relying on a heap is a common way to implement priority queues, for integer data, faster implementations exist. This can even apply to data types that have a finite range, such as floats:

* When the set of keys is {1, 2, ..., ''C''}, a [[van Emde Boas tree]] would support the ''[[minimum]]'', ''[[maximum]]'', ''insert'', ''delete'', ''search'', ''[[extract-min]]'', ''[[extract-max]]'', ''[[predecessor (graph theory)|predecessor]]'' and ''[[successor (graph theory)|successor]]'' operations in ''O''(log log ''C'') time, but has a space cost for small queues of about ''O''(2&lt;sup&gt;''m''/2&lt;/sup&gt;), where ''m'' is the number of bits in the priority value.&lt;ref&gt;P. van Emde Boas. Preserving order in a forest in less than logarithmic time. In ''Proceedings of the 16th Annual Symposium on Foundations of Computer Science'', pages 75-84. IEEE Computer Society, 1975.&lt;/ref&gt;
* The [[Fusion tree]] algorithm by [[Michael Fredman|Fredman]] and Willard implements the ''minimum'' operation in ''O''(1) time and ''insert'' and ''extract-min'' operations in &lt;math&gt;O(\sqrt{\log n})&lt;/math&gt; time however it is stated by the author that, &quot;Our algorithms have theoretical interest only; The constant factors involved in the execution times preclude practicality.&quot;.&lt;ref&gt;[[Michael Fredman|Michael L. Fredman]] and Dan E. Willard. Surpassing the information theoretic bound with fusion trees. ''Journal of Computer and System Sciences'', 48(3):533-551, 1994&lt;/ref&gt;

For applications that do many &quot;[[Peek (data type operation)|peek]]&quot; operations for every &quot;extract-min&quot; operation, the time complexity for peek actions can be reduced to ''O''(1) in all tree and heap implementations by caching the highest priority element after every insertion and removal. For insertion, this adds at most a constant cost, since the newly inserted element is compared only to the previously cached minimum element. For deletion, this at most adds an additional &quot;peek&quot; cost, which is typically cheaper than the deletion cost, so overall time complexity is not significantly impacted.

== Equivalence of priority queues and sorting algorithms ==

=== Using a priority queue to sort ===

The [[operational semantics|semantics]] of priority queues naturally suggest a sorting method: insert all the elements to be sorted into a priority queue, and sequentially remove them; they will come out in sorted order. This is actually the procedure used by several [[sorting algorithm]]s, once the layer of [[abstraction (computer science)|abstraction]] provided by the priority queue is removed. This sorting method is equivalent to the following sorting algorithms:


{|class=&quot;wikitable sortable&quot;

! Name !! Priority Queue Implementation !! Best !! Average !! Worst 
|- align=&quot;center&quot;
| [[Heapsort]]
| [[Heap (data structure)|Heap]]
| &lt;math&gt;n log(n)&lt;/math&gt;
|&lt;math&gt;n log(n)&lt;/math&gt;
|&lt;math&gt;n log(n)&lt;/math&gt;

|- align=&quot;center&quot;
| [[Smoothsort]]
| Leonardo Heap
|&lt;math&gt;n&lt;/math&gt;
|&lt;math&gt;n log(n)&lt;/math&gt;
|&lt;math&gt;n log (n)&lt;/math&gt;

|- align=&quot;center&quot;
| [[Selection sort]]
| Unordered [[Array#In_computer_science|Array]]
|&lt;math&gt;n^2&lt;/math&gt;
|&lt;math&gt;n^2&lt;/math&gt;
|&lt;math&gt;n^2&lt;/math&gt;

|- align=&quot;center&quot;
| [[Insertion Sort]]
| Ordered [[Array#In_computer_science|Array]]
|&lt;math&gt;n &lt;/math&gt;
|&lt;math&gt;n^2 &lt;/math&gt;
|&lt;math&gt;n^2 &lt;/math&gt;

|- align=&quot;center&quot;
| [[Tree sort]]
| [[self-balancing binary search tree]]
|&lt;math&gt;n log(n)&lt;/math&gt;
|&lt;math&gt;n log(n)&lt;/math&gt;
|&lt;math&gt;n^2&lt;/math&gt;

|}

=== Using a sorting algorithm to make a priority queue ===

A sorting algorithm can also be used to implement a priority queue. Specifically, Thorup says:&lt;ref&gt;{{cite doi|10.1145/1314690.1314692}}&lt;/ref&gt;

&lt;blockquote&gt;
We present a general deterministic linear space reduction from priority queues to sorting implying that if we can sort up to ''n'' keys in ''S''(''n'') time per key, then there is a priority queue supporting ''delete'' and ''insert'' in ''O''(''S''(''n'')) time and ''find-min'' in constant time.
&lt;/blockquote&gt;

That is, if there is a sorting algorithm which can sort in ''O''(''S'') time per key, where ''S'' is some function of ''n'' and [[word size]],&lt;ref&gt;http://courses.csail.mit.edu/6.851/spring07/scribe/lec17.pdf&lt;/ref&gt; then one can use the given procedure to create a priority queue where pulling the highest-priority element is ''O''(1) time, and inserting new elements (and deleting elements) is ''O''(''S'') time. For example, if one has an ''O''(''n''&amp;nbsp;log&amp;nbsp;log&amp;nbsp;''n'') sort algorithm, one can create a priority queue with ''O''(1) pulling and ''O''(log&amp;nbsp;log&amp;nbsp;''n'') insertion.

== Libraries ==

A priority queue is often considered to be a &quot;[[container (abstract data type)|container data structure]]&quot;.

The [[Standard Template Library]] (STL), and the [[C++]] 1998 standard, specifies &lt;code&gt;priority_queue&lt;/code&gt; as one of the STL [[container (programming)|container]] [[adaptor (programming)|adaptor]] [[Template (programming)|class template]]s. It implements a max-priority-queue, and has three parameters: a comparison object for sorting such as a functor (defaults to less&lt;T&gt; if unspecified), the underlying container for storing the data structures (defaults to std::vector&lt;T&gt;), and two iterators to the beginning and end of a sequence. Unlike actual STL containers, it does not allow [[Iterator|iteration]] of its elements (it strictly adheres to its abstract data type definition). STL also has utility functions for manipulating another random-access container as a binary max-heap. The [[Boost (C++ libraries)]] also have an implementation in the library heap.

Python's [https://docs.python.org/library/heapq.html heapq] module implements a binary min-heap on top of a list.

[[Java (programming language)|Java]]'s library contains a {{Javadoc:SE|java/util|PriorityQueue}} class, which implements a min-priority-queue.

[[Go (programming language)|Go]]'s library contains a [http://golang.org/pkg/container/heap/ container/heap] module, which implements a min-heap on top of any compatible data structure.

The [[Standard PHP Library]] extension contains the class [http://us2.php.net/manual/en/class.splpriorityqueue.php SplPriorityQueue].

Apple's [[Core Foundation]] framework contains a [http://developer.apple.com/library/mac/#documentation/CoreFoundation/Reference/CFBinaryHeapRef/Reference/reference.html CFBinaryHeap] structure, which implements a min-heap.

== Applications ==

=== Bandwidth management ===

Priority queuing can be used to manage limited resources such as [[Bandwidth (computing)|bandwidth]] on a transmission line from a [[computer network|network]] [[router (computing)|router]]. In the event of outgoing [[traffic]] queuing due to insufficient bandwidth, all other queues can be halted to send the traffic from the highest priority queue upon arrival. This ensures that the prioritized traffic (such as real-time traffic, e.g. an [[Real-time Transport Protocol|RTP]] stream of a [[Voice over Internet Protocol|VoIP]] connection) is forwarded with the least delay and the least likelihood of being rejected due to a queue reaching its maximum capacity. All other traffic can be handled when the highest priority queue is empty. Another approach used is to send disproportionately more traffic from higher priority queues.

Many modern protocols for [[Local Area Network]]s also include the concept of Priority Queues at the [[Media Access Control]] (MAC) sub-layer to ensure that high-priority applications (such as [[VoIP]] or [[IPTV]]) experience lower latency than other applications which can be served with [[Best effort]] service. Examples include [[IEEE 802.11e]] (an amendment to [[IEEE 802.11]] which provides [[Quality of Service]]) and [[ITU-T]] [[G.hn]] (a standard for high-speed [[Local area network]] using existing home wiring ([[Power line communication|power lines]], phone lines and [[Ethernet over coax|coaxial cables]]).

Usually a limitation (policer) is set to limit the bandwidth that traffic from the highest priority queue can take, in order to prevent high priority packets from choking off all other traffic. This limit is usually never reached due to high level control instances such as the [[Cisco Systems, Inc.|Cisco]] [[Callmanager]], which can be programmed to inhibit calls which would exceed the programmed bandwidth limit.
&lt;!-- this was marked IMHO in the original
Priority queues exist on ISO-layer 2 (which is ethernet or WAN interfaces such as T1 / E1) and are filled by entry-criterions such as [[Diffserv]] Codepoints or IP-Precedence. Network equipment usually can be programmed to pick up prio packets by the layer 4 info (IP protocol and port) or the new one by a mechanism called [[NBAR]].
--&gt;

=== Discrete event simulation ===

Another use of a priority queue is to manage the events in a [[discrete event simulation]]. The events are added to the queue with their simulation time used as the priority. The execution of the simulation proceeds by repeatedly pulling the top of the queue and executing the event thereon.

''See also'': [[Scheduling (computing)]], [[queueing theory]]

=== Dijkstra's algorithm ===

When the graph is stored in the form of adjacency list or matrix, priority queue can be used to extract minimum efficiently when implementing [[Dijkstra's algorithm]], although one also needs the ability to alter the priority of a particular vertex in the priority queue efficiently.

=== Huffman coding ===

[[Huffman coding]] requires one to repeatedly obtain the two lowest-frequency trees. A priority queue makes this efficient.

=== Best-first search algorithms ===

[[Best-first search]] algorithms, like the [[A* search algorithm]], find the shortest path between two [[vertex (graph theory)|vertices]] or [[Node (graph theory)|nodes]] of a [[weighted graph]], trying out the most promising routes first. A priority queue (also known as the ''fringe'') is used to keep track of unexplored routes; the one for which the estimate (a lower bound in the case of A*) of the total path length is smallest is given highest priority. If memory limitations make best-first search impractical, variants like the [[SMA*]] algorithm can be used instead, with a [[double-ended priority queue]] to allow removal of low-priority items.

=== ROAM triangulation algorithm ===

The Real-time Optimally Adapting Meshes ([[ROAM]]) algorithm computes a dynamically changing triangulation of a terrain. It works by splitting triangles where more detail is needed and merging them where less detail is needed. The algorithm assigns each triangle in the terrain a priority, usually related to the error decrease if that triangle would be split. The algorithm uses two priority queues, one for triangles that can be split and another for triangles that can be merged. In each step the triangle from the split queue with the highest priority is split, or the triangle from the merge queue with the lowest priority is merged with its neighbours.

=== Prim's Algorithm for Minimum Spanning Tree ===
Using [[Binary heap |min heap priority queue]] in [[Prim's algorithm]] to find [[Minimum spanning tree]] of a [[Connected graph |connected]] and [[Undirected graph| undirected graph]], one can achieve a good running time of algorithm. This min heap priority queue uses min heap data structure which supports operations such as ''Insert'', ''Minimum'', ''Extract-Min'', ''Decrease-key''.
&lt;ref name=&quot;CLR&quot;&gt;
{{cite book
| author =[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], [[Clifford Stein]] 
| title = INTRODUCTION TO ALGORITHMS
| volume = 3
| year= 2009
| publisher= [[MIT Press]]
| isbn= 978-81-203-4007-7
| page = 634
| url= http://books.google.co.in/books/about/Introduction_To_Algorithms.html?id=NLngYyWFl_YC
| quote = In order to implement Prim's algorithm efficiently, we need a fast way to select
a new edge to add to the tree formed by the edges in A. In the pseudo-code  
}}
&lt;/ref&gt;In this implementation, the [[weighted graph|weight]] of the edges is used to decide the priority of the [[Vertex (graph theory) |vertices]]. Lower the weight, higher the priority and higher the weight, lower the priority.

== See also ==

* [[Batch queue]]
* [[Command queue]]
* [[Job scheduler]]

== References ==

{{Reflist}}

== Further reading ==

* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 6.5: Priority queues, pp.&amp;nbsp;138&amp;ndash;142.

== External links ==

* [http://en.cppreference.com/w/cpp/container/priority_queue C++ reference for &lt;code&gt;std::priority_queue&lt;/code&gt;]
* [http://leekillough.com/heaps/ Descriptions] by [[Lee Killough (programmer)|Lee Killough]]
* [http://bitbucket.org/trijezdci/pqlib/src/ PQlib] - Open source Priority Queue library for C
* [https://github.com/vy/libpqueue libpqueue] is a generic priority queue (heap) implementation (in C) used by the Apache HTTP Server project.
* [http://www.theturingmachine.com/algorithms/heaps.html Survey of known priority queue structures] by Stefan Xenos
* [http://video.google.com/videoplay?docid=3499489585174920878 UC Berkeley - Computer Science 61B - Lecture 24: Priority Queues] (video) - introduction to priority queues using binary heap

{{Data structures}}

[[Category:Priority queues| ]]
[[Category:Abstract data types]]</text>
      <sha1>6mihlnzwo6cbjglttr8o66tm9dlvulb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Van Emde Boas tree</title>
    <ns>0</ns>
    <id>1189425</id>
    <revision>
      <id>626637150</id>
      <parentid>619185540</parentid>
      <timestamp>2014-09-22T16:03:23Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>added [[Category:Priority queues]] using [[WP:HC|HotCat]]</comment>
      <text xml:space="preserve" bytes="13621">{{Expert-subject |computer science |talk=Bug in delete function |reason=bug in pseudocode |date=March 2013 }}
{| class=&quot;infobox&quot; style=&quot;width: 20em&quot;
! colspan=&quot;2&quot; style=&quot;font-size: 125%; text-align: center&quot; | Van Emde Boas tree
|-
! [[List of data structures|Type]]
| Non-binary [[tree data structure|tree]]
|-
! Invented
| 1975
|-
! Invented by
| {{ill|de|Peter van Emde Boas}}
|-
! colspan=&quot;2&quot; style=&quot;text-align: center; background-color: #CCCCFF; color: #000000;&quot; | Asymptotic complexity&lt;br /&gt;in [[big O notation]]
|-
! Space
| ''O''(''M'')
|-
! Search
| ''O''(log log ''M'')
|-
! Insert
| ''O''(log log ''M'')
|-
! Delete
| ''O''(log log ''M'')
|}

A '''Van Emde Boas tree''' (or '''Van Emde Boas priority queue'''; {{IPA-nl|vɑn 'ɛmdə 'boɑs}}), also known as a '''vEB tree''', is a [[tree data structure]] which implements an [[associative array]] with ''m''-bit integer keys. It performs all operations in [[Big-O notation|O]](log&amp;nbsp;''m'') time, or equivalently in O(log log&amp;nbsp;''M'') time, where M=2&lt;sup&gt;m&lt;/sup&gt; is the maximum number of elements that can be stored in the tree. The ''M'' is not to be confused with the actual number of elements stored in the tree, by which the performance of other tree data-structures is often measured. The vEB tree has good space efficiency when it contains a large number of elements, as discussed below. It was invented by a team led by {{ill|de|Peter van Emde Boas}} in 1975.&lt;ref&gt;[[Peter van Emde Boas]]: ''Preserving order in a forest in less than logarithmic time'' (''Proceedings of the 16th Annual Symposium on Foundations of Computer Science'' 10: 75-84, 1975)&lt;/ref&gt;

==Supported operations==
A vEB supports the operations of an ''ordered [[associative array]]'', which includes the usual associative array operations along with two more ''order'' operations, ''FindNext'' and ''FindPrevious'':&lt;ref&gt;[[Gudmund Skovbjerg Frandsen]]: ''[http://www.daimi.au.dk/~gudmund/dynamicF04/vEB.pdf Dynamic algorithms: Course notes on van Emde Boas trees (PDF)]'' ([[University of Aarhus]], Department of Computer Science)&lt;/ref&gt;
*''Insert'': insert a key/value pair with an ''m''-bit key
*''Delete'': remove the key/value pair with a given key
*''Lookup'': find the value associated with a given key
*''FindNext'': find the key/value pair with the smallest key at least a given ''k''
*''FindPrevious'': find the key/value pair with the largest key at most a given ''k''

A vEB tree also supports the operations ''Minimum'' and ''Maximum'', which return the minimum and maximum element stored in the tree respectively.&lt;ref&gt;* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Third Edition. [[MIT Press]], 2009. ISBN 0-262-53305-8. Chapter 20: The van Emde Boas tree, pp.&amp;nbsp;531–560.&lt;/ref&gt; These both run in ''O''(1) time, since the minimum and maximum element are stored as attributes in each tree.

==How it works==

[[Image:VebDiagram.svg|thumb |alt=Example Van Emde Boas tree |An example Van Emde Boas tree with dimension 5 and the root's aux structure after 1, 2, 3, 5, 8 and 10 have been inserted.]] For the sake of simplicity, let ''log&lt;sub&gt;2&lt;/sub&gt; m = k'' for some integer ''k''.  Define M=2&lt;sup&gt;m&lt;/sup&gt;.  A vEB tree ''T'' over the universe {0,...,''M''-1} has a root node that stores an array ''T.children'' of length ''M&lt;sup&gt;1/2&lt;/sup&gt;''.  ''T.children[i]'' is a pointer to a vEB tree that is responsible for the values {''iM&lt;sup&gt;1/2&lt;/sup&gt;,...,(i+1)M&lt;sup&gt;1/2&lt;/sup&gt;-1''}.  Additionally, ''T'' stores two values ''T.min'' and ''T.max'' as well as an auxiliary vEB tree ''T.aux''.

Data is stored in a vEB tree as follows: The smallest value currently in the tree is stored in ''T.min'' and largest value is stored in ''T.max''.  Note that ''T.min'' is not stored anywhere else in the vEB tree, while ''T.max'' is. If ''T'' is empty then we use the convention that ''T.max=-1'' and ''T.min=M''.  Any other value ''x'' is stored in the subtree ''T.children[i]'' where &lt;math&gt;i=\lfloor x/M^{1/2}\rfloor&lt;/math&gt;.  The auxiliary tree ''T.aux'' keeps track of which children are non-empty, so ''T.aux'' contains the value ''j'' if and only if ''T.children[j]'' is non-empty.

===FindNext===

The operation ''FindNext(T, x)'' that searches for the successor of an element ''x'' in a vEB tree proceeds as follows: If ''x''≤''T.min'' then the search is complete, and the answer is ''T.min''.  If ''x&gt;T.max'' then the next element does not exist, return M. Otherwise, let ''i=x/M&lt;sup&gt;1/2&lt;/sup&gt;''.  If ''x≤T.children[i].max'' then the value being searched for is contained in ''T.children[i]'' so the search proceeds recursively in ''T.children[i]''.  Otherwise, We search for the value ''i'' in ''T.aux''.  This gives us the index ''j'' of the first subtree that contains an element larger than ''x''.  The algorithm then returns ''T.children[j].min''. The element found on the children level needs to be composed with the high bits to form a complete next element.

 '''function''' FindNext(T, x)
     '''if''' x ≤ T.min '''then'''
         '''return''' T.min
     '''if''' x &gt; T.max '''then''' ''// no next element''
         '''return''' M
     i = floor(x/&lt;math&gt;\sqrt{M}&lt;/math&gt;)
     lo = x % &lt;math&gt;\sqrt{M}&lt;/math&gt;
     hi = x - lo
     '''if''' lo ≤ T.children[i].max '''then'''
         '''return''' hi + FindNext(T.children[i], lo)
     '''return''' hi + T.children[FindNext(T.aux, i)].min
 '''end'''

Note that, in any case, the algorithm performs ''O''(1) work and then possibly recurses on a subtree over a universe of size ''M&lt;sup&gt;1/2&lt;/sup&gt;'' (an ''m/2'' bit universe). This gives a recurrence for the running time of ''T(m)=T(m/2) + O(1)'', which resolves to ''O(log ''m'') = ''O''(log log ''M'').

===Insert===

The call ''insert(T, x)'' that inserts a value ''x'' into a vEB tree T operates as follows:

If ''T'' is empty then we set ''T.min = T.max = x'' and we are done.

Otherwise, if ''x&amp;lt;T.min'' then we insert ''T.min'' into the subtree ''i'' responsible for ''T.min'' and then set ''T.min = x''.  If ''T.children[i]'' was previously empty, then we also insert ''i'' into ''T.aux''

Otherwise, if ''x&amp;gt;T.max'' then we insert ''x'' into the subtree ''i'' responsible for ''x'' and then set ''T.max = x''.  If ''T.children[i]'' was previously empty, then we also insert ''i'' into ''T.aux''

Otherwise, ''T.min&amp;lt; x &amp;lt; T.max'' so we insert ''x'' into the subtree ''i'' responsible for ''x''.  If T.children[i] was previously empty, then we also insert ''i'' into ''T.aux''.

In code:
 '''function''' Insert(T, x)
     '''if''' T.min &gt; T.max '''then''' ''// T is empty''
         T.min = T.max = x;
         '''return'''
     '''if''' T.min == T.max '''then'''
         '''if''' x &lt; T.min '''then'''
             T.min = x
             '''return'''
         '''if''' x &gt; T.max '''then'''
             T.max = x
     '''if''' x &lt; T.min '''then'''
         swap(x, T.min)
     '''if''' x &gt; T.max '''then'''
         T.max = x
     i = floor(x / &lt;math&gt;\sqrt{M})&lt;/math&gt;
     Insert(T.children[i], x % &lt;math&gt;\sqrt{M}&lt;/math&gt;)
     '''if''' T.children[i].min == T.children[i].max '''then'''
         Insert(T.aux, i)
 '''end'''

The key to the efficiency of this procedure is that inserting an element into an empty vEB tree takes ''O''(1) time.  So, even though the algorithm sometimes makes two recursive calls, this only occurs when the first recursive call was into an empty subtree.  This gives the same running time recurrence of  ''T(m)=T(m/2) + O(1)'' as before.

===Delete===

Deletion from vEB trees is the trickiest of the operations. The call ''Delete(T, x)'' that deletes a value ''x'' from a vEB tree T operates as follows:

If ''T.min = T.max = x'' then ''x'' is the only element stored in the tree and we set ''T.min = M'' and ''T.max = -1'' to indicate that the tree is empty.

Otherwise, if ''x = T.min'' then we need to find the second-smallest value ''y'' in the vEB tree, delete it from its current location, and set ''T.min=y''.  The second-smallest value ''y'' is either ''T.max'' or ''T.children[T.aux.min].min'', so it can be found in ''O''(1) time.  In the latter case we delete ''y'' from the subtree that contains it.

Similarly, if ''x = T.max'' then we need to find the second-largest value ''y'' in the vEB tree and set ''T.max=y''.  The second-largest value ''y'' is either ''T.min'' or ''T.children[T.aux.max].max'', so it can be found in ''O''(1) time.  We also delete ''x'' from the subtree that contains it.

In case where x is not T.min or T.max, and T has no other elements, we know x is not in T and return without further operations.

Otherwise, we have the typical case where ''x≠T.min'' and ''x≠T.max''.  In this case we delete x from the subtree ''T.children[i]'' that contains ''x''.

In any of the above cases, if we delete the last element ''x'' or ''y'' from any subtree ''T.children[i]'' then we also delete ''i'' from ''T.aux''

In code:
 '''function''' Delete(T, x)
     '''if''' T.min == T.max == x '''then'''
         T.min = M
         T.max = -1
         '''return'''
     '''if''' x == T.min '''then'''
         '''if''' T.aux is empty '''then'''
             T.min = T.max
             '''return'''
         '''else'''
             x = T.children[T.aux.min].min
             T.min = x
     '''if''' x == T.max '''then'''
         '''if''' T.aux is empty '''then'''
             T.max = T.min
             '''return'''
         '''else'''
             T.max = T.children[T.aux.max].max
     '''if''' T.aux is empty '''then'''
         '''return'''
     i = floor(x / &lt;math&gt;\sqrt{M}&lt;/math&gt;)
     Delete(T.children[i], x % &lt;math&gt;\sqrt{M}&lt;/math&gt;)
     '''if''' T.children[i] is empty '''then'''
         Delete(T.aux, i)
 '''end'''

Again, the efficiency of this procedure hinges on the fact that deleting from a vEB tree that contains only one element takes only constant time.  In particular, the last line of code only executes if ''x'' was the only element in ''T.children[i]'' prior to the deletion.

===Discussion===

The assumption that ''log m'' is an integer is unnecessary.  The operations ''x/&lt;math&gt;\sqrt{M}&lt;/math&gt;'' and ''x%&lt;math&gt;\sqrt{M}&lt;/math&gt;'' can be replaced by taking only higher-order ceil(m/2) and the lower-order floor(m/2) bits of ''x'', respectively.  On any existing machine, this is more efficient than division or remainder computations.

The implementation described above uses pointers and occupies a total space of &lt;math&gt;O(M) = O(2^m) &lt;/math&gt;.
This can be seen as follows. The recurrence is &lt;math&gt; S(M) = O( \sqrt{M}) + (\sqrt{M}+1) \cdot S(\sqrt{M}) &lt;/math&gt;.
Resolving that would lead to &lt;math&gt; S(M) \in (1 + \sqrt{M})^{\log \log M}  + \log \log M \cdot O( \sqrt{M} )&lt;/math&gt;.
One can, fortunately, also show that &lt;math&gt;S(M) = M-2 &lt;/math&gt; by induction.&lt;ref&gt;{{cite web|last=Rex|first=A|title=Determining the space complexity of van Emde Boas trees|url=http://mathoverflow.net/questions/2245/determining-the-space-complexity-of-van-emde-boas-trees|accessdate=2011-05-27}}&lt;/ref&gt;

In practical implementations, especially on machines with ''shift-by-k'' and ''find first zero'' instructions, performance can further be improved by switching to a [[bit array]] once ''m'' equal to the [[Word (data type)|word size]] (or a small multiple thereof) is reached. Since all operations on a single word are constant time, this does not affect the asymptotic performance, but it does avoid the majority of the pointer storage and several pointer dereferences, achieving a significant practical savings in time and space with this trick.

An obvious optimization of vEB trees is to discard empty subtrees. This makes vEB trees quite compact when they contain many elements, because no subtrees are created until something needs to be added to them. Initially, each element added creates about log(''m'') new trees containing about ''m/2'' pointers all together. As the tree grows, more and more subtrees are reused, especially the larger ones. In a full tree of 2&lt;sup&gt;''m''&lt;/sup&gt; elements, only O(2&lt;sup&gt;''m''&lt;/sup&gt;) space is used. Moreover, unlike a binary search tree, most of this space is being used to store data: even for billions of elements, the pointers in a full vEB tree number in the thousands.

However, for small trees the overhead associated with vEB trees is enormous: on the order of &lt;math&gt;\sqrt{M}&lt;/math&gt;. This is one reason why they are not popular in practice. One way of addressing this limitation is to use only a fixed number of bits per level, which results in a [[trie]].  Alternatively, each table may be replaced by a [[hash table]], reducing the space to O(''n'') (where ''n'' is the number of elements stored in the data structure) at the expense of making the data structure randomized. Other structures, including [[y-fast trie]]s and [[x-fast trie]]s have been proposed that have comparable update and query times and also use randomized hash tables to reduce the space to ''O''(''n'') or ''O''(''n'' log ''M'').

==References==
{{reflist}}

===Further reading===
{{refbegin}}
* Erik Demaine, Shantonu Sen, and Jeff Lindy. Massachusetts Institute of Technology. 6.897: Advanced Data Structures (Spring 2003). [http://theory.csail.mit.edu/classes/6.897/spring03/scribe_notes/L1/lecture1.pdf Lecture 1 notes: Fixed-universe successor problem, van Emde Boas]. [http://theory.csail.mit.edu/classes/6.897/spring03/scribe_notes/L2/lecture2.pdf Lecture 2 notes: More van Emde Boas, ...].
* {{cite doi|10.1007/BF01683268}}

{{refend}}

{{CS-Trees}}

[[Category:Trees (data structures)]]
[[Category:Computer science articles needing expert attention]]
[[Category:Priority queues]]</text>
      <sha1>rrrtmp97evuyb3o92rtk4fb6uaojb5u</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Monotone priority queue</title>
    <ns>0</ns>
    <id>43920111</id>
    <revision>
      <id>626639205</id>
      <parentid>626639050</parentid>
      <timestamp>2014-09-22T16:19:05Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <minor/>
      <text xml:space="preserve" bytes="2142">In [[computer science]], a '''monotone priority queue''' is a variant of the [[priority queue]]/[[heap (data structure)|heap]] data structure. It offers ''insert'' and ''extract-min'' (or ''extract-max''; this article assumes min-priority queues, [[without loss of generality]]) operations, like an ordinary priority queue, but imposes the restriction that a key (item) may only be inserted if its priority is greater than that of the last key extracted from the queue. This entails that the sequence of keys extracted from the queue from a [[Monotonic function|monotonically increasing]] sequence.&lt;ref name=&quot;mehlhorn&quot;&gt;{{cite book |last1=Mehlhorn |first1=Kurt |first2=Peter |last2=Sanders |title=Algorithms and Data Structures: The Basic Toolbox |publisher=Springer |year=2008}}&lt;/ref&gt;{{rp|128}} This restriction is met by several applications, including [[discrete event simulation]] and the [[best-first search|best-first]] version of [[branch and bound]].&lt;ref name=&quot;mehlhorn&quot;/&gt;{{rp|128}}

Specialized monotone PQ data structures can be used to obtain [[Analysis of algorithms|asymptotically]] better running times for algorithms using them, compared to standard priority queues. For example, in graph with integer edge costs, these structures can be used to speed up [[Dijkstra's algorithm]] for shortest-path finding&lt;ref name=&quot;Cherkassky&quot;&gt;{{cite journal |last1=Cherkassky |first1=Boris V. |first2=Andrew V. |last2=Goldberg |first3=Craig |last3=Silverstein |title=Buckets, heaps, lists, and monotone priority queues |journal=SIAM J. Computing |volume=28 |number=4 |year=1999 |pages=1326–1346 |url=http://xenon.stanford.edu/~csilvers/papers/hotq-soda.ps}}&lt;/ref&gt; (for arbitrary edge costs, it is unknown whether a speedup can be achieved&lt;ref name=&quot;mehlhorn&quot;/&gt;{{rp|201}}).

==References==
{{reflist}}

==Further reading==
* {{cite conference |last=Raman |first=Rajeev |title=Priority queues: Small, monotone and trans-dichotomous |conference=ESA |year=1996 |pages=121–137 |url=http://www.researchgate.net/publication/220770017_Priority_Queues_Small_Monotone_and_Trans-dichotomous/file/3deec522ec2bd5ad21.pdf}}

[[Category:Priority queues]]</text>
      <sha1>l3uqtbe6uukjf5rjpwix45wq148lg2e</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Heap (data structure)</title>
    <ns>0</ns>
    <id>13996</id>
    <revision>
      <id>625242702</id>
      <parentid>625182259</parentid>
      <timestamp>2014-09-12T14:19:40Z</timestamp>
      <contributor>
        <username>Mindmatrix</username>
        <id>160367</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/58.69.145.203|58.69.145.203]] ([[User talk:58.69.145.203|talk]]) to last version by Mindmatrix</comment>
      <text xml:space="preserve" bytes="10854">{{About|the programming data structure|the dynamic memory area|Dynamic memory allocation}}

{{more footnotes|date=November 2013}}
[[Image:Max-Heap.svg|thumb|right|240px|Example of a [[Complete binary tree|complete binary]] max-heap with node keys being integers from 1 to 100]]

In [[computer science]], a '''heap''' is a specialized [[Tree (data structure)|tree]]-based [[data structure]] that satisfies the ''heap property:'' If A is a parent [[Node (computer science)|node]] of B then the key of node A is ordered with respect to the key of node B with the same ordering applying across the heap. Either the keys of parent nodes are always greater than or equal to those of the children and the highest key is in the root node (this kind of heap is called ''max heap'') or the keys of parent nodes are less than or equal to those of the children and the lowest key is in the root node (''min heap''). Heaps are crucial in several efficient [[graph theory|graph]] [[algorithm]]s such as [[Dijkstra's algorithm]], and in the sorting algorithm [[heapsort]]. A common implementation of a heap is the [[binary heap]], in which the tree is a complete binary tree (see figure).

In a heap the highest (or lowest) priority element is always stored at the root, hence the name '''heap'''. A heap is not a sorted structure and can be regarded as partially ordered. As visible from the Heap-diagram, there is no particular relationship among nodes on any given level, even among the siblings. When a heap is a complete binary tree, it has a smallest possible height - a heap with N nodes always has O(log N) height. A heap is a useful data structure when you need to remove the object with the highest (or lowest) priority.

Note that, as shown in the graphic, there is no implied ordering between siblings or cousins and no implied sequence for an [[Inorder traversal|in-order traversal]] (as there would be in, e.g., a [[binary search tree]]). The heap relation mentioned above applies only between nodes and their parents, grandparents, etc.  The maximum number of children each node can have depends on the type of heap, but in many types it is at most two, which is known as a binary heap.

The heap is one maximally efficient implementation of an [[abstract data type]] called a [[priority queue]], and in fact priority queues are often referred to as &quot;heaps&quot;, regardless of how they may be implemented. Note that despite the similarity of the name &quot;heap&quot; to &quot;[[stack (abstract data type)|stack]]&quot; and &quot;[[queue (abstract data type)|queue]]&quot;, the latter two are abstract data types, while a heap is a specific data structure, and &quot;priority queue&quot; is the proper term for the abstract data type.{{Citation needed|date=November 2013}}

A ''heap'' data structure should not be confused with ''the heap'' which is a common name for the pool of memory from which [[Dynamic memory allocation|dynamically allocated memory]] is allocated. The term was originally used only for the data structure.

==Implementation and operations==

Heaps are usually implemented in an array, and do not require pointers between elements.

Full and almost full [[binary heap]]s may be represented in a very space-efficient way using an [[array data structure|array]] alone. The first (or last) element will contain the root. The next two elements of the array contain its children. The next four contain the four children of the two child nodes, etc. Thus the children of the node at position ''n'' would be at positions 2''n'' and 2''n''+1 in a one-based array, or 2''n''+1 and 2''n''+2 in a zero-based array. This allows moving up or down the tree by doing simple index computations. Balancing a heap is done by swapping elements which are out of order. As we can build a heap from an array without requiring extra memory (for the nodes, for example), [[heapsort]] can be used to sort an array in-place.

The operations commonly performed with a heap are:
* ''create-heap'': create an empty heap
* ''heapify'': create a heap out of given array of elements
* ''find-max'' or ''find-min'': find the maximum item of a max-heap or a minimum item of a min-heap (aka, ''[[Peek (data type operation)|peek]]'')
* ''delete-max'' or ''delete-min'': removing the root node of a max- or min-heap, respectively
* ''increase-key'' or ''decrease-key'': updating a key within a max- or min-heap, respectively
* ''insert'': adding a new key to the heap
* ''merge'': joining two heaps to form a valid new heap containing all the elements of both.
* ''meld(h1,h2)'': Return the heap formed by taking the union of the item-disjoint heaps h1 and h2. Melding destroys h1 and h2.
* ''size'': return the number of items in the heap.
* ''isEmpty()'': returns true if the heap is empty, false otherwise.
* ''buildHeap(list)'': builds a new heap from a list of keys.
* ''ExtractMin()'' [or ''ExtractMax()'']: Returns the node of minimum value from a min heap [or maximum value from a max heap] after removing it from the heap
* ''Union()'': Creates a new heap by joining two heaps given as input.
* ''Shift-up'': Move a node up in the tree, as long as needed (depending on the heap condition: min-heap or max-heap)
* ''Shift-down'': Move a node down in the tree, similar to Shift-up
Different types of heaps implement the operations in different ways, but notably, insertion is often done by adding the new element at the end of the heap in the first available free space. This will tend to violate the heap property, and so the elements are then reordered until the heap property has been reestablished. Construction of a binary (or ''d''-ary) heap out of a given array of elements may be performed faster than a sequence of consecutive insertions into an originally empty heap using the classic [[Heapsort#Variations|Floyd's algorithm]], with the worst-case number of comparisons equal to 2''N'' − 2''s''&lt;sub&gt;2&lt;/sub&gt;(''N'') − ''e''&lt;sub&gt;2&lt;/sub&gt;(''N'') (for a binary heap), where ''s''&lt;sub&gt;2&lt;/sub&gt;(''N'') is the sum of all digits of the binary representation of ''N'' and ''e''&lt;sub&gt;2&lt;/sub&gt;(''N'') is the exponent of 2 in the prime factorization of ''N''.&lt;ref&gt;{{citation
 | last1 = Suchenek | first1 = Marek A.
 | title = Elementary Yet Precise Worst-Case Analysis of Floyd's Heap-Construction Program
 | doi = 10.3233/FI-2012-751
 | pages = 75–92
 | publisher = IOS Press
 | journal = Fundamenta Informaticae
 | volume = 120
 | issue = 1
 | year = 2012
 | url = http://www.deepdyve.com/lp/ios-press/elementary-yet-precise-worst-case-analysis-of-floyd-s-heap-50NW30HMxU}}.&lt;/ref&gt;

==Variants==
* [[2-3 heap]]
* [[B-heap]]
* [[Beap]]
* [[Binary heap]]
* [[Binomial heap]]
* [[Brodal queue]]
* [[D-ary heap|''d''-ary heap]]
* [[Fibonacci heap]]
* [[Leftist tree|Leftist heap]]
* [[Pairing heap]]
* [[Skew heap]]
* [[Soft heap]]
* [[Weak heap]]
* [[Leaf heap]]
* [[Radix heap]]
* [[Randomized meldable heap]]
* [[Ternary heap]]
* [[Treap]]

==Comparison of theoretic bounds for variants==
{{Heap Running Times}}

==Applications==
The heap data structure has many applications.

* [[Heapsort]]: One of the best sorting methods being in-place and with no quadratic worst-case scenarios.
* [[Selection algorithm]]s: A heap allows access to the min or max element in constant time, and other selections (such as median or kth-element) can be done in sub-linear time on data that is in a heap.&lt;ref&gt;{{citation
 | last = Frederickson | first = Greg N.
 | contribution = An Optimal Algorithm for Selection in a Min-Heap
 | doi = 10.1006/inco.1993.1030
 | pages = 197–214
 | publisher = Academic Press
 | title = Information and Computation
 | volume = 104
 | issue = 2
 | year = 1993
 | url = http://ftp.cs.purdue.edu/research/technical_reports/1991/TR%2091-027.pdf}}&lt;/ref&gt;
* [[List of algorithms#Graph algorithms|Graph algorithms]]: By using heaps as internal traversal data structures, run time will be reduced by polynomial order. Examples of such problems are [[Prim's algorithm|Prim's minimal-spanning-tree algorithm]] and [[Dijkstra's algorithm|Dijkstra's shortest-path algorithm]].

*[[Priority Queue]]: A priority queue is an abstract concept like &quot;a list&quot; or &quot;a map&quot;; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with a heap or a variety of other methods.

*[[Order statistics]]: The Heap data structure can be used to efficiently find the kth smallest (or largest) element in an array.

==Implementations==
* The [[C++]] [[Standard Template Library]] provides the &lt;tt&gt;make_heap&lt;/tt&gt;, &lt;tt&gt;push_heap&lt;/tt&gt; and &lt;tt&gt;pop_heap&lt;/tt&gt; algorithms for heaps (usually implemented as binary heaps), which operate on arbitrary random access [[iterator]]s. It treats the iterators as a reference to an array, and uses the array-to-heap conversion. It also provides the container adaptor &lt;tt&gt;priority_queue&lt;/tt&gt;, which wraps these facilities in a container-like class. However, there is no standard support for the decrease/increase-key operation.
* The [[Boost (C++ libraries)|Boost C++ libraries]] include a heaps library. Unlike the STL it supports decrease and increase operations, and supports additional types of heap: specifically, it supports ''d''-ary, binomial, Fibonacci, pairing and skew heaps.
* The [[Java (programming language)|Java]] 2 platform (since version 1.5) provides the binary heap implementation with class [http://docs.oracle.com/javase/6/docs/api/java/util/PriorityQueue.html &lt;tt&gt;java.util.PriorityQueue&amp;lt;E&amp;gt;&lt;/tt&gt;] in [[Java Collections Framework]].
* [[Python (programming language)|Python]] has a [https://docs.python.org/library/heapq.html &lt;tt&gt;heapq&lt;/tt&gt;] module that implements a priority queue using a binary heap.
* [[PHP]] has both max-heap (&lt;tt&gt;SplMaxHeap&lt;/tt&gt;) and min-heap (&lt;tt&gt;SplMinHeap&lt;/tt&gt;) as of version 5.3 in the Standard PHP Library.
* [[Perl]] has implementations of binary, binomial, and Fibonacci heaps in the [https://metacpan.org/module/Heap &lt;tt&gt;Heap&lt;/tt&gt;] distribution available on [[CPAN]].
* The [[Go (programming language)|Go]] library contains a [http://golang.org/pkg/container/heap/ &lt;tt&gt;heap&lt;/tt&gt;] package with heap algorithms that operate on an arbitrary type that satisfy a given interface.
* Apple's [[Core Foundation]] library contains a [https://developer.apple.com/library/mac/#documentation/CoreFoundation/Reference/CFBinaryHeapRef/Reference/reference.html &lt;tt&gt;CFBinaryHeap&lt;/tt&gt;] structure.

==See also==
* [[Sorting algorithm]]
* [[Stack (abstract data type)]]
* [[Queue (abstract data type)]]
* [[Tree (data structure)]]
* [[Treap]], a form of binary search tree based on heap-ordered trees

==References==
{{Reflist}}

==External links==
{{Commons category|Heaps}}
{{Wikibooks|Data Structures|Min and Max Heaps}}
*[http://mathworld.wolfram.com/Heap.html Heap] at Wolfram MathWorld

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Heap (Data Structure)}}
[[Category:Heaps (data structures)| ]]</text>
      <sha1>fxvvp00es3q9rz0jhkpp4d97t8zbial</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Adaptive heap sort</title>
    <ns>0</ns>
    <id>4261544</id>
    <revision>
      <id>544308678</id>
      <parentid>471785300</parentid>
      <timestamp>2013-03-15T07:42:11Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q4680746]]</comment>
      <text xml:space="preserve" bytes="717">The '''adaptive heap sort''' is a [[sorting algorithm]] that is similar to [[heap sort]], but uses a [[randomized binary search tree]] to structure the input according to any preexisting order.  The randomized binary search tree is used to select candidates that are put into the heap, so the heap doesn't need to keep track of all elements.  Adaptive heap sort is a part of the [[Adaptive sort|adaptive sorting family]].

The first adaptive heapsort was [[Smoothsort| Dijkstra's Smoothsort]].

==See also==
* [[Adaptive sort]]

==External links==
*{{DADS|Adaptive heap sort|adaptiveHeapSort}}

[[Category:Sorting algorithms]]
[[Category:Comparison sorts]]
[[Category:Heaps (data structures)]]

{{datastructure-stub}}</text>
      <sha1>2muvxix5gcxfyv70xfk1wdy24w8hew8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>B-heap</title>
    <ns>0</ns>
    <id>30047903</id>
    <revision>
      <id>572434907</id>
      <parentid>514390688</parentid>
      <timestamp>2013-09-11T02:31:25Z</timestamp>
      <contributor>
        <username>Twimoki</username>
        <id>7226010</id>
      </contributor>
      <comment>Added reference to original invention</comment>
      <text xml:space="preserve" bytes="1571">A '''B-heap''' is a [[binary heap]] implemented to keep subtrees in a single [[Page (computer memory)|page]].  This reduces the number of pages accessed by up to a factor of ten for big heaps when using [[virtual memory]], compared with the traditional implementation.&lt;ref&gt;{{cite journal
 | first = Poul-Henning | last = Kamp
 | url = http://queue.acm.org/detail.cfm?id=1814327
 | title = You're Doing It Wrong
 | journal = [[ACM Queue]]
 | date = June 11, 2010
 }}&lt;/ref&gt;
The traditional mapping of elements to locations in an [[Array data structure|array]] puts (almost) every level in a different page.

There are other heap variants which are efficient in computers using virtual memory or caches, such as [[cache-oblivious algorithm]]s, k-heaps,&lt;ref&gt;{{cite doi|10.1093/comjnl/34.5.428}}&lt;/ref&gt; and [[van Emde Boas tree|van Emde Boas layouts]].&lt;ref&gt;{{cite doi|10.1007/BF01683268}}&lt;/ref&gt;

==See also==
* [[D-ary heap]]

==References==
{{Reflist}}

==External links==
*Implementations at http://www.varnish-cache.org/trac/browser/lib/libvarnish/binary_heap.c and http://phk.freebsd.dk/B-Heap/binheap.c
*[https://github.com/valyala/gheap Generic heap implementation with B-heap support].
*For more on van Emde Boas layouts see Benjamin Sach [http://www.cs.bris.ac.uk/Research/Seminars/departmental/2008-03-13_DeptSeminar_BenSach.pdf Descent into Cache-Oblivion] or [http://blogs.msdn.com/b/devdev/archive/2007/06/12/cache-oblivious-data-structures.aspx Cache-oblivious data structures].

{{DEFAULTSORT:B-Heap}}
[[Category:Heaps (data structures)]]


{{datastructure-stub}}</text>
      <sha1>rncydnvmps3sag9oxefzh4ecbcblvpt</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Beap</title>
    <ns>0</ns>
    <id>3335635</id>
    <revision>
      <id>471785310</id>
      <parentid>449863419</parentid>
      <timestamp>2012-01-17T01:07:08Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Moving category Heaps (structure) to [[:Category:Heaps (data structures)]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2012 January 12]].</comment>
      <text xml:space="preserve" bytes="1687">'''Beap''', or '''bi-parental [[Heap (data_structure)|heap]]''', is a [[data structure]] where a node usually has two parents (unless it is the first or last on a level) and two children (unless it is on the last level). Unlike a heap, a beap allows [[sublinear]] search. The beap was introduced by Ian Munro and Hendra Suwanda. A related data structure is the [[Young tableau]].

[[Image:beap.jpg|frame|Beap]]

==Performance== 

The height of the structure is approximately &lt;math&gt;\sqrt{n}&lt;/math&gt;. Also, assuming the last level is full, the number of elements on that level is also &lt;math&gt;\sqrt{n}&lt;/math&gt;. In fact, because of these properties all basic operations (insert, remove, find) run in &lt;math&gt;O(\sqrt{n})&lt;/math&gt; time on average. Find operations in the heap can be &lt;math&gt;O(n)&lt;/math&gt; in the worst case. Removal and insertion of new elements involves propagation of elements up or down (much like in a heap) in order to restore the beap invariant. An additional perk is that beap provides constant time access to the smallest element and &lt;math&gt;O(\sqrt{n})&lt;/math&gt; time for the maximum element.

Actually, a &lt;math&gt;O(\sqrt{n})&lt;/math&gt; find operation can be implemented if parent pointers at each node are maintained. You would start at the absolute bottom-most element of the top node (similar to the left-most child in a heap) and move either up or right to find the element of interest.

==References==

J. Ian Munro and Hendra Suwanda. &quot;Implicit data structures for fast search and update&quot;. ''[[Journal of Computer and System Sciences]]'', 21(2):236250, 1980.

J.W.J Williams in Algorithms 232, &quot;Heapsort&quot;, ''[[Comm. ACM 7]]'' (June 1964), 347-348

[[Category:Heaps (data structures)]]</text>
      <sha1>rchmg27cd20n32vb7o9rxp3cy2s4m0j</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Binomial heap</title>
    <ns>0</ns>
    <id>254138</id>
    <revision>
      <id>623984688</id>
      <parentid>623663667</parentid>
      <timestamp>2014-09-03T09:28:13Z</timestamp>
      <contributor>
        <username>Dexbot</username>
        <id>16752040</id>
      </contributor>
      <minor/>
      <comment>Removing Link GA template ([[d:Wikidata:Development plan#Badges|handled by wikidata]])</comment>
      <text xml:space="preserve" bytes="9837">{{no footnotes|date=March 2013}}
In [[computer science]], a '''binomial heap''' is a [[Heap (data structure)|heap]] similar to a [[binary heap]] but also supports quick merging of two heaps. This is achieved by using a special tree structure. It is important as an implementation of the [[mergeable heap]] [[abstract data type]] (also called [[meldable heap]]), which is a [[priority queue]] supporting merge operation.

==Binomial Heap==
A binomial heap is implemented as a collection of [[binomial]] [[tree data structure|tree]]s (compare with a [[binary heap]], which has a shape of a single [[binary tree]]). A '''binomial tree''' is defined recursively:

* A binomial tree of order 0 is a single node
* A binomial tree of order ''k'' has a root node whose children are roots of binomial trees of orders ''k''−1, ''k''−2, ..., 2, 1, 0 (in this order).

[[Image:Binomial Trees.svg|center|thumb|500px|Binomial trees of order 0 to 3: Each tree has a root node with subtrees of all lower ordered binomial trees, which have been highlighted. For example, the order 3 binomial tree is connected to an order 2, 1, and 0 (highlighted as blue, green and red respectively) binomial tree.]]

A binomial tree of order ''k'' has 2&lt;sup&gt;k&lt;/sup&gt; nodes, height ''k''.

Because of its unique structure, a binomial tree of order ''k'' can be constructed from two trees of order ''k''−1 trivially by attaching one of them as the leftmost child of root of the other one. This feature is central to the ''merge'' operation of a binomial heap, which is its major advantage over other conventional heaps.

The name comes from the shape: a binomial tree of order &lt;math&gt;n&lt;/math&gt; has &lt;math&gt;\tbinom n d&lt;/math&gt; nodes at depth &lt;math&gt;d&lt;/math&gt;. (See [[Binomial coefficient]].)

==Structure of a binomial heap==
A binomial heap is implemented as a set of binomial trees that satisfy the ''binomial heap properties'':

* Each binomial tree in a heap obeys the ''[[minimum-heap property]]'': the key of a node is greater than or equal to the key of its parent.

* There can only be either ''one'' or ''zero'' binomial trees for each order, including zero order.

The first property ensures that the root of each binomial tree contains the smallest key in the tree, which applies to the entire heap.

The second property implies that a binomial heap with ''n'' nodes consists of at most [[Binary logarithm|log]] ''n'' + 1 binomial trees. In fact, the number and orders of these trees are uniquely determined by the number of nodes ''n'': each binomial tree corresponds to one digit in the [[binary numeral system|binary]] representation of number ''n''. For example number 13 is 1101 in binary, &lt;math&gt;2^3 + 2^2 + 2^0&lt;/math&gt;, and thus a binomial heap with 13 nodes will consist of three binomial trees of orders 3, 2, and 0 (see figure below).

&lt;center&gt;[[Image:Binomial-heap-13.svg|325px|Example of a binomial heap]]&lt;br&gt;''Example of a binomial heap containing 13 nodes with distinct keys.&lt;br/&gt;The heap consists of three binomial trees with orders 0, 2, and 3.''&lt;/center&gt;

==Implementation==
Because no operation requires random access to the root nodes of the binomial trees, the roots of the binomial trees can be stored in a [[linked list]], ordered by increasing order of the tree.

===Merge===
[[Image:Binomial heap merge1.svg|right|thumb|200px|To merge two binomial trees of the same order, first compare the root key. Since 7&gt;3, the black tree on the left(with root node 7) is attached to the grey tree on the right(with root node 3) as a subtree. The result is a tree of order 3.]]

As mentioned above, the simplest and most important operation is the merging of two binomial trees of the same order within two binomial heaps. Due to the structure of binomial trees, they can be merged trivially. As their root node is the smallest element within the tree, by comparing the two keys, the smaller of them is the minimum key, and becomes the new root node. Then the other tree becomes a subtree of the combined tree. This operation is basic to the complete merging of two binomial heaps.

 '''function''' mergeTree(p, q)
     '''if''' p.root.key &lt;= q.root.key
         '''return''' p.addSubTree(q)
     '''else'''
         '''return''' q.addSubTree(p)


[[Image:Binomial heap merge2.svg|right|thumb|300px|This shows the merger of two binomial heaps. This is accomplished by merging two binomial trees of the same order one by one. If the resulting merged tree has the same order as one binomial tree in one of the two heaps, then those two are merged again.]]
The operation of '''merging''' two heaps is perhaps the most interesting and can be used as a subroutine in most other operations. The lists of roots of both heaps are traversed simultaneously, similarly as in the [[merge algorithm]].

If only one of the heaps contains a tree of order ''j'', this tree is moved to the merged heap. If both heaps contain a tree of order ''j'', the two trees are merged to one tree of order ''j''+1 so that the minimum-heap property is satisfied. Note that it may later be necessary to merge this tree with some other tree of order ''j''+1 present in one of the heaps. In the course of the algorithm, we need to examine at most three trees of any order (two from the two heaps we merge and one composed of two smaller trees).

Because each binomial tree in a binomial heap corresponds to a bit in the binary representation of its size, there is an analogy between the merging of two heaps and the binary addition of the ''sizes'' of the two heaps, from right-to-left. Whenever a carry occurs during addition, this corresponds to a merging of two binomial trees during the merge.

Each tree has order at most log ''n'' and therefore the running time is ''O''(log ''n'').

 '''function''' merge(p, q)
     '''while''' '''not''' (p.end() '''and''' q.end())
         tree = mergeTree(p.currentTree(), q.currentTree())
         
         '''if''' '''not''' heap.currentTree().empty()
             tree = mergeTree(tree, heap.currentTree())
         
         heap.addTree(tree)
         heap.next(); p.next(); q.next()



===Insert===
'''Inserting''' a new element to a heap can be done by simply creating a new heap containing only this element and then merging it with the original heap. Due to the merge, insert takes O(log ''n'') time. However, across a series of ''n'' consecutive insertions, '''insert''' has an ''amortized'' time of O(1) (i.e. constant).

===Find minimum===
To find the '''minimum''' element of the heap, find the minimum among the roots of the binomial trees. This can again be done easily in ''O''(log ''n'') time, as there are just ''O''(log ''n'') trees and hence roots to examine.

By using a pointer to the binomial tree that contains the minimum element, the time for this operation can be reduced to ''O''(1). The pointer must be updated when performing any operation other than Find minimum. This can be done in ''O''(log ''n'') without raising the running time of any operation.

===Delete minimum===
To '''delete the minimum element''' from the heap, first find this element, remove it from its binomial tree, and obtain a list of its subtrees. Then transform this list of subtrees into a separate binomial heap by reordering them from smallest to largest order. Then merge this heap with the original heap. Since each tree has at most log ''n'' children, creating this new heap is ''O''(log ''n''). Merging heaps is ''O''(log ''n''), so the entire delete minimum operation is ''O''(log ''n'').

 '''function''' deleteMin(heap)
     min = heap.trees().first()
     '''for each''' current '''in''' heap.trees()
         '''if''' current.root &lt; min '''then''' min = current
     '''for each''' tree '''in''' min.subTrees()
         tmp.addTree(tree)
     heap.removeTree(min)
     merge(heap, tmp)

===Decrease key===
After '''decreasing''' the key of an element, it may become smaller than the key of its parent, violating the minimum-heap property. If this is the case, exchange the element with its parent, and possibly also with its grandparent, and so on, until the minimum-heap property is no longer violated. Each binomial tree has height at most log ''n'', so this takes ''O''(log ''n'') time.

===Delete===
To '''delete''' an element from the heap, decrease its key to negative infinity (that is, some value lower than any element in the heap) and then delete the minimum in the heap.

==Summary of running times==
{{Heap Running Times}}

==Applications==
* [[Discrete event simulation]]
* [[Priority queue]]s

==See also==
* [[Fibonacci heap]]
* [[Soft heap]]
* [[Skew binomial heap]]

==References==
* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Chapter 19: Binomial Heaps, pp.&amp;nbsp;455&amp;ndash;475.
* Vuillemin, J. (1978). [http://portal.acm.org/citation.cfm?id=359478 A data structure for manipulating priority queues.] ''Communications of the ACM'' '''21''', 309–314.

==External links==
&lt;!-- * [http://es.wikipedia.org/wiki/Heap_Binomial Binomial Heaps (Spanish)]  This is in wikidata and on the left handsite with all other iw-links--&gt; 
* [http://www.cs.yorku.ca/~aaw/Sotirios/BinomialHeap.html Java applet simulation of binomial heap]
* [http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/511508 Python implementation of binomial heap]
* [http://www.cs.unc.edu/~bbb/#binomial_heaps Two C implementations of binomial heap] (a generic one and one optimized for integer keys)
* [http://hackage.haskell.org/packages/archive/TreeStructures/latest/doc/html/src/Data-Heap-Binomial.html Haskell implementation of binomial heap]
* [https://github.com/vy/binomial-heap Common Lisp implementation of binomial heap]

{{Data structures}}

[[Category:Heaps (data structures)]]</text>
      <sha1>o9y3x1fzalywfwqz9mipm3903s1aip8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Brodal queue</title>
    <ns>0</ns>
    <id>33238984</id>
    <revision>
      <id>575676381</id>
      <parentid>557876432</parentid>
      <timestamp>2013-10-04T03:33:04Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>{{algorithm-stub}}</comment>
      <text xml:space="preserve" bytes="1308">In [[computer science]], the '''Brodal queue''' is a [[Heap (data structure)|heap]]/[[priority queue]] structure with very low [[Best, worst and average case|worst case]] [[Asymptotic analysis|time bounds]]: &lt;math&gt;O(1)&lt;/math&gt; for insertion, find-minimum, meld (merge two queues) and decrease-key and &lt;math&gt;O(\mathrm{log}(n))&lt;/math&gt; for delete-minimum and general deletion; they are the first heap variant with these bounds. Brodal queues are named after their inventor [[Gerth Stølting Brodal]].&lt;ref name=&quot;Brodal&quot;&gt;Gerth Stølting Brodal (1996). Worst-case efficient priority queues. Proc. 7th ACM-SIAM Symposium on Discrete Algorithms, pp. 52—58&lt;/ref&gt;

While having better asymptotic bounds than other priority queue structures, they are, in the words of Brodal himself, &quot;quite complicated&quot; and &quot;[not] applicable in practice.&quot;&lt;ref name=&quot;Brodal&quot;/&gt; Brodal and [[Chris Okasaki|Okasaki]] describe a [[persistent data structure|persistent]] ([[purely functional|functional]]) version of Brodal queues.&lt;ref name=&quot;BrodalOkasaki&quot;&gt;Gerth Stølting Brodal and Chris Okasaki (1996). [https://users.info.unicaen.fr/~karczma/TEACH/Doc/brodal_okasaki.pdf Optimal purely functional priority queues]. J. Functional Programming.&lt;/ref&gt;

==References==
&lt;references/&gt;

[[Category:Heaps (data structures)]]


{{algorithm-stub}}</text>
      <sha1>8gvathu66xyvb99tbla9dw08yo0lxhq</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>D-ary heap</title>
    <ns>0</ns>
    <id>11960848</id>
    <revision>
      <id>576961204</id>
      <parentid>562181496</parentid>
      <timestamp>2013-10-13T08:00:09Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <minor/>
      <comment>/* Applications */ [[Andrew V. Goldberg]]</comment>
      <text xml:space="preserve" bytes="11741">{{DISPLAYTITLE:''d''-ary heap}}
The '''{{math|''d''}}-ary heap''' or '''{{math|''d''}}-heap''' is a [[priority queue]] [[data structure]], a generalization of the [[binary heap]] in which the nodes have {{math|''d''}} children instead of 2.&lt;ref name=&quot;j75&quot;/&gt;&lt;ref name=&quot;t83&quot;/&gt;&lt;ref name=&quot;w07&quot;/&gt; Thus, a binary heap is a 2-heap. According to Tarjan&lt;ref name=&quot;t83&quot;/&gt; and Jensen et al.,&lt;ref&gt;{{citation
 | last1 = Jensen | first1 = C.
 | last2 = Katajainen | first2 = J.
 | last3 = Vitale | first3 = F.
 | title = An extended truth about heaps
 | url = http://www.cphstl.dk/Report/In-place-multiway-heaps/experimental-study.pdf
 | year = 2004}}.&lt;/ref&gt; {{math|''d''}}-ary heaps were invented by [[Donald B. Johnson]] in 1975.&lt;ref name=&quot;j75&quot;&gt;{{citation
 | last = Johnson | first = D. B. | authorlink = Donald B. Johnson
 | doi = 10.1016/0020-0190(75)90001-0
 | journal = Information Processing Letters
 | pages = 53–57
 | title = Priority queues with update and finding minimum spanning trees
 | volume = 4
 | year = 1975
 | issue = 3}}.&lt;/ref&gt;

This data structure allows decrease priority operations to be performed more quickly than binary heaps, at the expense of slower delete minimum operations. This tradeoff leads to better running times for algorithms such as [[Dijkstra's algorithm]] in which decrease priority operations are more common than delete min operations.&lt;ref name=&quot;j75&quot;/&gt;&lt;ref name=&quot;t2&quot;/&gt; Additionally, {{math|''d''}}-ary heaps have better [[memory cache]] behavior than a binary heap, allowing them to run more quickly in practice despite having a theoretically larger worst-case running time.&lt;ref name=&quot;nmm91&quot;/&gt;&lt;ref name=&quot;k10&quot;/&gt; Like binary heaps, {{math|''d''}}-ary heaps are an [[In-place algorithm|in-place data structure]] that uses no additional storage beyond that needed to store the array of items in the heap.&lt;ref name=&quot;t83&quot;/&gt;&lt;ref name=&quot;mp05&quot;&gt;{{citation
 | last1 = Mortensen | first1 = C. W.
 | last2 = Pettie | first2 = S.
 | contribution = The complexity of implicit and space efficient priority queues
 | doi = 10.1007/11534273_6
 | pages = 49–60
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | title = [[SWAT and WADS conferences|Algorithms and Data Structures: 9th International Workshop, WADS 2005, Waterloo, Canada, August 15-17, 2005, Proceedings]]
 | volume = 3608
 | year = 2005
 | isbn = 978-3-540-28101-6}}.&lt;/ref&gt;

==Data structure==
The {{math|''d''}}-ary heap consists of an [[Array data structure|array]] of {{math|''n''}} items, each of which has a priority associated with it. These items may be viewed as the nodes in a complete {{math|''d''}}-ary tree, listed in [[breadth-first search|breadth first traversal order]]: the item at position 0 of the array forms the root of the tree, the items at positions 1–{{math|''d''}} are its children, the next {{math|''d''&lt;sup&gt;2&lt;/sup&gt;}} items are its grandchildren, etc. Thus, the parent of the item at position {{math|''i''}} (for any {{math|''i'' &gt; 0}}) is the item at position {{math|floor((''i'' &amp;minus; 1)/''d'')}} and its children are the items at positions {{math|''di'' + 1}} through {{math|''di'' + ''d''}}. According to the [[binary heap|heap property]], in a min-heap, each item has a priority that is at least as large as its parent; in a max-heap, each item has a priority that is no larger than its parent.&lt;ref name=&quot;t83&quot;&gt;{{citation
 | last = Tarjan | first = R. E. | author-link = Robert Tarjan
 | contribution = 3.2. ''d''-heaps
 | pages = 34–38
 | publisher = [[Society for Industrial and Applied Mathematics]]
 | series = CBMS-NSF Regional Conference Series in Applied Mathematics
 | title = Data Structures and Network Algorithms
 | volume = 44
 | year = 1983}}.&lt;/ref&gt;&lt;ref name=&quot;w07&quot;/&gt;

The minimum priority item in a min-heap (or the maximum priority item in a max-heap) may always be found at position 0 of the array. To remove this item from the priority queue, the last item ''x'' in the array is moved into its place, and the length of the array is decreased by one. Then, while item ''x'' and its children do not satisfy the heap property, item ''x'' is swapped with one of its children (the one with the smallest priority in a min-heap, or the one with the largest priority in a max-heap), moving it downward in the tree and later in the array, until eventually the heap property is satisfied. The same downward swapping procedure may be used to increase the priority of an item in a min-heap, or to decrease the priority of an item in a max-heap.&lt;ref name=&quot;t83&quot;/&gt;&lt;ref name=&quot;w07&quot;/&gt;

To insert a new item into the heap, the item is appended to the end of the array, and then while the heap property is violated it is swapped with its parent, moving it upward in the tree and earlier in the array, until eventually the heap property is satisfied. The same upward-swapping procedure may be used to decrease the priority of an item in a min-heap, or to increase the priority of an item in a max-heap.&lt;ref name=&quot;t83&quot;/&gt;&lt;ref name=&quot;w07&quot;/&gt;

To create a new heap from an array of {{math|''n''}} items, one may loop over the items in reverse order, starting from the item at position {{math|''n'' &amp;minus; 1}} and ending at the item at position 0, applying the downward-swapping procedure for each item.&lt;ref name=&quot;t83&quot;/&gt;&lt;ref name=&quot;w07&quot;/&gt;

==Analysis==
In a {{math|''d''}}-ary heap with {{math|''n''}} items in it, both the upward-swapping procedure and the downward-swapping procedure may perform as many as {{math|1=log&lt;sub&gt;''d''&lt;/sub&gt; ''n'' = log ''n'' / log ''d''}} swaps. In the upward-swapping procedure, each swap involves a single comparison of an item with its parent, and takes constant time. Therefore, the time to insert a new item into the heap, to decrease the priority of an item in a min-heap, or to increase the priority of an item in a max-heap, is {{math|O(log ''n'' / log ''d'')}}. In the downward-swapping procedure, each swap involves {{math|''d''}} comparisons and takes {{math|O(''d'')}} time: it takes {{math|''d'' &amp;minus; 1}} comparisons to determine the minimum or maximum of the children and then one more comparison against the parent to determine whether a swap is needed. Therefore, the time to delete the root item, to increase the priority of an item in a min-heap, or to decrease the priority of an item in a max-heap, is {{math|O(''d'' log ''n'' / log ''d'')}}.&lt;ref name=&quot;t83&quot;/&gt;&lt;ref name=&quot;w07&quot;/&gt;

When creating a {{math|''d''}}-ary heap from a set of ''n'' items, most of the items are in positions that will eventually hold leaves of the {{math|''d''}}-ary tree, and no downward swapping is performed for those items. At most {{math|''n''/''d'' + 1}} items are non-leaves, and may be swapped downwards at least once, at a cost of {{math|O(''d'')}} time to find the child to swap them with. At most {{math|''n''/''d''&lt;sup&gt;2&lt;/sup&gt; + 1}} nodes may be swapped downward two times, incurring an additional {{math|O(''d'')}} cost for the second swap beyond the cost already counted in the first term, etc. Therefore, the total amount of time to create a heap in this way is
:&lt;math&gt;\sum_{i=1}^{\log_d n} \left(\frac{n}{d^i}+1\right) O(d) = O(n).&lt;/math&gt;&lt;ref name=&quot;t83&quot;/&gt;&lt;ref name=&quot;w07&quot;/&gt;

The exact value of the above (the worst-case number of comparisons during the construction of d-ary heap) is known to be equal to:

:&lt;math&gt; \frac{d}{d-1} (n - s_d (n)) - (d-1 - (n  \mod  d)) ( e_d ( \lfloor \frac{n}{d} \rfloor) + 1) &lt;/math&gt;,&lt;ref name=&quot;Suchenek&quot;&gt;{{citation
 | last1 = Suchenek | first1 = Marek A.
 | title = Elementary Yet Precise Worst-Case Analysis of Floyd's Heap-Construction Program
 | doi = 10.3233/FI-2012-751
 | pages = 75–92
 | publisher = IOS Press
 | journal = Fundamenta Informaticae
 | volume = 120
 | issue = 1
 | year = 2012
 | url = http://www.deepdyve.com/lp/ios-press/elementary-yet-precise-worst-case-analysis-of-floyd-s-heap-50NW30HMxU}}.&lt;/ref&gt;
where s&lt;sub&gt;d&lt;/sub&gt;(n) is the sum of all digits of the standard base-d representation of n and e&lt;sub&gt;d&lt;/sub&gt;(n) is the exponent of d in the factorization of n.
This reduces to
:&lt;math&gt; 2 n - 2 s_2 (n) - e_2 (n) &lt;/math&gt;, &lt;ref name=&quot;Suchenek&quot; /&gt;
for d = 2, and to
:&lt;math&gt; \frac{3}{2} (n - s_3 (n)) - 2 e_3 (n) - e_3 (n-1) &lt;/math&gt;,&lt;ref name=&quot;Suchenek&quot; /&gt;

for d = 3.


The space usage of the {{math|''d''-ary}} heap, with insert and delete-min operations, is linear, as it uses no extra storage other than an array containing a list of the items in the heap.&lt;ref name=&quot;t83&quot;/&gt;&lt;ref name=&quot;mp05&quot;/&gt; If changes to the priorities of existing items need to be supported, then one must also maintain pointers from the items to their positions in the heap, which again uses only linear storage.&lt;ref name=&quot;t83&quot;/&gt;

==Applications==
[[Dijkstra's algorithm]] for [[shortest path]]s in graphs and [[Prim's algorithm]] for [[minimum spanning tree]]s both use a min-heap in which there are {{math|''n''}} delete-min operations and as many as {{math|''m''}} decrease-priority operations, where {{math|''n''}} is the number of vertices in the graph and ''m'' is the number of edges. By using a {{math|''d''}}-ary heap with {{math|1=''d'' = ''m''/''n''}}, the total times for these two types of operations may be balanced against each other, leading to a total time of {{math|O(''m'' log&lt;sub&gt;''m''/''n''&lt;/sub&gt; ''n'')}} for the algorithm, an improvement over the {{math|O(''m'' log ''n'')}} running time of binary heap versions of these algorithms whenever the number of edges is significantly larger than the number of vertices.&lt;ref name=&quot;j75&quot;/&gt;&lt;ref name=&quot;t2&quot;&gt;{{harvtxt|Tarjan|1983}}, pp. 77 and 91.&lt;/ref&gt; An alternative priority queue data structure, the [[Fibonacci heap]], gives an even better theoretical running time of {{math|O(''m'' + ''n'' log ''n'')}}, but in practice {{math|''d''}}-ary heaps are generally at least as fast, and often faster, than Fibonacci heaps for this application.&lt;ref&gt;{{citation
 | last1 = Cherkassky | first1 = B. V.
 | last2 = Goldberg | first2 = A. V. | author2-link = Andrew V. Goldberg
 | last3 = Radzik | first3 = T.
 | doi = 10.1007/BF02592101
 | issue = 2
 | journal = Mathematical Programming
 | pages = 129–174
 | title = Shortest paths algorithms: Theory and experimental evaluation
 | volume = 73
 | year = 1996}}.&lt;/ref&gt;

4-heaps may perform better than binary heaps in practice, even for delete-min operations.&lt;ref name=&quot;t83&quot;/&gt;&lt;ref name=&quot;w07&quot;&gt;{{citation
 | last = Weiss | first = M. A.
 | contribution = ''d''-heaps
 | edition = 2nd
 | isbn = 0-321-37013-9
 | page = 216
 | publisher = Addison-Wesley
 | title = Data Structures and Algorithm Analysis
 | year = 2007}}.&lt;/ref&gt; Additionally,
a {{math|''d''}}-ary heap typically runs much faster than a binary heap for heap sizes that exceed the size of the computer's [[cache memory]]:
A binary heap typically requires more [[cache miss]]es and [[virtual memory]] [[page fault]]s than a {{math|''d''}}-ary heap, each one taking far more time than the extra work incurred by the additional comparisons a {{math|''d''}}-ary heap makes compared to a binary heap.&lt;ref name=&quot;nmm91&quot;&gt;{{citation
 | last1 = Naor | first1 = D.
 | last2 = Martel | first2 = C. U.
 | last3 = Matloff | first3 = N. S.
 | year = 1991
 | doi = 10.1093/comjnl/34.5.428
 | issue = 5
 | journal = Computer Journal
 | pages = 428–437
 | title = Performance of priority queue structures in a virtual memory environment
 | volume = 34}}.&lt;/ref&gt;&lt;ref name=&quot;k10&quot;&gt;{{citation
 | last = Kamp | first = Poul-Henning
 | journal = ACM Queue
 | title = You're doing it wrong
 | url = http://queue.acm.org/detail.cfm?id=1814327
 | volume = 8
 | issue = 6
 | year = 2010}}.&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*[https://github.com/valyala/gheap C++ implementation of generalized heap with D-Heap support]

{{DEFAULTSORT:D-Ary Heap}}
[[Category:Heaps (data structures)]]</text>
      <sha1>hf4gxj6ccbi0j5hu0786v2fbf8sf0jm</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Pairing heap</title>
    <ns>0</ns>
    <id>3402053</id>
    <revision>
      <id>626473130</id>
      <parentid>626353082</parentid>
      <timestamp>2014-09-21T13:02:09Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>major copyedit to lede, mention O(1) conjecture for decrease-key, splay tree inspiration</comment>
      <text xml:space="preserve" bytes="10282">A '''pairing heap''' is a type of [[heap (data structure)|heap]] [[data structure]] with relatively simple implementation and excellent practical [[amortized]] performance. Pairing heaps [[heap property|heap-ordered]] multiway [[Tree (data structure)|tree structures]], and can be considered simplified [[Fibonacci heap]]s. They are considered a &quot;robust choice&quot; for implementing such algorithms as [[Prim's algorithm|Prim's MST algorithm]],&lt;ref name=&quot;mehlhorn&quot;&gt;{{cite book |last1=Mehlhorn |first1=Kurt |first2=Peter |last2=Sanders |title=Algorithms and Data Structures: The Basic Toolbox |publisher=Springer |year=2008}}&lt;/ref&gt;{{rp|231}}, and support the following operations (assuming a min-heap):

* ''find-min'': simply return the top element of the heap.
* ''merge'': compare the two root elements, the smaller remains the root of the result, the larger element and its subtree is appended as a child of this root.
* ''insert'': create a new heap for the inserted element and ''merge'' into the original heap.
* ''decrease-key'' (optional): remove the subtree rooted at the key to be decreased, replace the key with a smaller key, then ''merge'' the result back into the heap.
* ''delete-min'': remove the root and ''merge'' its subtrees.  Various strategies are employed.

The analysis of pairing heaps' time complexity was initially inspired by that of [[splay tree]]s.&lt;ref name=&quot;FSST&quot;/&gt;
The amortized time per ''delete-min'' is {{math|''O''(log ''n'')}}.&lt;ref name=FSST&gt;{{cite journal
 | last1 = Fredman | first1 = Michael L. | author1-link = Michael Fredman
 | last2 = Sedgewick | first2 = Robert | author2-link = Robert Sedgewick (computer scientist)
 | last3 = Sleator | first3 = Daniel D. | author3-link = Daniel Sleator
 | last4 = Tarjan | first4 = Robert E. | author4-link = Robert Tarjan
 | doi = 10.1007/BF01840439
 | issue = 1
 | journal = Algorithmica
 | pages = 111–129
 | title = The pairing heap: a new form of self-adjusting heap
 | url = http://www.lb.cs.cmu.edu/afs/cs.cmu.edu/user/sleator/www/papers/pairing-heaps.pdf
 | volume = 1
 | year = 1986}}&lt;/ref&gt;  The operations ''find-min'', ''merge'', and ''insert'' run in constant time, {{math|''O''(1)}}.&lt;ref name=Iacono&gt;{{cite conference
 | last = Iacono | first = John
 | contribution = Improved upper bounds for pairing heaps
 | url = http://john2.poly.edu/papers/swat00/paper.pdf
 | doi = 10.1007/3-540-44985-X_5
 | pages = 63–77
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | title = Proc. 7th Scandinavian Workshop on Algorithm Theory
 | volume = 1851
 | year = 2000
 | isbn = 978-3-540-67690-4}}&lt;/ref&gt;

Determining the precise asymptotic running time of pairing heaps when a ''decrease-key'' operation is needed has turned out to be difficult. Initially, the time complexity of this operation was conjectured on empirical grounds to be {{math|''O''(1)}},&lt;ref name=&quot;StaskoVitter&quot;/&gt;
but [[Michael Fredman|Fredman]] proved that the amortized time per ''decrease-key'' is at least &lt;math&gt;\Omega(\log\log n)&lt;/math&gt; for some sequences of operations.&lt;ref name=Fredman&gt;{{cite journal
 | last = Fredman | first = Michael L. | author-link = Michael Fredman
 | doi = 10.1145/320211.320214
 | issue = 4
 | journal = Journal of the ACM
 | pages = 473–501
 | title = On the efficiency of pairing heaps and related data structures
 | url = http://wwwens.uqac.ca/azinflou/Fichiers840/EfficiencyPairingHeap.pdf
 | volume = 46
 | year = 1999}}&lt;/ref&gt;
Pettie then derived an upper bound of &lt;math&gt;O(2^{2\sqrt{\log\log n}})&lt;/math&gt; amortized time for ''decrease-key'', which is &lt;math&gt;o(\log n)&lt;/math&gt;.&lt;ref name=Pettie&gt;{{citation
 | last = Pettie | first = Seth
 | contribution = Towards a final analysis of pairing heaps
 | doi = 10.1109/SFCS.2005.75
 | url = http://www.eecs.umich.edu/~pettie/papers/focs05.pdf
 | pages = 174–183
 | title = [[Symposium on Foundations of Computer Science|Proc. 46th Annual IEEE Symposium on Foundations of Computer Science]]
 | year = 2005
 | isbn = 0-7695-2468-0}}&lt;/ref&gt;
No tight &lt;math&gt;\Theta(\log\log n)&lt;/math&gt; bound is known.&lt;ref name=&quot;Pettie&quot;/&gt;

Although this is worse than other priority queue algorithms such as [[Fibonacci heap]]s, which perform ''decrease-key'' in &lt;math&gt;O(1)&lt;/math&gt; amortized time, the performance in practice is excellent.  [[John Stasko|Stasko]] and [[Jeff Vitter|Vitter]]&lt;ref name=StaskoVitter&gt;{{citation
 | last1 = Stasko | first1 = John T. |author-link = John Stasko
 | last2 = Vitter | first2 = Jeffrey S. | author2-link = Jeffrey Vitter
 | doi = 10.1145/214748.214759
 | issue = 3
 | journal = Communications of the ACM
 | pages = 234–249
 | title = Pairing heaps: experiments and analysis
 | id = {{citeseerx|10.1.1.106.2988}}
 | volume = 30
 | year = 1987}}&lt;/ref&gt; and Moret and Shapiro&lt;ref name=MoretShapiro&gt;{{citation
 | last1 = Moret | first1 = Bernard M. E.
 | last2 = Shapiro | first2 = Henry D.
 | contribution = An empirical analysis of algorithms for constructing a minimum spanning tree
 | id = {{citeseerx|10.1.1.53.5960}}
 | doi = 10.1007/BFb0028279
 | pages = 400–411
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | title = [[Workshop on Algorithms and Data Structures|Proc. 2nd Workshop on Algorithms and Data Structures]]
 | volume = 519
 | year = 1991
 | isbn = 3-540-54343-0}}&lt;/ref&gt; conducted experiments on pairing heaps and other heap data structures.  They concluded that the pairing heap is as fast as, and often faster than, other efficient data structures like the [[binary heap]]s.

==Structure==

A pairing heap is either an empty heap, or a pair consisting of a root element and a possibly empty list of pairing heaps. The heap ordering property requires that all the root elements of the subheaps in the list are not smaller than the root element of the heap. The following description assumes a purely functional heap that does not support the ''decrease-key'' operation.

 &lt;code&gt;'''type''' PairingHeap[Elem] = Empty | Heap(elem: Elem, subheaps: List[PairingHeap[Elem]])&lt;/code&gt;

A pointer-based implementation for [[RAM machine]]s, supporting ''decrease-key'', can be achieved using three pointers per node, by representing the children of a node by a [[singly-linked list]]: a pointer to the node's first child, one to its next sibling, and one to the parent. Alternatively, the parent pointer can be omitted by letting the last child point back to the parent, if a single boolean flag is added to indicate &quot;end of list&quot;. This achieves a more compact structure at the expense of a constant overhead factor per operation.&lt;ref name=&quot;FSST&quot;/&gt;

==Operations==

===''find-min''===

The function ''find-min'' simply returns the root element of the heap:

&lt;code&gt;
 '''function''' find-min(heap)
   '''if''' heap == Empty
     '''error'''
   '''else'''
     '''return''' heap.elem
&lt;/code&gt;

===''merge''===

Merging with an empty heap returns the other heap, otherwise a new heap is returned that has the minimum of the two root elements as its root element and just adds the heap with the larger root to the list of subheaps:

&lt;code&gt;
 '''function''' merge(heap1, heap2)
   '''if''' heap1 == Empty
     '''return''' heap2
   '''elsif''' heap2 == Empty
     '''return''' heap1
   '''elsif''' heap1.elem &lt; heap2.elem
     '''return''' Heap(heap1.elem, heap2 :: heap1.subheaps)
   '''else'''
     '''return''' Heap(heap2.elem, heap1 :: heap2.subheaps)
&lt;/code&gt;

===''insert''===

The easiest way to insert an element into a heap is to merge the heap with a new heap containing just this element and an empty list of subheaps:

&lt;code&gt;
 '''function''' insert(elem, heap)
   '''return''' merge(Heap(elem, []), heap)
&lt;/code&gt;

===''delete-min''===

The only non-trivial fundamental operation is the deletion of the minimum element from the heap. The standard strategy first merges the subheaps in pairs (this is the step that gave this datastructure its name) from left to right and then merges the resulting list of heaps from right to left:

&lt;code&gt;
 '''function''' delete-min(heap)
   '''if''' heap == Empty
     '''error'''
   '''else'''
     '''return''' merge-pairs(heap.subheaps)
&lt;/code&gt;

This uses the auxiliary function ''merge-pairs'':

&lt;code&gt;
 '''function''' merge-pairs(l)
   '''if''' length(l) == 0
     '''return''' Empty
   '''elsif''' length(l) == 1
     '''return''' l[0]
   '''else'''
     '''return''' merge(merge(l[0], l[1]), merge-pairs(l[2.. ]))
&lt;/code&gt;

That this does indeed implement the described two-pass left-to-right then right-to-left merging strategy can be seen from this reduction:

&lt;code&gt;
    merge-pairs([H1, H2, H3, H4, H5, H6, H7])
 =&gt; merge(merge(H1, H2), merge-pairs([H3, H4, H5, H6, H7]))
      # merge H1 and H2 to H12, then the rest of the list
 =&gt; merge('''H12''', merge(merge(H3, H4), merge-pairs([H5, H6, H7])))
      # merge H3 and H4 to H34, then the rest of the list
 =&gt; merge(H12, merge('''H34''', merge(merge(H5, H6), merge-pairs([H7]))))
      # merge H5 and H6 to H56, then the rest of the list
 =&gt; merge(H12, merge(H34, merge('''H56''', H7)))
      # switch direction, merge the last two resulting heaps, giving H567
 =&gt; merge(H12, merge(H34, '''H567'''))
      # merge the last two resulting heaps, giving H34567
 =&gt; merge(H12, '''H34567''') 
      # finally, merge the first merged pair with the result of merging the rest
 =&gt; '''H1234567'''
&lt;/code&gt;

==Summary of running times==
{{Heap Running Times}}

==References==

{{reflist|30em}}

==External links==
* Louis Wasserman discusses pairing heaps and their implementation in [[Haskell (programming language)|Haskell]] in [http://themonadreader.files.wordpress.com/2010/05/issue16.pdf The Monad Reader, Issue 16] (pp.&amp;nbsp;37–52).
* [http://www.cise.ufl.edu/~sahni/dsaaj/enrich/c13/pairing.htm pairing heaps], [[Sartaj Sahni]]
* {{Citation |url=http://www.siam.org/proceedings/soda/2009/SODA09_052_elmasrya.pdf |title=Pairing Heaps with O(log log n) decrease Cost |author=Amr Elmasry |journal=Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms {SODA '09} |year=2009 |pages=471–476 |location=New York}}
* [http://www.swi-prolog.org/pldoc/doc/swi/library/heaps.pl heaps library] in [[SWI-Prolog]], uses pairing heaps
* [https://gist.github.com/1248317 Open source implementation of pairing heaps in Erlang]

[[Category:Heaps (data structures)]]</text>
      <sha1>hj4a1hs8pw5wpp10sg6jwjrbumgg8mw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Queap</title>
    <ns>0</ns>
    <id>31075298</id>
    <revision>
      <id>607753106</id>
      <parentid>578215404</parentid>
      <timestamp>2014-05-09T09:38:00Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>Added 1 doi to a journal cite using [[Project:AWB|AWB]] (10104)</comment>
      <text xml:space="preserve" bytes="10091">{{One source|date=April 2013}}
[[File:Queap.svg|thumb | right | 350x260px |A Queap Q with k = 6 and n = 9]]

In [[computer science]], a '''queap''' is a [[priority queue]] [[data structure]]. The data structure allows insertions and deletions of arbitrary elements, as well as retrieval of the highest-priority element. Each deletion takes [[amortized time]] logarithmic in the number of items that have been in the structure for a longer time than the removed item. Insertions take constant amortized time.

The data structure consists of a [[doubly linked list]] and a [[2-3-4 tree|2-4 tree]] data structure, each modified to keep track of its minimum-priority element.
The basic operation of the structure is to keep newly inserted elements in the doubly linked list, until a deletion would remove one of the list items, at which point they are all moved into the 2-4 tree. The 2-4 tree stores its elements in insertion order, rather than the more conventional priority-sorted order.

Both the data structure and its name were devised by John Iacono and Stefan Langerman.&lt;ref&gt;{{cite journal
|author1=John Iacono
|author2=Stefan Langerman
|title=Queaps
|journal=Algorithmica
|volume=42
|number=1
|pages=49–56
|year=2005
|publisher=Springer
|doi=10.1007/s00453-004-1139-5
}}&lt;/ref&gt;

==Description==

A queap is a priority queue that inserts elements in O(1) amortized time, and removes the minimum element in O(log(''k''&amp;nbsp;+&amp;nbsp;2)) if there are ''k'' items that have been in the heap for a longer time than the element to be extracted. The queap has a property called the queueish property: the time to search for element ''x'' is O(lg ''q''(''x'')) where ''q''(''x'') is equal to ''n''&amp;nbsp;&amp;minus;&amp;nbsp;1&amp;nbsp;&amp;minus;&amp;nbsp;''w''(''x'') and ''w''(''x'') is the number of distinct items that has been accessed by operations such as searching, inserting, or deleting. ''q''(''x'') is defined as how many elements have not been accessed since ''x''&lt;nowiki&gt;'&lt;/nowiki&gt;s last access. Indeed, the queueish property is the complement of the splay tree working set property: the time to search for element ''x'' is O(lg ''w''(''x'')).

A queap can be represented by two data structures: a doubly linked list and a modified version of 2-4 tree. The doubly linked list, ''L'', is used for a series of insert and locate-min operations. The queap keeps a pointer to the minimum element stored in the list. To add element ''x'' to list ''l'', the element ''x'' is added to the end of the list and a bit variable in element ''x'' is set to one. This operation is done to determine if the element is either in the list or in a 2-4 tree. 

A 2-4 tree is used when a delete operation occurs. If the item ''x'' is already in tree ''T'', the item is removed using the 2-4 tree delete operation. Otherwise, the item ''x'' is in list ''L'' (done by checking if the bit variable is set). All the elements stored in list ''L'' are then added to the 2-4 tree, setting the bit variable of each element to zero. ''x'' is then removed from ''T''. 

A queap uses only the 2-4 tree structure properties, not a search tree. The modified 2-4 tree structure is as follows. Suppose list ''L'' has the following set of elements: &lt;math&gt;x_1, x_2, x_3, \dots , x_k&lt;/math&gt;. When the deletion operation is invoked, the set of elements stored in ''L'' is then added to the leaves of the 2-4 tree in that order, proceeded by a dummy leaf containing an infinite key. Each internal node of ''T'' has a pointer &lt;math&gt;h_v&lt;/math&gt;, which points to the smallest item in subtree ''v''. Each internal node on path ''P'' from the root to &lt;math&gt;x_0&lt;/math&gt; has a pointer &lt;math&gt;c_v&lt;/math&gt;, which points to the smallest key in &lt;math&gt;T - T_v - \{r\}&lt;/math&gt;. The &lt;math&gt;h_v&lt;/math&gt; pointers of each internal node on path ''P'' are ignored. The queap has a pointer to &lt;math&gt;c_{x_0}&lt;/math&gt;, which points to the smallest element in ''T''.

An application of queaps includes a unique set of high priority events and extraction of the highest priority event for processing.

==Operations==

Let ''minL'' be a pointer that points to the minimum element in the doubly linked list ''L'', &lt;math&gt;c_{x_0}&lt;/math&gt; be the minimum element stored in the 2-4 tree, ''T'', ''k'' be the number of elements stored in ''T'', and ''n'' be the total number of elements stored in queap ''Q''. The operations are as follows:

'''''New(Q):''''' Initializes a new empty queap.

: Initialize an empty doubly linked list ''L'' and 2-4 tree ''T''. Set ''k'' and ''n'' to zero.

'''''Insert(Q, x):''''' Add the element ''x'' to queap ''Q''.

: Insert the element ''x'' in list ''L''. Set the bit in element ''x'' to one to demonstrate that the element is in the list ''L''. Update the ''minL'' pointer if ''x'' is the smallest element in the list. Increment ''n'' by 1.

'''''Minimum(Q):''''' Retrieve a pointer to the smallest element from queap ''Q''.

: If ''key(minL)'' &lt; ''key''(&lt;math&gt;c_{x_0}&lt;/math&gt;), return ''minL''. Otherwise return &lt;math&gt;c_{x_0}&lt;/math&gt;.

'''''Delete(Q, x):''''' Remove element x from queap ''Q''.

: If the bit of the element ''x'' is set to one, the element is stored in list ''L''. Add all the elements from ''L'' to ''T'', setting the bit of each element to zero. Each element is added to the parent of the right most child of ''T'' using the insert operation of the 2-4 tree. ''L'' becomes empty. Update &lt;math&gt;h_v&lt;/math&gt; pointers for all the nodes ''v'' whose children are new/modified, and repeat the process with the next parent until the parent is equal to the root. Walk from the root to node  &lt;math&gt;x_0&lt;/math&gt;, and update the &lt;math&gt;c_v&lt;/math&gt; values. Set ''k'' equal to ''n''. 

: If the bit of the element ''x'' is set to zero, ''x'' is a leaf of ''T''. Delete x using the 2-4 tree delete operation. Starting from node ''x'', walk in ''T'' to node &lt;math&gt;x_0&lt;/math&gt;, updating &lt;math&gt;h_v&lt;/math&gt; and &lt;math&gt;c_v&lt;/math&gt; pointers. Decrement n and k by 1.

'''''DeleteMin(Q):''''' Delete and return the smallest element from queap ''Q''.

: Invoke the ''Minimum(Q)'' operation. The operation returns ''min''. Invoke the ''Delete(Q, min)'' operation. Return ''min''.

'''''CleanUp(Q):''''' Delete all the elements in list ''L'' and tree ''T''.

: Starting from the first element in list ''L'', traverse the list, deleting each node.

: Starting from the root of the tree ''T'', traverse the tree using the [[Tree traversal|post-order traversal]] algorithm, deleting each node in the tree.

==Analysis==

The running time is analyzed using the [[amortized analysis]]. The potential function for queap Q will be &lt;math&gt;\phi(Q)=c|L|&lt;/math&gt; where &lt;math&gt;Q=(T, L)&lt;/math&gt;.

'''''Insert(Q, x):''''' The cost of the operation is ''O(1)''. The size of list ''L'' grows by one, the potential increases by some constant ''c''.

'''''Minimum(Q):''''' The operation does not alter the data structure so the amortized cost is equal to its actual cost, O(1).

'''''Delete(Q, x):''''' There are two cases.

===Case 1===

If ''x'' is in tree ''T'', then the amortized cost is not modified. The delete operation is ''O(1)'' amortized 2-4 tree. Since ''x'' was removed from the tree, &lt;math&gt;h_v&lt;/math&gt; and &lt;math&gt;c_v&lt;/math&gt; pointers may need updating. At most, there will be &lt;math&gt;O(lgq(x))&lt;/math&gt; updates.

===Case 2===

If ''x'' is in list ''L'', then all the elements from ''L'' are inserted in ''T''. This has a cost of &lt;math&gt;a|L|&lt;/math&gt; of some constant ''a'', amortized over the 2-4 tree. After inserting and updating the &lt;math&gt;h_v&lt;/math&gt; and &lt;math&gt;c_v&lt;/math&gt; pointers, the total time spent is bounded by &lt;math&gt;2a|L|&lt;/math&gt;.
The second operation is to delete ''x'' from ''T'', and to walk on the path from x to &lt;math&gt;x_0&lt;/math&gt;, correcting &lt;math&gt;h_v&lt;/math&gt; and &lt;math&gt;c_v&lt;/math&gt; values. The time is spent at most  &lt;math&gt;2a|L| + O(lgq(x))&lt;/math&gt;.  If &lt;math&gt;c &gt; 2a&lt;/math&gt;, then the amortized cost will be &lt;math&gt;O(lgq(x))&lt;/math&gt;.
'''''Delete(Q, x):''''' is the addition of the amortized cost of '''''Minimum(Q)''''' and '''''Delete(Q, x)''''', which is &lt;math&gt;O(lgq(x))&lt;/math&gt;.

==Code example==

A small [[Java language|java]] implementation of a queap:
&lt;pre&gt;
public class Queap
{
        public int n, k;
        public List&lt;Element&gt; l; //Element is a generic data type
        public QueapTree t;    //a 2-4 tree, modified for Queap purpose
        public Element minL;

        private Queap() {
                n = 0;
                k = 0;
                l = new LinkedList&lt;Element&gt;();
                t = new QueapTree();
        }

        public static Queap New() {
                return new Queap();
        }

        public static void Insert(Queap Q, Element x) {
                if (Q.n == 0)
                        Q.minL = x;
                Q.l.add(x);
                x.inList = true;
                if (x.compareTo(Q.minL) &lt; 0)
                        Q.minL = x;
        }

        public static Element Minimum(Queap Q) {
                //t is a 2-4 tree and x0, cv are tree nodes.
                if (Q.minL.compareTo(Q.t.x0.cv.key) &lt; 0)
                        return Q.minL;

                return Q.t.x0.cv.key;
        }

        public static void Delete(Queap Q, QueapNode x) {
                Q.t.deleteLeaf(x);
                --Q.n;
                --Q.k;
        }

        public static void Delete(Queap Q, Element x) {
                QueapNode n;
                if (x.inList) {
                        //set inList of all the elements in the list to false
                        n = Q.t.insertList(Q.l, x);
                        Q.k = Q.n;
                        Delete(Q, n);
                }
                else if ((n = Q.t.x0.cv).key == x)
                        Delete(Q, n);
        }

        public static Element DeleteMin(Queap Q) {
                Element min = Minimum(Q);
                Delete(Q, min);
                return min;
        }
}
&lt;/pre&gt;

==See also==
* [[Queue (data structure)]]
* [[Priority queue]]
* [[Splay tree]]
* [[2-4 tree]]
* [[Doubly linked list]]
* [[Amortized analysis]]

==References==
{{reflist}}

[[Category:Heaps (data structures)]]
[[Category:Algorithmic information theory]]</text>
      <sha1>2zvxkatbqplca012pshi407qkvrrqjg</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Skew heap</title>
    <ns>0</ns>
    <id>3707999</id>
    <revision>
      <id>556383534</id>
      <parentid>556383469</parentid>
      <timestamp>2013-05-23T05:55:51Z</timestamp>
      <contributor>
        <ip>67.80.28.85</ip>
      </contributor>
      <comment>oops, wrong picture</comment>
      <text xml:space="preserve" bytes="4915">A '''skew heap''' (or '''self-adjusting heap''') is a [[heap (data structure)|heap]] [[data structure]] implemented as a [[binary tree]]. Skew heaps are advantageous because of their ability to merge more quickly than binary heaps. In contrast with [[binary heap]]s, there are no structural constraints, so there is no guarantee that the height of the tree is logarithmic. Only two conditions must be satisfied:
* The general heap order must be enforced
* Every operation (add, remove_min, merge) on two skew heaps must be done using a special ''skew heap merge''.

A skew heap is a self-adjusting form of a [[leftist tree|leftist heap]] which attempts to maintain balance by unconditionally swapping all nodes in the merge path when merging two heaps. (The merge operation is also used when adding and removing values.) 

With no structural constraints, it may seem that a skew heap would be horribly inefficient. However, [[amortized analysis|amortized complexity analysis]] can be used to demonstrate that all operations on a skew heap can be done in O(log n).&lt;ref&gt;http://www.cse.yorku.ca/~andy/courses/4101/lecture-notes/LN5.pdf&lt;/ref&gt;

== Definition ==
Skew heaps may be described with the following [[Recursion|recursive]] definition:

*A heap with only one element is a skew heap.
*The result of ''skew merging'' two skew heaps &lt;math&gt;sh_1&lt;/math&gt; and &lt;math&gt;sh_2&lt;/math&gt; is also a skew heap.

== Operations ==
=== Merging two heaps ===
When two skew heaps are to be merged, we can use a similar process as the merge of two [[Leftist tree|leftist heaps]]:

* Compare roots of two heaps; let p be the heap with the smaller root, and q be the other heap. Let r be the name of the resulting new heap.
* Let the root of r be the root of p (the smaller root), and let r's right subtree be p's left subtree.
* Now, compute r's left subtree by recursively merging p's right subtree with q.

Before:
[[Image:SkewHeapMerge1.svg]]

&lt;br /&gt;
after
[[Image:SkewHeapMerge7.svg]]

=== Non-recursive merging ===
Alternatively, there is a non-recursive approach which is more wordy, and does require some sorting at the outset.

*Split each heap into subtrees by cutting every rightmost path. (From the root node, sever the right node and make the right child its own subtree.) This will result in a set of trees in which the root either only has a left child or no children at all.
*Sort the subtrees in ascending order based on the value of the root node of each subtree.
*While there are still multiple subtrees, iteratively recombine the last two (from right to left).
** If the root of the second-to-last subtree has a left child, swap it to be the right child.
** Link the root of the last subtree as the left child of the second-to-last subtree.

[[Image:SkewHeapMerge1.svg]]

[[Image:SkewHeapMerge2.svg]]

[[Image:SkewHeapMerge3.svg]]

[[Image:SkewHeapMerge4.svg]]

[[Image:SkewHeapMerge5.svg]]

[[Image:SkewHeapMerge6.svg]]

[[Image:SkewHeapMerge7.svg]]

=== Adding values ===

Adding a value to a skew heap is like merging a tree with one node together with the original tree.

=== Removing values ===

Removing the first value in a heap can be accomplished by removing the root and merging its child subtrees.

=== Implementation ===

In many functional languages, skew heaps become extremely simple to implement.  Here is a complete sample implementation in Haskell.

&lt;source lang=&quot;haskell&quot;&gt;
data SkewHeap a = Empty
                | Node a (SkewHeap a) (SkewHeap a)

singleton :: Ord a =&gt; a -&gt; SkewHeap a
singleton x = Node x Empty Empty

union :: Ord a =&gt; SkewHeap a -&gt; SkewHeap a -&gt; SkewHeap a
Empty              `union` t2                 = t2
t1                 `union` Empty              = t1
t1@(Node x1 l1 r1) `union` t2@(Node x2 l2 r2)
   | x1 &lt;= x2                                 = Node x1 (t2 `union` r1) l1
   | otherwise                                = Node x2 (t1 `union` r2) l2

insert :: Ord a =&gt; a -&gt; SkewHeap a -&gt; SkewHeap a
insert x heap = singleton x `union` heap

extractMin :: Ord a =&gt; SkewHeap a -&gt; Maybe (a, SkewHeap a)
extractMin Empty        = Nothing
extractMin (Node x l r) = Just (x, l `union` r)
&lt;/source&gt;

== References ==
*{{cite journal|last1=[[Daniel Sleator|Sleator]]|first1=Daniel Dominic|last2=[[Robert Tarjan|Tarjan]]|first2=Robert Endre|year=1986|title=Self-Adjusting Heaps|journal=[[SIAM Journal on Computing]]|volume=15|issue=1|pages=52–69|issn=0097-5397|doi=10.1137/0215004|url=http://www.cs.cmu.edu/~sleator/papers/Adjusting-Heaps.htm}}
* [http://www.cse.yorku.ca/~andy/courses/4101/lecture-notes/LN5.pdf CSE 4101 lecture notes, York University]
{{reflist}}

==External links==
*[http://www.cse.yorku.ca/~aaw/Pourhashemi/ Animations comparing leftist heaps and skew heaps, York University]
*[http://people.cis.ksu.edu/~rhowell/viewer/heapviewer.html Java applet for simulating heaps, Kansas State University]

[[Category:Binary trees]]
[[Category:Heaps (data structures)]]</text>
      <sha1>jprryvtbfe3stavgqpia1yf2mfura98</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Smoothsort</title>
    <ns>0</ns>
    <id>100450</id>
    <revision>
      <id>622973475</id>
      <parentid>611209002</parentid>
      <timestamp>2014-08-27T03:37:30Z</timestamp>
      <contributor>
        <username>EmausBot</username>
        <id>11292982</id>
      </contributor>
      <minor/>
      <comment>Bot: Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:Q1714823]]</comment>
      <text xml:space="preserve" bytes="8124">{{Infobox Algorithm
|class=[[Sorting algorithm]]
|image=[[Image:Smoothsort.gif|none|A run of the smoothsort algorithm sorting an array that is mainly in order but with a few out-of-sequence elements.]]
|data=[[Array data structure|Array]]
|time=&lt;math&gt;\scriptstyle O(n\log n)&lt;/math&gt;
|best-time=&lt;math&gt;\scriptstyle O(n)&lt;/math&gt;
|average-time=&lt;math&gt;\scriptstyle O(n\log n)&lt;/math&gt;
|space=&lt;math&gt;\scriptstyle O(n)&lt;/math&gt; total, &lt;math&gt;\scriptstyle O(1)&lt;/math&gt; auxiliary
|optimal=When the data is already sorted
}}

'''Smoothsort'''&lt;ref&gt;{{Cite EWD|796a|Smoothsort – an alternative to sorting in situ}}&lt;/ref&gt; (method) is a [[comparison sort|comparison-based]] [[sorting algorithm]]. It is a variation of [[heapsort]] developed by [[Edsger Dijkstra]] in 1981. Like heapsort, smoothsort's upper bound is [[Big O notation|O]](''n'' log&amp;nbsp;''n''). The advantage of smoothsort is that it comes closer to O(''n'') time if the [[Adaptive sort|input is already sorted to some degree]], whereas heapsort averages O(''n'' log&amp;nbsp;''n'') regardless of the initial sorted state.

==Overview==
Like [[heapsort]], smoothsort builds up an implicit heap data structure in the array to be sorted, then sorts the array by continuously extracting the maximum element from that heap.  Unlike heapsort, smoothsort does not use a [[binary heap]], but rather a custom heap based on the [[Leonardo numbers]] L(n).  The heap structure consists of a string of heaps, the sizes of which are all Leonardo numbers, and whose roots are stored in ascending order.  The advantage of this custom heap over binary heaps is that if the sequence is already sorted, it takes only &lt;math&gt;\scriptstyle O(n)&lt;/math&gt; time to construct and deconstruct the heap, hence the better runtime.

Breaking the input up into a sequence of heaps is simple – the leftmost nodes of the array are made into the largest heap possible, and the remainder is likewise divided up.  It can be proven &lt;ref&gt;[http://www.keithschwarz.com/smoothsort/ Smoothsort Demystified]. Keithschwarz.com. Retrieved on 2010-11-20.&lt;/ref&gt; that:

* Any array of any length can so be divided up into sections of size L(x).
* No two heaps will have the same size. The string will therefore be a string of heaps strictly descending in size.
* No two heaps will have sizes that are consecutive Leonardo numbers, except for possibly the final two.

Each heap, having a size of L(x), is structured from left to right as a sub-heap of size {{nowrap|L(x − 1)}}, a sub-heap of size {{nowrap|L(x − 2)}}, and a root node, with the exception of heaps with a size of L(1) and L(0), which are singleton nodes. Each heap maintains the heap property that a root node is always at least as large as the root nodes of its child heaps (and therefore at least as large as all nodes in its child heaps), and the string of heaps as a whole maintains the string property that the root node of each heap is at least as large as the root node of the heap to the left.

The consequence of this is that the rightmost node in the string will always be the largest of the nodes, and, importantly, an array that is already sorted needs no rearrangement to be made into a valid series of heaps. This is the source of the adaptive qualities of the algorithm.

The algorithm is simple. We start by dividing up our unsorted array into a single heap of one element, followed by an unsorted portion. A one-element array is trivially a valid sequence of heaps. This sequence is then grown by adding one element at a time to the right, performing swaps to keep the sequence property and the heap property, until it fills the entire original array.

From this point on, the rightmost element of the sequence of heaps will be the largest element in any of the heaps, and will therefore be in its correct, final position. We then reduce the series of heaps back down to a single heap of one element by removing the rightmost node (which stays in place) and performing re-arrangements to restore the heap condition. When we are back down to a single heap of one element, the array is sorted.

==Operations==

Ignoring (for the moment) Dijkstra's optimisations, two operations are necessary – increase the string by adding one element to the right, and decrease the string by removing the right most element (the root of the last heap), preserving the heap and string conditions.

===Grow the string by adding an element to the right===

* If the last two heaps are of size {{nowrap|L(x + 1)}} and L(x) (i.e., consecutive leonardo numbers), the new element becomes the root node of a bigger heap of size L(x+2). This heap will not necessarily have the heap property.
* If the last two heaps of the string are not consecutive Leonardo numbers, then the rightmost element becomes a new heap of size 1. This 1 is taken to be L(1), unless the rightmost heap already has size L(1), in which case the new one-element heap is taken to be of size L(0).

After this, the heap and string properties must be restored, which is usually done via a variant of [[insertion sort]].  This is done as follows:

# The rightmost heap (the one that has just been created) becomes the &quot;current&quot; heap
# While there is a heap to the left of the current heap and its root is larger than the current root ''and'' both of its child heap roots
#* Then swap the new root with the root on the heap to the left (this will not disturb the heap property of the current heap). That heap then becomes the current heap.
# Perform a &quot;filter&quot; operation on the current heap to establish the heap property:
#*While the current heap has a size greater than 1 and either child heap of the current heap has a root node greater than the root of the current heap
#** Swap the greater child root with the current root. That child heap becomes the current heap.

The filter operation is greatly simplified by the use of Leonardo numbers, as a heap will always either be a single node, or will have two children. One does not need to manage the condition of one of the child heaps not being present.

====Optimisation====

* If the new heap is going to become part of a larger heap by the time we are done, then don't bother establishing the string property: it only needs to be done when a heap has reached its final size.
** To do this, look at how many elements are left after the new heap of size L(x). If there are more than {{nowrap|L(x − 1) + 1}}, then this new heap is going to be merged.

* Do not maintain the heap property of the rightmost heap. If that heap becomes one of the final heaps of the string, then maintaining the string property will restore the heap property. Of course, whenever a new heap is created, then the rightmost heap is no longer the rightmost and the heap property needs to be restored.

===Shrink the string by removing the rightmost element===

If the rightmost heap has a size of 1 (i.e., L(1) or L(0)), then nothing needs to be done. Simply remove that rightmost heap.

If the rightmost heap does not have a size of 1, then remove the root, exposing the two sub-heaps as members of the string.  Restore the string property first on the left one and then on the right one.

====Optimisation====

* When restoring the string property, we do not need to compare the root of the heap to the left with the two child nodes of the heaps that have just been exposed, because we know that these newly exposed heaps have the heap property. Just compare it to the root.

==Memory usage==

The smoothsort algorithm needs to be able to hold in memory the sizes of all of the heaps in the string.  Since all these values are distinct, this is usually done using a [[bit vector]].  Moreover, since there are at most O(log n) numbers in the sequence, these bits can be encoded in O(1) machine words, assuming a [[transdichotomous model|transdichotomous machine model]].

==Notes==
{{Reflist}}
* [http://www.enterag.ch/hartwig/order/smoothsort.pdf Commented transcription of EWD796a]

{{sorting}}

[[Category:Sorting algorithms]]
[[Category:Comparison sorts]]
[[Category:Heaps (data structures)]]
[[Category:Articles with example Java code]]
[[Category:Dutch inventions]]</text>
      <sha1>kg9tdocnkv4y9qypq94v4ga9r600utj</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Soft heap</title>
    <ns>0</ns>
    <id>546678</id>
    <revision>
      <id>608189114</id>
      <parentid>602320241</parentid>
      <timestamp>2014-05-12T07:46:46Z</timestamp>
      <contributor>
        <ip>194.199.27.192</ip>
      </contributor>
      <text xml:space="preserve" bytes="6294">{{for|the [[Canterbury scene]] band|Soft Heap}}

In [[computer science]], a '''soft heap''' is a variant on the simple [[heap (data structure)|heap]] [[data structure]] that has constant [[amortized analysis|amortized]] time for 5 types of operations. This is achieved by carefully &quot;corrupting&quot; (increasing) the keys of at most a certain number of values in the heap. The constant time operations are:
* '''create'''(''S''): Create a new soft heap
* '''insert'''(''S'', ''x''): Insert an element into a soft heap
* '''meld'''(''S'', '' S' ''): Combine the contents of two soft heaps into one, destroying both
* '''delete'''(''S'', ''x''): Delete an element from a soft heap
* '''findmin'''(''S''): Get the element with minimum key in the soft heap

Other heaps such as [[Fibonacci heap]]s achieve most of these bounds without any corruption, but cannot provide a constant-time bound on the critical ''delete'' operation. The amount of corruption can be controlled by the choice of a parameter ε, but the lower this is set, the more time insertions require ([[Big-O notation|O]](log 1/ε) for an error rate of ε).

More precisely, the guarantee offered by the soft heap is the following: for a fixed value ''ε'' between 0 and 1/2, at any point in time there will be at most ''ε*n'' corrupted keys in the heap, where ''n'' is the number of elements inserted so far. Note that this does not guarantee that only a fixed percentage of the keys ''currently'' in the heap are corrupted: in an unlucky sequence of insertions and deletions, it can happen that all elements in the heap will have corrupted keys. Similarly, we have no guarantee that in a sequence of elements extracted from the heap with ''findmin'' and ''delete'', only a fixed percentage will have corrupted keys: in an unlucky scenario only corrupted elements are extracted from the heap.

The soft heap was designed by [[Bernard Chazelle]] in 2000. The term &quot;corruption&quot; in the structure is the result of what Chazelle called &quot;carpooling&quot; in a soft heap. Each node in the soft heap contains a linked-list of keys and one common key. The common key is an upper bound on the values of the keys in the linked-list. Once a key is added to the linked-list, it is considered corrupted because its value is never again relevant in any of the soft heap operations: only the common keys are compared. This is what makes soft heaps &quot;soft&quot;; you can't be sure whether or not any particular value you put into it will be corrupted. The purpose of these corruptions is effectively to lower the [[information entropy]] of the data, enabling the data structure to break through [[information theory|information-theoretic]] barriers regarding heaps.


== Applications ==

Surprisingly, despite its limitations and its unpredictable nature, soft heaps are useful in the design of deterministic algorithms. They were used to achieve the best complexity to date for finding a [[minimum spanning tree]]. They can also be used to easily build an optimal [[selection algorithm]], as well as ''near-sorting'' algorithms, which are algorithms that place every element near its final position, a situation in which [[insertion sort]] is fast.

One of the simplest examples is the selection algorithm. Say we want to find the ''k''th largest of a group of ''n'' numbers. First, we choose an error rate of 1/3; that is, at most about 33% of the keys we insert will be corrupted. Now, we insert all ''n'' elements into the heap &amp;mdash; we call the original values the &quot;correct&quot; keys, and the values stored in the heap the &quot;stored&quot; keys. At this point, at most ''n''/3 keys are corrupted, that is, for at most ''n''/3 keys is the &quot;stored&quot; key larger than the &quot;correct&quot; key, for all the others the stored key equals the correct key.

Next, we delete the minimum element from the heap ''n''/3 times (this is done according to the &quot;stored&quot; key). As the total number of insertions we have made so far is still n, there are still at most ''n''/3 corrupted keys in the heap. Accordingly, at least 2''n''/3 &amp;minus; ''n''/3 = ''n''/3 of the keys remaining in the heap are not corrupted. 

Let ''L'' be the element with the largest correct key among the elements we removed. The stored key of ''L'' is possibly larger than its correct key (if ''L'' was corrupted), and even this larger value is smaller than all the stored keys of the remaining elements in the heap (as we were removing minimums). Therefore, the correct key of ''L'' is smaller than the remaining ''n''/3 uncorrupted elements in the soft heap. Thus, ''L'' divides the elements somewhere between 33%/66% and 66%/33%. We then partition the set about ''L'' using the ''partition'' algorithm from [[quicksort]] and apply the same algorithm again to either the set of numbers less than ''L'' or the set of numbers greater than ''L'', neither of which can exceed 2''n''/3 elements. Since each insertion and deletion requires O(1) amortized time, the total deterministic time is T(''n'') =  T(2''n''/3) + O(''n''). Using [[Master_theorem#Case_3|case 3]] of the [[master theorem]] (with ε=1 and c=2/3), we know that T(''n'') = Θ(''n'').

The final algorithm looks like this:

  '''function''' softHeapSelect(a[1..n], k)
      '''if''' k = 1 '''then return''' minimum(a[1..n])
      create(S)
      '''for''' i '''from''' 1 '''to''' n
          insert(S, a[i])
      '''for''' i '''from''' 1 '''to''' n/3
          x := findmin(S)
          delete(S, x)
      xIndex := partition(a, x)  ''// Returns new index of pivot x''
      '''if''' k &lt; xIndex
          softHeapSelect(a[1..xIndex-1], k)
      '''else'''
          softHeapSelect(a[xIndex..n], k-xIndex+1)

==References==
* Chazelle, B. 2000. [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.9705 The soft heap: an approximate priority queue with optimal error rate.] ''J. ACM'' 47, 6 (Nov. 2000), 1012-1027.
* Kaplan, H. and Zwick, U. 2009. [http://epubs.siam.org/doi/pdf/10.1137/1.9781611973068.53 A simpler implementation and analysis of Chazelle's soft heaps.] In ''Proceedings of the Nineteenth Annual ACM -SIAM Symposium on Discrete Algorithms'' (New York, New York, January 4––6, 2009). Symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics, Philadelphia, PA, 477-485.

[[Category:Heaps (data structures)]]</text>
      <sha1>4h4kjrsncge5v2pzhej3ek3yemt2gto</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Treap</title>
    <ns>0</ns>
    <id>249855</id>
    <revision>
      <id>621643495</id>
      <parentid>596093314</parentid>
      <timestamp>2014-08-17T16:21:50Z</timestamp>
      <contributor>
        <username>Chmarkine</username>
        <id>15398482</id>
      </contributor>
      <minor/>
      <comment>/* External links */change to https if the server sends [[HTTP Strict Transport Security|HSTS]] header, replaced: http://github.com → https://github.com (2) using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="13362">{{Infobox data structure
|name=Treap
|type=Randomized [[Binary Search Tree]]
|space_avg=O(n)
|space_worst=O(n)
|search_avg=O(log n)
|search_worst=amortized O(log n)
|insert_avg=O(log n)
|insert_worst=amortized O(log n)
|delete_avg=O(log n)
|delete_worst=amortized O(log n)
}}
{{Probabilistic}}
In [[computer science]], the '''treap''' and the '''randomized binary search tree''' are two closely related forms of [[binary search tree]] [[data structure]]s that maintain a dynamic set of ordered keys and allow [[binary search]]es among the keys. After any sequence of insertions and deletions of keys, the shape of the tree is a [[random variable]] with the same probability distribution as a [[random binary tree]]; in particular, with high probability its height is proportional to the [[logarithm]] of the number of keys, so that each search, insertion, or deletion operation takes logarithmic time to perform.

== Description ==
[[Image:TreapAlphaKey.svg|thumb|left|A treap with alphabetic key and numeric max heap order]]
The treap was first described by [[Cecilia R. Aragon]] and [[Raimund Seidel]] in 1989;&lt;ref name=&quot;paper89&quot;&gt;{{Citation | contribution=Randomized Search Trees |
first1=Cecilia R. | last1=Aragon | first2=Raimund | last2=Seidel |
url=http://faculty.washington.edu/aragon/pubs/rst89.pdf |
title=[[Symposium on Foundations of Computer Science|Proc. 30th Symp. Foundations of Computer Science (FOCS 1989)]] | pages=540–545 | year=1989 |
doi=10.1109/SFCS.1989.63531 | isbn=0-8186-1982-1 | publisher=IEEE Computer Society Press | location=Washington, D.C.}}
&lt;/ref&gt;&lt;ref name=&quot;paper96&quot;&gt;
{{Citation | title=Randomized Search Trees |
first1=Raimund | last1=Seidel | first2=Cecilia R. | last2=Aragon |
journal=Algorithmica | volume=16 | issue=4/5 | pages=464–497 | year=1996 |
url=http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.30.8602 |
doi=10.1007/s004539900061}}&lt;/ref&gt; its name is a [[portmanteau word|portmanteau]] of [[Tree data structure|tree]] and [[heap (data structure)|heap]].
It is a [[Cartesian tree]]&lt;ref&gt;{{citation
 | last = Vuillemin | first = Jean
 | doi = 10.1145/358841.358852
 | issue = 4
 | journal = Commun. ACM
 | location = New York, NY, USA
 | pages = 229–239
 | publisher = ACM
 | title = A unifying look at data structures
 | volume = 23
 | year = 1980}}.&lt;/ref&gt; in which each key is given a (randomly chosen) numeric priority. As with any binary search tree, the [[inorder traversal]] order of the nodes is the same as the sorted order of the keys. The structure of the tree is determined by the requirement that it be heap-ordered: that is, the priority number for any non-leaf node must be greater than or equal to the priority of its children. Thus, as with Cartesian trees more generally, the root node is the maximum-priority node, and its left and right subtrees are formed in the same manner from the subsequences of the sorted order to the left and right of that node.

An equivalent way of describing the treap is that it could be formed by inserting the nodes highest-priority-first into a binary search tree without doing any rebalancing. Therefore, if the priorities are independent random numbers (from a distribution over a large enough space of possible priorities to ensure that two nodes are very unlikely to have the same priority) then the shape of a treap has the same probability distribution as the shape of a [[random binary search tree]], a search tree formed by inserting the nodes without rebalancing in a randomly chosen insertion order.  Because random binary search trees are known to have logarithmic height with high probability, the same is true for treaps.

Aragon and Seidel also suggest assigning higher priorities to frequently accessed nodes, for instance by a process that, on each access, chooses a random number and replaces the priority of the node with that number if it is higher than the previous priority. This modification would cause the tree to lose its random shape; instead, frequently accessed nodes would be more likely to be near the root of the tree, causing searches for them to be faster.

Blelloch and Reid-Miller&lt;ref&gt;{{citation
 | last1 = Blelloch | first1 = Guy E., 
 | last2 = Reid-Miller | first2 = Margaret, 
 | contribution = Fast set operations using treaps
 | doi = 10.1145/277651.277660
 | isbn = 0-89791-989-0
 | location = New York, NY, USA
 | pages = 16–26
 | publisher = ACM
 | title = [[Symposium on Parallel Algorithms and Architectures|Proc. 10th ACM Symp. Parallel Algorithms and Architectures (SPAA 1998)]]
 | year = 1998}}.&lt;/ref&gt; describe an application of treaps to a problem of maintaining [[set (computer science)|set]]s of items and performing [[set union]], [[set intersection]], and [[set difference]] operations, using a treap to represent each set. Naor and Nissim&lt;ref&gt;{{citation
 | last1 = Naor | first1 = M. | author1-link = Moni Naor
 | last2 = Nissim | first2 = K.
 | date = April 2000
 | doi = 10.1109/49.839932
 | issue = 4
 | journal = IEEE Journal on Selected Areas in Communications
 | pages = 561–570
 | title = Certificate revocation and certificate update
 | url = http://eprints.kfupm.edu.sa/29443/1/29443.pdf
 | volume = 18}}.&lt;/ref&gt; describe another application, for maintaining [[Public key certificate|authorization certificates]] in [[public-key cryptography|public-key cryptosystems]].

== Operations ==
Specifically, the treap supports the following operations:
*To search for a given key value, apply a standard [[binary search algorithm]] in a binary search tree, ignoring the priorities.
*To insert a new key ''x'' into the treap, generate a random priority ''y'' for ''x''. Binary search for ''x'' in the tree, and create a new node at the leaf position where the binary search determines a node for ''x'' should exist. Then, as long as ''x'' is not the root of the tree and has a larger priority number than its parent ''z'', perform a [[tree rotation]] that reverses the parent-child relation between ''x'' and ''z''.
*To delete a node ''x'' from the treap, if ''x'' is a leaf of the tree, simply remove it. If ''x'' has a single child ''z'', remove ''x'' from the tree and make ''z'' be the child of the parent of ''x'' (or make ''z'' the root of the tree if ''x'' had no parent). Finally, if ''x'' has two children, swap its position in the tree with the position of its immediate successor ''z'' in the sorted order, resulting in one of the previous cases. In this final case, the swap may violate the heap-ordering property for ''z'', so additional rotations may need to be performed to restore this property.
*To split a treap into two smaller treaps, those smaller than key ''x'', and those larger than key ''x'', insert ''x'' into the treap with maximum priority—larger than the priority of any node in the treap. After this insertion, ''x'' will be the root node of the treap, all values less than ''x'' will be found in the left subtreap, and all values greater than ''x'' will be found in the right subtreap. This costs as much as a single insertion into the treap.
*Merging two treaps that are the product of a former split, one can safely assume that the greatest value in the first treap is less than the smallest value in the second treap. Insert a value ''x'', such that ''x'' is larger than this max-value in the first treap, and smaller than the min-value in the second treap, and assign it the minimum priority. After insertion it will be a leaf node, and can easily be deleted. The result is one treap merged from the two original treaps. This is effectively &quot;undoing&quot; a split, and costs the same.

==Randomized binary search tree==
The randomized binary search tree, introduced by Martínez and Roura subsequently to the work of Aragon and Seidel on treaps,&lt;ref&gt;{{Citation | title=Randomized binary search trees | journal=Journal of the ACM | volume=45 | issue=2 | year=1997 | first1=Conrado | last1=Martínez | first2=Salvador | last2=Roura | pages=288–323 | url=http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.243 | doi=10.1145/274787.274812}}&lt;/ref&gt; stores the same nodes with the same random distribution of tree shape, but maintains different information within the nodes of the tree in order to maintain its randomized structure.

Rather than storing random priorities on each node, the randomized binary search tree stores a small integer at each node, the number of its descendants (counting itself as one); these numbers may be maintained during tree rotation operations at only a constant additional amount of time per rotation. When a key ''x'' is to be inserted into a tree that already has ''n'' nodes, the insertion algorithm chooses with probability 1/(''n''&amp;nbsp;+&amp;nbsp;1) to place ''x'' as the new root of the tree, and otherwise it calls the insertion procedure recursively to insert ''x'' within the left or right subtree (depending on whether its key is less than or greater than the root). The numbers of descendants are used by the algorithm to calculate the necessary probabilities for the random choices at each step. Placing ''x'' at the root of a subtree may be performed either as in the treap by inserting it at a leaf and then rotating it upwards, or by an alternative algorithm described by Martínez and Roura that splits the subtree into two pieces to be used as the left and right children of the new node.

The deletion procedure for a randomized binary search tree uses the same information per node as the insertion procedure, and like the insertion procedure it makes a sequence of O(log&amp;nbsp;''n'') random decisions in order to join the two subtrees descending from the left and right children of the deleted node into a single tree. If the left or right subtree of the node to be deleted is empty, the join operation is trivial; otherwise, the left or right child of the deleted node is selected as the new subtree root with probability proportional to its number of descendants, and the join proceeds recursively.

==Comparison==
The information stored per node in the randomized binary tree is simpler than in a treap (a small integer rather than a high-precision random number), but it makes a greater number of calls to the random number generator (O(log&amp;nbsp;''n'') calls per insertion or deletion rather than one call per insertion) and the insertion procedure is slightly more complicated due to the need to update the numbers of descendants per node. A minor technical difference is that, in a treap, there is a small probability of a collision (two keys getting the same priority), and in both cases there will be statistical differences between a true random number generator and the [[PRNG|pseudo-random number generator]] typically used on digital computers. However, in any case the differences between the theoretical model of perfect random choices used to design the algorithm and the capabilities of actual random number generators are vanishingly small.

Although the treap and the randomized binary search tree both have the same random distribution of tree shapes after each update, the history of modifications to the trees performed by these two data structures over a sequence of insertion and deletion operations may be different. For instance, in a treap, if the three numbers 1, 2, and 3 are inserted in the order 1, 3, 2, and then the number 2 is deleted, the remaining two nodes will have the same parent-child relationship that they did prior to the insertion of the middle number. In a randomized binary search tree, the tree after the deletion is equally likely to be either of the two possible trees on its two nodes, independently of what the tree looked like prior to the insertion of the middle number.

==References==
{{reflist}}

==External links==
*[http://faculty.washington.edu/aragon/treaps.html Collection of treap references and info] by Cecilia Aragon
* [http://opendatastructures.org/versions/edition-0.1e/ods-java/7_2_Treap_Randomized_Binary.html Open Data Structures - Section 7.2 - Treap: A Randomized Binary Search Tree]
*[http://people.ksp.sk/~kuko/bak/index.html Treap Applet] by Kubo Kovac
*[http://www.ibr.cs.tu-bs.de/lehre/ss98/audii/applets/BST/Treap-Example.html Animated treap]
*[http://www.cs.uiuc.edu/class/sp09/cs473/notes/08-treaps.pdf Randomized binary search trees]. Lecture notes from a course by Jeff Erickson at UIUC. Despite the title, this is primarily about treaps and [[skip list]]s; randomized binary search trees are mentioned only briefly.
*[http://code.google.com/p/treapdb/ A high performance key-value store based on treap] by Junyi Sun
*[http://www.fernando-rodriguez.com/a-high-performance-alternative-to-dictionary VB6 implementation of treaps]. Visual basic 6 implementation  of treaps as a COM object.
*[http://code.google.com/p/as3-commons/source/browse/trunk/as3-commons-collections/src/main/actionscript/org/as3commons/collections/Treap.as ActionScript3 implementation of a treap]
*[https://pypi.python.org/pypi/treap/ Pure Python and Cython in-memory treap and duptreap]
*[http://www.codeproject.com/Articles/8184/Treaps-in-C Treaps in C#]. By Roy Clemmons
*[https://github.com/steveyen/gtreap Pure Go in-memory, immutable treaps]
*[https://github.com/steveyen/gkvlite Pure Go persistent treap key-value storage library]

{{CS-Trees}}

[[Category:Heaps (data structures)]]
[[Category:Binary trees]]
[[Category:Probabilistic data structures]]</text>
      <sha1>3l3c1bdp5mz3ai31sccsngin0itrnqx</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Kinetic heap</title>
    <ns>0</ns>
    <id>35772150</id>
    <revision>
      <id>595783428</id>
      <parentid>503868336</parentid>
      <timestamp>2014-02-16T21:38:12Z</timestamp>
      <contributor>
        <username>Edemaine</username>
        <id>1773</id>
      </contributor>
      <comment>/* References */ pdf link for Tarjan paper</comment>
      <text xml:space="preserve" bytes="9583">[[File:Kinetic heap overview.png|right]]
A '''Kinetic Heap''' is a [[kinetic data structure]], obtained by the [[Kinetic data structure#Certificates Approach|kinetization]] of a [[heap (data structure)|heap]]. It is designed to store elements (keys associated with priorities) where the priority is changing as a continuous function of time. As a type of [[Kinetic Priority Queue|kinetic priority queue]], it maintains the maximum priority element stored in it. 
The kinetic heap data structure works by storing the elements as a tree that satisfies the following heap property - if {{math|&lt;var&gt;B&lt;/var&gt;}} is a [[child node]] of {{math|&lt;var&gt;A&lt;/var&gt;}}, then the priority of the element in {{math|&lt;var&gt;A&lt;/var&gt;}} must be higher than the priority of the element in {{math|&lt;var&gt;B&lt;/var&gt;}}. This heap property is enforced using [[kinetic data structure#Certificates Approach|certificates]] along every edge so, like other kinetic data structures, a kinetic heap also contains a priority queue (the event queue) to maintain certificate failure times.

== Implementation and operations ==
A regular heap can be kinetized by augmenting with a certificate [{{math|&lt;var&gt;A&lt;/var&gt;&gt;&lt;var&gt;B&lt;/var&gt;}}] for every pair of nodes{{math|&lt;var&gt;A&lt;/var&gt;}}, {{math|&lt;var&gt;B&lt;/var&gt;}} such that {{math|&lt;var&gt;B&lt;/var&gt;}} is a child node of {{math|&lt;var&gt;A&lt;/var&gt;}}. If the value stored at a node {{math|&lt;var&gt;X&lt;/var&gt;}} is a function {{math|f&lt;sub&gt;&lt;var&gt;X&lt;/var&gt;&lt;/sub&gt;(&lt;var&gt;t&lt;/var&gt;)}} of time, then this certificate is only valid while {{math|f&lt;sub&gt;&lt;var&gt;A&lt;/var&gt;&lt;/sub&gt;(&lt;var&gt;t&lt;/var&gt;) &gt; f&lt;sub&gt;&lt;var&gt;B&lt;/var&gt;&lt;/sub&gt;(&lt;var&gt;t&lt;/var&gt;)}}. Thus, the failure of this certificate must be scheduled in the event queue at a time {{math|&lt;var&gt;t&lt;/var&gt;}} such that {{math|f&lt;sub&gt;&lt;var&gt;A&lt;/var&gt;&lt;/sub&gt;(&lt;var&gt;t&lt;/var&gt;) &gt; f&lt;sub&gt;&lt;var&gt;B&lt;/var&gt;&lt;/sub&gt;(&lt;var&gt;t&lt;/var&gt;)}}.

All certificate failures are scheduled on the &quot;event queue&quot;, which is assumed to be an efficient priority queue whose operations take {{math|O(log &lt;var&gt;n&lt;/var&gt;)}} time.

=== Dealing with certificate failures ===
[[File:Kinetic heap swap.png|right]]
When a certificate [{{math|&lt;var&gt;A&lt;/var&gt;&gt;&lt;var&gt;B&lt;/var&gt;}}] fails, the data structure must swap {{math|&lt;var&gt;A&lt;/var&gt;}} and {{math|&lt;var&gt;B&lt;/var&gt;}} in the heap, and update the certificates that each of them was present in.

For example, if &lt;math&gt;B&lt;/math&gt; (call its [[child nodes]] &lt;math&gt;Y,Z&lt;/math&gt;) was a child node of &lt;math&gt;A&lt;/math&gt; (call its child nodes&lt;math&gt;B,C&lt;/math&gt; and its [[parent node]] &lt;math&gt;X&lt;/math&gt;), and the certificate [{{math|&lt;var&gt;A&lt;/var&gt;&gt;&lt;var&gt;B&lt;/var&gt;}}] fails, then the data structure must swap&lt;math&gt;B&lt;/math&gt; and &lt;math&gt;A&lt;/math&gt;, then replace the old certificates (and the corresponding scheduled events) [{{math|&lt;var&gt;A&lt;/var&gt;&gt;&lt;var&gt;B&lt;/var&gt;}}], [{{math|&lt;var&gt;A&lt;/var&gt;&lt;&lt;var&gt;X&lt;/var&gt;}}], [{{math|&lt;var&gt;A&lt;/var&gt;&gt;&lt;var&gt;C&lt;/var&gt;}}], [{{math|&lt;var&gt;B&lt;/var&gt;&gt;&lt;var&gt;Y&lt;/var&gt;}}], [{{math|&lt;var&gt;B&lt;/var&gt;&gt;&lt;var&gt;Z&lt;/var&gt;}}] with new certificates [{{math|&lt;var&gt;B&lt;/var&gt;&gt;&lt;var&gt;A&lt;/var&gt;}}], [{{math|&lt;var&gt;B&lt;/var&gt;&lt;&lt;var&gt;X&lt;/var&gt;}}], [{{math|&lt;var&gt;B&lt;/var&gt;&gt;&lt;var&gt;C&lt;/var&gt;}}], [{{math|&lt;var&gt;A&lt;/var&gt;&gt;&lt;var&gt;Y&lt;/var&gt;}}] and [{{math|&lt;var&gt;A&lt;/var&gt;&gt;&lt;var&gt;Z&lt;/var&gt;}}].

Thus, assuming [[Degeneracy (mathematics)|non-degeneracy]] of the events (no two events happen at the same time), only a constant number of events need to be de-scheduled and re-scheduled even in the worst case.

=== Operations ===
A kinetic heap supports the following operations:
* {{math|'''create-heap'''(&lt;var&gt;h&lt;/var&gt;)}}: create an empty kinetic heap {{math|&lt;var&gt;h&lt;/var&gt;}}
* {{math|'''find-max'''(&lt;var&gt;h, t&lt;/var&gt;)}} (or '''find-min'''): - return the {{math|max}} (or {{math|min}} for a {{math|min-heap}}) value stored in the heap  {{math|&lt;var&gt;h&lt;/var&gt;}} at the current virtual time {{math|&lt;var&gt;t&lt;/var&gt;}}.
* {{math|'''insert'''(&lt;var&gt;X&lt;/var&gt;, f&lt;sub&gt;&lt;var&gt;X&lt;/var&gt;&lt;/sub&gt;, &lt;var&gt;t&lt;var&gt;)}}: - insert a key {{math|&lt;var&gt;X&lt;/var&gt;}} into the kinetic heap at the current virtual time {{math|&lt;var&gt;t&lt;/var&gt;}}, whose value changes as a continuous function {{math|f&lt;sub&gt;&lt;var&gt;X&lt;/sub&gt;&lt;/var&gt;(&lt;var&gt;t&lt;/var&gt;)}} of time {{math|&lt;var&gt;t&lt;/var&gt;}}. The insertion is done as in a normal heap in {{math|O(log &lt;var&gt;n&lt;/var&gt;)}} time, but {{math|O(log &lt;var&gt;n&lt;/var&gt;)}} certificates might need to be changed as a result, so the total time for rescheduling certificate failures is  {{math|O(log &lt;sup&gt;2&lt;/sup&gt; &lt;var&gt;n&lt;/var&gt;)}}
* {{math|'''delete'''(&lt;var&gt;X&lt;/var&gt;, &lt;var&gt;t&lt;/var&gt;)}} - delete a key {{math||&lt;var&gt;X&lt;/var&gt;}} at the current virtual time {{math|&lt;var&gt;t&lt;/var&gt;}}. The deletion is done as in a normal heap in {{math|O(log &lt;var&gt;n&lt;/var&gt;)}} time, but {{math|O(log &lt;var&gt;n&lt;/var&gt;)}} certificates might need to be changed as a result, so the total time for rescheduling certificate failures is  {{math|O(log &lt;sup&gt;2&lt;/sup&gt; &lt;var&gt;n&lt;/var&gt;)}}.

== Performance ==
Kinetic heaps perform well according to the four metrics ([[kinetic data structure#Performance|responsiveness]], [[kinetic data structure#Performance|locality]], [[kinetic data structure#Performance|compactness]] and [[kinetic data structure#Performance|efficiency]]) of kinetic data structure quality defined by Basch et al.&lt;ref name=&quot;mobile&quot;/&gt; The analysis of the first three qualities is straightforward:
* '''[[kinetic data structure#Performance|Responsiveness]]:'''A kinetic heap is responsive, since each certificate failure causes the concerned keys to be swapped and leads to only five certificates being replaced in the worst case.
* '''[[kinetic data structure#Performance|Locality]]:''' Each node is present in one certificate each along with its parent node and two child nodes (if present), meaning that each node can be involved in a total of three scheduled events in the worst case, thus kinetic heaps are local.
* '''[[kinetic data structure#Performance|Compactness]]:''' Each edge in the heap corresponds to exactly one scheduled event, therefore the number of scheduled events is exactly {{math||&lt;var&gt;n&lt;/var&gt;-1}} where {{math|&lt;var&gt;n&lt;/var&gt;}} is the number of nodes in the kinetic heap. Thus, kinetic heaps are compact.

=== Analysis of efficiency ===
The efficiency of a kinetic heap in the general case is largely unknown.&lt;ref name=&quot;mobile&quot;/&gt;&lt;ref name=&quot;heap analysis&quot;/&gt;&lt;ref name=&quot;hanger&quot;/&gt;However, in the special case of [[Kinetic data structure#Types of Trajectories|affine motion]] {{math|f(&lt;var&gt;t&lt;/var&gt;) {{=}}  a&lt;var&gt;t&lt;/var&gt; + b}} of the priorities, kinetic heaps are known to be very efficient.&lt;ref name=&quot;heap analysis&quot;/&gt;

==== Affine motion, no insertions or deletions ====
In this special case, the maximum number of events processed by a kinetic heap can be shown to be exactly the number of edges in the [[transitive closure]] of the tree structure of the heap, which is {{math|O(&lt;var&gt;n&lt;/var&gt;log&lt;var&gt;n&lt;/var&gt;)}} for a tree of height {{math|O(log&lt;var&gt;n&lt;/var&gt;)}} &lt;ref name=&quot;heap analysis&quot;/&gt;

==== Affine motion, with insertions and deletions ====
If {{math|&lt;var&gt;n&lt;/var&gt;}} insertions and deletions are made on a kinetic heap that starts empty, the maximum number of events processed is &lt;math&gt;O(n \sqrt{n\log n})&lt;/math&gt;.&lt;ref name=&quot;sweep&quot;/&gt; However, this bound is not believed to be tight,&lt;ref name=&quot;heap analysis&quot;/&gt; and the only known lower bound is &lt;math&gt;\Omega(n\log n)&lt;/math&gt;.&lt;ref name=&quot;sweep&quot;/&gt;

== Variants ==
This article deals with &quot;simple&quot; kinetic heaps as described above, but other variants have been developed for specialized applications,&lt;ref name=&quot;tarjan&quot;/&gt; such as: 
* [[Fibonacci kinetic heap]]
* [[Incremental kinetic heap]]

Other heap-like kinetic priority queues are:
* [[Kinetic heater]]
* [[Kinetic hanger]]

== References ==
{{Reflist| refs=
&lt;ref name=&quot;heap analysis&quot;&gt;{{cite web | url=http://www.uniriotec.br/~fonseca/kh.pdf | title=Kinetic heap-ordered trees: Tight analysis and improved algorithms | publisher=Information Processing Letters | accessdate=May 17, 2012 | author=da Fonseca, Guilherme D. and de Figueiredo, Celina M. H. | pages=165–169}}&lt;/ref&gt;
&lt;ref name=&quot;hanger&quot;&gt;
{{cite web | url=http://www.uniriotec.br/~fonseca/hanger.pdf | title=Kinetic hanger |publisher=Information Processing Letters | accessdate=May 17, 2012 | author=da Fonseca, Guilherme D. and de Figueiredo, Celina M. H. and Carvalho, Paulo C. P. | pages=151–157}}
&lt;/ref&gt;
&lt;ref name=&quot;mobile&quot;&gt;
{{cite conference | url=http://dl.acm.org/citation.cfm?id=314435 | title=Data structures for mobile data | publisher=Society for Industrial and Applied Mathematics | accessdate=May 17, 2012 | author=Basch, J., Guibas, L. J., Hershberger, J | booktitle=Proceedings of the eighth annual ACM-SIAM symposium on Discrete algorithms | year=1997 | conference=SODA | pages=747 -756}}
&lt;/ref&gt;
&lt;ref name=&quot;sweep&quot;&gt;
{{cite conference | url=http://dl.acm.org/citation.cfm?id=263089 | title=Sweeping lines and line segments with a heap | publisher=ACM | accessdate=May 17, 2012 | author=Basch, J, Guibas, L. J., Ramkumar, G. D. | booktitle=Proceedings of the thirteenth annual symposium on Computational geometry | year=1997 | conference=SCG | pages=469-471}}
&lt;/ref&gt;
&lt;ref name=&quot;tarjan&quot;&gt;

{{cite conference| url=http://wwwnew.cs.princeton.edu/courses/archive/fall03/cs528/handouts/faster%20kinetic%20heaps.pdf | title = Faster kinetic heaps and their use
in broadcast scheduling | publisher=ACM | accessdate=May 17, 2012| author = K. H., Tarjan, R. and T. K. | booktitle=Proc. 12th ACM-SIAM Symposium on Discrete Algorithms |pages=836–844| year=2001}}
&lt;/ref&gt;

}}
{{cite web |url=http://graphics.stanford.edu/projects/lgl/papers/g-KDS_DS-Handbook-04/g-KDS_DS-Handbook-04.pdf| title=Kinetic Data Structures - Handbook |accessdate=May 17, 2012 | author=Guibas, Leonidas}}
&lt;!--- Categories ---&gt;

[[Category:Articles created via the Article Wizard]]
[[Category:Kinetic data structures]]
[[Category:Heaps (data structures)]]</text>
      <sha1>qm215vvi1tuy6e7kkougl1wxk4shikk</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Kinetic hanger</title>
    <ns>0</ns>
    <id>35846433</id>
    <revision>
      <id>503868217</id>
      <parentid>498830414</parentid>
      <timestamp>2012-07-24T01:26:47Z</timestamp>
      <contributor>
        <username>Dthomsen8</username>
        <id>6277327</id>
      </contributor>
      <comment>Reviewed by Dthomsen8</comment>
      <text xml:space="preserve" bytes="4320">A '''Kinetic hanger''' is a randomized version of a [[kinetic heap]] whose performance is easy to analyze tightly. A kinetic hanger satisfies the heap property (the priority of each element is higher than the priority of its children) but relaxes the requirement that the tree structure must be strictly balanced, thus insertions and deletions can be randomized. As a result, the structure of the kinetic hanger has the property that it is drawn uniformly at random from the space of all possible heap-like structures on its elements.

== Implementation ==
The kinetic hanger structure (including [[Kinetic data structure#Certificates Approach|certificates]] and event queue) is exactly the same as the kinetic heap structure, but without the balancing requirement. Thus, it consists of an [[efficient]] priority queue (the event queue) to maintain the certificate failure times, as well as a main (not necessarily balanced) [[heap (data structure)|heap]]-like [[tree (data structure)|tree]] structure in which the elements are stored. There is a certificate associated with each edge that enforces the heap property (priority of parent &gt; priority of child) along that edge.

The characteristic operation in a kinetic hanger is &quot;''hanging''&quot;, which is defined as follows (a distinction is made between a node in the tree structure and the element stored in it).
''Hang(Node n, Element e)''
# If there is no element at ''n'', put ''e'' in ''n'' and return
# If the element ''x'' in ''n'' has a higher priority than ''e'', choose a child ''c'' of ''n'' randomly and recursively call ''Hang(c, e)''
# If the element ''x'' in ''n'' has a lower priority than ''e'', put ''e'' in ''n'' choose a child ''c'' of ''n'' randomly and recursively call ''Hang(c, x)''

The main difference between the kinetic hanger and the kinetic heap is in the key operations, which are implemented as follows in a kinetic hanger:
* '''Build-hanger:''' First sort elements by priority and then call ''hang'' on the root for each element in order. Then calculate and schedule certificate failure times in the event queue. This takes O(n log n) time, similar to a kinetic heap.
* '''Insert:''' The kinetic hanger inserts top-down (instead of bottom-up) by &quot;''hanging''&quot; the new element at the root node. This takes O(log n) time, but O(log n) certificates might have to be changed on the way down, thus total time is O{{math|(log&lt;sup&gt;2&lt;/sup&gt;&lt;var&gt;n&lt;/var&gt;)}}
* '''Delete:''' This is a simpler operation than in a heap, since the balancing of tree structure doesn't need to be maintained. Thus, the element is simply replaced with the larger of its children, and then that child is recursively deleted. Again, this takes O(log n) time, but O(log n) certificates might have to be updated, so the total time is O{{math|(log&lt;sup&gt;2&lt;/sup&gt;&lt;var&gt;n&lt;/var&gt;)}}.

All these operations result in a uniformly random structure for the hanger, with an expected height of O(log n).

== Analysis ==
This structure is:
* '''[[Kinetic data structure#Performance|Responsive]]:''' processing a certificate failure takes O(log n) time, just like in a kinetic heap
* '''[[Kinetic data structure#Performance|Local]]:''' each element is involved in O(1) certificates, just like in a kinetic heap
* '''Compact:''' there are a total of O(n) certificates, just like in a kinetic heap
* '''[[Kinetic data structure#Performance|Efficient]]:'''  it has the same efficiency as a [[kinetic tournament]] or [[kinetic heater]] - for a collection of space-time trajectories where each pair intersects at most {{math|&lt;var&gt;s&lt;/var&gt;}} times, the kinetic hanger processes {{math|O(&amp;lambda;&lt;sub&gt;&lt;var&gt;s&lt;/var&gt;+2&lt;/sub&gt;log &lt;var&gt;n&lt;/var&gt;)}} events in {{math|O(&amp;lambda;&lt;sub&gt;&lt;var&gt;s&lt;/var&gt;+2&lt;/sub&gt;log&lt;sup&gt;2&lt;/sup&gt;&lt;var&gt;n&lt;/var&gt;)}} time, where  {{math|&amp;lambda;&lt;sub&gt;&lt;var&gt;s&lt;/var&gt;+2&lt;/sub&gt;}} is a [[Davenport-Schinzel sequence]]

== References ==
{{Reflist}}
{{cite web | url=http://www.uniriotec.br/~fonseca/hanger.pdf | title=Kinetic hanger |publisher=Information Processing Letters | accessdate=May 17, 2012 |author=da Fonseca, Guilherme D. and de Figueiredo, Celina M. H. and Carvalho, Paulo C. P. | pages=151–157}}
&lt;!--- Categories ---&gt;

[[Category:Articles created via the Article Wizard]]
[[Category:Kinetic data structures]]
[[Category:Heaps (data structures)]]
[[Category:Probabilistic data structures]]</text>
      <sha1>i0nk1vkfkkxnxy0zf8lucqlifikbt29</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Kinetic heater</title>
    <ns>0</ns>
    <id>35846430</id>
    <revision>
      <id>533162595</id>
      <parentid>503868405</parentid>
      <timestamp>2013-01-15T06:20:30Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (8855)</comment>
      <text xml:space="preserve" bytes="3781">A '''Kinetic Heater''' is a [[kinetic priority queue]] similar to a [[kinetic heap]], that makes use of randomization to simplify its analysis in a way similar to a [[treap]]. Specifically, each element has a random key associated with it in addition to its priority (which changes as a continuous function of time as in all [[kinetic data structure]]s). The kinetic heater is then simultaneously a [[binary search tree]] on the element keys, and a [[heap (data structure)|heap]] on the element priorities. The kinetic heater achieves (expected) asymptotic performance bounds equal to the best kinetic priority queues. In practice however, it is less efficient since the extra random keys need to be stored, and the procedure to handle certificate failure is a (relatively complicated) rotation instead of a simple swap.&lt;ref name=&quot;hanger&quot;/&gt;

== Implementation ==
If every element has a key and a priority associated with it, then there is a unique tree structure that is simultaneously a search tree on the keys and a heap on the priorities - this structure corresponds to the treap (if the priorities are randomly chosen) or the kinetic heater (if the keys are randomly chosen).

The validity of the tree structure is ensured by creating a certificate at each edge that enforces the heap property on that edge. The main operational difference between a kinetic heap and a kinetic heater is in how they respond to certificate failures. When a certificate on an edge fails, a kinetic heater will perform a rotation around the nodes that failed (instead of the swap that a kinetic heap would perform).

[[File:Rotation in a kinetic heater.png]]

For example, consider the elements ''B'' (with parent ''F'') and its left child ''D'' (with right child ''C''). When the certificate [''B''&gt;''D''] on the edge ''BD'' fails, the tree will be [[tree rotation|rotated]] around this edge. Thus in this case the resulting structure has ''D'' in place of ''B'', ''C'' becomes a child of ''B'' instead of''D'', and there are three certificate changes [B&gt;D] replaced with [D&gt;B], [D&gt;C] replaced with [B&gt;C] and [F&gt;B] replaced with [F&gt;D]. Everything else stays the same.

== Analysis ==
This kinetic data structure is:
* '''[[Kinetic data structure#Performance|Responsive]]''': There are O(1) certificate updates that need to be done when a certificate fails, which takes O(log n) time
* '''[[Kinetic data structure#Performance|Local]]''': Each element is involved in O(1) certificates
* '''[[Kinetic data structure#Performance|Compact]]''': There are O(n) total certificates
* '''[[Kinetic data structure#Performance|Efficient]]''': It has the same (expected) asymptotic performance as [[kinetic hanger]], [[kinetic tournament]] - for a collection of space-time trajectories where each pair intersects at most {{math|&lt;var&gt;s&lt;/var&gt;}} times, the kinetic heater processes {{math|O(&amp;lambda;&lt;sub&gt;&lt;var&gt;s+2&lt;var&gt;&lt;/sub&gt;log &lt;var&gt;n&lt;/var&gt;)}} events in{{math|O(&amp;lambda;&lt;sub&gt;&lt;var&gt;s+2&lt;var&gt;&lt;/sub&gt;log&lt;sup&gt;2&lt;/sup&gt;&lt;var&gt;n&lt;/var&gt;)}} time, where  {{math|&amp;lambda;&lt;sub&gt;&lt;var&gt;s+2&lt;var&gt;&lt;/sub&gt;}} is a [[Davenport-Schinzel sequence]].

== References ==
{{Reflist|refs=
&lt;ref name=&quot;hanger&quot;&gt;
{{cite web | url=http://www.uniriotec.br/~fonseca/hanger.pdf | title=Kinetic hanger |publisher=Information Processing Letters | accessdate=May 17, 2012 |author=da Fonseca, Guilherme D. and de Figueiredo, Celina M. H. and Carvalho, Paulo C. P. | pages=151–157}}
&lt;/ref&gt;
}}
{{cite web | url=http://citeseer.ist.psu.edu/viewdoc/download?doi=10.1.1.41.2301&amp;rep=rep1&amp;type=pdf | title=Kinetic Data Structures | accessdate=May 17, 2012| author=Basch, J}}
&lt;!--- Categories ---&gt;

[[Category:Articles created via the Article Wizard]]
[[Category:Kinetic data structures]]
[[Category:Heaps (data structures)]]
[[Category:Probabilistic data structures]]</text>
      <sha1>s7hhsttb0o5fgqm8mxgm13qcnrbjiug</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Binary heap</title>
    <ns>0</ns>
    <id>69890</id>
    <revision>
      <id>625792566</id>
      <parentid>625791244</parentid>
      <timestamp>2014-09-16T10:19:15Z</timestamp>
      <contributor>
        <username>CiaPan</username>
        <id>258338</id>
      </contributor>
      <comment>Undid revision 604010894 by [[Special:Contributions/62.205.89.164|62.205.89.164]] ([[User talk:62.205.89.164|talk]]) – that wasn't an error, 'sift up' is correct</comment>
      <text xml:space="preserve" bytes="21640">{{Infobox data structure
|name=Binary Heap
|type=tree
|
&lt;!-- NOTE:
    Base of logarithms doesn't matter in big O notation. O(log n) is the same as O(lg n) or O(ln n) or O(log_2 n). A change of base is just a constant factor. So don't change these O(log n) complexities to O(lg n) or something else just to indicate a base-2 log. The base doesn't matter.
--&gt;
|space_avg=O(n)
|space_worst=O(n)
|search_avg=O(n)
|search_worst=O(n)
|insert_avg=O(1)
|insert_worst=O(log n)
|delete_avg=O(log n)
|delete_worst=O(log n)
|peek_avg=O(1)
|peek_worst=O(1)
}}
[[File:Max-Heap.svg|thumb|right|240px|Example of a complete binary max heap]]
[[File:Min-heap.png|thumb|right|240px|Example of a complete binary min heap]]
A '''binary heap''' is a [[heap (data structure)|heap]] [[data structure]] created using a [[binary tree]]. It can be seen as a binary tree with two additional constraints:

;Shape property: The tree is a ''[[Complete Binary Tree|complete binary tree]]''; that is, all levels of the tree, except possibly the last one (deepest) are fully filled, and, if the last level of the tree is not complete, the nodes of that level are filled from left to right.
;Heap property: All nodes are ''either '''greater than or equal to''' or '''less than or equal to''' each of its children, according to a comparison [[predicate (mathematical logic)|predicate]] defined for the heap.

Heaps with a mathematical &quot;greater than or equal to&quot; (≥) comparison predicate are called ''max-heaps''; those with a mathematical &quot;less than or equal to&quot; (≤) comparison predicate are called ''min-heaps''.  Min-heaps are often used to implement [[priority queue]]s.&lt;ref&gt;{{cite web
|url=https://docs.python.org/library/heapq.html
|title=heapq – Heap queue algorithm
|work=Python Standard Library
}}&lt;/ref&gt;&lt;ref&gt;{{cite web
|url=http://download.oracle.com/javase/6/docs/api/java/util/PriorityQueue.html
|title=Class PriorityQueue
|work=Java™ Platform Standard Ed. 6
}}&lt;/ref&gt;

Since the ordering of siblings in a heap is not specified by the heap property, a single node's two children can be freely interchanged unless doing so violates the shape property (compare with [[treap]]).

The binary heap is a special case of the [[d-ary heap]] in which d = 2.

== Heap operations ==
Both the insert and remove operations modify the heap to conform to the shape property first, by adding or removing from the end of the heap. Then the heap property is restored by traversing up or down the heap. Both operations take O(log ''n'') time.

=== Insert ===
To add an element to a heap we must perform an ''up-heap'' operation (also known as ''bubble-up'', ''percolate-up'', ''sift-up'', ''trickle up'', ''heapify-up'', or ''cascade-up''), by following this algorithm:

#Add the element to the bottom level of the heap.
#Compare the added element with its parent; if they are in the correct order, stop.
#If not, swap the element with its parent and return to the previous step.

The number of operations required is dependent on the number of levels the new element must rise to satisfy the heap property, thus the insertion operation has a time complexity of O(log ''n''). However, in 1974, Thomas Porter and Istvan Simon proved that the function for the average number of levels an inserted node moves up is upper bounded by the constant 1.6067.&lt;ref&gt;{{cite web|title=Random Insertion into a Priority Queue Structure|url=ftp://reports.stanford.edu/pub/cstr/reports/cs/tr/74/460/CS-TR-74-460.pdf|work=Stanford University Reports|publisher=Stanford University|accessdate=31 January 2014|author=Thomas Porter|author2=Istvan Simon|page=13|year=1974}}&lt;/ref&gt; The average number of operations required for an insertion into a binary heap is 2.6067 since one additional comparison is made that does not result in the inserted node moving up a level. Thus, on average, binary heap insertion has a constant, O(1), time complexity.  Intuitively, this makes sense since approximately 50% of the elements are leaves and approximately 75% of the elements are in the bottom two levels, it is likely that the new element to be inserted will only move a few levels upwards to maintain the heap.

As an example of binary heap insertion, say we have a max-heap

::[[File:Heap add step1.svg|150px]]

and we want to add the number 15 to the heap. We first place the 15 in the position marked by the X. However, the heap property is violated since 15 is greater than 8, so we need to swap the 15 and the 8. So, we have the heap looking as follows after the first swap:

::[[File:Heap add step2.svg|150px]]

However the heap property is still violated since 15 is greater than 11, so we need to swap again:

::[[File:Heap add step3.svg|150px]]

which is a valid max-heap. There is no need to check the children after this. Before we placed 15 on X, the heap was valid, meaning 11 is greater than 5. If 15 is greater than 11, and 11 is greater than 5, then 15 must be greater than 5, because of the [[transitive relation]].

=== Delete ===
The procedure for deleting the root from the heap (effectively extracting the maximum element in a max-heap or the minimum element in a min-heap) and restoring the properties is called ''down-heap'' (also known as ''bubble-down'', ''percolate-down'', ''sift-down'', ''trickle down'', ''heapify-down'', ''cascade-down'' and ''extract-min/max'').

#Replace the root of the heap with the last element on the last level.
#Compare the new root with its children; if they are in the correct order, stop.
#If not, swap the element with one of its children and return to the previous step. (Swap with its smaller child in a min-heap and its larger child in a max-heap.)

So, if we have the same max-heap as before

::[[File:Heap delete step0.svg|150px]]

We remove the 11 and replace it with the 4.

::[[File:Heap remove step1.svg|150px]]

Now the heap property is violated since 8 is greater than 4. In this case, swapping the two elements, 4 and 8, is enough to restore the heap property and we need not swap elements further:

::[[File:Heap remove step2.svg|150px]]

The downward-moving node is swapped with the ''larger'' of its children in a max-heap (in a min-heap it would be swapped with its smaller child), until it satisfies the heap property in its new position. This functionality is achieved by the '''Max-Heapify''' function as defined below in [[pseudocode]] for an [[Array data structure|array]]-backed heap ''A'' of length ''heap_length''[''A''].  Note that &quot;A&quot; is indexed starting at 1, not 0 as is common in many real programming languages.

'''Max-Heapify''' (''A'', ''i''):&lt;br/&gt;
{{pad|2em}}''left'' ← 2''i''&lt;br/&gt;
{{pad|2em}}''right'' ← 2''i'' + 1&lt;br/&gt;
{{pad|2em}}''largest'' ← ''i''&lt;br/&gt;
{{pad|2em}}'''if''' ''left'' ≤ ''heap_length''[''A''] '''and''' ''A''[''left''] &gt; A[''largest''] '''then''':&lt;br/&gt;
{{pad|4em}}''largest'' ← ''left''&lt;br/&gt;
{{pad|2em}}'''if''' ''right'' ≤ ''heap_length''[''A''] '''and''' ''A''[''right''] &gt; ''A''[''largest''] '''then''':&lt;br/&gt;
{{pad|4em}}''largest'' ← ''right''&lt;br/&gt;
{{pad|2em}}'''if''' ''largest'' ≠ ''i'' '''then''':&lt;br/&gt;
{{pad|4em}}'''swap''' ''A[''i''] ↔ ''A''[''largest'']&lt;br/&gt;
{{pad|4em}}Max-Heapify(''A'', ''largest'')

For the above algorithm to correctly re-heapify the array, the node at index ''i'' and its two direct children must violate the heap property. If they do not, the algorithm will fall through with no change to the array. The down-heap operation (without the preceding swap) can also be used to modify the value of the root, even when an element is not being deleted.

In the worst case, the new root has to be swapped with its child on each level until it reaches the bottom level of the heap, meaning that the delete operation has a time complexity relative to the height of the tree, or O(log ''n'').

==Building a heap==
A heap could be built by successive insertions. This approach requires &lt;math&gt;O(n \log n)&lt;/math&gt; time because each insertion takes &lt;math&gt;O(\log n)&lt;/math&gt; time and there are &lt;math&gt;n&lt;/math&gt; elements. However this is not the optimal method. The optimal method starts by arbitrarily putting the elements on a binary tree, respecting the shape property (the tree could be represented by an array, see below). Then starting from the lowest level and moving upwards, shift the root of each subtree downward as in the deletion algorithm until the heap property is restored. More specifically if all the subtrees starting at some height &lt;math&gt;h&lt;/math&gt; (measured from the bottom) have already been &quot;heapified&quot;, the trees at height &lt;math&gt;h+1&lt;/math&gt; can be heapified by sending their root down along the path of maximum valued children when building a max-heap, or minimum valued children when building a min-heap. This process takes &lt;math&gt;O(h)&lt;/math&gt; operations (swaps) per node. In this method most of the heapification takes place in the lower levels. Since the height of the heap is &lt;math&gt; \left\lfloor \lg (n) \right\rfloor&lt;/math&gt;, the number of nodes at height &lt;math&gt;h&lt;/math&gt; is &lt;math&gt;\le \left\lceil 2^{\left(\lg n - h\right) - 1} \right\rceil = \left\lceil \frac{2^{\lg n}}{2^{h+1}}\right\rceil = \left\lceil\frac{n}{2^{h+1}}\right\rceil&lt;/math&gt;. Therefore, the cost of heapifying all subtrees is:

:&lt;math&gt;
\begin{align}
\sum_{h=0}^{\lceil \lg n \rceil} \frac{n}{2^{h+1}}O(h) &amp; =
O\left(n\sum_{h=0}^{\lceil \lg n \rceil} \frac{h}{2^{h + 1}}\right) \\
&amp; \le O\left(n\sum_{h=0}^{\infty} \frac{h}{2^h}\right) \\
&amp; = O(n)

\end{align}
&lt;/math&gt;

This uses the fact that the given infinite [[series (mathematics)|series]] ''h'' / 2&lt;sup&gt;''h''&lt;/sup&gt; [[Convergent series|converges]] to 2.

The exact value of the above (the worst-case number of comparisons during the heap construction) is known to be equal to:

:&lt;math&gt; 2 n - 2 s_2 (n) - e_2 (n) &lt;/math&gt;,&lt;ref&gt;{{citation
 | last1 = Suchenek | first1 = Marek A.
 | title = Elementary Yet Precise Worst-Case Analysis of Floyd's Heap-Construction Program
 | doi = 10.3233/FI-2012-751
 | pages = 75–92
 | publisher = IOS Press
 | journal = Fundamenta Informaticae
 | volume = 120
 | issue = 1
 | year = 2012
 | url = http://www.deepdyve.com/lp/ios-press/elementary-yet-precise-worst-case-analysis-of-floyd-s-heap-50NW30HMxU}}.&lt;/ref&gt;
where s&lt;sub&gt;2&lt;/sub&gt;(n) is the sum of all digits of the binary representation of n and e&lt;sub&gt;2&lt;/sub&gt;(n) is the exponent of 2 in the prime factorization of n.

The '''Build-Max-Heap''' function that follows, converts an array ''A'' which stores a complete
binary tree with n nodes to a max-heap by repeatedly using '''Max-Heapify''' in a bottom up manner.
It is based on the observation that the array elements indexed by
''[[floor function|floor]]''(n/2) + 1, ''floor''(n/2) + 2, ..., n
are all leaves for the tree, thus each is a one-element heap. '''Build-Max-Heap''' runs
'''Max-Heapify''' on each of the remaining tree nodes.

'''Build-Max-Heap''' (''A''):&lt;br/&gt;
{{pad|2em}}''heap_length''[''A''] ← ''length''[''A'']&lt;br/&gt;
{{pad|2em}}'''for''' ''i'' ← ''floor''(''length''[''A'']/2) '''downto''' 1 '''do'''&lt;br/&gt;
{{pad|4em}}'''Max-Heapify'''(''A'', ''i'')

== Heap implementation ==&lt;!-- This section is linked from [[Heapsort]] --&gt;
[[File:Binary tree in array.svg|300px|right|frame|A small complete binary tree stored in an array]]
[[File:Binary Heap with Array Implementation.JPG|400px|thumb|right|Comparison between a binary heap and an array implementation.]]

Heaps are commonly implemented with an [[Array data structure|array]]. Any binary tree can be stored in an array, but because a binary heap is always a complete binary tree, it can be stored compactly. No space is required for [[pointer (computer programming)|pointer]]s; instead, the parent and children of each node can be found by arithmetic on array indices. These properties make this heap implementation a simple example of an [[implicit data structure]] or [[Binary tree#Ahnentafel list|Ahnentafel list]]. Details depend on the root position, which in turn may depend on constraints of a [[programming language]] used for implementation, or programmer preference. Specifically, sometimes the root is placed at index 1, sacrificing space in order to simplify arithmetic. The ''[[Peek (data type operation)|peek]]'' operation (''find-min'' or ''find-max'') simply returns the value of the root, and is thus O(1).

Let ''n'' be the number of elements in the heap and ''i'' be an arbitrary valid index of the array storing the heap. If the tree root is at index 0, with valid indices 0 through ''n − ''1, then each element ''a'' at index ''i'' has
* children at indices 2''i ''+ 1 and 2''i ''+ 2
* its parent ⌊(''i ''&amp;minus; 1) ∕ 2⌋ where ⌊…⌋ is the [[floor function]].
Alternatively, if the tree root is at index 1, with valid indices 1 through ''n'', then each element ''a'' at index ''i'' has
* children at indices 2''i'' and 2''i ''+1
* its parent at index ⌊''i ∕'' 2⌋.

This implementation is used in the [[heapsort]] algorithm, where it allows the space in the input array to be reused to store the heap (i.e. the algorithm is done [[In-place algorithm|in-place]]). The implementation is also useful for use as a [[Priority queue]] where use of a [[dynamic array]] allows insertion of an unbounded number of items.

The upheap/downheap operations can then be stated in terms of an array as follows: suppose that the heap property holds for the indices ''b'', ''b''+1, ..., ''e''. The sift-down function extends the heap property to ''b''−1, ''b'', ''b''+1, ..., ''e''.
Only index ''i'' = ''b''−1 can violate the heap property.
Let ''j'' be the index of the largest child of ''a''[''i''] (for a max-heap, or the smallest child for a min-heap) within the range ''b'', ..., ''e''.
(If no such index exists because 2''i'' &gt; ''e'' then the heap property holds for the newly extended range and nothing needs to be done.)
By swapping the values ''a''[''i''] and ''a''[''j''] the heap property for position ''i'' is established.
At this point, the only problem is that the heap property might not hold for index ''j''.
The sift-down function is applied [[tail recursion|tail-recursively]] to index ''j'' until the heap property is established for all elements.

The sift-down function is fast. In each step it only needs two comparisons and  one swap. The index value where it is working doubles in each iteration, so that at most log&lt;sub&gt;2&lt;/sub&gt; ''e'' steps are required.

For big heaps and using [[virtual memory]], storing elements in an array according to the above scheme is inefficient: (almost) every level is in a different [[Page (computer memory)|page]].  [[B-heap]]s are binary heaps that keep subtrees in a single page, reducing the number of pages accessed by up to a factor of ten.&lt;ref&gt;
Poul-Henning Kamp.
[http://queue.acm.org/detail.cfm?id=1814327 &quot;You're Doing It Wrong&quot;].
ACM Queue.
June 11, 2010.
&lt;/ref&gt;

The operation of merging two binary heaps takes Θ(''n'') for equal-sized heaps. The best you can do is (in case of array implementation) simply concatenating the two heap arrays and build a heap of the result.&lt;ref&gt;
Chris L. Kuszmaul.
[http://nist.gov/dads/HTML/binaryheap.html &quot;binary heap&quot;].
Dictionary of Algorithms and Data Structures, Paul E. Black, ed., U.S. National Institute of Standards and Technology. 16 November 2009.
&lt;/ref&gt; A heap on ''n'' elements can be merged with a heap on ''k'' elements using O(log ''n'' log ''k'') key comparisons, or, in case of a pointer-based implementation, in O(log ''n'' log ''k'') time.&lt;ref&gt;J.-R. Sack and  T. Strothotte
[http://www.springerlink.com/content/k24440h5076w013q/ &quot;An Algorithm for Merging Heaps&quot;],
Acta Informatica 22, 171-186 (1985).&lt;/ref&gt; An algorithm for splitting a heap on ''n'' elements into two heaps on ''k'' and ''n-k'' elements, respectively, based on a new view
of heaps as an ordered collections of subheaps was presented in.&lt;ref&gt;. J.-R. Sack and  T. Strothotte 
[http://www.sciencedirect.com/science/article/pii/089054019090026E &quot;A characterization of heaps and its applications&quot;]
Information and Computation
Volume 86, Issue 1, May 1990, Pages 69–86.&lt;/ref&gt; The algorithm requires  O(log ''n'' * log ''n'')  comparisons. The view  also presents a new and conceptually simple algorithm for merging heaps. When merging is a common task, a different heap implementation is recommended, such as  [[binomial heap]]s, which can be merged in O(log ''n'').

Additionally, a binary heap can be implemented with a traditional binary tree data structure, but there is an issue with finding the adjacent element on the last level on the binary heap when adding an element. This element can be determined algorithmically or by adding extra data to the nodes, called &quot;threading&quot; the tree—instead of merely storing references to the children, we store the [[inorder]] successor of the node as well.

It is possible to modify the heap structure to allow extraction of both the smallest and largest element in  [[Big O notation|&lt;math&gt;O&lt;/math&gt;]]&lt;math&gt;(\log n)&lt;/math&gt; time.&lt;ref name=&quot;sym&quot;&gt;{{cite web
| url = http://cg.scs.carleton.ca/~morin/teaching/5408/refs/minmax.pdf
| author = Atkinson, M.D., [[Jörg-Rüdiger Sack|J.-R. Sack]], N. Santoro, and T. Strothotte
| title = Min-max heaps and generalized priority queues.
| publisher = Programming techniques and Data structures. Comm. ACM, 29(10): 996–1000
| date = 1 October 1986
}}&lt;/ref&gt;  To do this, the rows alternate between min heap and max heap.  The algorithms are roughly the same, but, in each step, one must consider the alternating rows with alternating comparisons.  The performance is roughly the same as a normal single direction heap. This idea can be generalised to a min-max-median heap.

== Derivation of index equations ==
In an array-based heap, the children and parent of a node can be located via simple arithmetic on the node's index. This section derives the relevant equations for heaps with their root at index 0, with additional notes on heaps with their root at index 1.

To avoid confusion, we'll define the '''level''' of a node as its distance from the root, such that the root itself occupies level 0.

=== Child nodes ===

For a general node located at index &lt;math&gt;i&lt;/math&gt; (beginning from 0), we will first derive the index of its right child, &lt;math&gt;\text{right} = 2i + 2&lt;/math&gt;.

Let node &lt;math&gt;i&lt;/math&gt; be located in level &lt;math&gt;L&lt;/math&gt;, and note that any level &lt;math&gt;l&lt;/math&gt; contains exactly &lt;math&gt;2^l&lt;/math&gt; nodes. Furthermore, there are exactly &lt;math&gt;2^{l + 1} - 1&lt;/math&gt; nodes contained in the layers up to and including layer &lt;math&gt;l&lt;/math&gt; (think of binary arithmetic; 0111...111 = 1000...000 - 1). Because the root is stored at 0, the &lt;math&gt;k&lt;/math&gt;th node will be stored at index &lt;math&gt;(k - 1)&lt;/math&gt;. Putting these observations together yields the following expression for the '''index of the last node in layer l'''.

::&lt;math&gt;\text{last}(l) = (2^{l + 1} - 1) - 1 = 2^{l + 1} - 2&lt;/math&gt;

Let there be &lt;math&gt;j&lt;/math&gt; nodes after node &lt;math&gt;i&lt;/math&gt; in layer L, such that

::&lt;math&gt;\begin{alignat}{2}
i = &amp; \quad \text{last}(L) - j\\
  = &amp; \quad (2^{L + 1} -2) - j\\
\end{alignat}
&lt;/math&gt;

Each of these &lt;math&gt;j&lt;/math&gt; nodes must have exactly 2 children, so there must be &lt;math&gt;2j&lt;/math&gt; nodes separating &lt;math&gt;i&lt;/math&gt;'s right child from the end of its layer (&lt;math&gt;L + 1&lt;/math&gt;).

::&lt;math&gt;\begin{alignat}{2}
\text{right} = &amp; \quad \text{last(L + 1)} -2j\\
             = &amp; \quad (2^{L + 2} -2) -2j\\
             = &amp; \quad 2(2^{L + 1} -2 -j) + 2\\
             = &amp; \quad 2i + 2
\end{alignat}
&lt;/math&gt;

As required.

Noting that the left child of any node is always 1 place before its right child, we get &lt;math&gt;\text{left} = 2i + 1&lt;/math&gt;.

If the root is located at index 1 instead of 0, the last node in each level is instead at index &lt;math&gt;2^{l + 1} - 1&lt;/math&gt;. Using this throughout yields &lt;math&gt;\text{left} = 2i&lt;/math&gt; and &lt;math&gt;\text{right} = 2i + 1&lt;/math&gt; for heaps with their root at 1.

=== Parent node ===

Every node is either the left or right child of its parent, so we know that either of the following is true.

# &lt;math&gt;i = 2 \times (\text{parent}) + 1&lt;/math&gt;
# &lt;math&gt;i = 2 \times (\text{parent}) + 2&lt;/math&gt;

Hence,
::&lt;math&gt;\text{parent} = \frac{i - 1}{2} \textbf{ or } \frac{i - 2}{2}&lt;/math&gt;

Now consider the expression &lt;math&gt;\left\lfloor \dfrac{i - 1}{2} \right\rfloor&lt;/math&gt;.

If node &lt;math&gt;i&lt;/math&gt; is a left child, this gives the result immediately, however, it also gives the correct result if node &lt;math&gt;i&lt;/math&gt; is a right child. In this case, &lt;math&gt;(i - 2)&lt;/math&gt; must be even, and hence &lt;math&gt;(i - 1)&lt;/math&gt; must be odd.

::&lt;math&gt;\begin{alignat}{2}
\left\lfloor \dfrac{i - 1}{2} \right\rfloor = &amp; \quad \left\lfloor \dfrac{i - 2}{2} + \dfrac{1}{2} \right\rfloor\\
= &amp; \quad \frac{i - 2}{2}\\
= &amp; \quad \text{parent}
\end{alignat}
&lt;/math&gt;

Therefore, irrespective of whether a node is a left or right child, its parent can be found by the expression:

::&lt;math&gt;\text{parent} = \left\lfloor \dfrac{i - 1}{2} \right\rfloor&lt;/math&gt;

==See also==
* [[Heap (data structure)|Heap]]
* [[Heapsort]]

==Notes==
{{notelist}}

==References==
{{reflist}}

==External links==
*[http://people.ksp.sk/~kuko/bak/index.html Binary Heap Applet] by Kubo Kovac
*[http://www.policyalmanac.org/games/binaryHeaps.htm Using Binary Heaps in A* Pathfinding]
*[http://opendatastructures.org/versions/edition-0.1e/ods-java/10_1_BinaryHeap_Implicit_Bi.html Open Data Structures - Section 10.1 - BinaryHeap: An Implicit Binary Tree]
*[http://robin-thomas.github.io/max-heap/ Implementation of binary max heap in C] by Robin Thomas
*[http://robin-thomas.github.io/min-heap/ Implementation of binary min heap in C] by Robin Thomas

{{Data structures}}

{{DEFAULTSORT:Binary Heap}}
[[Category:Heaps (data structures)]]</text>
      <sha1>qr7mwlbs16esw2jrgpzb5orv9jv7wio</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>2–3 heap</title>
    <ns>0</ns>
    <id>675651</id>
    <revision>
      <id>551020984</id>
      <parentid>537025002</parentid>
      <timestamp>2013-04-18T19:27:20Z</timestamp>
      <contributor>
        <username>Bgwhite</username>
        <id>264323</id>
      </contributor>
      <minor/>
      <comment>/* References */Add [[Wikipedia:DEFAULTSORT|DEFAULTSORT]] - using [[Project:AWB|AWB]] (9094)</comment>
      <text xml:space="preserve" bytes="680">In [[computer science]], a '''2–3 heap''' is a [[data structure]], a variation on the [[heap (data structure)|heap]], designed by [[Tadao Takaoka]] in 1999. The structure is similar to the [[Fibonacci heap]], and borrows from the [[2–3 tree]].

Time costs for some common heap operations are:

* ''Delete-min'' takes &lt;math&gt;O(log(n))&lt;/math&gt; [[amortized time]].
* ''Decrease-key'' takes constant amortized time.
* ''Insertion'' takes constant amortized time.

== References ==

* Tadao Takaoka.  [http://www.cosc.canterbury.ac.nz/~tad/2-3heaps.pdf ''Theory of 2–3 Heaps''], Cocoon (1999).

{{DEFAULTSORT:2-3 heap}}
[[Category:Heaps (data structures)]]


{{datastructure-stub}}</text>
      <sha1>f4z23t9hu6bomwq4x803rbwd95sz6bn</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Addressable heap</title>
    <ns>0</ns>
    <id>40964448</id>
    <revision>
      <id>580137038</id>
      <parentid>580127518</parentid>
      <timestamp>2013-11-04T10:39:24Z</timestamp>
      <contributor>
        <username>Oleksandr Shturmov</username>
        <id>19190609</id>
      </contributor>
      <comment>categorized</comment>
      <text xml:space="preserve" bytes="1724">{{Orphan|date=November 2013}}

In [[computer science]], an ''Addressable heap'' is an [[abstract data type]], which is a [[mergeable heap]] supporting access to the elements of the heap via handles (also called [[Reference (computer science)|references]]), and allows to remove, or decrease the key of the element referenced by a particular handle.

== Definition ==

An addressable heap supports the following operations:&lt;ref&gt;Kurt Mehlborn and Peter Sanders. ''Algorithms and Data Structures -- The Basic Toolbox''.  2008. Springer-Verlag. Berlin, Heidelberg, Germany. ISBN 978-3-540-77977-3.&lt;/ref&gt;

* &lt;code&gt;Make-Heap()&lt;/code&gt;, creating an empty heap.
* &lt;code&gt;Insert(H,x)&lt;/code&gt;, inserting an element &lt;code&gt;x&lt;/code&gt; into the heap &lt;code&gt;H&lt;/code&gt;, and returning a handle to it.
* &lt;code&gt;Min(H)&lt;/code&gt;, returning a handle to the minimum element, or &lt;code&gt;Nil&lt;/code&gt; if no such element exists.
* &lt;code&gt;Extract-Min(H)&lt;/code&gt;, extracting and returning a handle to the minimum element, or &lt;code&gt;Nil&lt;/code&gt; if no such element exists.
* &lt;code&gt;Remove(h)&lt;/code&gt;, removing the element referenced by &lt;code&gt;h&lt;/code&gt; (from its respective heap).
* &lt;code&gt;Decrease-Key(h,k)&lt;/code&gt;, decreasing the key of the element referenced by &lt;code&gt;h&lt;/code&gt; to &lt;code&gt;k&lt;/code&gt;; illegal if &lt;code&gt;k&lt;/code&gt; is larger than the key referenced by &lt;code&gt;h&lt;/code&gt;.
* &lt;code&gt;Merge(H1,H2)&lt;/code&gt;, combining the elements of &lt;code&gt;H1&lt;/code&gt; and &lt;code&gt;H2&lt;/code&gt;.

== Examples ==

Examples of addressable heaps include:

* [[Fibonacci heap]]s
* [[Binomial heap]]s

A more complete list with performance comparisons can be found [[Heap (data structure)#Comparison of theoretic bounds for variants|here]].

== References ==
{{reflist}}

[[Category:Heaps (data structures)]]</text>
      <sha1>20rykuxofi9c1g3fpufvdzqjds6t1hd</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Mergeable heap</title>
    <ns>0</ns>
    <id>40964192</id>
    <revision>
      <id>608823133</id>
      <parentid>580137091</parentid>
      <timestamp>2014-05-16T11:53:48Z</timestamp>
      <contributor>
        <username>Ruud Koot</username>
        <id>170083</id>
      </contributor>
      <minor/>
      <comment>fmt</comment>
      <text xml:space="preserve" bytes="1707">In [[computer science]], a '''mergeable heap''' (also called a '''meldable heap''') is an [[abstract data type]], which is a [[Heap (data structure)|heap]] supporting a merge operation.

== Definition ==

A mergeable heap supports the following operations:&lt;ref&gt;Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. ''Introduction to Algorithms''. 2009, 3rd ed. The MIT Press. ISBN 978-0-262-53305-8.&lt;/ref&gt;

* &lt;code&gt;Make-Heap()&lt;/code&gt;, creating an empty heap.
* &lt;code&gt;Insert(H,x)&lt;/code&gt;, inserting an element &lt;code&gt;x&lt;/code&gt; into the heap &lt;code&gt;H&lt;/code&gt;.
* &lt;code&gt;Min(H)&lt;/code&gt;, returning the minimum element, or &lt;code&gt;Nil&lt;/code&gt; if no such element exists.
* &lt;code&gt;Extract-Min(H)&lt;/code&gt;, extracting and returning the minimum element, or &lt;code&gt;Nil&lt;/code&gt; if no such element exists.
* &lt;code&gt;Merge(H1,H2)&lt;/code&gt;, combining the elements of &lt;code&gt;H1&lt;/code&gt; and &lt;code&gt;H2&lt;/code&gt;.

== Trivial implementation ==

It is straightforward to implement a mergeable heap given a simple heap:

&lt;code&gt;Merge(H1,H2):&lt;/code&gt;
# &lt;code&gt;x &amp;larr; Extract-Min(H2)&lt;/code&gt;
# &lt;code&gt;'''while''' x ≠ Nil&lt;/code&gt;
## &lt;code&gt;Insert(H1, x)&lt;/code&gt;
## &lt;code&gt;x &amp;larr; Extract-Min(H2)&lt;/code&gt;

This can however be wasteful as each &lt;code&gt;Extract-Min(H)&lt;/code&gt; and &lt;code&gt;Insert(H,x)&lt;/code&gt; typically have to maintain the [[Heap (data structure)|heap property]].

== More efficient implementations ==

Examples of mergeable heaps include:

* [[Fibonacci heap]]s
* [[Binomial heap]]s
* [[Pairing heap]]s
* [[Binary heap]]s

A more complete list with performance comparisons can be found [[Heap (data structure)#Comparison of theoretic bounds for variants|here]].

== References ==
{{reflist}}

[[Category:Heaps (data structures)]]</text>
      <sha1>dxe2a59tc96ipu4ges9ykib3rwjjbez</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Shadow heap</title>
    <ns>0</ns>
    <id>42442221</id>
    <revision>
      <id>603562486</id>
      <parentid>603392174</parentid>
      <timestamp>2014-04-10T06:58:16Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (10072)</comment>
      <text xml:space="preserve" bytes="8329">In [[computer science]], a '''shadow heap''' is a [[mergeable heap]] [[data structure]] which supports efficient heap merging in the [[Amortized analysis|amortized]] sense. More specifically, '''shadow heaps''' make use of the shadow merge algorithm to achieve insertion in [[Big O notation|''O'']](f(''n'')) amortized time and deletion in ''O''((log ''n'' log log ''n'')/f(''n'')) amortized time, for any choice of 1 ≤ f(''n'') ≤ log log ''n''.&lt;ref&gt;{{cite techreport
 | last1 = Kuszmaul | first1 = Christopher L.
 | title = Efficient Merge and Insert Operations for Binary Heaps and Trees
 | institution = NASA Advanced Supercomputing Division
 | year = 2000
 | number = 00-003
 | url = http://www.nas.nasa.gov/assets/pdf/techreports/2000/nas-00-003.pdf}}&lt;/ref&gt;

Throughout this article, it is assumed that ''A'' and ''B'' are binary heaps with |''A''| ≤ |''B''|.

==Shadow merge==
Shadow merge is an [[algorithm]] for merging two [[binary heap]]s efficiently if these heaps are implemented as [[Array data structure|array]]s. Specifically, the running time of shadow merge on two heaps &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; is &lt;math&gt;O(|A| + \min\{\log |B| \log \log |B|, \log |A| \log |B|\})&lt;/math&gt;.

===Algorithm===

We wish to merge the two binary min-heaps &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt;. The algorithm is as follows:

# Concatenate the array &lt;math&gt;A&lt;/math&gt; at the end of the array &lt;math&gt;B&lt;/math&gt; to obtain an array &lt;math&gt;C&lt;/math&gt;.
# Identify the ''shadow'' of &lt;math&gt;A&lt;/math&gt; in &lt;math&gt;C&lt;/math&gt;; that is, the ancestors of the last &lt;math&gt;|A|&lt;/math&gt; nodes in &lt;math&gt;C&lt;/math&gt; which destroy the heap property. 
# Identify the following two parts of the shadow from &lt;math&gt;C&lt;/math&gt;:
#* The ''path'' &lt;math&gt;P&lt;/math&gt;: the set of nodes in the shadow for which there are at most 2 at any depth of &lt;math&gt;C&lt;/math&gt;;
#* The ''subtree'' &lt;math&gt;T&lt;/math&gt;: the remainder of the shadow.
# Extract and sort the smallest &lt;math&gt;|P|&lt;/math&gt; nodes from the shadow into an array &lt;math&gt;S&lt;/math&gt;.
# Transform &lt;math&gt;S&lt;/math&gt; as follows:
#* If &lt;math&gt;|S| &gt; |C|&lt;/math&gt;, then starting from the smallest element in the sorted array, sequentially insert each element of &lt;math&gt;S&lt;/math&gt; into &lt;math&gt;C&lt;/math&gt;, replacing them with &lt;math&gt;C&lt;/math&gt;'s smallest elements.
#* If &lt;math&gt;|S| \leq |C|&lt;/math&gt;, then extract and sort the &lt;math&gt;|P|&lt;/math&gt; smallest elements from &lt;math&gt;C&lt;/math&gt;, and merge this sorted list with &lt;math&gt;S&lt;/math&gt;.
# Replace the elements of &lt;math&gt;S&lt;/math&gt; into their original positions in &lt;math&gt;C&lt;/math&gt;.
# Make a heap out of &lt;math&gt;T&lt;/math&gt;.

===Running time===

Again, let &lt;math&gt;P&lt;/math&gt; denote the path, and &lt;math&gt;T&lt;/math&gt; denote the subtree of the concatenated heap &lt;math&gt;C&lt;/math&gt;. The number of nodes in &lt;math&gt;P&lt;/math&gt; is at most twice the depth of &lt;math&gt;C&lt;/math&gt;, which is &lt;math&gt;O(\log |B|)&lt;/math&gt;. Moreover, the number of nodes in &lt;math&gt;T&lt;/math&gt; at depth &lt;math&gt;d&lt;/math&gt; is at most 3/4 the number of nodes at depth &lt;math&gt;d + 1&lt;/math&gt;, so the subtree has size &lt;math&gt;O(|A|)&lt;/math&gt;. Since there are at most 2 nodes at each level on &lt;math&gt;P&lt;/math&gt;, then reading the smallest &lt;math&gt;|P|&lt;/math&gt; elements of the shadow into the sorted array &lt;math&gt;S&lt;/math&gt; takes &lt;math&gt;O(\log |B|)&lt;/math&gt; time.

If &lt;math&gt;|S| &gt; |C|&lt;/math&gt;, then combining &lt;math&gt;P&lt;/math&gt; and &lt;math&gt;C&lt;/math&gt; as in step 5 above takes time &lt;math&gt;O(\log |A| \log |B|)&lt;/math&gt;. Otherwise, the time taken in this step is &lt;math&gt;O(|A| + \log |B| \log \log |B|)&lt;/math&gt;. Finally, making a heap of the subtree &lt;math&gt;T&lt;/math&gt; takes &lt;math&gt;O(|A|)&lt;/math&gt; time. This amounts to a total running time for shadow merging of &lt;math&gt;O(|A| + \min\{\log |A| \log |B|, \log |B| \log \log |B|\})&lt;/math&gt;.

==Structure==

A shadow heap &lt;math&gt;H&lt;/math&gt; consists of threshold function &lt;math&gt;f(H)&lt;/math&gt;, and an [[Array data structure|array]] for which the usual array-implemented [[binary heap]] property is upheld in its first entries, and for which the heap property is not necessarily upheld in the other entries. Thus, the shadow heap is essentially a binary heap &lt;math&gt;B&lt;/math&gt; adjacent to an array &lt;math&gt;A&lt;/math&gt;. To add an element to the shadow heap, place it in the array &lt;math&gt;A&lt;/math&gt;. If the array becomes too large according to the specified threshold, we first build a heap out of &lt;math&gt;A&lt;/math&gt; using [[Heapsort#Variations|Floyd's algorithm]] for heap construction,&lt;ref&gt;{{citation
 | last1 = Suchenek | first1 = Marek A.
 | title = Elementary Yet Precise Worst-Case Analysis of Floyd's Heap-Construction Program
 | doi = 10.3233/FI-2012-751
 | pages = 75–92
 | publisher = IOS Press
 | journal = Fundamenta Informaticae
 | volume = 120
 | issue = 1
 | year = 2012
 | url = http://www.deepdyve.com/lp/ios-press/elementary-yet-precise-worst-case-analysis-of-floyd-s-heap-50NW30HMxU}}&lt;/ref&gt; and then merge this heap with &lt;math&gt;B&lt;/math&gt; using shadow merge. Finally, the merging of shadow heaps is simply done through sequential insertion of one heap into the other using the above insertion procedure.

==Analysis==

We are given a shadow heap &lt;math&gt;H = (B, A)&lt;/math&gt;, with threshold function &lt;math&gt;\log |H| \leq f(H) \leq \log |H| \log \log |H|&lt;/math&gt; as above. Suppose that the threshold function is such that any change in &lt;math&gt;|B|&lt;/math&gt; induces no larger a change than in &lt;math&gt;f(H)&lt;/math&gt;. We derive the desired running time bounds for the [[mergeable heap]] operations using the [[potential method]] for [[amortized analysis]]. The potential &lt;math&gt;\Psi(H)&lt;/math&gt; of the heap is chosen to be:

:&lt;math&gt;\Psi(H) = |A| (1 + \min\{\log |B| \log \log |B|, \log |B| \log |A|\}/f(H))&lt;/math&gt;

Using this potential, we can obtain the desired amortized running times:

'''create(''H'')''': initializes a new empty shadow heap &lt;math&gt;H&lt;/math&gt;
:Here, the potential &lt;math&gt;\Psi&lt;/math&gt; is unchanged, so the amortized cost of creation is &lt;math&gt;O(1)&lt;/math&gt;, the actual cost.

'''insert(''x'', ''H'')''': inserts &lt;math&gt;x&lt;/math&gt; into the shadow heap &lt;math&gt;H&lt;/math&gt;
:There are two cases:
:*If the merge is employed, then the drop in the potential function is exactly the actual cost of merging &lt;math&gt;B&lt;/math&gt; and &lt;math&gt;A&lt;/math&gt;, so the amortized cost is &lt;math&gt;O(1)&lt;/math&gt;.
:*If the merge is not done, then the amortized cost is &lt;math&gt;O(1 + \min\{\log |B| \log \log |B|, \log |B| \log |A|\}/f(H))&lt;/math&gt;
:By choice of the threshold function, we thus obtain that the amortized cost of insertion is:
:&lt;math&gt;O(\log |H| \log \log |H|/f(H))&lt;/math&gt;

'''delete_min(''H'')''': deletes the minimum priority element from &lt;math&gt;H&lt;/math&gt;
:Finding and deleting the minimum takes actual time &lt;math&gt;O(|A| + \log |B|)&lt;/math&gt;. Moreover, the potential function can only increase after this deletion if the value of &lt;math&gt;f(H)&lt;/math&gt; decreases. By choice of &lt;math&gt;f&lt;/math&gt;, we have that the amortized cost of this operation is the same as the actual cost.

==Related algorithms &amp; data structures==

A naive binary heap merging algorithm will merge the two heaps &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; in in time &lt;math&gt;O(|B|)&lt;/math&gt; by simply concatenating both heaps and making a heap out of the resulting array using [[Heapsort#Variations|Floyd's algorithm]] for heap construction. Alternatively, the heaps can simply be merged by sequentially inserting each element of &lt;math&gt;A&lt;/math&gt; into &lt;math&gt;B&lt;/math&gt;, taking time &lt;math&gt;O(|A| \log |B|)&lt;/math&gt;.

[[Jörg-Rüdiger Sack|Sack]] and [[Thomas Strothotte|Strothotte]] proposed an algorithm for merging the binary heaps in &lt;math&gt;O(|A| + \log |A| \log |B|)&lt;/math&gt; time.&lt;ref&gt;{{citation
 | last1 = Sack | first1 = Jörg-R.
 | last2 = Strothotte | first2 = Thomas
 | title = An Algorithm for Merging Heaps
 | doi = 10.1007/BF00264229
 | pages = 171–186
 | publisher = Springer-Verlag
 | journal = Acta Informatica
 | volume = 22
 | issue = 2
 | year = 1985}}.&lt;/ref&gt; Their algorithm is known to be more efficient than the second naive solution described above roughly when &lt;math&gt;|A| &gt; \log |B|&lt;/math&gt;. Shadow merge performs asymptotically better than their algorithm, even in the worst case.

There are several other heaps which support faster merge times. For instance, [[Fibonacci heap]]s can be merged in &lt;math&gt;O(1)&lt;/math&gt; time. Since binary heaps require &lt;math&gt;\Omega(|A|)&lt;/math&gt; time to merge,&lt;ref&gt;{{Introduction to Algorithms|edition=1}}&lt;/ref&gt; shadow merge remains efficient.

==References==
{{reflist}}
*
*
*
*

{{Data structures}}

{{DEFAULTSORT:Shadow Heap}}
[[Category:Heaps (data structures)]]</text>
      <sha1>oixb1ryaffjoyk0rdu07lccq9vrbx7q</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Heapsort</title>
    <ns>0</ns>
    <id>13995</id>
    <revision>
      <id>626685827</id>
      <parentid>626399311</parentid>
      <timestamp>2014-09-22T22:12:47Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>copyedit lede</comment>
      <text xml:space="preserve" bytes="22785">{{Use dmy dates|date=July 2012}}
{{Infobox Algorithm|class=[[Sorting algorithm]]
|image=[[Image:Sorting heapsort anim.gif]]
|caption=A run of the heapsort algorithm sorting an array of randomly permuted values. In the first stage of the algorithm the array elements are reordered to satisfy the [[Heap (data structure)|heap property]]. Before the actual sorting takes place, the heap tree structure is shown briefly for illustration.
|data=[[Array data structure|Array]]
|time=&lt;math&gt;O(n\log n)&lt;/math&gt;
|average-time=&lt;math&gt;O(n\log n)&lt;/math&gt;
|best-time=&lt;math&gt;\Omega(n), O(n\log n)&lt;/math&gt;&lt;ref&gt;http://dx.doi.org/10.1006/jagm.1993.1031&lt;/ref&gt;
|space=&lt;math&gt;O(1)&lt;/math&gt; auxiliary 
|optimal=Never
}}
'''Heapsort''' is a [[comparison sort|comparison-based]] [[sorting algorithm]]. Heapsort can be thought of as an improved [[selection sort]]: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the smallest element and moving that to the sorted region. The improvement consists of the use of a [[heap (data structure)|heap]] data structure rather than a linear-time search to find the minimum.&lt;ref&gt;{{cite book |first=Steven |last=Skiena |authorlink=Steven Skiena |title=The Algorithm Design Manual |publisher=Springer |year=2008 |page=109 |quote=[H]eapsort is nothing but an implementation of selection sort
using the right data structure. |doi=10.1007/978-1-84800-070-4_4}}&lt;/ref&gt;

Although somewhat slower in practice on most machines than a well-implemented [[quicksort]], it has the advantage of a more favorable worst-case [[big O notation|O]](''n'' log ''n'') runtime.  Heapsort is an [[in-place algorithm]], but it is not a [[stable sort]].

Heapsort was invented by [[J. W. J. Williams]] in 1964.&lt;ref&gt;{{harvnb|Williams|1964}}&lt;/ref&gt; This was also the birth of the heap, presented already by Williams as a useful data structure in its own right.&lt;ref name=&quot;brass&quot;&gt;{{cite book |first=Peter |last=Brass |title=Advanced Data Structures |publisher=Cambridge University Press |year=2008 |isbn=9780521880374 |page=209}}&lt;/ref&gt; In the same year, [[Robert Floyd|R. W. Floyd]] published an improved version that could sort an array in-place, continuing his earlier research into the [[treesort]] algorithm.&lt;ref name=&quot;brass&quot;/&gt;

== Overview ==
The heapsort algorithm can be divided into two parts.

In the first step, a [[Heap (data structure)|heap]] is [[Binary heap#Building a heap|built]] out of the data. The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions.  For a zero-based array, the root node is stored at index 0; if &lt;code&gt;i&lt;/code&gt; is the index of the current node, then
&lt;pre&gt;
  iParent     = floor((i-1) / 2)
  iLeftChild  = 2*i + 1
  iRightChild = 2*i + 2
&lt;/pre&gt;

In the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap. Once all objects have been removed from the heap, the result is a sorted array.

Heapsort can be performed in place. The array can be split into two parts, the sorted array and the heap. The storage of heaps as arrays is diagrammed [[Binary heap#Heap implementation|here]].  The heap's invariant is preserved after each extraction, so the only cost is that of extraction.

== Variations ==
*The most important variation to the simple variant is an improvement by Floyd that uses only one comparison in each [[Binary heap#Insert|siftup]] run, which must be followed by a [[Binary heap#Delete|siftdown]] for the original child. The worst-case number of comparisons during the Floyd's heap-construction phase of Heapsort is known to be equal to 2N − 2s&lt;sub&gt;2&lt;/sub&gt;(N) − e&lt;sub&gt;2&lt;/sub&gt;(N), where s&lt;sub&gt;2&lt;/sub&gt;(N) is the sum of all digits of the binary representation of N and e&lt;sub&gt;2&lt;/sub&gt;(N) is the exponent of 2 in the prime factorization of N.&lt;ref&gt;{{citation
 | last1 = Suchenek | first1 = Marek A.
 | title = Elementary Yet Precise Worst-Case Analysis of Floyd's Heap-Construction Program
 | doi = 10.3233/FI-2012-751
 | pages = 75–92
 | publisher = IOS Press
 | journal = Fundamenta Informaticae
 | volume = 120
 | issue = 1
 | year = 2012
 | url = http://www.deepdyve.com/lp/ios-press/elementary-yet-precise-worst-case-analysis-of-floyd-s-heap-50NW30HMxU}}&lt;/ref&gt;
*A version called bottom-up heapsort was announced in 1993 as beating quicksort (with median-of-three pivot selection) on arrays of size ≥16000.&lt;ref&gt;{{cite journal |last=Wegener |first=Ingo |title=Bottom-up heapsort, a new variant of heapsort beating, on an average, quicksort (if {{mvar|n}} is not very small) |journal=Theoretical Computer Science |volume=118 |issue=1 |year=1993 |pages=81–98}}&lt;/ref&gt; It requires only {{math|1.5 ''n'' log ''n'' + ''O''(''n'')}} comparisons in the worst case and {{math|''n'' log ''n'' + ''O''(1)}} on average. A 2008 re-evaluation of the algorithm showed it to be no faster than ordinary heapsort, though, presumably because modern [[branch prediction]] nullifies the cost of the comparisons that bottom-up heapsort manages to avoid.&lt;ref&gt;{{cite book |last1=Mehlhorn |first1=Kurt |first2=Peter |last2=Sanders |title=Algorithms and data structures: The basic toolbox |publisher=Springer |year=2008 |page=142}}&lt;/ref&gt;
*[[Ternary heapsort]]&lt;ref&gt;&quot;Data Structures Using Pascal&quot;, 1991, page 405, gives a ternary heapsort as a student exercise. &quot;Write a sorting routine similar to the heapsort except that it uses a ternary heap.&quot;&lt;/ref&gt; uses a ternary heap instead of a binary heap; that is, each element in the heap has three children. It is more complicated to program, but does a constant number of times fewer swap and comparison operations.  This is because each step in the shift operation of a ternary heap requires three comparisons and one swap, whereas in a binary heap two comparisons and one swap are required. The ternary heap does two steps in less time than the binary heap requires for three steps, which multiplies the index by a factor of 9 instead of the factor 8 of three binary steps.{{Citation needed|date=September 2014}}
*The '''[[smoothsort]]''' algorithm&lt;ref&gt;http://www.cs.utexas.edu/users/EWD/ewd07xx/EWD796a.PDF&lt;/ref&gt;&lt;ref&gt;http://www.cs.utexas.edu/~EWD/transcriptions/EWD07xx/EWD796a.html&lt;/ref&gt; is a variation of heapsort developed by [[Edsger W. Dijkstra|Edsger Dijkstra]] in 1981. Like heapsort, smoothsort's upper bound is [[Big O notation|O]](''n'' log&amp;nbsp;''n''). The advantage of smoothsort is that it comes closer to O(''n'') time if the [[Adaptive sort|input is already sorted to some degree]], whereas heapsort averages O(''n'' log&amp;nbsp;''n'') regardless of the initial sorted state. Due to its complexity, smoothsort is rarely used.
*Levcopoulos and Petersson&lt;ref&gt;{{citation
 | last1 = Levcopoulos | first1 = Christos
 | last2 = Petersson | first2 = Ola
 | contribution = Heapsort—Adapted for Presorted Files
 | doi = 10.1007/3-540-51542-9_41
 | location = London, UK
 | pages = 499–509
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | title = WADS '89: Proceedings of the Workshop on Algorithms and Data Structures
 | volume = 382 | doi = 10.1007/3-540-51542-9_41
 | year = 1989}}.&lt;/ref&gt; describe a variation of heapsort based on a [[Cartesian tree]] that does not add an element to the heap until smaller values on both sides of it have already been included in the sorted output. As they show, this modification can allow the algorithm to sort more quickly than O(''n''&amp;nbsp;log&amp;nbsp;''n'') for inputs that are already nearly sorted.

== Comparison with other sorts ==
Heapsort primarily competes with [[quicksort]], another very efficient general purpose nearly-in-place comparison-based sort algorithm.

Quicksort is typically somewhat faster due to some factors, but the worst-case running time for quicksort is [[Big O notation|O]](''n''&lt;sup&gt;2&lt;/sup&gt;), which is unacceptable for large data sets and can be deliberately triggered given enough knowledge of the implementation, creating a security risk. See [[quicksort]] for a detailed discussion of this problem and possible solutions.

Thus, because of the O(''n'' log ''n'') upper bound on heapsort's running time and constant upper bound on its auxiliary storage, embedded systems with real-time constraints or systems concerned with security often use heapsort.{{Citation needed|date=September 2014}}

Heapsort also competes with [[merge sort]], which has the same time bounds. Merge sort requires Ω(n) auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster in practice on machines with small or slow [[data cache]]s. On the other hand, merge sort has several advantages over heapsort:
* Merge sort on arrays has considerably better data cache performance, often outperforming heapsort on modern desktop computers because merge sort frequently accesses contiguous memory locations (good [[locality of reference]]); heapsort references are spread throughout the heap.
* Heapsort is not a [[stable sort]]; merge sort is stable.
* Merge sort [[parallel algorithm|parallelizes]] well and can achieve close to [[linear speedup]] with a trivial implementation; heapsort is not an obvious candidate for a parallel algorithm.
* Merge sort can be adapted to operate on '''singly''' [[linked list]]s with O(1) extra space. Heapsort can be adapted to operate on '''doubly''' linked lists with only O(1) extra space overhead.{{Citation needed|date=June 2012}}
* Merge sort is used in [[external sorting]]; heapsort is not. Locality of reference is the issue.

[[Introsort]] is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort.

== Pseudocode ==

The following is the &quot;simple&quot; way to implement the algorithm in [[pseudocode]]. Arrays are '''[[Comparison of programming languages (array)|zero-based]]''' and ''swap'' is used to exchange two elements of the array. Movement 'down' means from the root towards the leaves, or from lower indices to higher. Note that during the sort, the largest element is at the root of the heap at a[0], while at the end of the sort, the largest element is in a[end].

 '''function''' heapsort(a, count) '''is'''
     '''input:''' an unordered array ''a'' of length ''count''
  
     ''(Build the heap in array a so that largest value is at the root)''
     heapify(a, count)
 
     ''(The following loop maintains the [[Loop invariant|invariants]] that a[0:end] is a heap and every element''
      ''beyond end is greater than everything before it (so a[end:count] is in sorted order))''
     end ← count - 1
     '''while''' end &gt; 0 '''do'''
         ''(a[0] is the root and largest value. The swap moves it in front of the sorted elements.)''
         swap(a[end], a[0])
         ''(the heap size is reduced by one)''
         end ← end - 1
         ''(the swap ruined the heap property, so restore it)''
         siftDown(a, 0, end)

The sorting routine uses two subroutines, ''heapify'' and ''siftDown''. The former is the common in-place heap construction routine, while the second is a common subroutine for implementing ''heapify''.
 
 ''(Put elements of a in heap order, in-place)''
 '''function''' heapify(a, count) '''is'''
     ''(start is assigned the index in a of the last parent node)''
     ''(the last element in a 0-based array is at index count-1; find the parent of that element )''
     start ← floor ((count - 2 ) / 2)
     
     '''while''' start ≥ 0 '''do'''
         ''(sift down the node at index start to the proper place such that all nodes below''
         '' the start index are in heap order)''
         siftDown(a, start, count-1)
         ''(go to the next parent node)''
         start ← start - 1
     ''(after sifting down the root all nodes/elements are in heap order)''
 
 '''function''' siftDown(a, start, end) '''is'''
     root ← start
 
     '''while''' root * 2 + 1 ≤ end '''do'''    ''(While the root has at least one child)''
         child ← root * 2 + 1       ''(left child)''
         swap ← root                ''(keeps track of child to swap with)''
 
         '''if''' a[swap] &lt; a[child]
             swap ← child
         ''(if there is a right child and that child is greater)''
         '''if''' child+1 ≤ end '''and''' a[swap] &lt; a[child+1]
             swap ← child + 1
         '''if''' swap ≠ root
             swap(a[root], a[swap])
             root ← swap            ''(repeat to continue sifting down the child now)''
         '''else'''
             '''return'''

The heapify function can be thought of as building a heap from the bottom up, successively shifting downward to establish the [[Heap (data structure)|heap property]]. An alternative version (shown below) that builds the heap top-down and sifts upward may be conceptually simpler to grasp. This &quot;siftUp&quot; version can be visualized as starting with an empty heap and successively inserting elements, whereas the &quot;siftDown&quot; version given above treats the entire input array as a full, &quot;broken&quot; heap and &quot;repairs&quot; it starting from the last non-trivial sub-heap (that is, the last parent node).

[[File:Binary heap bottomup vs topdown.svg|thumb|right|Difference in time complexity between the &quot;siftDown&quot; version and the &quot;siftUp&quot; version.]]
Also, the &quot;siftDown&quot; version of heapify [[Binary heap#Building a heap|has {{math|''O''(''n'')}} time complexity]], while the &quot;siftUp&quot; version given below has {{math|''O''(''n'' log ''n'')}} time complexity due to its equivalence with inserting each element, one at a time, into an empty heap.&lt;ref&gt;{{cite web|title=Priority Queues|url=http://faculty.simpson.edu/lydia.sinapova/www/cmsc250/LN250_Weiss/L10-PQueues.htm|accessdate=24 May 2011}}&lt;/ref&gt;
This may seem counter-intuitive since, at a glance, it is apparent that the former only makes half as many calls to its logarithmic-time sifting function as the latter; i.e., they seem to differ only by a constant factor, which never has an impact on asymptotic analysis.

To grasp the intuition behind this difference in complexity, note that the number of swaps that may occur during any one siftUp call ''increases'' with the depth of the node on which the call is made. The crux is that there are many (exponentially many) more &quot;deep&quot; nodes than there are &quot;shallow&quot; nodes in a heap, so that siftUp may have its full logarithmic running-time on the approximately linear number of calls made on the nodes at or near the &quot;bottom&quot; of the heap. On the other hand, the number of swaps that may occur during any one siftDown call ''decreases'' as the depth of the node on which the call is made increases. Thus, when the &quot;siftDown&quot; heapify begins and is calling siftDown on the bottom and most numerous node-layers, each sifting call will incur, at most, a number of swaps equal to the &quot;height&quot; (from the bottom of the heap) of the node on which the sifting call is made. In other words, about half the calls to siftDown will have at most only one swap, then about a quarter of the calls will have at most two swaps, etc.

The heapsort algorithm itself has {{math|''O''(''n'' log ''n'')}} time complexity using either version of heapify.

  '''function''' heapify(a,count) is
      ''(end is assigned the index of the first (left) child of the root)''
      end := 1
      
      '''while''' end &lt; count
          ''(sift up the node at index end to the proper place such that all nodes above''
          '' the end index are in heap order)''
          siftUp(a, 0, end)
          end := end + 1
      ''(after sifting up the last node all nodes are in heap order)''
  
  '''function''' siftUp(a, start, end) '''is'''
      '''input: ''' ''start represents the limit of how far up the heap to sift.''
                    ''end is the node to sift up.''
      child := end 
      '''while''' child &gt; start
          parent := floor((child - 1) / 2)
          '''if''' a[parent] &lt; a[child] '''then''' ''(out of max-heap order)''
              swap(a[parent], a[child])
              child := parent ''(repeat to continue sifting up the parent now)''
          '''else'''
              '''return'''

== Example ==
Let { 6, 5, 3, 1, 8, 7, 2, 4 } be the list that we want to sort from the smallest to the largest. (NOTE, for 'Building the Heap' step: Larger nodes don't stay below smaller node parents. They are swapped with parents, and then recursively checked if another swap is needed, to keep larger numbers above smaller numbers on the heap binary tree.)
[[File:Heapsort-example.gif|350px|thumb|right|An example on heapsort.]]
'''1. Build the heap'''
{| class=&quot;wikitable&quot;
|-
! Heap !! newly added element !! swap elements
|-
| nil || 6 ||  
|-
| 6 || 5 ||  
|-
| 6, 5 || 3 ||  
|-
| 6, 5, 3 || 1 ||  
|-
| 6, 5, 3, 1 || 8 ||  
|-
| 6, '''5''', 3, 1, '''8 '''||   || 5, 8 
|-
| '''6''', '''8''', 3, 1, 5 ||   || 6, 8 
|-
| 8, 6, 3, 1, 5 || 7 ||  
|-
| 8, 6, '''3''', 1, 5, '''7''' ||   || 3, 7 
|-
| 8, 6, 7, 1, 5, 3 || 2 ||   
|-
| 8, 6, 7, 1, 5, 3, 2 || 4 ||  
|-
| 8, 6, 7, '''1''', 5, 3, 2, '''4''' ||   || 1, 4
|-
| 8, 6, 7, 4, 5, 3, 2, 1 ||   ||  
|}

'''2. Sorting.'''
{| class=&quot;wikitable&quot;
|-
! Heap !! swap elements !! delete element !! sorted array !! details
|-
| '''8''', 6, 7, 4, 5, 3, 2, '''1''' || 8, 1 ||   ||   || swap 8 and 1 in order to delete 8 from heap
|-
| 1, 6, 7, 4, 5, 3, 2, '''8''' ||   || 8 ||   || delete 8 from heap and add to sorted array
|-
| '''1''', 6, '''7''', 4, 5, 3, 2 || 1, 7 ||   || 8 || swap 1 and 7 as they are not in order in the heap
|-
| 7, 6, '''1''', 4, 5, '''3''', 2 || 1, 3 ||   || 8 || swap 1 and 3 as they are not in order in the heap 
|-
| '''7''', 6, 3, 4, 5, 1, '''2''' || 7, 2 ||   || 8 || swap 7 and 2 in order to delete 7 from heap
|-
| 2, 6, 3, 4, 5, 1, '''7''' ||   || 7 || 8 || delete 7 from heap and add to sorted array
|-
| '''2''', '''6''', 3, 4, 5, 1 || 2, 6 ||   || 7, 8 || swap 2 and 6 as they are not in order in the heap
|-
| 6, '''2''', 3, 4, '''5''', 1 || 2, 5 ||   || 7, 8 || swap 2 and 5 as they are not in order in the heap
|-
| '''6''', 5, 3, 4, 2, '''1'''|| 6, 1 ||   || 7, 8 || swap 6 and 1 in order to delete 6 from heap
|-
| 1, 5, 3, 4, 2, '''6''' ||   || 6 || 7, 8 || delete 6 from heap and add to sorted array
|-
| '''1''', '''5''', 3, 4, 2 || 1, 5 ||   || 6, 7, 8 || swap 1 and 5 as they are not in order in the heap 
|-
| 5, '''1''', 3, '''4''', 2 || 1, 4 ||   || 6, 7, 8 || swap 1 and 4 as they are not in order in the heap 
|-
| '''5''', 4, 3, 1, '''2''' || 5, 2 ||   || 6, 7, 8 || swap 5 and 2 in order to delete 5 from heap 
|-
| 2, 4, 3, 1, '''5''' ||   || 5 || 6, 7, 8 || delete 5 from heap and add to sorted array 
|-
| '''2''', '''4''', 3, 1 || 2, 4 ||   || 5, 6, 7, 8 || swap 2 and 4 as they are not in order in the heap 
|-
| '''4''', 2, 3, '''1''' || 4, 1 ||   || 5, 6, 7, 8 || swap 4 and 1 in order to delete 4 from heap 
|-
| 1, 2, 3, '''4''' ||   || 4 || 5, 6, 7, 8 || delete 4 from heap and add to sorted array 
|-
| '''1''', 2, '''3''' || 1, 3 ||   || 4, 5, 6, 7, 8 || swap 1 and 3 as they are not in order in the heap 
|-
| '''3''', 2, '''1''' || 3, 1 ||   || 4, 5, 6, 7, 8 || swap 3 and 1 in order to delete 3 from heap
|-
| 1, 2, '''3''' ||   || 3 || 4, 5, 6, 7, 8 || delete 3 from heap and add to sorted array 
|-
| '''1''', '''2''' || 1, 2 ||   || 3, 4, 5, 6, 7, 8 || swap 1 and 2 as they are not in order in the heap 
|-
| '''2''', '''1''' || 2, 1 ||   || 3, 4, 5, 6, 7, 8 || swap 2 and 1 in order to delete 2 from heap
|-
| 1, '''2''' ||   || 2 || 3, 4, 5, 6, 7, 8 || delete 2 from heap and add to sorted array 
|-
| '''1''' ||   || 1 || 2, 3, 4, 5, 6, 7, 8 || delete 1 from heap and add to sorted array 
|-
|   ||   ||   || 1, 2, 3, 4, 5, 6, 7, 8 || completed
|}

== Notes ==
{{reflist|30em}}

== References ==
* {{Citation |first=J. W. J. |last=Williams |author-link=J. W. J. Williams |title=Algorithm 232 - Heapsort |year=1964 |journal=[[Communications of the ACM]] |volume=7 |issue=6 |pages=347–348 |doi= }}
* {{Citation |first=Robert W. |last=Floyd |author-link=Robert W. Floyd |title=Algorithm 245 - Treesort 3 |year=1964 |journal=[[Communications of the ACM]] |volume=7 |issue=12 |page=701 |doi= 10.1145/355588.365103}}
* {{Citation |first=Svante |last=Carlsson |author-link=Svante Carlsson |title=Average-case results on heapsort |year=1987 |journal=BIT |volume=27 |issue=1 |pages=2–17 |doi= 10.1007/bf01937350}}
* {{Citation |first=Donald |last=Knuth |author-link=Donald Knuth |series=[[The Art of Computer Programming]] |volume=3 |title=Sorting and Searching |edition=third |publisher=Addison-Wesley |year=1997 |isbn=0-201-89685-0 |pages=144–155 |contribution=&amp;sect;5.2.3, Sorting by Selection }}
* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Chapters 6 and 7 Respectively: Heapsort and Priority Queues
* [http://www.cs.utexas.edu/users/EWD/ewd07xx/EWD796a.PDF A PDF of Dijkstra's original paper on Smoothsort]
* [http://cis.stvincent.edu/html/tutorials/swd/heaps/heaps.html Heaps and Heapsort Tutorial] by David Carlson, St. Vincent College

== External links ==
{{wikibooks|Algorithm implementation|Sorting/Heapsort|Heapsort}}
* [http://www.sorting-algorithms.com/heap-sort Animated Sorting Algorithms: Heap Sort] –  graphical demonstration and discussion of heap sort
* [http://olli.informatik.uni-oldenburg.de/heapsort_SALA/english/start.html Courseware on Heapsort from Univ. Oldenburg] - With text, animations and interactive exercises
* [http://www.nist.gov/dads/HTML/heapSort.html NIST's Dictionary of Algorithms and Data Structures: Heapsort]
* [http://www.codecodex.com/wiki/Heapsort Heapsort implemented in 12 languages]
* [http://www.azillionmonkeys.com/qed/sort.html Sorting revisited] by Paul Hsieh
* [http://coderaptors.com/?HeapSort A color graphical Java applet] that allows experimentation with initial state and shows statistics
* [http://employees.oneonta.edu/zhangs/powerPointPlatform/index.php A PowerPoint presentation demonstrating how Heap sort works] that is for educators.
* [http://opendatastructures.org/versions/edition-0.1e/ods-java/11_1_Comparison_Based_Sorti.html#SECTION001413000000000000000 Open Data Structures - Section 11.1.3 - Heap-Sort]

{{sorting}}

[[Category:Sorting algorithms]]
[[Category:Comparison sorts]]
[[Category:Heaps (data structures)]]
[[Category:Articles with example pseudocode]]

[[no:Sorteringsalgoritme#Heap sort]]</text>
      <sha1>6zdwvv01ed64ruzz75ay45i9da8b3xr</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Fibonacci heap</title>
    <ns>0</ns>
    <id>254142</id>
    <revision>
      <id>626738184</id>
      <parentid>626738183</parentid>
      <timestamp>2014-09-23T08:34:42Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor/>
      <comment>Reverting possible vandalism by [[Special:Contributions/14.139.238.98|14.139.238.98]] to version by Wingedsubmariner. False positive? [[User:ClueBot NG/FalsePositives|Report it]]. Thanks, [[User:ClueBot NG|ClueBot NG]]. (1963227) (Bot)</comment>
      <text xml:space="preserve" bytes="15307">In [[computer science]], a '''Fibonacci heap''' is a [[Heap (data structure)|heap data structure]] consisting of a collection of [[Tree (data structure)|tree]]s. It has a better [[amortized analysis|amortized]] running time than a [[binomial heap]]. Fibonacci heaps were developed by [[Michael Fredman|Michael L. Fredman]] and [[Robert Tarjan|Robert E. Tarjan]] in 1984 and first published in a scientific journal in 1987. The name of Fibonacci heap comes from [[Fibonacci number]]s which are used in the running time analysis.

Find-minimum is ''O''(1) amortized time.&lt;ref name=&quot;CLRS&quot;&gt;[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. [[MIT Press]] and McGraw-Hill, 2001. ISBN 0-262-03293-7. Chapter 20: Fibonacci Heaps, pp.476&amp;ndash;497. Third edition p518.&lt;/ref&gt; Operations insert, decrease key, and merge (union) work in constant amortized time.&lt;ref name=&quot;Fredman And Tarjan&quot;/&gt; Operations delete and delete minimum work in [[Big O notation|''O'']](log ''n'') amortized time.&lt;ref name=&quot;Fredman And Tarjan&quot;/&gt; This means that starting from an empty data structure, any sequence of ''a'' operations from the first group and ''b'' operations from the second group would take ''O''(''a''&amp;nbsp;+&amp;nbsp;''b''&amp;nbsp;log&amp;nbsp;''n'') time. In a [[binomial heap]] such a sequence of operations would take ''O''((''a''&amp;nbsp;+&amp;nbsp;''b'') log (''n'')) time. A Fibonacci heap is thus better than a binomial heap when ''b'' is [[asymptotic analysis|asymptotically]] smaller than ''a''.

Using Fibonacci heaps for [[priority queue]]s improves the asymptotic running time of important algorithms, such as [[Dijkstra's algorithm]] for computing the [[shortest path]] between two nodes in a graph.

== Structure ==
[[Image:Fibonacci heap.png|thumbnail|250px|Figure 1. Example of a Fibonacci heap. It has three trees of degrees 0, 1 and 3. Three vertices are marked (shown in blue). Therefore the potential of the heap is 9 (3 trees + 2 * marked-vertices).]]
A Fibonacci heap is a collection of [[tree data structure|tree]]s satisfying the [[minimum-heap property]], that is, the key of a child is always greater than or equal to the key of the parent. This implies that the minimum key is always at the root of one of the trees. Compared with binomial heaps, the structure of a Fibonacci heap is more flexible. The trees do not have a prescribed shape and in the extreme case the heap can have every element in a separate tree. This flexibility allows some operations to be executed in a &quot;lazy&quot; manner, postponing the work for later operations. For example merging heaps is done simply by concatenating the two lists of trees, and operation ''decrease key'' sometimes cuts a node from its parent and forms a new tree.

However at some point some order needs to be introduced to the heap to achieve the desired running time. In particular, degrees of nodes (here degree means the number of children) are kept quite low: every node has degree at most ''O''(log ''n'') and the size of a subtree rooted in a node of degree ''k'' is at least  ''F&lt;sub&gt;k&lt;/sub&gt;''&lt;sub&gt;&amp;nbsp;+&amp;nbsp;2&lt;/sub&gt;, where ''F&lt;sub&gt;k&lt;/sub&gt;'' is the ''k''th [[Fibonacci number]]. This is achieved by the rule that we can cut at most one child of each non-root node. When a second child is cut, the node itself needs to be cut from its parent and becomes the root of a new tree (see Proof of degree bounds, below). The number of trees is decreased in the operation ''delete minimum'', where trees are linked together.

As a result of a relaxed structure, some operations can take a long time while others are done very quickly. For the [[amortized analysis|amortized running time]] analysis we use the [[potential method]], in that we pretend that very fast operations take a little bit longer than they actually do. This additional time is then later combined and subtracted from the actual running time of slow operations. The amount of time saved for later use is measured at any given moment by a potential function. The potential of a Fibonacci heap is given by

:Potential = ''t'' + 2''m''

where ''t'' is the number of trees in the Fibonacci heap, and ''m'' is the number of marked nodes. A node is marked if at least one of its children was cut since this node was made a child of another node (all roots are unmarked).
The amortized time for an operation is given by the sum of the actual time and ''c'' times the difference in potential, where ''c'' is a constant (chosen to match the constant factors in the ''O'' notation for the actual time).

Thus, the root of each tree in a heap has one unit of time stored. This unit of time can be used later to link this tree with another tree at amortized time 0. Also, each marked node has two units of time stored. One can be used to cut the node from its parent. If this happens, the node becomes a root and the second unit of time will remain stored in it as in any other root.

== Implementation of operations ==
To allow fast deletion and concatenation, the roots of all trees are linked using a circular, [[doubly linked list]]. The children of each node are also linked using such a list. For each node, we maintain its number of children and whether the node is marked. Moreover we maintain a pointer to the root containing the minimum key.

Operation '''find minimum''' is now trivial because we keep the pointer to the node containing it. It does not change the potential of the heap, therefore both actual and amortized cost is constant. 

As mentioned above, '''merge''' is implemented simply by concatenating the lists of tree roots of the two heaps. This can be done in constant time and the potential does not change, leading again to constant amortized time.

Operation '''insert''' works by creating a new heap with one element and doing merge. This takes constant time, and the potential increases by one, because the number of trees increases. The amortized cost is thus still constant.

[[Image:Fibonacci heap extractmin1.png|170px|thumb|right|Fibonacci heap from Figure 1 after first phase of extract minimum. Node with key 1 (the minimum) was deleted and its children were added as separate trees.]]
Operation '''extract minimum''' (same as ''delete minimum'') operates in three phases. First we take the root containing the minimum element and remove it. Its children will become roots of new trees. If the number of children was ''d'', it takes time ''O''(''d'') to process all new roots and the potential increases by ''d''−1. Therefore the amortized running time of this phase is ''O''(''d'') = ''O''(log ''n'').

[[Image:Fibonacci heap extractmin2.png|130px|thumb|left|Fibonacci heap from Figure 1 after extract minimum is completed. First, nodes 3 and 6 are linked together. Then the result is linked with tree rooted at node 2. Finally, the new minimum is found.]]
However to complete the extract minimum operation, we need to update the pointer to the root with minimum key. Unfortunately there may be up to ''n'' roots we need to check. In the second phase we therefore decrease the number of roots by successively linking together roots of the same degree. When two roots ''u'' and ''v'' have the same degree, we make one of them a child of the other so that the one with the smaller key remains the root. Its degree will increase by one. This is repeated until every root has a different degree. To find trees of the same degree efficiently we use an array of length ''O''(log ''n'') in which we keep a pointer to one root of each degree. When a second root is found of the same degree, the two are linked and the array is updated. The actual running time is ''O''(log ''n'' + ''m'') where ''m'' is the number of roots at the beginning of the second phase. At the end we will have at most ''O''(log ''n'') roots (because each has a different degree). Therefore the difference in the potential function from before this phase to after it is: ''O''(log ''n'') − ''m'', and the amortized running time is then at most ''O''(log ''n'' + ''m'') + ''c''(''O''(log ''n'') − ''m''). With a sufficiently large choice of ''c'', this simplifies to ''O''(log ''n'').

In the third phase we check each of the remaining roots and find the minimum. This takes ''O''(log ''n'') time and the potential does not change. The overall amortized running time of extract minimum is therefore ''O''(log ''n'').

[[Image:Fibonacci heap-decreasekey.png|250px|thumb|right|Fibonacci heap from Figure 1 after decreasing key of node 9 to 0. This node as well as its two marked ancestors are cut from the tree rooted at 1 and placed as new roots.]]
Operation '''decrease key''' will take the node, decrease the key and if the heap property becomes violated (the new key is smaller than the key of the parent), the node is cut from its parent. If the parent is not a root, it is marked. If it has been marked already, it is cut as well and its parent is marked. We continue upwards until we reach either the root or an unmarked node. In the process we create some number, say ''k'', of new trees. Each of these new trees except possibly the first one was marked originally but as a root it will become unmarked. One node can become marked. Therefore the number of marked nodes changes by &amp;minus;(''k''&amp;nbsp;&amp;minus;&amp;nbsp;1)&amp;nbsp;+&amp;nbsp;1&amp;nbsp;=&amp;nbsp;&amp;minus;&amp;nbsp;''k''&amp;nbsp;+&amp;nbsp;2. Combining these 2 changes, the potential changes by 2(&amp;minus;''k''&amp;nbsp;+&amp;nbsp;2)&amp;nbsp;+&amp;nbsp;''k''&amp;nbsp;=&amp;nbsp;&amp;minus;''k''&amp;nbsp;+&amp;nbsp;4. The actual time to perform the cutting was ''O''(''k''), therefore (again with a sufficiently large choice of ''c'') the amortized running time is constant.

Finally, operation '''delete''' can be implemented simply by decreasing the key of the element to be deleted to minus infinity, thus turning it into the minimum of the whole heap. Then we call extract minimum to remove it. The amortized running time of this operation is ''O''(log ''n'').

==Proof of degree bounds==
The amortized performance of a Fibonacci heap depends on the degree (number of children) of any tree root being ''O''(log ''n''), where ''n'' is the size of the heap.  Here we show that the size of the (sub)tree rooted at any node ''x'' of degree ''d'' in the heap must have size at least ''F&lt;sub&gt;d&lt;/sub&gt;''&lt;sub&gt;+2&lt;/sub&gt;, where ''F&lt;sub&gt;k&lt;/sub&gt;'' is the ''k''th [[Fibonacci number]].  The degree bound follows from this and the fact (easily proved by induction) that &lt;math&gt;F_{d+2} \ge \varphi^d&lt;/math&gt; for all integers &lt;math&gt;d\ge 0&lt;/math&gt;, where &lt;math&gt;\varphi = (1+\sqrt 5)/2 \doteq 1.618&lt;/math&gt;.  (We then have &lt;math&gt;n \ge F_{d+2} \ge \varphi^d&lt;/math&gt;, and taking the log to base &lt;math&gt;\varphi&lt;/math&gt; of both sides gives &lt;math&gt;d\le \log_{\varphi} n&lt;/math&gt; as required.)

Consider any node ''x'' somewhere in the heap (''x'' need not be the root of one of the main trees).  Define '''size'''(''x'') to be the size of the tree rooted at ''x'' (the number of descendants of ''x'', including ''x'' itself).  We prove by induction on the height of ''x'' (the length of a longest simple path from ''x'' to a descendant leaf), that '''size'''(''x'')&amp;nbsp;≥&amp;nbsp;''F&lt;sub&gt;d&lt;/sub&gt;''&lt;sub&gt;+2&lt;/sub&gt;, where ''d'' is the degree of ''x''.

'''Base case:''' If ''x'' has height 0, then ''d''&amp;nbsp;=&amp;nbsp;0, and '''size'''(''x'')&amp;nbsp;=&amp;nbsp;1&amp;nbsp;=&amp;nbsp;''F''&lt;sub&gt;2&lt;/sub&gt;.

'''Inductive case:'''  Suppose ''x'' has positive height and degree ''d''&amp;gt;0.  Let ''y''&lt;sub&gt;1&lt;/sub&gt;, ''y''&lt;sub&gt;2&lt;/sub&gt;, ..., ''y&lt;sub&gt;d&lt;/sub&gt;'' be the children of ''x'', indexed in order of the times they were most recently made children of ''x'' (''y''&lt;sub&gt;1&lt;/sub&gt; being the earliest and ''y&lt;sub&gt;d&lt;/sub&gt;'' the latest), and let ''c''&lt;sub&gt;1&lt;/sub&gt;, ''c''&lt;sub&gt;2&lt;/sub&gt;, ..., ''c&lt;sub&gt;d&lt;/sub&gt;'' be their respective degrees.  We '''claim''' that ''c&lt;sub&gt;i&lt;/sub&gt;''&amp;nbsp;≥&amp;nbsp;''i''-2 for each ''i'' with 2≤''i''≤''d'': Just before ''y&lt;sub&gt;i&lt;/sub&gt;'' was made a child of ''x'', ''y''&lt;sub&gt;1&lt;/sub&gt;,...,''y&lt;sub&gt;i&lt;/sub&gt;''&lt;sub&gt;−1&lt;/sub&gt; were already children of ''x'', and so ''x'' had degree at least ''i''−1 at that time.  Since trees are combined only when the degrees of their roots are equal, it must have been that ''y&lt;sub&gt;i&lt;/sub&gt;'' also had degree at least ''i''-1 at the time it became a child of ''x''.  From that time to the present, ''y&lt;sub&gt;i&lt;/sub&gt;'' can only have lost at most one child (as guaranteed by the marking process), and so its current degree ''c&lt;sub&gt;i&lt;/sub&gt;'' is at least ''i''−2.  This proves the '''claim'''.

Since the heights of all the ''y&lt;sub&gt;i&lt;/sub&gt;'' are strictly less than that of ''x'', we can apply the inductive hypothesis to them to get '''size'''(''y&lt;sub&gt;i&lt;/sub&gt;'')&amp;nbsp;≥&amp;nbsp;''F&lt;sub&gt;c&lt;sub&gt;i&lt;/sub&gt;&lt;/sub&gt;''&lt;sub&gt;+2&lt;/sub&gt;&amp;nbsp;≥&amp;nbsp;''F''&lt;sub&gt;(''i''−2)+2&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''F&lt;sub&gt;i&lt;/sub&gt;''.  The nodes ''x'' and ''y''&lt;sub&gt;1&lt;/sub&gt; each contribute at least 1 to '''size'''(''x''), and so we have

&lt;math&gt;\textbf{size}(x) \ge 2 + \sum_{i=2}^d \textbf{size}(y_i) \ge 2 + \sum_{i=2}^d F_i = 1 + \sum_{i=0}^d F_i.&lt;/math&gt;

A routine induction proves that &lt;math&gt;1 + \sum_{i=0}^d F_i = F_{d+2}&lt;/math&gt; for any &lt;math&gt;d\ge 0&lt;/math&gt;, which gives the desired lower bound on '''size'''(''x'').

==Worst case==
Although the total running time of a sequence of operations starting with an empty structure is bounded by the bounds given above, some (very few) operations in the sequence can take very long to complete (in particular delete and delete minimum have linear running time in the worst case). For this reason Fibonacci heaps and other amortized data structures may not be appropriate for [[real-time computing|real-time systems]]. It is possible to create a data structure which has the same worst-case performance as the Fibonacci heap has amortized performance.&lt;ref name=&quot;bare_url&quot;&gt;{{Citation |id = [[CiteSeerX]]: {{url|1=citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.8133|2=10.1.1.43.8133}} |title=Worst-Case Efficient Priority Queues |year=1996 |author=Gerth Stølting Brodal |journal=Proc. 7th ACM-SIAM Symposium on Discrete Algorithms |publisher = [[Society for Industrial and Applied Mathematics]]|pages=52–58 |doi=10.1145/313852.313883 |isbn=0-89871-366-8}}&lt;/ref&gt; However the resulting structure (a [[Brodal queue]]) is, in the words of the creator, &quot;quite complicated&quot; and &quot;[not] applicable in practice.&quot;

==Summary of running times==
{{Heap Running Times}}

== References ==
{{reflist}}

== External links ==
* [http://www.cs.yorku.ca/~aaw/Jason/FibonacciHeapAnimation.html Java applet simulation of a Fibonacci heap]
* [http://www.mathworks.com/matlabcentral/fileexchange/30072-fibonacci-heap MATLAB implementation of Fibonacci heap]
* [http://www.labri.fr/perso/pelegrin/code/#fibonacci De-recursived and memory efficient C implementation of Fibonacci heap] (free/libre software, [http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html CeCILL-B license])
* [http://github.com/evansenter/f_heap Ruby implementation of the Fibonacci heap (with tests)]
* [http://www.cs.princeton.edu/~wayne/cs423/fibonacci/FibonacciHeapAlgorithm.html Pseudocode of the Fibonacci heap algorithm]
* [http://stackoverflow.com/q/6273833/194609 Various Java Implementations for Fibonacci heap]

{{Data structures}}

{{DEFAULTSORT:Fibonacci Heap}}
[[Category:Fibonacci numbers]]
[[Category:Heaps (data structures)]]</text>
      <sha1>fayucdfh98jy6bcg4ae5v312hmolew1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Rolling hash</title>
    <ns>0</ns>
    <id>4071549</id>
    <revision>
      <id>616260631</id>
      <parentid>610053709</parentid>
      <timestamp>2014-07-09T18:32:20Z</timestamp>
      <contributor>
        <ip>2001:4898:80E8:EE31:0:0:0:2</ip>
      </contributor>
      <comment>/* Rabin-Karp rolling hash */ De-abbreved chars to characters</comment>
      <text xml:space="preserve" bytes="6641">A '''rolling hash''' is a [[hash function]] where the input is hashed in a window that moves through the input.

A few hash functions allow a rolling hash to be computed very quickly—the new hash value is rapidly calculated given only the old hash value, the old value removed from the window, and the new value added to the window—similar to the way a [[moving average]] function can be computed much more quickly than other low-pass filters.

One of the main applications is the [[Rabin-Karp string search algorithm]], which uses the rolling hash described below.

Another popular application is [[rsync]] program which uses a checksum based on Mark Adler's [[adler-32]] as its rolling hash.

Another application is the Low Bandwidth Network Filesystem (LBFS), which uses a [[Rabin fingerprint]] as its rolling hash.

At best, rolling hash values are [[pairwise independent]]&lt;ref name=&quot;lemirekaser&quot;&gt;Daniel Lemire, Owen Kaser: Recursive n-gram hashing is pairwise independent, at best, Computer Speech &amp; Language 24 (4), pages 698-710, 2010. [[arXiv:0705.4676]]&lt;/ref&gt; or strongly [[universal hashing|universal]]. They cannot be [[k-independent hashing|3-wise independent]], for example.

== Rabin-Karp rolling hash ==

The [[Rabin-Karp string search algorithm]] is normally used with a very simple rolling hash function that only uses multiplications and additions:

&lt;math&gt;H = c_1 a^{k-1} + c_2 a^{k-2} + c_3 a^{k-3} + ... + c_k a^{0}&lt;/math&gt;
where &lt;math&gt;a&lt;/math&gt; is a constant and &lt;math&gt;c_1, ..., c_k&lt;/math&gt; are the input characters.

In order to avoid manipulating huge &lt;math&gt;H&lt;/math&gt; values, all math is done [[modular arithmetic|modulo]] &lt;math&gt;n&lt;/math&gt;. The choice of &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;n&lt;/math&gt; is critical to get good hashing; see [[linear congruential generator]] for more discussion.

Removing and adding characters simply involves adding or subtracting the first or last term. Shifting all characters by one position to the left requires multiplying the entire sum &lt;math&gt;H&lt;/math&gt; by &lt;math&gt;a&lt;/math&gt;. Shifting all characters by one position to the right requires dividing the entire sum &lt;math&gt;H&lt;/math&gt; by &lt;math&gt;a&lt;/math&gt;. Note that in modulo arithmetic, &lt;math&gt;a&lt;/math&gt; can be chosen to have a [[modular multiplicative inverse|multiplicative inverse]] &lt;math&gt;a^{-1}&lt;/math&gt; by which &lt;math&gt;H&lt;/math&gt; can be multiplied to get the result of the division without actually performing a division.

== Content based slicing using Rabin-Karp hash ==
 
One of the interesting use cases of the rolling hash function is that it can create dynamic, content-based chunks of a stream or file. This is especially useful when it is required to send only the changed chunks of a large file over a network and a simple byte addition at the front of the file would cause all the fixed size windows to become updated, while in reality, only the first ‘chunk’ has been modified.

The simplest approach to calculate the dynamic chunks is to calculate the rolling hash and if it matches a pattern (like the lower N bits are all zeroes) then it’s a chunk boundary. This approach will ensure that any change in the file will only affect its current and possibly the next chunk, but nothing else.

When the boundaries are known, the chunks need to be compared by their hash values to detect which one was modified and needs transfer across the network.&lt;ref&gt;{{cite web | first = Adam | last = Horvath | url = http://blog.teamleadnet.com/2012/10/rabin-karp-rolling-hash-dynamic-sized.html | title = Rabin Karp rolling hash - dynamic sized chunks based on hashed content | date = October 24, 2012 }}&lt;/ref&gt;

== Cyclic polynomial ==

Hashing by cyclic polynomial&lt;ref&gt;Jonathan D. Cohen, [http://www.cparity.com/projects/AcmClassification/samples/256168.pdf Recursive Hashing Functions for n-Grams], ACM Trans. Inf. Syst. 15 (3), 1997&lt;/ref&gt;&amp;mdash;sometimes called Buzhash&amp;mdash;is also simple, but it has the benefit of avoiding multiplications, using [[Barrel shifter|barrel shifts]] instead. It is a form of [[tabulation hashing]]: it presumes that there is some hash function &lt;math&gt;h&lt;/math&gt; from characters to integers in the interval &lt;math&gt;[0,2^L)&lt;/math&gt;. This hash function might be simply an array or a [[hash table]] mapping characters to random integers. Let the function &lt;math&gt;s&lt;/math&gt; be a cyclic binary rotation (or [[Barrel shifter|barrel shift]]): it rotates the bits by 1 to the left, pushing the latest bit in the first position. E.g., &lt;math&gt;s(10011)=00111&lt;/math&gt;. Let &lt;math&gt;\oplus&lt;/math&gt; be the bit-wise [[exclusive or]]. The hash values are defined as

&lt;math&gt; H = s^{k-1}(h( c_1 )) \oplus s^{k-2}( h( c_2) )  \oplus \ldots \oplus  s( h( c_{k-1}) ) \oplus   h( c_k)&lt;/math&gt;

where the multiplications by powers of two can be implemented by binary shifts. The result is a number in &lt;math&gt;[0,2^L)&lt;/math&gt;.

Computing the hash values in a rolling fashion is done as follows. Let &lt;math&gt;H&lt;/math&gt; be the previous hash value. Rotate &lt;math&gt;H&lt;/math&gt; once: &lt;math&gt;H\leftarrow s(H)&lt;/math&gt;.  If &lt;math&gt;c_1&lt;/math&gt; is the character to be removed, rotate it &lt;math&gt;k&lt;/math&gt; times:  &lt;math&gt;s^{k}(h( c_1 ))&lt;/math&gt;. Then simply set 

&lt;math&gt;H\leftarrow s(H) \oplus s^{k}(h( c_1 )) \oplus h(c_{k+1})&lt;/math&gt; 

where &lt;math&gt;c_{k+1}&lt;/math&gt; is the new character.

Hashing by cyclic polynomials is strongly universal or pairwise independent: simply keep the first &lt;math&gt;L-k+1&lt;/math&gt; bits. That is, take the result &lt;math&gt;H&lt;/math&gt; and dismiss any &lt;math&gt;k-1&lt;/math&gt; consecutive bits.&lt;ref name=&quot;lemirekaser&quot; /&gt; In practice, this can be achieved by an integer division &lt;math&gt;H \rightarrow H \div 2^{k-1}&lt;/math&gt;.

== Computational complexity ==

All rolling hash functions are linear in the number of characters, but their complexity with respect to the length of the window (&lt;math&gt;k&lt;/math&gt;) varies. Rabin-Karp rolling hash requires the multiplications of two &lt;math&gt;k&lt;/math&gt;-bit numbers, [[integer multiplication]] is in &lt;math&gt;O(k \log k 2^{O(\log^*k)})&lt;/math&gt;.&lt;ref&gt;M. Fürer, Faster integer multiplication, in: STOC ’07, 2007, pp. 57–66.&lt;/ref&gt; Hashing [[ngram]]s by cyclic polynomials can be done in linear time.&lt;ref name=&quot;lemirekaser&quot; /&gt;

== Software ==
*[http://code.google.com/p/ngramhashing/ ngramhashing] is a [[Free software]] C++ implementation of several rolling hash functions
*[http://code.google.com/p/rollinghashjava/ rollinghashjava] is an Apache licensed Java implementation of rolling hash functions

==See also==
*[[MinHash]]
*[[w-shingling]]

== External links ==
*[http://courses.csail.mit.edu/6.006/spring11/rec/rec06.pdf MIT 6.006: Introduction to Algorithms 2011- Lecture Notes - Rolling Hash]

== Footnotes ==
&lt;references/&gt;

[[Category:Hash functions]]</text>
      <sha1>tryy3mokd6l7mi1dltapoic2x485l7d</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Locality preserving hashing</title>
    <ns>0</ns>
    <id>2992597</id>
    <revision>
      <id>501690233</id>
      <parentid>483297800</parentid>
      <timestamp>2012-07-11T08:58:53Z</timestamp>
      <contributor>
        <username>Phil Boswell</username>
        <id>24373</id>
      </contributor>
      <comment>{{cite doi}}</comment>
      <text xml:space="preserve" bytes="1025">In [[computer science]], a '''locality preserving hashing''' is a [[hash function]] ''f'' that maps a point or points in a multidimensional [[coordinate space]] to a scalar value, such that if we have three points ''A'', ''B'' and ''C'' such that 
:&lt;math&gt;|A-B| &lt; |B-C| \Rightarrow |f(A) - f(B)| &lt; |f(B) - f(C)|. \,&lt;/math&gt;

In other words, these are hash functions where the relative distance between the input values is preserved in the relative distance between of the output hash values; input values that are closer to each other will produce output hash values that are closer to each other.

This is in contrast to [[cryptography|cryptographic]] hash functions and [[checksum]]s, which are designed to have [[Avalanche effect|maximum output difference between adjacent inputs]].

Locality preserving hashes are related to [[space-filling curve]]s and [[locality sensitive hashing]].

==External links==
*{{cite doi|10.1145/258533.258656}}
*{{cite doi|10.1007/BF01185209}}

[[Category:Hash functions]]


{{comp-sci-stub}}</text>
      <sha1>0nohg03mdrxs97ky1debqdro1sm1lb7</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Bitstate hashing</title>
    <ns>0</ns>
    <id>21890341</id>
    <revision>
      <id>287405139</id>
      <parentid>284423335</parentid>
      <timestamp>2009-05-02T07:44:34Z</timestamp>
      <contributor>
        <username>Andreas Kaufmann</username>
        <id>72502</id>
      </contributor>
      <minor/>
      <comment>Removed category [[:Category:Search algorithms|Search algorithms]] (using [[WP:HOTCAT|HotCat]])</comment>
      <text xml:space="preserve" bytes="1524">'''Bitstate hashing''' is a [[hashing]] method invented in 1968 by Morris.&lt;ref&gt;Morris, R. (1968). ''Scatter Storage Techniques''&lt;/ref&gt; It is used for state hashing, where each state (e.g. of an automaton) is represented by a number and it is passed to some [[hash function]].

The result of the function is then taken as the index to an array of bits (a [[bit-field]]), where one looks for 1 if the state was already seen before or stores 1 by itself if not.

It usually serves as a yes–no technique without a need of storing whole state bit representation.

A shortcoming of this framework is losing precision like in other hashing techniques. Hence some tools use this technique with more than one hash function so that the bit-field gets widened by the number of used functions, each having its own row. And even after all functions return values (the indices) point to fields with contents equal to 1, the state may be uttered as visited with much higher probability.

== Use ==
* It is utilized in [[SPIN_model_checker|SPIN]] model checker for decision whether a state was already visited by nested-[[depth-first search]] searching algorithm or not. They mention savings of 98% of memory in the case of using one hash function (175 MB to 3 MB) and 92% when two hash functions are used (13 MB). The state coverage dropped to 97% in the former case. &lt;ref&gt;Holzmann, G. J. (2003) Addison Wesley. ''Spin Model Checker, The: Primer and Reference Manual''&lt;/ref&gt;

== References ==
&lt;references/&gt;



[[Category:Hash functions]]</text>
      <sha1>dlq3aacmg10wa867ukxabvkak9w3myb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Jenkins hash function</title>
    <ns>0</ns>
    <id>22263235</id>
    <revision>
      <id>604548721</id>
      <parentid>557423573</parentid>
      <timestamp>2014-04-17T05:10:53Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor/>
      <comment>fixed [[Help:CS1 errors#bad_date|CS1 errors: dates]] to meet [[MOS:DATEFORMAT]] (also [[WP:AWB/GF|General fixes]]) using [[Project:AWB|AWB]] (10069)</comment>
      <text xml:space="preserve" bytes="3734">The '''Jenkins hash functions''' are a collection of (non-[[cryptographic hash function|cryptographic]]) [[hash functions]] for multi-[[byte]] keys designed by [[Robert John Jenkins Junior|Bob Jenkins]].  They can be used also as [[checksum]]s to detect accidental data corruption or detect identical records in a [[database]].  The first one was formally published in 1997.

==The hash functions==

===one-at-a-time===
Jenkins's '''one-at-a-time''' hash is adapted here from a WWW page by Bob Jenkins,&lt;ref name=&quot;dobbsx&quot;&gt;{{cite web|first=Bob|last= Jenkins |year=c. 2006|url=http://www.burtleburtle.net/bob/hash/doobs.html |title=A hash function for hash Table lookup|accessdate = April 16, 2009}}&lt;/ref&gt; which is an expanded version of his [[Dr. Dobbs Journal|Dr. Dobbs]] article.&lt;ref name=&quot;dobbs&quot;&gt;{{cite journal|first=Bob |last=Jenkins |year=1997|title=Hash functions|journal=Dr. Dobbs Journal|date= September 1997}}&lt;/ref&gt;

&lt;source lang=&quot;c&quot;&gt;
uint32_t jenkins_one_at_a_time_hash(char *key, size_t len)
{
    uint32_t hash, i;
    for(hash = i = 0; i &lt; len; ++i)
    {
        hash += key[i];
        hash += (hash &lt;&lt; 10);
        hash ^= (hash &gt;&gt; 6);
    }
    hash += (hash &lt;&lt; 3);
    hash ^= (hash &gt;&gt; 11);
    hash += (hash &lt;&lt; 15);
    return hash;
} 
&lt;/source&gt;

[[Image:JenkinsOneAtATime-3.svg|thumb|255px|right|Avalanche behavior of Jenkins One-at-a-time hash over 3-byte keys]]

The [[avalanche effect|avalanche]] behavior of this hash is shown on the right.

Each of the 24 rows corresponds to a single bit in the 3-byte input key, and each of the 32 columns corresponds to a bit in the output hash.  Colors are chosen by how well the input key bit affects the given output hash bit: a green square indicates good mixing behavior, a yellow square weak mixing behavior, and red would indicate no mixing. Only a few bits in the last byte of the input key are weakly mixed to a minority of bits in the output hash.

The standard implementation of the [[Perl]] programming language includes Jenkins's one-at-a-time hash and [[SipHash]], and uses Jenkins's one-at-a-time hash by default.&lt;ref&gt;
[http://stackoverflow.com/questions/11214270/what-hashing-function-algorithm-does-perl-use &quot;What hashing function/algorithm does Perl use ?&quot;]
&lt;/ref&gt;&lt;ref&gt;
[http://www.perlmonks.org/?node_id=381061 &quot;RFC: perlfeaturedelta&quot;]:
&quot;one-at-a-time hash algorithm ... [was added in version] 5.8.0&quot;
&lt;/ref&gt;&lt;ref&gt;
[http://perl5.git.perl.org/perl.git/blob/HEAD:/hv_func.h &quot;perl: hv_func.h&quot;]
&lt;/ref&gt;

===lookup2===

The '''lookup2''' function was an interim successor to one-at-a-time.  It is the function referred to as &quot;My Hash&quot; in the 1997 Dr. Dobbs journal article, though it has been obsoleted by subsequent functions that Jenkins has released.

===lookup3===
The '''lookup3''' function consumes input in 12 byte (96 bit) chunks.&lt;ref&gt;{{Cite web|first= Bob|last= Jenkins | url=http://www.burtleburtle.net/bob/c/lookup3.c|title= lookup3.c source code| accessdate= April 16, 2009}}&lt;/ref&gt; It may be appropriate when speed is more important than simplicity.  Note, though, that any speed improvement from the use of this hash is only likely to be useful for large keys, and that the increased complexity may also have speed consequences such as preventing an optimizing compiler from inlining the hash function.

===SpookyHash===

In 2011 Jenkins released a new 128-bit hash function called SpookyHash.&lt;ref&gt;{{Cite web|first= Bob|last= Jenkins | url=http://www.burtleburtle.net/bob/hash/spooky.html|title=SpookyHash: a 128-bit noncryptographic hash| accessdate= Jan 29, 2012}}&lt;/ref&gt;  SpookyHash is significantly faster than lookup3.

==See also==
* [[CityHash]]
* [[Fowler Noll Vo hash]]
* [[MurmurHash]]

==References==
{{Reflist}}

[[Category:Hash functions]]</text>
      <sha1>3afwiqurab6xl5s27qwcyhbbmkn9hvr</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Pearson hashing</title>
    <ns>0</ns>
    <id>101219</id>
    <revision>
      <id>613569891</id>
      <parentid>613285598</parentid>
      <timestamp>2014-06-19T15:11:41Z</timestamp>
      <contributor>
        <username>Elryacko</username>
        <id>966088</id>
      </contributor>
      <text xml:space="preserve" bytes="5619">'''Pearson hashing'''&lt;ref name=acmref&gt;{{Citation
 |title= Fast Hashing of Variable-Length Text Strings
 |first= Peter K.
 |last= Pearson
 |journal= [[Communications of the ACM]]
 |volume= 33
 |issue= 6
 |pages= 677
 |date= June 1990
 |url= http://portal.acm.org/citation.cfm?id=78978
 |issn=
 |doi= 10.1145/78973.78978}}&lt;/ref&gt;&lt;ref&gt;[http://cs.mwsu.edu/~griffin/courses/2133/downloads/Spring11/p677-pearson.pdf Online PDF file of the CACM paper].&lt;/ref&gt; is a [[hash function]] designed for fast execution on processors with 8-bit [[processor register|register]]s. Given an input consisting of any number of bytes, it produces as output a single byte that is strongly dependent&lt;ref name=acmref/&gt; on every byte of the input. Its implementation requires only a few instructions, plus a 256-byte [[lookup table]] containing a [[permutation]] of the values 0 through 255.

This hash function is a [[CBC-MAC]] that uses an 8-bit [[substitution cipher]] implemented via the substitution table. An 8-bit
[[cipher]] has negligible cryptographic security, so the Pearson hash function is not [[cryptographically strong]]; but it offers these benefits:

* It is extremely simple.
* It executes quickly on resource-limited processors.
* There is no simple class of inputs for which [[hash collision|collision]]s (identical outputs) are especially likely.
* Given a small, privileged set of inputs (e.g., [[reserved word]]s for a [[compiler]]), the permutation table can be adjusted so that those inputs yield distinct hash values, producing what is called a [[perfect hash function]].

One of its drawbacks when compared with other hashing algorithms designed for 8-bit processors is the suggested 256 byte lookup table, which can be prohibitively large for a small [[microcontroller]] with a program memory size on the order of hundreds of bytes. A workaround to this is to use a simple permutation function instead of a table stored in program memory. However, using a too simple function, such as ''T[i] = 255-i'' partly defeats the usability as a hash function as [[anagram]]s will result in the same hash value; using a too complex function, on the other hand, will affect speed negatively.

The algorithm can be described by the following [[pseudocode]], which computes the hash of message&amp;nbsp;''C'' using the permutation table&amp;nbsp;''T'':

&lt;code lang=&quot;pseudo&quot;&gt;
 h := 0
 '''for each''' c '''in''' C '''loop'''
   index := h '''xor''' c
   h := T[index]
 '''end loop'''
 '''return''' h
&lt;/code&gt;

==C implementation to generate 64-bit (16 hex chars) hash==

&lt;source lang=&quot;C&quot; line&gt;
   void xPear16(const unsigned char *x, size_t len, char *hex, size_t hexlen) {
      size_t i, j;
      unsigned char hh[8];
      static const unsigned char T[256] = {
      // 256 values 0-255 in any (random) order suffices
       98,  6, 85,150, 36, 23,112,164,135,207,169,  5, 26, 64,165,219, //  1
       61, 20, 68, 89,130, 63, 52,102, 24,229,132,245, 80,216,195,115, //  2
       90,168,156,203,177,120,  2,190,188,  7,100,185,174,243,162, 10, //  3
      237, 18,253,225,  8,208,172,244,255,126,101, 79,145,235,228,121, //  4
      123,251, 67,250,161,  0,107, 97,241,111,181, 82,249, 33, 69, 55, //  5
       59,153, 29,  9,213,167, 84, 93, 30, 46, 94, 75,151,114, 73,222, //  6
      197, 96,210, 45, 16,227,248,202, 51,152,252,125, 81,206,215,186, //  7
       39,158,178,187,131,136,  1, 49, 50, 17,141, 91, 47,129, 60, 99, //  8
      154, 35, 86,171,105, 34, 38,200,147, 58, 77,118,173,246, 76,254, //  9
      133,232,196,144,198,124, 53,  4,108, 74,223,234,134,230,157,139, // 10
      189,205,199,128,176, 19,211,236,127,192,231, 70,233, 88,146, 44, // 11
      183,201, 22, 83, 13,214,116,109,159, 32, 95,226,140,220, 57, 12, // 12
      221, 31,209,182,143, 92,149,184,148, 62,113, 65, 37, 27,106,166, // 13
        3, 14,204, 72, 21, 41, 56, 66, 28,193, 40,217, 25, 54,179,117, // 14
      238, 87,240,155,180,170,242,212,191,163, 78,218,137,194,175,110, // 15
       43,119,224, 71,122,142, 42,160,104, 48,247,103, 15, 11,138,239  // 16
      };

      for (j = 0; j &lt; 8; j++) {
         unsigned char h = T[(x[0] + j) % 256];
         for (i = 1; i &lt; len; i++) {
            h = T[h ^ x[i]];
         }
         hh[j] = h;
      }

      snprintf(hex, hexlen, &quot;%02X%02X%02X%02X%02X%02X%02X%02X&quot;,
         hh[0], hh[1], hh[2], hh[3],
         hh[4], hh[5], hh[6], hh[7]);
   }
&lt;/source&gt;

For a given string or chunk of data, Pearson's original algorithm produces only an 8 bit byte or integer, 0-255. But the algorithm makes it extremely easy to generate whatever length of hash is desired. The scheme used above is a very straightforward implementation of the algorithm. As Pearson noted: a change to any bit in the string causes his algorithm to create a completely different hash (0-255). In the code above, following every completion of the inner loop, the first byte of the string is incremented by one.

Every time that simple change to the first byte of the data is made, a different Pearson hash, h, is generated. xPear16 builds a 16 hex byte hash by concatenating a series of 8-bit Pearson (h) hashes. Instead of producing a value from 0 to 255, it generates a value from 0 to 18,446,744,073,709,551,615.

Pearson's algorithm can be made to generate hashes of any desired length, simply by adding 1 to the first byte of the string, re-computing h for the string, and concatenating the results. Thus the same core logic can be made to generate 32-bit or 128-bit hashes.

==References ==
&lt;references/&gt;

[[Category:Error detection and correction]]
[[Category:Hash functions]]
[[Category:Articles with example pseudocode]]</text>
      <sha1>4hje4gf45lbt6kfu0aikiom9i9ckcwe</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Zobrist hashing</title>
    <ns>0</ns>
    <id>1963880</id>
    <revision>
      <id>598540239</id>
      <parentid>553904227</parentid>
      <timestamp>2014-03-07T12:37:11Z</timestamp>
      <contributor>
        <username>דוד</username>
        <id>19877</id>
      </contributor>
      <comment>/* Calculation of the hash value */  indentation</comment>
      <text xml:space="preserve" bytes="5064">'''Zobrist hashing''' (also referred to as '''Zobrist keys''' or '''Zobrist signatures''' &lt;ref&gt;[http://web.archive.org/web/20070822204038/http://www.seanet.com/~brucemo/topics/zobrist.htm Zobrist keys: a means of enabling position comparison.]&lt;/ref&gt;) is a [[hash function]] construction used in [[computer program]]s that play [[abstract board game]]s, such as [[computer chess|chess]] and [[computer go|Go]], to implement [[transposition table]]s, a special kind of [[hash table]] that is indexed by a board position and used to avoid analyzing the same position more than once. Zobrist hashing is named for its inventor, [[Albert Lindsey Zobrist (computer scientist)|Albert Lindsey Zobrist]].&lt;ref&gt;Albert Lindsey Zobrist, [https://www.cs.wisc.edu/techreports/1970/TR88.pdf ''A New Hashing Method with Application for Game Playing''], Tech. Rep. 88, Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, (1969).&lt;/ref&gt;  It has also been applied as a method for recognizing substitutional alloy configurations in simulations of crystalline materials.&lt;ref name=MonteCarlo/&gt;

==Calculation of the hash value==
Zobrist hashing starts by [[pseudorandom number generator|randomly generating]] [[bitstring]]s for each possible element of a board game, i.e. for each combination of a piece and a position (in the game of chess, that's 12 pieces × 64 board positions, or 14 if a king that may still castle and a pawn that may capture ''[[en passant]]'' are treated separately). Now any board configuration can be broken up into independent piece/position components, which are mapped to the random bitstrings generated earlier. The final Zobrist hash is computed by combining those bitstrings using bitwise [[XOR]]. Example pseudocode for the game of chess:

    constant indices
        white_pawn := 1
        white_rook := 2
        # etc.
        black_king := 12
    
    function init_zobrist():
        # fill a table of random numbers/bitstrings
        table := a 2-d array of size 64×12
        for i from 1 to 64:  # loop over the board, represented as a linear array
            for j from 1 to 12:      # loop over the pieces
                table[i][j] = random_bitstring()
    
    function hash(board):
        h := 0
        for i from 1 to 64:      # loop over the board positions
            if board[i] != empty:
                j := the piece at board[i], as listed in the constant indices, above
                h := h XOR table[i][j]
        return h

==Use of the hash value==
If the bitstrings are long enough, different board positions will almost certainly hash to different values; however longer bitstrings require proportionally more computer resources to manipulate. Many game engines store only the hash values in the transposition table, omitting the position information itself entirely to reduce memory usage, and assuming that [[hash collision]]s will not occur, or will not greatly influence the results of the table if they do.

Zobrist hashing is the  first known instance of [[tabulation hashing]]. The result is a [[K-independent hashing|3-wise independent hash family]]. In particular, it is strongly [[Universal hashing|universal]].

As an example, in [[chess]], each of the 64 squares can at any time be empty, or contain one of the 6 game pieces, which are either black or white. That is, each square can be in one of 1 + 6 x 2 = 13 possible states at any time. Thus one needs to generate at most 13 x 64 = 832 random bitstrings. Given a position, one obtains its Zobrist hash by finding out which pieces are on which squares, and combining the relevant bitstrings together.

==Updating the hash value==
Rather than computing the hash for the entire board every time, as the pseudocode above does, the hash value of a board can be updated simply by XORing out the bitstring(s) for positions that have changed, and XORing in the bitstrings for the new positions. For instance, if a pawn on a chessboard square is replaced by a [[rook (chess)|rook]] from another square, the resulting position would be produced by XORing the existing hash with the bitstrings for:
  'pawn at this square'      (XORing ''out'' the pawn at this square)
  'rook at this square'      (XORing ''in'' the rook at this square)
  'rook at source square'    (XORing ''out'' the rook at the source square)
  'nothing at source square' (XORing ''in'' nothing at the source square).
This makes Zobrist hashing very efficient for traversing a [[game tree]].

In [[computer go]], this technique is also used for [[Rules of Go#Ko|superko]] detection.

==Wider usage==
The same method has been used to recognize [[substitutional alloy]] configurations during [[Monte Carlo simulations]] in order to prevent wasting computational effort on states that have already been calculated.&lt;ref name=MonteCarlo&gt;{{cite doi|10.1016/j.cpc.2004.09.007}}&lt;/ref&gt;

==See also==
* [[Alpha-beta pruning]]

== References ==
&lt;references/&gt;

[[Category:Computer chess]]
[[Category:Computer Go]]
[[Category:Game artificial intelligence]]
[[Category:Hash functions]]</text>
      <sha1>4wilougmm2a869oq6w8wgwq2rz89b7b</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>MurmurHash</title>
    <ns>0</ns>
    <id>25081196</id>
    <revision>
      <id>621958088</id>
      <parentid>621761414</parentid>
      <timestamp>2014-08-19T19:49:41Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (10388)</comment>
      <text xml:space="preserve" bytes="11387">{{Use dmy dates|date=January 2012}}
'''MurmurHash''' is a non-[[Cryptographic hash function|cryptographic]] [[hash function]] suitable for general hash-based lookup.&lt;ref name=&quot;Hadoop&quot;&gt;{{cite web|url=http://hbase.apache.org/docs/current/api/org/apache/hadoop/hbase/util/MurmurHash.html |title=Hadoop in Java |publisher=Hbase.apache.org |date=24 July 2011 |accessdate=13 January 2012}}&lt;/ref&gt;&lt;ref&gt;[http://laboratorios.fi.uba.ar/lsi/chouza-tesisingenieriainformatica.pdf Chouza et al].&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.inesc-id.pt/ficheiros/publicacoes/5453.pdf |title=Couceiro et al. |format=PDF |language=pt |accessdate=13 January 2012}}&lt;/ref&gt;  It was created by Austin Appleby in 2008,&lt;ref&gt;{{cite web|url=http://murmurhash.googlepages.com/ |title=MurmurHash on GooglePages |publisher=Murmurhash.googlepages.com |accessdate=13 January 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Tanjent (tanjent) wrote,3 March 2008 13:31:00 |url=http://tanjent.livejournal.com/756623.html |title=MurmurHash first announcement |publisher=Tanjent.livejournal.com |accessdate=13 January 2012}}&lt;/ref&gt; and exists in a number of variants,&lt;ref name=&quot;Murmur160&quot;&gt;{{cite web|url=http://simonhf.wordpress.com/2010/09/25/murmurhash160/ |title=MurmurHash2-160 |publisher=Simonhf.wordpress.com |date=25 September 2010 |accessdate=13 January 2012}}&lt;/ref&gt; all of which have been released into the public domain. When compared to other popular hash functions, MurmurHash performed well in a random distribution of regular keys.&lt;ref name=&quot;StackExchange&quot;&gt;{{cite web|url=http://programmers.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed |publisher=stackexchange.com |title=Which hashing algorithm is best for uniqueness and speed}}&lt;/ref&gt;

Unlike [[cryptographic hash function]]s, it is not designed to be difficult to reverse by an adversary, making it unsuitable for cryptographic purposes.

==Variants==

The current version is MurmurHash3,&lt;ref&gt;{{cite web|title=MurmurHash3 on smhasher|url=http://code.google.com/p/smhasher/wiki/MurmurHash3}}&lt;/ref&gt;&lt;ref name=&quot;Horvath&quot;&gt;{{cite web | first = Adam | last = Horvath | url = http://blog.teamleadnet.com/2012/08/murmurhash3-ultra-fast-hash-algorithm.html | title = MurMurHash3, an ultra fast hash algorithm for C# / .NET | date = Aug 10, 2012 }}&lt;/ref&gt; which yields a 32-bit or 128-bit hash value.

The older MurmurHash2&lt;ref&gt;{{cite web|title=MurmurHash2 on smhasher|url=http://code.google.com/p/smhasher/wiki/MurmurHash2}}&lt;/ref&gt; yields a 32-bit or 64-bit value. Slower versions of MurmurHash2 are available for big-endian and aligned-only machines.  The MurmurHash2A variant adds the [[Merkle–Damgård construction]] so that it can be called incrementally. There are two variants which generate 64-bit values; MurmurHash64A, which is optimized for 64-bit processors, and MurmurHash64B, for 32-bit ones. MurmurHash2-160 generates the 160-bit hash, and MurmurHash1 is obsolete.

==Implementations==

The canonical implementation is in [[C++]], but there are efficient ports for a variety of popular languages, including [[Python (programming language)|Python]],&lt;ref&gt;{{cite web|url=http://code.google.com/p/pyfasthash/ |title=pyfasthash in Python |publisher=Google |accessdate=13 January 2012}}&lt;/ref&gt; [[C (programming language)|C]],&lt;ref&gt;{{cite web|url=http://www.qdecoder.org/qlibc/ |title=C implementation in qLibc by Seungyoung Kim}}&lt;/ref&gt; [[C Sharp (programming language)|C#]],&lt;ref name=&quot;Horvath&quot;/&gt;&lt;ref&gt;{{cite web|last=Landman |first=Davy |url=http://landman-code.blogspot.com/2009/02/c-superfasthash-and-murmurhash2.html |title=Davy Landman in C# |publisher=Landman-code.blogspot.com |accessdate=13 January 2012}}&lt;/ref&gt; [[Perl]],&lt;ref&gt;{{cite web|url=http://metacpan.org/module/Digest::MurmurHash |title=Toru Maesaka in Perl |publisher=metacpan.org |accessdate=13 January 2012}}&lt;/ref&gt; [[Ruby (programming language)|Ruby]],&lt;ref&gt;{{cite web|author=Bruce Williams &lt;http://codefluency.com&gt;, for Ruby Central &lt;http://rubycentral.org&gt; |url=http://rubyforge.org/projects/murmurhash |title=Ruby |publisher=Rubyforge.org |date=3 May 2009 |accessdate=13 January 2012}}&lt;/ref&gt; [[PHP]],&lt;ref&gt;{{cite web|url=http://murmur.vaizard.org/en/ |title=Murmurhash3 PHP extension |publisher=Murmur.vaizard.org |accessdate=13 January 2012}}&lt;/ref&gt; [[Haskell (programming language)|Haskell]],&lt;ref&gt;{{cite web|url=http://hackage.haskell.org/package/murmur-hash |title=Haskell |publisher=Hackage.haskell.org |accessdate=13 January 2012}}&lt;/ref&gt; [[Scala (programming language)|Scala]],&lt;ref&gt;{{cite web|url=https://github.com/scala/scala/blob/master/src/library/scala/util/hashing/MurmurHash3.scala|title=Scala standard library implementation|date=14 December 2012}}&lt;/ref&gt; [[Java (programming language)|Java]],&lt;ref&gt;[http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/hash/Hashing.html MurmurHash3 in Java], part of Guava&lt;/ref&gt;&lt;ref&gt;[http://dmy999.com/article/50/murmurhash-2-java-port Derek Young in Java], public domain&lt;/ref&gt; [[Erlang (programming language)|Erlang]],&lt;ref&gt;[https://github.com/bipthelin/murmerl3 MurmurHash3 in Erlang]&lt;/ref&gt; and [[JavaScript]]&lt;ref&gt;{{cite web|author=raycmorgan (owner) |url=http://gist.github.com/588423 |title=Javascript implementation by Ray Morgan |publisher=Gist.github.com |accessdate=13 January 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=garycourt |url=http://github.com/garycourt/murmurhash-js |title=MurmurHash.js by Gary Court |publisher=Github.com|title=MurmurHash.js on Github|accessdate=13 January 2012}}&lt;/ref&gt; together with online version.&lt;ref&gt;{{cite web|url=http://murmurhash.shorelabs.com |title=Online version of MurmurHash |publisher=shorelabs.com |accessdate=12 August 2014}}&lt;/ref&gt;

It has been adopted into a number of open-source projects, most notably libstdc++ (ver 4.6), Perl,&lt;ref&gt;{{cite web|url=http://search.cpan.org/~drolsky/perl-5.17.7/pod/perl5176delta.pod#New_hash_function_Murmurhash-32_%28v3%29 |title=perl5176delta |accessdate=31 December 2012}}&lt;/ref&gt; nginx (ver 1.0.1),&lt;ref&gt;{{cite web|url=http://nginx.org/en/CHANGES |title=nginx |accessdate=13 January 2012}}&lt;/ref&gt; [[Rubinius]],&lt;ref&gt;{{cite web|url=https://github.com/rubinius/rubinius/commit/1d69526c484cc9435a7198e41b8995db6c3acf1a |title=Rubinius |accessdate=29 February 2012}}&lt;/ref&gt; libmemcached (the [[C (programming language)|C]] driver for [[Memcached]]),&lt;ref&gt;[http://libmemcached.org/libMemcached.html libmemcached]&lt;/ref&gt; maatkit,&lt;ref&gt;{{cite web|url=http://code.google.com/p/maatkit/source/detail?r=3273 |title=maatkit |publisher=Google |date=24 March 2009 |accessdate=13 January 2012}}&lt;/ref&gt; [[Hadoop]],&lt;ref name=&quot;Hadoop&quot;/&gt; Kyoto Cabinet,&lt;ref&gt;{{cite web|url=http://fallabs.com/kyotocabinet/spex.html |title=Kyoto Cabinet specification |publisher=Fallabs.com |date=4 March 2011 |accessdate=13 January 2012}}&lt;/ref&gt; [[RaptorDB]],&lt;ref&gt;{{cite web|last=Gholam |first=Mehdi |url=http://www.codeproject.com/KB/database/RaptorDB.aspx |title=RaptorDB CodeProject page |publisher=Codeproject.com |date=13 November 2011 |accessdate=13 January 2012}}&lt;/ref&gt; [[OlegDB]],&lt;ref&gt;{{cite web|url=https://olegdb.org/documentation.html |title=OlegDB Documentation |accessdate=24 January 2013}}&lt;/ref&gt; [[Apache Cassandra|Cassandra]],&lt;ref&gt;{{cite web|url=http://wiki.apache.org/cassandra/Partitioners|title=Partitioners|publisher=apache.org |date=2013-11-15|accessdate=2013-12-19}}&lt;/ref&gt; [[Clojure]] &lt;ref&gt;{{cite web|url=https://github.com/clojure/clojure/blob/master/src/jvm/clojure/lang/Murmur3.java|title=Murmur3.java in Clojure source code on Github|publisher=clojure.org|accessdate=2014-03-11}}&lt;/ref&gt; and [[vowpal wabbit]] &lt;ref&gt;{{cite web|url=https://github.com/JohnLangford/vowpal_wabbit/blob/master/vowpalwabbit/hash.cc|title=hash.cc in vowpalwabbit source code}}&lt;/ref&gt;

==Algorithm==

 Murmur3_32(''key'', ''len'', ''seed'')
     // Note: In this version, all integer arithmetic is performed with unsigned 32 bit integers.
     //       In the case of overflow, the result is constrained by the application of modulo &lt;math&gt;2^{32}&lt;/math&gt; arithmetic.
     
     ''c1'' &lt;math&gt;\gets&lt;/math&gt; 0xcc9e2d51
     ''c2'' &lt;math&gt;\gets&lt;/math&gt; 0x1b873593
     ''r1'' &lt;math&gt;\gets&lt;/math&gt; 15
     ''r2'' &lt;math&gt;\gets&lt;/math&gt; 13
     ''m'' &lt;math&gt;\gets&lt;/math&gt; 5
     ''n'' &lt;math&gt;\gets&lt;/math&gt; 0xe6546b64
  
     ''hash'' &lt;math&gt;\gets&lt;/math&gt; seed
 
     for each fourByteChunk of key
         k &lt;math&gt;\gets&lt;/math&gt; fourByteChunk
 
         k &lt;math&gt;\gets&lt;/math&gt; k * c1
         k &lt;math&gt;\gets&lt;/math&gt; (k &lt;&lt; r1) '''OR''' (k &gt;&gt; (32-r1))
         k &lt;math&gt;\gets&lt;/math&gt; k * c2
 
         hash &lt;math&gt;\gets&lt;/math&gt; hash '''XOR''' k
         hash &lt;math&gt;\gets&lt;/math&gt; (hash &lt;&lt; r2) '''OR''' (hash &gt;&gt; (32-r2))
         hash &lt;math&gt;\gets&lt;/math&gt; hash * m + n
 
     with any remainingBytesInKey
         remainingBytes &lt;math&gt;\gets&lt;/math&gt; SwapEndianOrderOf(remainingBytesInKey)
         // Note: Endian swapping is only necessary on big-endian machines.
         //       The purpose is to place the meaningful digits towards the low end of the value,
         //       so that these digits have the greatest potential to affect the low range digits
         //       in the subsequent multiplication.  Consider that locating the meaningful digits
         //       in the high range would produce a greater effect upon the high digits of the
         //       multiplication, and notably, that such high digits are likely to be discarded
         //       by the modulo arithmetic under overflow.  We don't want that.
         
         remainingBytes &lt;math&gt;\gets&lt;/math&gt; remainingBytes * c1
         remainingBytes &lt;math&gt;\gets&lt;/math&gt; (remainingBytes &lt;&lt; r1) '''OR''' (remainingBytes &gt;&gt; (32 - r1))
         remainingBytes &lt;math&gt;\gets&lt;/math&gt; remainingBytes * c2
 
         hash &lt;math&gt;\gets&lt;/math&gt; hash '''XOR''' remainingBytes
  
     hash &lt;math&gt;\gets&lt;/math&gt; hash '''XOR''' len
 
     hash &lt;math&gt;\gets&lt;/math&gt; hash '''XOR''' (hash &gt;&gt; 16)
     hash &lt;math&gt;\gets&lt;/math&gt; hash * 0x85ebca6b
     hash &lt;math&gt;\gets&lt;/math&gt; hash '''XOR''' (hash &gt;&gt; 13)
     hash &lt;math&gt;\gets&lt;/math&gt; hash * 0xc2b2ae35
     hash &lt;math&gt;\gets&lt;/math&gt; hash '''XOR''' (hash &gt;&gt; 16)

'''A sample C implementation follows:'''

&lt;syntaxhighlight lang=&quot;c&quot;&gt;uint32_t murmur3_32(const char *key, uint32_t len, uint32_t seed) {
	static const uint32_t c1 = 0xcc9e2d51;
	static const uint32_t c2 = 0x1b873593;
	static const uint32_t r1 = 15;
	static const uint32_t r2 = 13;
	static const uint32_t m = 5;
	static const uint32_t n = 0xe6546b64;

	uint32_t hash = seed;

	const int nblocks = len / 4;
	const uint32_t *blocks = (const uint32_t *) key;
	int i;
	for (i = 0; i &lt; nblocks; i++) {
		uint32_t k = blocks[i];
		k *= c1;
		k = (k &lt;&lt; r1) | (k &gt;&gt; (32 - r1));
		k *= c2;

		hash ^= k;
		hash = ((hash &lt;&lt; r2) | (hash &gt;&gt; (32 - r2))) * m + n;
	}

	const uint8_t *tail = (const uint8_t *) (key + nblocks * 4);
	uint32_t k1 = 0;

	switch (len &amp; 3) {
	case 3:
		k1 ^= tail[2] &lt;&lt; 16;
	case 2:
		k1 ^= tail[1] &lt;&lt; 8;
	case 1:
		k1 ^= tail[0];

		k1 *= c1;
		k1 = (k1 &lt;&lt; r1) | (k1 &gt;&gt; (32 - r1));
		k1 *= c2;
		hash ^= k1;
	}

	hash ^= len;
	hash ^= (hash &gt;&gt; 16);
	hash *= 0x85ebca6b;
	hash ^= (hash &gt;&gt; 13);
	hash *= 0xc2b2ae35;
	hash ^= (hash &gt;&gt; 16);

	return hash;
}
&lt;/syntaxhighlight&gt;

==See also==
*[[Fowler–Noll–Vo hash function]]
*[[Jenkins hash function]]
*[[CityHash]]

==References==
{{reflist|colwidth=30em}}

{{DEFAULTSORT:Murmurhash}}
[[Category:Hash functions]]
[[Category:Articles with example pseudocode]]
[[Category:Articles with example C code]]</text>
      <sha1>p1w02db50wx9jcxxzx2q7pqzmf2nwcs</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>CityHash</title>
    <ns>0</ns>
    <id>31472239</id>
    <revision>
      <id>585599231</id>
      <parentid>585599189</parentid>
      <timestamp>2013-12-11T14:57:14Z</timestamp>
      <contributor>
        <username>AzureDiamond</username>
        <id>19172751</id>
      </contributor>
      <minor/>
      <comment>/* External links */</comment>
      <text xml:space="preserve" bytes="2063">{{about|the family of hash functions|the running club|Hash House Harriers}}

'''CityHash''' is a family of non-[[Cryptographic hash function|cryptographic]] [[hash functions]], designed for fast hashing of [[String (computer science)|strings]]. It has 32-, 64-, 128-, and 256-bit variants.

Google developed the algorithm in-house starting in 2010.&lt;ref name=&quot;googlecode&quot;&gt;http://code.google.com/p/cityhash/&lt;/ref&gt; The [[C++]] source code for the [[reference implementation]] of the algorithm was released in 2011 under an [[MIT license]], with credit to Geoff Pike and Jyrki Alakuijala.&lt;ref name=&quot;announcement&quot;&gt;http://google-opensource.blogspot.com/2011/04/introducing-cityhash.html&lt;/ref&gt; The authors expect the algorithm to outperform previous work by a factor of 1.05 to 2.5, depending on the CPU and mix of string lengths being hashed.&lt;ref name=&quot;readme&quot;/&gt; CityHash is influenced by and partly based on [[MurmurHash]].&lt;ref name=&quot;sourcecode&quot;&gt;http://code.google.com/p/cityhash/source/browse/trunk/src/city.cc&lt;/ref&gt;

Some particularly fast CityHash functions depend on CRC32 instructions that are present in [[SSE4.2]]. However, most CityHash functions are designed to be portable, though they will run best on little-endian 32-bit or 64-bit CPUs.&lt;ref name=&quot;readme&quot;&gt;http://code.google.com/p/cityhash/source/browse/trunk/README&lt;/ref&gt;

== Concerns ==

CityHash releases do not maintain backward compatibility with previous versions.&lt;ref name=&quot;news&quot;&gt;http://code.google.com/p/cityhash/source/browse/trunk/NEWS&lt;/ref&gt;  Users should not use CityHash for persistent storage, or else not upgrade CityHash.

The README warns that CityHash has not been tested much on [[big-endian]] platforms.&lt;ref name=&quot;readme&quot;/&gt;

== References ==
{{reflist}}

== External links ==
*[http://code.google.com/p/cityhash/ Official site]
*[http://google-opensource.blogspot.com/2011/04/introducing-cityhash.html Introducing CityHash], Announcement by Google
*[http://ee380.stanford.edu/Abstracts/121017-slides.pdf], Slides from Geoff Pike's talk at Stanford University

[[Category:Hash functions]]</text>
      <sha1>kvzu4xz3jnkufig16b5ti8274nbt6mg</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>K-independent hashing</title>
    <ns>0</ns>
    <id>31142742</id>
    <revision>
      <id>592399517</id>
      <parentid>491107019</parentid>
      <timestamp>2014-01-25T23:48:13Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* Mathematical Definitions */Journal cites, added 1 DOI using [[Project:AWB|AWB]] (9888)</comment>
      <text xml:space="preserve" bytes="5891">A family of hash functions is said to be '''&lt;math&gt;k&lt;/math&gt;-independent''' or '''&lt;math&gt;k&lt;/math&gt;-universal'''&lt;ref name=CLRS&gt;
{{cite book
 | first     = Thomas H.
 | last      = Cormen
 | coauthors = Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford
 | title     = Introduction to Algorithms
 | publisher = MIT Press
 | year      = 2009
 | isbn      = 0-262-03384-4
 | edition   = 3rd
 }}
&lt;/ref&gt; if selecting a [[hash function]] at random from the family guarantees that the hash codes of any designated &lt;math&gt;k&lt;/math&gt; keys are [[Independence (probability theory)|independent random variables]] (see precise mathematical definitions below). Such families allow good average case performance in randomized algorithms or data structures, even if the input data is chosen by an adversary. The trade-offs between the degree of independence and the efficiency of evaluating the hash function are well studied, and many &lt;math&gt;k&lt;/math&gt;-independent families have been proposed.

== Introduction ==
{{see also|Hash function}}

The goal of hashing is usually to map keys from some large domain (universe) &lt;math&gt;U&lt;/math&gt; into a smaller range, such as &lt;math&gt;m&lt;/math&gt; bins (labelled &lt;math&gt;[m] = \{0, \dots, m-1\}&lt;/math&gt;). In the analysis of randomized algorithms and data structures, it is often desirable for the hash codes of various keys to &quot;behave randomly&quot;. For instance, if the hash code of each key were an independent random choice in &lt;math&gt;[m]&lt;/math&gt;, the number of keys per bin could be analyzed using the [[Chernoff bound]]. A deterministic hash function cannot offer any such guarantee in an adversarial setting, as the adversary may choose the keys to be the precisely the [[Image (mathematics)|preimage]] of a bin. Furthermore, a deterministic hash function does not allow for ''rehashing'': sometimes the input data turns out to be bad for the hash function (e.g. there are too many collisions), so one would like to change the hash function.

The solution to these problems is to pick a function ''randomly'' from a large family of hash functions. The randomness in choosing the hash function can be used to guarantee some desired random behavior of the hash codes of any keys of interest. The first definition along these lines was [[universal hashing]], which guarantees a low collision probability for any two designated keys. The concept of &lt;math&gt;k&lt;/math&gt;-independent hashing, introduced by Wegman and Carter in 1981,&lt;ref name=WC81&gt;
{{cite journal
 | last1 = Wegman
 | first1 = Mark N. | author1-link = Mark N. Wegman
 | last2 = Carter
 | first2 = J. Lawrence
 | title = New hash functions and their use in authentication and set equality
 | journal = Journal of Computer and System Sciences
 | volume = 22
 | issue = 3
 | pages = 265–279
 | year = 1981
 | doi = 10.1016/0022-0000(81)90033-7
 | id = Conference version in FOCS'79
 | url = http://www.fi.muni.cz/~xbouda1/teaching/2009/IV111/Wegman_Carter_1981_New_hash_functions.pdf
 | accessdate = 9 February 2011
}}&lt;/ref&gt; strengthens the guarantees of random behavior to families of &lt;math&gt;k&lt;/math&gt; designated keys, and adds a guarantee on the uniform distribution of hash codes.

=== Mathematical Definitions ===
The strictest definition, introduced by Wegman and Carter&lt;ref name=WC81 /&gt; under the name &quot;strongly universal&lt;math&gt;_k&lt;/math&gt; hash family&quot;, is the following. A family of hash functions &lt;math&gt;H=\{ h:U \to [m] \}&lt;/math&gt; is &lt;math&gt;k&lt;/math&gt;-independent if for any &lt;math&gt;k&lt;/math&gt; distinct keys &lt;math&gt;(x_1, \dots, x_k) \in U^k&lt;/math&gt; and any &lt;math&gt;k&lt;/math&gt; hash codes (not necessarily distinct) &lt;math&gt;(y_1, \dots, y_k) \in [m]^k&lt;/math&gt;, we have:
: &lt;math&gt;\Pr_{h \in H} \left[ h(x_1)=y_1 \land \cdots \land h(x_k)=y_k \right] = m^{-k}&lt;/math&gt;

This definition is equivalent to the following two conditions:
# for any fixed &lt;math&gt;x\in U&lt;/math&gt;, as &lt;math&gt;h&lt;/math&gt; is drawn randomly from &lt;math&gt;H&lt;/math&gt;, &lt;math&gt;h(x)&lt;/math&gt; is uniformly distributed in &lt;math&gt;[m]&lt;/math&gt;.
# for any fixed, distinct keys &lt;math&gt;x_1, \dots, x_k \in U&lt;/math&gt;, as &lt;math&gt;h&lt;/math&gt; is drawn randomly from &lt;math&gt;H&lt;/math&gt;, &lt;math&gt;h(x_1), \dots, h(x_k)&lt;/math&gt; are independent random variables.

Often it is inconvenient to achieve the perfect joint probability of &lt;math&gt;m^{-k}&lt;/math&gt; due to rounding issues. Following,&lt;ref name=Siegel&gt;
{{cite journal
 | last1 = Siegel
 | first1 = Alan
 | title = On universal classes of extremely random constant-time hash functions and their time-space tradeoff
 | journal = SIAM Journal on Computing
 | volume = 33
 | issue = 3
 | pages = 505–543
 | year = 2004
 | id = Conference version in FOCS'89
 | url = http://www.cs.nyu.edu/faculty/siegel/FASTH.pdf
 | doi=10.1137/S0097539701386216}}
&lt;/ref&gt; one may define a &lt;math&gt;(\mu, k)&lt;/math&gt;-independent family to satisfy:

: &lt;math&gt;\forall&lt;/math&gt; distinct &lt;math&gt;(x_1, \dots, x_k) \in U^k&lt;/math&gt; and &lt;math&gt;\forall (y_1, \dots, y_k) \in [m]^k&lt;/math&gt;, &lt;math&gt;~~\Pr_{h \in H} \left[ h(x_1)=y_1 \land \cdots \land h(x_k)=y_k \right] \le  \mu / m^k&lt;/math&gt;

Observe that, even if &lt;math&gt;\mu&lt;/math&gt; is close to 1, &lt;math&gt;h(x_i)&lt;/math&gt; are no longer independent random variables, which is often a problem in the analysis of randomized algorithms. Therefore, a more common alternative to dealing with rounding issues is to prove that the hash family is close in [[statistical distance]] to a &lt;math&gt;k&lt;/math&gt;-independent family, which allows black-box use of the independence properties.

==See also==
* [[Universal hashing]]
* [[Tabulation hashing]], a technique for generating 3-independent hash functions

== References ==
&lt;references /&gt;

== Further reading ==
* {{cite book
 | last1 = Motwani
 | first1 = Rajeev
 | last2 = Raghavan
 | first2 = Prabhakar
 | title = Randomized Algorithms
 | publisher = Cambridge University Press
 | year = 1995
 | isbn = 0-521-47465-5
 | page = 221
}}

[[Category:Hash functions]]
[[Category:Search algorithms]]
[[Category:Error detection and correction]]</text>
      <sha1>njvh8bz2pmsyoagu0gi9s4cy5kyjxi2</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hash filter</title>
    <ns>0</ns>
    <id>3933044</id>
    <revision>
      <id>498969929</id>
      <parentid>498969866</parentid>
      <timestamp>2012-06-23T10:51:21Z</timestamp>
      <contributor>
        <username>Arthur Frayn</username>
        <id>8954</id>
      </contributor>
      <comment>* [[Bloom filter]]</comment>
      <text xml:space="preserve" bytes="1141">{{Refimprove|date=January 2008}}

A '''hash filter''' creates a [[hash sum]] from data, typically [[e-mail]], and compares the sum against other previously defined sums. Depending on the purpose of the filter, the data can then be included or excluded in a [[function (computer science)|function]] based on whether it matches an existing sum.

For example, when a message is received by an e-mail server with a hash filter, the contents of the e-mail is converted into a hash sum. If this sum corresponds to the hash sum of another e-mail which has been categorized as [[e-mail spam|spam]], the received e-mail is prevented from being delivered. Spammers attempt to evade this by adding random strings to the text content and random pixel changes (&quot;confetti&quot;) to image content (see [[image spam]]).

==See also==
* [[Bloom filter]]
* [[Hash buster]]
* [[Locality-sensitive hashing]]

==References==
*[http://www.ianywhere.com/developer/product_manuals/sqlanywhere/1000/en/html/dbugen10/ug-queryopt-s-bloom.html Hash Filter algorithm (SQL Anywhere Server - SQL Usage)] at iAnywhere.com

[[Category:Hash functions]]

{{security-software-stub}}</text>
      <sha1>jx0510l8xbz833rknj79rsuh3zvu26t</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hash function</title>
    <ns>0</ns>
    <id>13790</id>
    <revision>
      <id>625643383</id>
      <parentid>625643221</parentid>
      <timestamp>2014-09-15T10:35:51Z</timestamp>
      <contributor>
        <ip>117.239.185.114</ip>
      </contributor>
      <text xml:space="preserve" bytes="36786">{{Refimprove|date=July 2010}}
[[File:Hash table 4 1 1 0 0 1 0 LL.svg|thumb|240px|right|A hash function that maps names to integers from 0 to 15. There is a collision between keys &quot;John Smith&quot; and &quot;Sandra Dee&quot;.]]
A '''hash function''' is any [[function]] that can be used to map digital [[data (computing)|data]] of arbitrary size to digital data of fixed size, with slight differences in input data producing very big differences in output data.&lt;ref&gt;Ovie Carroll and Mark Krotoski, &quot;Using 'Digital Fingerprints' (or Hash Values) for Investigations and Cases Involving Electronic Evidence,&quot; 62 United States Attorneys’ Bulletin 44-82 (May 2014)[http://www.justice.gov/usao/eousa/foia_reading_room/usab6203.pdf (extensive article citing and linking to primary sources and reviewing functions of hash values)]&lt;/ref&gt; The values returned by a hash function are called '''hash values''', '''hash codes''', '''hash sums''', or simply '''hashes'''. 
One practical use is a data structure called a [[hash table]], widely used in computer software for rapid data lookup. Hash functions accelerate table or database lookup by detecting duplicated records in a large file.  An example is finding similar stretches in DNA sequences. They are also useful in [[cryptography]]. A [[cryptographic hash function]] allows one to easily verify that some input data matches a stored hash value, but makes it hard to reconstruct the data from the hash alone. This principle is used by the [[Pretty Good Privacy|PGP]] algorithm for data validation and by many password checking systems.

Hash functions are related to (and often confused with) [[checksums]], check digits, fingerprints, randomization functions, error-correcting codes, and ciphers. Although these concepts overlap to some extent, each has its own uses and requirements and is designed and optimized differently. The Hash Keeper database maintained by the American National Drug Intelligence Center, for instance, is more aptly described as a catalog of file fingerprints than of hash values.

== Uses ==

=== Hash tables ===
Hash functions are primarily used in [[hash table]]s, to quickly locate a data record (e.g.,&amp;nbsp;a [[dictionary]] definition) given its [[primary key|search key]] (the headword). Specifically, the hash function is used to map the search key to an index; the index gives the place in the hash table where the corresponding record should be stored. Hash tables, in turn, are used to implement [[associative array]]s and [[set (abstract data type)|dynamic sets]].

Typically, the domain of a hash function (the set of possible keys) is larger than its range (the number of different table indexes), and so it will map several different keys to the same index. Therefore, each slot of a hash table is associated with (implicitly or explicitly) a [[set (mathematics)|set]] of records, rather than a single record. For this reason, each slot of a hash table is often called a ''bucket'', and hash values are also called ''bucket indices''.

Thus, the hash function only hints at the record's location — it tells where one should start looking for it. Still, in a half-full table, a good hash function will typically narrow the search down to only one or two entries.

=== Caches ===
Hash functions are also used to build [[cache (computing)|caches]] for large data sets stored in slow media. A cache is generally simpler than a hashed search table, since any collision can be resolved by discarding or writing back the older of the two colliding items. This is also used in file comparison

=== Bloom filters ===
{{Main|Bloom filter}}
Hash functions are an essential ingredient of the [[Bloom filter]], a space-efficient [[probabilistic]] [[data structure]] that is used to test whether an [[element (mathematics)|element]] is a member of a [[set (computer science)|set]].

=== Finding duplicate records ===
{{Main|Hash table}}
When storing records in a large unsorted file, one may use a hash function to map each record to an index into a table ''T'', and collect in each bucket ''T''[''i''] a [[list (computing)|list]] of the numbers of all records with the same hash value ''i''.  Once the table is complete, any two duplicate records will end up in the same bucket. The duplicates can then be found by scanning every bucket ''T''[''i''] which contains two or more members, fetching those records, and comparing them. With a table of appropriate size, this method is likely to be much faster than any alternative approach (such as sorting the file and comparing all consecutive pairs).

=== Protecting data ===
{{Main|Security of cryptographic hash functions}}
A hash value can be used to uniquely identify secret information. This requires that the hash function is [[collision resistance|collision resistant]], which means that it is very hard to find data that generate the same hash value. These functions are categorized into cryptographic hash functions and provably secure hash functions. Functions in the second category are the most secure but also too slow for most practical purposes. Collision resistance is accomplished in part by generating very large hash values. For example [[SHA-1]], one of the most widely used cryptographic hash functions, generates 160 bit values.

=== Finding similar records ===
{{Main|Locality sensitive hashing}}
Hash functions can also be used to locate table records whose key is similar, but not identical, to a given key; or pairs of records in a large file which have similar keys. For that purpose, one needs a hash function that maps similar keys to hash values that differ by at most ''m'', where ''m'' is a small integer (say, 1 or 2). If one builds a table ''T'' of all record numbers, using such a hash function, then similar records will end up in the same bucket, or in nearby buckets. Then one need only check the records in each bucket ''T''[''i''] against those in buckets ''T''[''i''+''k''] where ''k'' ranges between −''m'' and&amp;nbsp;''m''.

This class includes the so-called [[acoustic fingerprint]] algorithms, that are used to locate similar-sounding entries in large collection of [[audio file]]s. For this application, the hash function must be as insensitive as possible to data capture or transmission errors, and to trivial changes such as timing and volume changes, compression, etc.&lt;ref name=&quot;AudioHash1&quot;&gt;[http://citeseer.ist.psu.edu/rd/11787382%2C504088%2C1%2C0.25%2CDownload/http://citeseer.ist.psu.edu/cache/papers/cs/25861/http:zSzzSzwww.extra.research.philips.comzSznatlabzSzdownloadzSzaudiofpzSzcbmi01audiohashv1.0.pdf/haitsma01robust.pdf &quot;Robust Audio Hashing for Content Identification by Jaap Haitsma, Ton Kalker and Job Oostveen&quot;]&lt;/ref&gt;

=== Finding similar substrings ===
The same techniques can be used to find equal or similar stretches in a large collection of strings, such as a document repository or a [[biological database|genomic database]]. In this case, the input strings are broken into many small pieces, and a hash function is used to detect potentially equal pieces, as above.

The [[Rabin–Karp string search algorithm|Rabin–Karp algorithm]] is a relatively fast [[string searching algorithm]] that works in [[big O notation|O(''n'')]] time on average. It is based on the use of hashing to compare strings.

=== Geometric hashing ===
This principle is widely used in [[computer graphics]], [[computational geometry]] and many other disciplines, to solve many [[proximity problem]]s in the plane or in three-dimensional space, such as finding [[closest pair problem|closest pairs]] in a set of points, similar shapes in a list of shapes, similar [[image processing|images]] in an [[image retrieval|image database]], and so on. In these applications, the set of all inputs is some sort of [[metric space]], and the hashing function can be interpreted as a [[partition (mathematics)|partition]] of that space into a grid of ''cells''. The table is often an array with two or more indices (called a ''[[grid file]]'', ''grid index'', ''bucket grid'', and similar names), and the hash function returns an index [[tuple]]. This special case of hashing  is known as  [[geometric hashing]] or ''the grid method''. Geometric hashing is also used in [[telecommunication]]s (usually under the name [[vector quantization]]) to [[code (communications)|encode]] and [[data compression|compress]] multi-dimensional signals.

=== Standard Uses of Hashing in Cryptography ===
{{Main|Cryptographic hash function}}
Some standard applications that employ hash functions include authentication, message integrity (using an [[HMAC]](Hashed MAC)), message fingerprinting, data corruption detection, and digital signature efficiency.

== Properties ==
Good hash functions, in the original sense of the term, are usually required to satisfy certain properties listed below.  The exact requirements are dependent on the application, for example a hash function well suited to indexing data will probably be a poor choice for a [[cryptographic hash function]].

=== Determinism ===
A hash procedure must be [[deterministic algorithm|deterministic]]—meaning that for a given input value it must always generate the same hash value. In other words, it must be a [[function (mathematics)|function]] of the data to be hashed, in the mathematical sense of the term. This requirement excludes hash functions that depend on external variable parameters, such as [[pseudo-random number generator]]s or the time of day. It also excludes functions that depend on the memory address of the object being hashed, because that address may change during execution (as may happen on systems that use certain methods of [[garbage collection (computer science)|garbage collection]]), although sometimes rehashing of the item is possible.

=== Uniformity ===
A good hash function  should map the expected inputs as evenly as possible over its output range.  That is, every hash value in the output range should be generated with roughly the same [[probability]]. The reason for this last requirement is that the cost of hashing-based methods goes up sharply as the number of ''collisions''—pairs of inputs that are mapped to the same hash value—increases.  Basically, if some hash values are more likely to occur than others, a larger fraction of the lookup operations will have to search through a larger set of colliding table entries.

Note that this criterion only requires the value to be ''uniformly distributed'', not ''random'' in any sense. A good randomizing function is (barring computational efficiency concerns) generally a good choice as a hash function, but the converse need not be true.

Hash tables often contain only a small subset of the valid inputs. For instance, a club membership list may contain only a hundred or so member names, out of the very large set of all possible names. In these cases, the uniformity criterion should hold for almost all typical subsets of entries that may be found in the table, not just for the global set of all possible entries.

In other words, if a typical set of ''m'' records is hashed to ''n'' table slots, the probability of a bucket receiving many more than ''m''/''n'' records should be vanishingly small. In particular, if ''m'' is less than ''n'', very few buckets should have more than one or two records. (In an ideal &quot;[[perfect hash function]]&quot;, no bucket should have more than one record; but a small number of collisions is virtually inevitable, even if ''n'' is much larger than ''m'' – see the [[birthday paradox]]).

When testing a hash function, the uniformity of the distribution of hash values can be evaluated by the [[chi-squared test]].

=== Defined range ===
It is often desirable that the output of a hash function have fixed size (but see below). If, for example, the output is constrained to 32-bit integer values, the hash values can be used to index into an array. Such hashing is commonly used to accelerate data searches.&lt;ref name=&quot;algorithms_in_java&quot;&gt;{{cite book|title=Algorithms in Java|first=Robert|last=Sedgewick|year=2002|publisher=Addison Wesley|edition=3|isbn=978-0201361209|chapter=14. Hashing}}&lt;/ref&gt; On the other hand, cryptographic hash functions produce much larger hash values, in order to ensure the computational complexity of brute-force inversion.&lt;ref name=&quot;handbook_of_applied_cryptography&quot;&gt;{{cite book|title=Handbook of Applied Cryptography|first1=Alfred J.|last1=Menezes|first2=Paul C.|last2=van Oorschot|first3=Scott A|last3=Vanstone|year=1996|publisher=CRC Press|isbn=0849385237}}&lt;/ref&gt; For example [[SHA-1]], one of the most widely used cryptographic hash functions, produces a 160-bit value.

Producing fixed-length output from variable length input can by accomplished by breaking the input data into chunks of specific size. Hash functions used for data searches use some arithmetic expression which iteratively processes chunks of the input (such as the characters in a string) to produce the hash value.&lt;ref name=&quot;algorithms_in_java&quot;/&gt; In cryptographic hash functions, these chunks are processed by a [[one-way compression function]], with the last chunk being padded if necessary. In this case, their size, which is called ''block size'', is much bigger than the size of the hash value.&lt;ref name=&quot;handbook_of_applied_cryptography&quot;/&gt; For example, in [[SHA-1]], the hash value is 160 bits and the block size 512 bits.

==== Variable range ====
In many applications, the range of hash values may be different for each run of the program, or may change along the same run (for instance, when a hash table needs to be expanded). In those situations, one needs a hash function which takes two parameters—the input data ''z'', and the number ''n'' of allowed hash values.

A common solution is to compute a fixed hash function with a very large range (say, 0 to 2&lt;sup&gt;32&lt;/sup&gt;&amp;nbsp;−&amp;nbsp;1), divide the result by ''n'', and use the division's [[modulo operation|remainder]]. If ''n'' is itself a power of 2, this can be done by [[Mask (computing)|bit masking]] and [[bit shifting]]. When this approach is used, the hash function must be chosen so that the result has fairly uniform distribution between 0 and ''n''&amp;nbsp;−&amp;nbsp;1, for any value of ''n'' that may occur in the application. Depending on the function, the remainder may be uniform only for certain values of ''n'', e.g. [[odd number|odd]] or [[prime number]]s.

We can allow the table size ''n'' to not be a power of 2 and still not have to perform any remainder or division operation, as these computations are sometimes costly. For example, let ''n'' be significantly less than 2&lt;sup&gt;''b''&lt;/sup&gt;. Consider a [[pseudorandom number generator]] (PRNG) function ''P''(key) that is uniform on the interval [0, 2&lt;sup&gt;''b''&lt;/sup&gt;&amp;nbsp;−&amp;nbsp;1]. A hash function uniform on the interval [0, n-1] is ''n'' ''P''(key)/2&lt;sup&gt;''b''&lt;/sup&gt;. We can replace the division by a (possibly faster) right [[bit shifting|bit shift]]: ''nP''(key) &gt;&gt; ''b''.

==== Variable range with minimal movement (dynamic hash function) ====
When the hash function is used to store values in a hash table that outlives the run of the program, and the hash table needs to be expanded or shrunk, the hash table is referred to as a dynamic hash table.

A hash function that will relocate the minimum number of records when the table is – where ''z'' is the key being hashed and ''n'' is the number of allowed hash values&amp;nbsp;– such that ''H''(''z'',''n''&amp;nbsp;+&amp;nbsp;1) = ''H''(''z'',''n'') with probability close to ''n''/(''n''&amp;nbsp;+&amp;nbsp;1).

[[Linear hashing]] and spiral storage are examples of dynamic hash functions that execute in constant time but relax the property of uniformity to achieve the minimal movement property.

[[Extendible hashing]] uses a dynamic hash function that requires space proportional to ''n'' to compute the hash function, and it becomes a function of the previous keys that have been inserted.

Several algorithms that preserve the uniformity property but require time proportional to ''n'' to compute the value of ''H''(''z'',''n'') have been invented.

=== Data normalization ===
In some applications, the input data may contain features that are irrelevant for comparison purposes. For example, when looking up a personal name, it may be desirable to ignore the distinction between upper and lower case letters. For such data, one must use a hash function that is compatible with the data [[equivalence relation|equivalence]] criterion being used: that is, any two inputs that are considered equivalent must yield the same hash value. This can be accomplished by normalizing the input before hashing it, as by upper-casing all letters.

=== Continuity ===
&quot;A hash function that is used to search for similar (as opposed to equivalent) data must be as [[continuous function|continuous]] as possible; two inputs that differ by a little should be mapped to equal or nearly equal hash values.&quot;&lt;ref name=&quot;Josiang&quot;&gt;[http://www.scribd.com/doc/199819816/Fundamental-Data-Structures &quot;Fundamental Data Structures - Josiang p.132&quot;]. Retrieved May 19, 2014.&lt;/ref&gt;

Note that continuity is usually considered a fatal flaw for checksums, [[cryptographic hash function]]s, and other related concepts. Continuity is desirable for hash functions only in some applications, such as hash tables used in [[Nearest neighbor search]].

=== Non-invertible ===
In cryptographic applications, hash functions are typically expected to be [[Inverse function|non-invertible]], meaning that it is not possible to reconstruct the input datum {{mvar|x}} from its hash value {{mvar|h}}({{mvar|x}}) alone without spending great amounts of computing time (see also [[One-way function]]).

== Hash function algorithms ==
For most types of hashing functions the choice of the function depends strongly on the nature of the input data, and their [[probability distribution]] in the intended application.

=== Trivial hash function ===
If the datum to be hashed is small enough, one can use the datum itself (reinterpreted as an integer) as the hashed value. The cost of computing this &quot;trivial&quot; ([[identity function|identity]]) hash function is effectively zero. This hash function is [[Perfect hash function|perfect]], as it maps each input to a distinct hash value.

The meaning of &quot;small enough&quot; depends on the size of the type that is used as the hashed value. For example, in [[Java (programming language)|Java]], the hash code is a 32-bit integer. Thus the 32-bit integer &lt;code&gt;Integer&lt;/code&gt; and 32-bit floating-point &lt;code&gt;Float&lt;/code&gt; objects can simply use the value directly; whereas the 64-bit integer &lt;code&gt;Long&lt;/code&gt; and 64-bit floating-point &lt;code&gt;Double&lt;/code&gt; cannot use this method.

Other types of data can also use this perfect hashing scheme. For example, when mapping [[character string]]s between [[case (typography)|upper and lower case]], one can use the binary encoding of each character, interpreted as an integer, to index a table that gives the alternative form of that character (&quot;A&quot; for &quot;a&quot;, &quot;8&quot; for &quot;8&quot;, etc.).  If each character is stored in 8 bits (as in [[ASCII]] or [[ISO Latin 1]]), the table has only 2&lt;sup&gt;8&lt;/sup&gt; = 256 entries; in the case of [[Unicode]] characters, the table would have 17×2&lt;sup&gt;16&lt;/sup&gt; = 1114112 entries.

The same technique can be used to map [[ISO 3166-1 alpha-2|two-letter country codes]] like &quot;us&quot; or &quot;za&quot; to country names (26&lt;sup&gt;2&lt;/sup&gt;=676 table entries), 5-digit zip codes like 13083 to city names (100000 entries), etc. Invalid data values (such as the country code &quot;xx&quot; or the zip code 00000) may be left undefined in the table, or mapped to some appropriate &quot;null&quot; value.

=== Perfect hashing ===
{{Main|Perfect hash function}}
[[File:Hash table 4 1 1 0 0 0 0 LL.svg|thumb|240px|right|A perfect hash function for the four names shown]]
A hash function that is [[injective function|injective]]—that is, maps each valid input to a different hash value—is said to be '''[[perfect hash function|perfect]]'''. With such a function one can directly locate the desired entry in a hash table, without any additional searching.

=== Minimal perfect hashing ===
[[File:Hash table 4 1 0 0 0 0 0 LL.svg|thumb|240px|right|A minimal perfect hash function for the four names shown]]
A perfect hash function for ''n'' keys is said to be '''minimal''' if its range consists of ''n'' ''consecutive'' integers, usually from 0 to ''n''−1. Besides providing single-step lookup, a minimal perfect hash function also yields a compact hash table, without any vacant slots. Minimal perfect hash functions are much harder to find than perfect ones with a wider range.

=== Hashing uniformly distributed data ===
If the inputs are bounded-length [[string (computer science)|strings]] and each input may [[statistical independence|independently]] occur with [[Uniform distribution (discrete)|uniform]] probability (such as [[telephone]] numbers, [[vehicle registration plate|car license plates]], [[invoice]] numbers, etc.), then a hash function needs to map roughly the same number of inputs to each hash value. For instance, suppose that each input is an integer ''z'' in the range 0 to ''N''−1, and the output must be an integer ''h'' in the range 0 to ''n''−1, where ''N'' is much larger than ''n''. Then the hash function could be  ''h'' = ''z'' '''mod''' ''n'' (the remainder of ''z'' divided by ''n''), or ''h'' = (''z'' × ''n'') ÷ ''N'' (the value ''z'' scaled down by ''n''/''N'' and truncated to an integer), or many other formulas.

''h'' = ''z'' '''mod''' ''n'' was used in many of the original random number generators, but was found to have a number of issues. One of which is that as ''n'' approaches ''N'', this function becomes less and less uniform.

=== Hashing data with other distributions ===
These simple formulas will not do if the input values are not equally likely, or are not independent. For instance, most patrons of a [[supermarket]] will live in the same geographic area, so their telephone numbers are likely to begin with the same 3 to 4 digits. In that case, if ''m'' is 10000 or so, the division formula (''z'' × ''m'') ÷ ''M'', which depends mainly on the leading digits, will generate a lot of collisions; whereas the remainder formula ''z'' '''mod''' ''m'', which is quite sensitive to the trailing digits, may still yield a fairly even distribution.

=== Hashing variable-length data ===
When the data values are long (or variable-length) [[character string]]s—such as personal names, [[URL|web page addresses]], or mail messages—their distribution is usually very uneven, with complicated dependencies. For example, text in any [[natural language]] has highly non-uniform distributions of [[character (computing)|characters]], and [[digraph (computing)|character pairs]], very characteristic of the language. For such data, it is prudent to use a hash function that depends on all characters of the string—and depends on each character in a different way.

In cryptographic hash functions, a [[Merkle–Damgård construction]] is usually used. In general, the scheme for hashing such data is to break the input into a sequence of small units ([[bit]]s, [[byte]]s, [[Word (data type)|words]], etc.) and combine all the units ''b''[1], ''b''[2], …, ''b''[''m''] sequentially, as follows

&lt;code&gt;
 S ← S0;                         // ''Initialize the state.''
 '''for''' k '''in''' 1, 2, ..., m '''do'''       // ''Scan the input data units:''
   S ← F(S, b[k]);               //   ''Combine data unit k into the state.''
 '''return''' G(S, n)                 // ''Extract the hash value from the state.''&lt;/code&gt;

This schema is also used in many text checksum and fingerprint algorithms. The state variable ''S'' may be a 32- or 64-bit unsigned integer; in that case, ''S0'' can be 0, and ''G''(''S'',''n'') can be just ''S'' '''mod''' ''n''. The best choice of ''F'' is a complex issue and depends on the nature of the data. If the units ''b''[''k''] are single bits, then ''F''(''S'',''b'') could be, for instance
&lt;code&gt;
  '''if''' highbit(S) = 0 '''then'''
    '''return''' 2 * S + b
  '''else'''
    '''return''' (2 * S + b) ^ P&lt;/code&gt;
Here ''highbit''(''S'') denotes the most significant bit of ''S''; the '&lt;tt&gt;*&lt;/tt&gt;' operator denotes unsigned integer multiplication with lost [[overflow (software)|overflow]]; '&lt;tt&gt;^&lt;/tt&gt;' is the bitwise [[exclusive or]] operation applied to words; and ''P'' is a suitable fixed word.&lt;ref name=&quot;bro2&quot;&gt;{{cite book |first=A. Z. |last=Broder |chapter=Some applications of Rabin's fingerprinting method |title=Sequences II: Methods in Communications, Security, and Computer Science |pages=143–152 |location= |publisher=Springer-Verlag |year=1993 |isbn= }}&lt;/ref&gt;

=== Special-purpose hash functions ===
In many cases, one can design a special-purpose ([[heuristic (computer science)|heuristic]]) hash function that yields many fewer collisions than a good general-purpose hash function. For example, suppose that the input data are file names such as &lt;tt&gt;FILE0000.CHK&lt;/tt&gt;, &lt;tt&gt;FILE0001.CHK&lt;/tt&gt;, &lt;tt&gt;FILE0002.CHK&lt;/tt&gt;, etc., with mostly sequential numbers. For such data, a function that extracts the numeric part ''k'' of the file name and returns ''k'' '''mod''' ''n'' would be nearly optimal. Needless to say, a function that is exceptionally good for a specific kind of data may have dismal performance on data with different distribution.

=== Rolling hash ===
{{Main|rolling hash}}
In some applications, such as [[string searching algorithm|substring search]], one must compute a hash function ''h'' for every ''k''-character [[substring]] of a given ''n''-character string ''t''; where  ''k'' is a fixed integer, and ''n'' is ''k''. The straightforward solution, which is to extract every such substring ''s'' of ''t'' and compute ''h''(''s'') separately, requires a number of operations proportional to ''k''·''n''. However, with the proper choice of ''h'', one can use the technique of rolling hash to compute all those hashes with an effort proportional to ''k''&amp;nbsp;+&amp;nbsp;''n''.

=== Universal hashing ===
A [[universal hashing]] scheme is a [[randomized algorithm]] that selects a hashing function ''h'' among a family of such functions, in such a way that the probability of a collision of any two distinct keys is 1/''n'', where ''n'' is the number of distinct hash values desired—independently of the two keys. Universal hashing ensures (in a probabilistic sense) that the hash function application will behave as well as if it were using a random function, for any distribution of the input data. It will however have more collisions than perfect hashing, and may require more operations than a special-purpose hash function. See also Unique Permutation Hashing.&lt;ref&gt;Shlomi Dolev, Limor Lahiani, Yinnon Haviv, &quot;Unique permutation hashing,&quot; Theoretical Computer Science Volume 475, 4 March 2013, Pages 59–65&lt;/ref&gt;

=== Hashing with checksum functions ===
One can adapt certain checksum or fingerprinting algorithms for use as hash functions. Some of those algorithms will map arbitrary long string data ''z'', with any typical real-world distribution—no matter how non-uniform and dependent—to a 32-bit or 64-bit string, from which one can extract a hash value in 0 through ''n''&amp;nbsp;−&amp;nbsp;1.

This method may produce a sufficiently uniform distribution of hash values, as long as the hash range size ''n'' is small compared to the range of the checksum or fingerprint function. However, some checksums fare poorly in the [[avalanche effect|avalanche test]], which may be a concern in some applications. In particular, the popular CRC32 checksum provides only 16 bits (the higher half of the result) that are usable for hashing{{citation needed|date=June 2012}}{{dubious|reason: Bret Mulvey's CRC-32 example code is incorrect|date=December 2013}}. Moreover, each bit of the input has a deterministic effect on each bit of the CRC32, that is one can tell without looking at the rest of the input, which bits of the output will flip if the input bit is flipped; so care must be taken to use all 32 bits when computing the hash from the checksum.&lt;ref&gt;Bret Mulvey, ''[http://bretmulvey.com/hash/8.html Evaluation of CRC32 for Hash Tables]'', in ''[http://bretmulvey.com/hash/ Hash Functions]''. Accessed April 10, 2009.&lt;/ref&gt;{{dubious|reason: Bret Mulvey's CRC-32 example code is incorrect|date=December 2013}}

=== Hashing with cryptographic hash functions ===
Some [[cryptographic hash function]]s, such as [[SHA-1]], have even stronger uniformity guarantees than checksums or fingerprints, and thus can provide very good general-purpose hashing functions.

In ordinary applications, this advantage may be too small to offset their much higher cost.&lt;ref&gt;Bret Mulvey, ''[http://bretmulvey.com/hash/9.html Evaluation of SHA-1 for Hash Tables]'', in ''[http://bretmulvey.com/hash/ Hash Functions]''. Accessed April 10, 2009.&lt;/ref&gt;  However, this method can provide uniformly distributed hashes even when the keys are chosen by a malicious agent. This feature may help to protect services against [[denial of service attack]]s.

=== Hashing By Nonlinear Table Lookup ===
Tables of random numbers (such as 256 random 32 bit integers) can provide high-quality nonlinear functions to be used
as hash functions or for other purposes such as cryptography. The key to be hashed would be split into 8-bit (one byte) parts and each part will be used as an index for the nonlinear table. The table values will be added by arithmetic or XOR addition to the hash output value. Because the table is just 1024 bytes in size, it will fit into the cache of modern microprocessors and allow for very fast execution of the hashing algorithm. As the table value is on average much longer than 8 bits, one bit of input will affect nearly all output bits. This is different from multiplicative hash functions where higher-value input bits do not affect lower-value output bits.

This algorithm has proven to be very fast and of high quality for hashing purposes (especially hashing of integer number keys).

=== Efficient Hashing Of Strings ===
* See also [[Universal hashing#Hashing strings|Universal hashing of strings]]
Modern microprocessors will allow for much faster processing, if 8-bit character strings are not hashed by processing one character at a time, but by interpreting the string as an array of 32 bit or 64 bit integers and hashing/accumulating these &quot;wide word&quot; integer values by means of arithmetic operations (e.g. multiplication by constant and bit-shifting). The remaining characters of the string which are smaller than the word length of the CPU must be handled differently (e.g. being processed one character at a time).

This approach has proven to speed up hash code generation by a factor of five or more on modern microprocessors of
a word size of 64 bit.{{citation needed|date=April 2013}}

Another approach&lt;ref&gt;http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.7520 Performance in Practice of String Hashing Functions&lt;/ref&gt; is to convert strings to a 32 or 64 bit numeric value and then apply a hash function.  One method that avoids the problem of strings having great similarity (&quot;Aaaaaaaaaa&quot; and &quot;Aaaaaaaaab&quot;) is to use a [[Cyclic redundancy check]] (CRC) of the string to compute a 32- or 64-bit value. While it is possible that two different strings will have the same CRC, the likelihood is very small and only requires that one check the actual string found to determine whether one has an exact match. CRCs will be different for strings such as &quot;Aaaaaaaaaa&quot; and &quot;Aaaaaaaaab&quot;. Although, CRC codes can be used as hash values&lt;ref&gt;{{cite web|url=http://www.strchr.com/hash_functions|title=Hash functions: An empirical comparison|author=Peter Kankowski}}&lt;/ref&gt; they are not cryptographically secure since they are not [[collision resistant]].&lt;ref&gt;{{Cite journal| last1=Cam-Winget | first1=Nancy | last2=Housley | first2=Russ | last3=Wagner | first3=David | last4=Walker | first4=Jesse | title=Security Flaws in 802.11 Data Link Protocols | journal=Communications of the ACM | volume=46 | issue=5 | pages=35–39 |date = May 2003| doi=10.1145/769800.769823 }}&lt;/ref&gt;

== Locality-sensitive hashing ==
[[Locality-sensitive hashing]] (LSH) is a method of performing probabilistic dimension reduction of high-dimensional data. The basic idea is to hash the input items so that similar items are mapped to the same buckets with high probability (the number of buckets being much smaller than the universe of possible input items). This is different from the conventional hash functions, such as those used in cryptography, as in this case the goal is to maximize the probability of &quot;collision&quot; of similar items rather than to avoid collisions.&lt;ref&gt;{{cite web|author=A. Rajaraman and J. Ullman| url=http://infolab.stanford.edu/~ullman/mmds.html |title=Mining of Massive Datasets, Ch. 3. |year=2010}}&lt;/ref&gt;

One example of LSH is [[MinHash]] algorithm used for finding similar documents (such as web-pages):

Let ''h'' be a hash function that maps the members of {{math|''A''}} and {{math|''B''}} to distinct integers, and for any set ''S'' define {{math|''h''&lt;sub&gt;min&lt;/sub&gt;(''S'')}} to be the member {{math|''x''}} of {{math|''S''}} with the minimum value of {{math|''h''(''x'')}}. Then {{math|1=''h''&lt;sub&gt;min&lt;/sub&gt;(''A'') = ''h''&lt;sub&gt;min&lt;/sub&gt;(''B'')}} exactly when the minimum hash value of the union {{math|''A'' &amp;cup; ''B''}} lies in the intersection {{math|''A'' &amp;cap; ''B''}}.
Therefore,
:{{math|1=Pr[''h''&lt;sub&gt;min&lt;/sub&gt;(''A'') = ''h''&lt;sub&gt;min&lt;/sub&gt;(''B'')] = ''J''(''A'',''B'').}} where J is [[Jaccard index]].
In other words, if {{math|''r''}} is a random variable that is one when {{math|1=''h''&lt;sub&gt;min&lt;/sub&gt;(''A'') = ''h''&lt;sub&gt;min&lt;/sub&gt;(''B'')}} and zero otherwise, then {{math|''r''}} is an [[Bias of an estimator|unbiased estimator]] of {{math|''J''(''A'',''B'')}}, although it has too high a [[variance]] to be useful on its own. The idea of the MinHash scheme is to reduce the variance by averaging together several variables constructed in the same way.

== Origins of the term ==
The term &quot;hash&quot; comes by way of analogy with its non-technical meaning, to &quot;chop and mix&quot;. Indeed, typical hash functions, like the '''[[modular arithmetic|mod]]''' operation, &quot;chop&quot; the input domain into many sub-domains that get &quot;mixed&quot; into the output range to improve the uniformity of the key distribution.{{citation needed|date=June 2014}}

[[Donald Knuth]] notes that [[Hans Peter Luhn]] of [[IBM]] appears to have been the first to use the concept, in a memo dated January 1953, and that [[Robert Morris (cryptographer)|Robert Morris]] used the term in a survey paper in [[Communications of the ACM|CACM]] which elevated the term from technical jargon to formal terminology.&lt;ref name=&quot;knuth&quot;&gt;{{cite book | author=[[Donald Knuth|Knuth, Donald]] | year=1973 | title=[[The Art of Computer Programming]], volume 3, Sorting and Searching | pages=506–542 }}&lt;/ref&gt;

== List of hash functions ==
{{Main|List of hash functions}}
*[[NIST hash function competition]]
*Bernstein hash&lt;ref&gt;{{cite web|url=http://www.cse.yorku.ca/~oz/hash.html |title=Hash Functions |work=cse.yorku.ca |date=September 22, 2003 |quote=the djb2 algorithm (k=33) was first reported by dan bernstein many years ago in comp.lang.c. |accessdate=November 1, 2012}}&lt;/ref&gt;
*[[Fowler-Noll-Vo hash function]] (32, 64, 128, 256, 512, or 1024 bits)
*[[Jenkins hash function]] (32 bits)
*[[Pearson hashing]] (8 bits) (64 bit hash C code provided)
*[[Zobrist hashing]]

== See also ==
{{Portal-inline|Computer science}}
{{columns-list|colwidth=20em|
*[[Bloom filter]]
*[[Coalesced hashing]]
*[[Cuckoo hashing]]
*[[Hopscotch hashing]]
*[[Cryptographic hash function]]
*[[Distributed hash table]]
*[[Geometric hashing]]
*[[Hash Code cracker]]
*[[Hash table]]
*[[HMAC]]
*[[Identicon]]
*[[Linear hash]]
*[[List of hash functions]]
*[[Locality sensitive hashing]]
*[[MD5]]
*[[Perfect hash function]]
*[[Rabin–Karp string search algorithm]]
*[[Rolling hash]]
*[[Transposition table]]
*[[Universal hashing]]
*[[MinHash]]
*[[Low-discrepancy sequence]]
}}

== References ==
{{Reflist}}

== External links ==
{{Wiktionary|hash}}
*[http://burtleburtle.net/bob/hash/index.html Hash Functions and Block Ciphers  by Bob Jenkins]
*[http://www.webcitation.org/query?url=http://www.geocities.com/drone115b/Goulburn06.pdf&amp;date=2009-10-25+21:06:51  The Goulburn Hashing Function] ([[Portable Document Format|PDF]]) by Mayur Patel
* [http://herakles.zcu.cz/~skala/PUBL/PUBL_2010/2010_WSEAS-Corfu_Hash-final.pdf Hash Function Construction for Textual and Geometrical Data Retrieval] Latest Trends on Computers, Vol.2, pp.&amp;nbsp;483–489, CSCC conference, Corfu, 2010

[[Category:Error detection and correction]]
[[Category:Hash functions| ]]
[[Category:Search algorithms]]</text>
      <sha1>2rmnxk5n6rmnrr07dp314yx5s380zqb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Identicon</title>
    <ns>0</ns>
    <id>9248918</id>
    <revision>
      <id>617985698</id>
      <parentid>617338390</parentid>
      <timestamp>2014-07-22T13:57:40Z</timestamp>
      <contributor>
        <username>ChristianLuijten</username>
        <id>19307072</id>
      </contributor>
      <comment>/* Applications */ Use wayback link to fix dead link to Gemini</comment>
      <text xml:space="preserve" bytes="4124">[[File:Identicon.svg|thumb|right|An arbitrary Identicon]]
An '''Identicon''' is a visual representation of a [[hash value]], usually of an [[IP address]], that serves to identify a user of a computer system as a form of [[Avatar (computing)|avatar]] while protecting the users' privacy. The original Identicon was a 9-block graphic, and the representation has been extended to other graphic forms by third parties.

== Invention ==

Don Park came up with the Identicon idea on January 18, 2007.  In his words:

{{Quote|text=I originally came up with this idea to be used as an easy means of visually distinguishing multiple units of information, anything that can be reduced to bits. It's not just IPs but also people, places, and things.  IMHO, too much of the web what we read are textual or numeric information which are not easy to distinguish at a glance when they are jumbled up together. So I think adding visual identifiers will make the user experience much more enjoyable.|sign=Don Park|source=&lt;ref&gt;{{Wayback|url=www.docuverse.com/blog/donpark/2007/01/18/visual-security-9-block-ip-identification|title=Don Park's Daily Habit|date=20080703155519}}&lt;/ref&gt;
}}

== Releases ==

The original Identicon source package 0.1 was server-side [[Java (programming language)|Java]].  Version 0.2 was cleaned up, added some documentation, fixed a color bug, added a cache, and a runtime [[JAR file|jar]]. Version 0.3 included client-side [[Canvas element|Canvas]] tags. The current version is 0.5.

Other similar applications have been created. The Vash&lt;ref&gt;[http://www.thevash.com The Vash]&lt;/ref&gt; is a visual hash generator dual-licensed with [[Affero]] and proprietary licenses.&lt;ref&gt;[http://developers.slashdot.org/story/11/07/15/2324212/Visual-Hash-Turns-Text-Or-Data-Into-Abstract-Art Visual Hash Turns Text Or Data Into Abstract Art]. [[Slashdot]].&lt;/ref&gt;

== Applications ==

* One use is embedding them in wiki pages and [[blog]] comments to identify authors.  The thought includes protecting an author from someone else using his name to comment.  It would be obvious because, in those cases where the ISPs provide unique instead of dynamic IPs, the IP addresses would generate different Identicons.
* Third-party software is available to generate identicons for the purposes of identifying eBay sellers.&lt;ref&gt;[http://www.munnin.com/en/program_identicon.php Munnin [ Product - Identicon &amp;#93; Quick seller identification in the item listing&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* The original Identicon idea has been expanded to include a couple of new, simple yet very effective, anti-[[phishing]] protection schemes. One of them requires client-side support; Park is interested in talking to browser vendors regarding its incorporation. He calls this expansion &quot;Gemini.&quot;&lt;ref&gt;{{Wayback| title = Identicon-based anti-phishing protection | url = http://www.docuverse.com/blog/donpark/2007/01/22/identicon-based-anti-phishing-protection | quote = In a roundabout way, it's related to my idea of using DRM to protect one-time passwords but very different once it hatched.|date=20080510221519}}&lt;/ref&gt;
*An Add-On for [[Firefox]] called IdentFavIcon that, on web pages without favicons, replaces them in the tab with Identicons based on the IP of the page.

== References ==

{{Reflist}}

== External links ==
* [https://github.com/donpark/identicon Don Park's original source code at GitHub]
* [https://github.com/pauloubuntu/identicon Mavenized project based on Don Park's original source code at GitHub]
* [http://haacked.com/archive/2007/01/22/Identicons_as_Visual_Fingerprints.aspx Identicons as Visual Fingerprints] by Phil Haack
* [http://digitalconsumption.com/forum/Visiglyphs-for-IP-visualisation Visiglyphs]
* [http://sourceforge.net/projects/identicons PHP-Identicons]
* [http://identicon.riaforge.org/ Identicon CFC - ColdFusion implementation of Identicons]
* [https://github.com/cupcake/sigil Sigil]
* [https://github.com/RobThree/NIdenticon NIdenticon - .Net implementation of Identicons]

[[Category:Hash functions]]
[[Category:Internet forum terminology]]
[[Category:Web 2.0 neologisms]]
[[Category:Identifiers]]</text>
      <sha1>9293zj77j8nxa2mtc6xrequ4jagcmip</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>List of hash functions</title>
    <ns>0</ns>
    <id>2155690</id>
    <revision>
      <id>622860796</id>
      <parentid>617278515</parentid>
      <timestamp>2014-08-26T09:19:34Z</timestamp>
      <contributor>
        <ip>80.12.110.202</ip>
      </contributor>
      <comment>/* Non-cryptographic hash functions */</comment>
      <text xml:space="preserve" bytes="4329">This is a list of [[hash function]]s, including [[cyclic redundancy check]]s, [[checksum]] functions, and [[cryptographic hash function]]s.

==Cyclic redundancy checks==
{{Main|Cyclic redundancy check}}

{| class=&quot;wikitable&quot;
!Name
!Length
!Type
|-
|[[BSD checksum]]
|16 bits
|[[cyclic redundancy check|CRC]]
|-
|checksum
|32 bits
|[[cyclic redundancy check|CRC]]
|-
|[[crc16]]
|16 bits
|[[cyclic redundancy check|CRC]]
|-
|[[crc32]]
|32 bits
|[[cyclic redundancy check|CRC]]
|-
|[[crc32 mpeg2]]
|32 bits
|[[cyclic redundancy check|CRC]]
|-
|[[crc64]]
|64 bits
|[[cyclic redundancy check|CRC]]
|-
|[[SYSV checksum]]
|16 bits
|[[cyclic redundancy check|CRC]]
|-
|}

[[Adler-32]] is often classified as a CRC, but it uses a different algorithm.

==Checksums==
{{Main|Checksum}}

{| class=&quot;wikitable&quot;
|-
!Name
!Length
!Type
|-
|[[sum (Unix)]]
|16 or 32 bits
|sum
|-|-
|sum8
|8 bits
|sum
|-
|[[sum16]]
|16 bits
|sum
|-
|sum24
|24 bits
|sum
|-
|sum32
|32 bits
|sum
|-
|[[Fletcher's checksum|fletcher-4]]
|4 bits
|sum
|-
|[[Fletcher's checksum|fletcher-8]]
|8 bits
|sum
|-
|[[Fletcher's checksum|fletcher-16]]
|16 bits
|sum
|-
|[[Fletcher's checksum|fletcher-32]]
|32 bits
|sum
|-
|[[Adler-32]]
|32 bits
|sum
|-
| [[longitudinal redundancy check| xor8]]
|8 bits
|sum
|-
|[[Luhn algorithm]]
|4 bits
|sum
|-
|[[Verhoeff algorithm]]
|4 bits
|sum
|-
|[[Damm algorithm]]
|1 decimal digit 
|[[Quasigroup]] [[Binary operation|operation]]
|-
|}

==Non-cryptographic hash functions==
{| class=&quot;wikitable&quot;
|-
!Name
!Length
!Type
|-
|[[Pearson hashing]]
|8 bits
|xor/table
|-
|Buzhash
|variable
|xor/table
|-
|[[Fowler–Noll–Vo hash function]]&lt;br&gt;(FNV Hash)
|32, 64, 128, 256,&lt;br&gt; 512, or 1024 bits
|xor/product or &lt;br&gt; product/xor
|-
|[[Zobrist hashing]]
| variable
| xor
|-
|[[Jenkins hash function]]
|32 or 64 bits
| xor/addition 
|-
|[[Java hashCode()]]
|32 bits
| 
|-
|Bernstein hash&lt;ref&gt;http://www.cse.yorku.ca/~oz/hash.html&lt;/ref&gt;
|32 bits
| 
|-
|elf64
|64 bits
|hash
|-
|[[MurmurHash]]
|32, 64, or 128 bits
|product/rotation
|-
|SpookyHash
|32, 64 or 128 bits
|see [[Jenkins hash function]]
|-
|[[CityHash]]
|64, 128, or 256 bits
|
|-
|xxHash &lt;ref&gt;https://code.google.com/p/xxhash/&lt;/ref&gt;
|32, 64 bits
|
|}

==Cryptographic hash functions==
{{Main|cryptographic hash function}}

{| class=&quot;wikitable&quot;
|-
!Name
!Length
!Type
|-
|[[BLAKE_(hash function)|BLAKE]]-256
|256 bits
|HAIFA structure&lt;ref name=haifa&gt;{{cite paper |author=Eli Biham and Orr Dunkelman |date={{date|2007-07-20}} |title=A Framework for Iterative Hash Functions - HAIFA |url=http://eprint.iacr.org/2007/278 }}&lt;/ref&gt;
|-
|[[BLAKE_(hash function)|BLAKE]]-512
|512 bits
|HAIFA structure&lt;ref name=haifa /&gt;
|-
|[[Elliptic curve only hash|ECOH]]
|224 to 512 bits
|hash
|-
|[[Fast Syndrome Based Hash|FSB]]
|160 to 512 bits
|hash
|-
|[[GOST_(hash_function)|GOST]]
|256 bits
|hash
|-
|[[Grøstl]]
|256 to 512 bits
|hash
|-
|[[HAS-160]]
|160 bits
|hash
|-
|[[HAVAL]]
|128 to 256 bits
|hash
|-
|[[JH_(hash function)|JH]]
|512 bits
|hash
|-
|[[MD2 (cryptography)|MD2]]
|128 bits
|hash
|-
|[[MD4]]
|128 bits
|hash
|-
|[[MD5]]
|128 bits
| [[Merkle-Damgård construction]]
|-
|[[MD6]]
|512 bits
|[[Merkle tree]] [[NLFSR]]
|-
|[[RadioGatún]]
|Up to 1216 bits
|hash
|-
|[[RIPEMD]]-64
|64 bits
|hash
|-
|[[RIPEMD]]-160
|160 bits
|hash
|-
|[[RIPEMD]]-320
|320 bits
|hash
|-
|[[SHA-1]]
|160 bits
|[[Merkle-Damgård construction]]
|-
|[[SHA-2|SHA-224]]
|224 bits
|[[Merkle-Damgård construction]]
|-
|[[SHA-2|SHA-256]]
|256 bits
|[[Merkle-Damgård construction]]
|-
|[[SHA-2|SHA-384]]
|384 bits
|[[Merkle-Damgård construction]]
|-
|[[SHA-2|SHA-512]]
|512 bits
|[[Merkle-Damgård construction]]
|-
|[[SHA-3]] (originally known as Keccak)
|arbitrary
|[[Sponge function]]
|-
|[[Skein (hash function)|Skein]]
|arbitrary
|[[Unique Block Iteration]]
|-
|[[SipHash]]
|64 bits
|non-collision-resistant PRF
|-
|[[Snefru]]
|128 or 256 bits
|hash
|-
|[[Spectral Hash]]
|512 bits
|Wide Pipe Merkle-Damgård construction
|-
|[[SWIFFT]]
|512 bits
|hash
|-
|[[Tiger (cryptography)|Tiger]]
|192 bits
|[[Merkle-Damgård construction]]
|-
|[[Whirlpool (cryptography)|Whirlpool]]
|512 bits
|hash
|-
|}

==See also==
*[[NIST hash function competition]]

==Notes==
&lt;references /&gt;

[[Category:Hash functions|*List]]
[[Category:Checksum algorithms| ]]
[[Category:Cryptography lists and comparisons|Hash functions]]</text>
      <sha1>o3847dkjenldd7y5v5tmzduqr8153dq</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>SipHash</title>
    <ns>0</ns>
    <id>37927149</id>
    <revision>
      <id>621644699</id>
      <parentid>621125991</parentid>
      <timestamp>2014-08-17T16:32:26Z</timestamp>
      <contributor>
        <username>Chmarkine</username>
        <id>15398482</id>
      </contributor>
      <minor/>
      <comment>/* Usage */change to https if the server sends [[HTTP Strict Transport Security|HSTS]] header, replaced: http://www.python.org → https://www.python.org using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="2802">'''SipHash''' is a family of [[pseudorandom function]]s created by [[Jean-Philippe Aumasson]] and [[Daniel J. Bernstein]] in 2012.&lt;ref name=&quot;paper&quot;&gt;{{cite web
| url=https://131002.net/siphash/siphash.pdf
| title=SipHash: a fast short-input PRF
| author=Jean-Philippe Aumasson and Daniel J. Bernstein
| date=2012-09-18}}&lt;/ref&gt;

==Overview==

SipHash computes 64-bit [[message authentication code]] from a variable-length message and 128-bit secret key. It was designed to be efficient even for short inputs, with performance comparable to non-cryptographic hash functions, such as [[CityHash]],&lt;ref name=&quot;paper&quot;/&gt;
thus can be used in [[hash table]]s
to prevent [[Denial-of-service attack|DoS]] collision attack (hash flooding) or to authenticate [[network packet]]s.

Functions in SipHash family are specified as SipHash-''c''-''d'', where ''c'' is the number of rounds per message block and ''d'' is the number of finalization rounds. The recommended parameters are SipHash-2-4 for best performance, and SipHash-4-8 for conservative security.

==Usage==

SipHash is used in [[hash table]] implementations of various software:&lt;ref&gt;{{cite web
| url=https://131002.net/siphash/#us
| title=SipHash: a fast short-input PRF, Users
| author=Jean-Philippe Aumasson, Daniel J. Bernstein}}&lt;/ref&gt;
* [[Perl]]
* [[Python (programming language)|Python]] (starting in version 3.4)&lt;ref&gt;{{cite web | url=https://www.python.org/dev/peps/pep-0456/ |  title=PEP 456 -- Secure and interchangeable hash algorithm | author=Christian Heimes | accessdate=20 November 2013}}&lt;/ref&gt;
* [[Redis]]
* [[Ruby (programming language)|Ruby]]
* [[Rust (programming language)|Rust]]
* [[systemd]]
* [[OpenDNS]]
* [[Haskell (programming language)|Haskell]]

Native Implementations
* [https://github.com/jedisct1/siphash-js Javascript]

==See also==
* [[Cryptographic hash function]]
* [[Hash function]]
* [[List of hash functions]]

==References==
&lt;references/&gt;

==External links==
* {{cite web
| url=https://131002.net/siphash/
| title=SipHash: a fast short-input PRF - Project Page
| author=Jean-Philippe Aumasson, Daniel J. Bernstein}}

* {{cite web
| url=https://131002.net/siphash/siphash.pdf
| title=SipHash: a fast short-input PRF
| author=Jean-Philippe Aumasson and Daniel J. Bernstein
| date=2012-09-18}}

* {{cite web
| url=https://131002.net/siphash/siphash_slides.pdf
| title=SipHash: a fast short-input PRF - Presentation slides
| author=Jean-Philippe Aumasson, Daniel J. Bernstein
| date=2012-08-15}}

* {{cite web
| url=http://media.ccc.de/browse/congress/2012/29c3-5152-en-hashflooding_dos_reloaded_h264.html
| title=Hash-flooding DoS reloaded: attacks and defenses (video)
| author=Jean-Philippe Aumasson, Daniel J. Bernstein, Martin Boßlet
| date=2012-12-29}}


{{Cryptography navbox | hash }}

[[Category:Hash functions]]</text>
      <sha1>3yp3g6qhfneywyghtigiaw4seebzpot</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Fowler–Noll–Vo hash function</title>
    <ns>0</ns>
    <id>1096165</id>
    <revision>
      <id>623138808</id>
      <parentid>623042018</parentid>
      <timestamp>2014-08-28T06:05:52Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>/* Non-Cryptographic Hash */[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (10422)</comment>
      <text xml:space="preserve" bytes="6501">'''Fowler–Noll–Vo''' is a non-[[Cryptographic hash function|cryptographic]] [[hash function]] created by Glenn Fowler, [[Landon Curt Noll]], and Phong Vo.

The basis of the FNV hash algorithm was taken from an idea sent as reviewer comments to the [[POSIX|IEEE POSIX P1003.2]] committee by Glenn Fowler and Phong Vo in 1991. In a subsequent ballot round, Landon Curt Noll improved on their algorithm. Some people tried this hash and found that it worked rather well.&lt;ref&gt;http://programmers.stackexchange.com/a/145633/108318&lt;/ref&gt; In an email message to Landon, they named it the ''Fowler/Noll/Vo'' or FNV hash.&lt;ref&gt;[http://www.isthe.com/chongo/tech/comp/fnv/index.html#history FNV hash history]&lt;/ref&gt;

==Overview==
The current versions are FNV-1 and FNV-1a, which supply a means of creating non-zero '''FNV offset basis'''.  FNV currently comes in 32-, 64-, 128-, 256-, 512-, and 1024-bit flavors.  For pure FNV implementations, this is determined solely by the availability of '''FNV prime'''s for the desired bit length; however, the FNV webpage discusses methods of adapting one of the above versions to a smaller length that may  or may not be a power of two.&lt;ref&gt;[http://www.isthe.com/chongo/tech/comp/fnv/index.html#xor-fold Changing the FNV hash size - xor-folding]&lt;/ref&gt;&lt;ref&gt;[http://www.isthe.com/chongo/tech/comp/fnv/index.html#other-folding Changing the FNV hash size - non-powers of 2]&lt;/ref&gt;

The FNV hash algorithms and [http://www.isthe.com/chongo/tech/comp/fnv/index.html#FNV-source sample FNV source code] have been released into the public domain.&lt;ref&gt;[http://www.isthe.com/chongo/tech/comp/fnv/index.html#public_domain FNV put into the public domain]&lt;/ref&gt;

FNV is not a [[cryptographic hash]].

==The hash==

One of FNV's key advantages is that it is very simple to implement.  Start with an initial hash value of '''FNV offset basis'''.  For each byte in the input, [[multiply]] ''hash'' by the '''FNV prime''', then [[XOR]] it with the byte from the input.  The alternate algorithm, FNV-1a, reverses the multiply and XOR steps.

===FNV-1 hash===

The FNV-1 hash algorithm is as follows:&lt;ref&gt;[http://www.isthe.com/chongo/tech/comp/fnv/index.html#FNV-1 The core of the FNV hash]&lt;/ref&gt;

    ''hash'' = '''''FNV_offset_basis'''''
    for each ''octet_of_data'' to be hashed
    	''hash'' = ''hash'' × '''''FNV_prime'''''
    	''hash'' = ''hash'' [[XOR]] ''octet_of_data''
    return ''hash''

In the above [[pseudocode]], all variables are [[signedness|unsigned]] [[integers]].  All variables, except for ''octet_of_data'', have the same number of [[bit]]s as the FNV hash.  The variable, ''octet_of_data'', is an 8 [[bit]] unsigned [[integer]].

As an example, consider the 64-[[bit]] FNV-1 hash:

* All variables, except for ''octet_of_data'', are 64-[[bit]] unsigned [[integers]].
* The variable, ''octet_of_data'', is an 8 [[bit]] unsigned [[integer]].
* The '''''FNV_offset_basis''''' is the 64-[[bit]] '''FNV offset basis''' value: 14695981039346656037 (in hex, 0xcbf29ce484222325).
* The '''''FNV_prime''''' is the 64-[[bit]] '''FNV prime''' value: 1099511628211 (in hex, 0x100000001b3).
* The [[multiply]] returns the lower 64-[[bit]]s of the [[Product (mathematics)|product]].
* The [[XOR]] is an 8-[[bit]] operation that modifies only the lower 8-[[bit]]s of the hash value.
* The ''hash'' value returned is a 64-[[bit]] [[Signedness|unsigned]] [[Integer (computer science)|integer]].

The values for '''FNV prime''' and '''FNV offset basis''' may be found in this table.&lt;ref&gt;[http://www.isthe.com/chongo/tech/comp/fnv/index.html#FNV-param Parameters of the FNV-1 hash]&lt;/ref&gt;

===FNV-1a hash===

The FNV-1a hash differs from the FNV-1 hash by only the order in which the [[multiply]] and [[XOR]] is performed:&lt;ref&gt;[http://www.isthe.com/chongo/tech/comp/fnv/index.html#FNV-1a FNV-1a alternate algorithm]&lt;/ref&gt;

    ''hash'' = '''''FNV_offset_basis'''''
    for each ''octet_of_data'' to be hashed
    	''hash'' = ''hash'' [[XOR]] ''octet_of_data''
    	''hash'' = ''hash'' × '''''FNV_prime'''''
    return ''hash''

The above [[pseudocode]] has the same assumptions that were noted for the FNV-1 [[pseudocode]].  The small change in order leads to much better [[avalanche effect|avalanche characteristics]].&lt;ref&gt;[http://murmurhash.googlepages.com/avalanche Avalanche]&lt;/ref&gt;

==Non-Cryptographic Hash==

The FNV hash was designed for fast [[Hash table]] and [[Checksum]] use, not [[cryptography]]. The authors have identified the following properties as making the algorithm unsuitable as a [[cryptographic hash function]]:&lt;ref&gt;[http://tools.ietf.org/html/draft-eastlake-fnv#section-6.1 The FNV Non-Cryptographic Hash Algorithm - Why is FNV Non-Cryptographic?]&lt;/ref&gt;

* '''Speed of Computation''' - As a hash designed primarily for hashtable and checksum use, FNV-1 and 1a were designed to be fast to compute. However, this same speed makes finding specific hash values (collisions) by brute force faster.
* '''Sticky State''' - Being an iterative hash based primarily on multiplication and XOR, the algorithm is sensitive to the number zero. Specifically, if the hash value were to become zero at any point during calculation, and the next byte hashed were also all zeroes, the hash would not change. This makes colliding messages trivial to create given a message that results in a hash value of zero at some point in its calculation. Additional operations, such as the addition of a third constant prime on each step, can mitigate this but may have detrimental effects on avalanche effect or random distribution of hash values.
* '''Diffusion''' - The ideal secure hash function is one in which each byte of input has an equally-complex effect on every bit of the hash. In the FNV hash, the ones place (the rightmost bit) is always the XOR of the rightmost bit of every input byte. This can be mitigated by XOR-folding (computing a hash twice the desired length, and then XORing the bits in the &quot;upper half&quot; with the bits in the &quot;lower half&quot;).

==See also==
* [[Pearson hashing]] (uses a constant linear permutation instead of a constant prime seed)
* [[Jenkins hash function]]
* [[MurmurHash]]

==Notes==
{{reflist|2}}

==External links==
* [http://www.isthe.com/chongo/tech/comp/fnv/index.html Landon Curt Noll's webpage on FNV] (with table of base &amp; prime parameters)
* [http://tools.ietf.org/html/draft-eastlake-fnv Internet draft by Fowler, Noll, Vo, and Eastlake] (2011, work in progress)

{{DEFAULTSORT:Fowler-Noll-Vo hash function}}
[[Category:Hash functions]]</text>
      <sha1>byspuvlqyzb8gnadwnph55rsjoufgdw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Toeplitz Hash Algorithm</title>
    <ns>0</ns>
    <id>42835786</id>
    <revision>
      <id>613319745</id>
      <parentid>613319512</parentid>
      <timestamp>2014-06-17T18:52:18Z</timestamp>
      <contributor>
        <username>JacobiJonesJr</username>
        <id>18766808</id>
      </contributor>
      <text xml:space="preserve" bytes="1423">
{{Infobox cryptographic hash function
| name           = Toeplitz Hash
| image          = 
| caption        = 
&lt;!-- General --&gt;
| designers      =
| publish date   = 
| series         = 
| derived from   =
| derived to     = 
| related to     = [[Scalable Networking Pack|Receive Side Scaling]]
| certification  = 
&lt;!-- Detail --&gt;
| digest size    = 
| structure      = 
| rounds         = 
| speed          = 
| cryptanalysis  = 
}}

The Toeplitz hash function is used in many [[Network interface controller|network interface controllers]] for receive side scaling.&lt;ref name=&quot;kernel-org-scaling&quot;&gt;{{cite web|url=https://www.kernel.org/doc/Documentation/networking/scaling.txt|title=Scaling in the Linux Networking Stack|accessdate=2014-05-22|archiveurl=http://web.archive.org/web/20140522233520/https://www.kernel.org/doc/Documentation/networking/scaling.txt|archivedate=22 May 2014|deadurl=no}}&lt;/ref&gt;&lt;ref name=&quot;microsoft-ndis-rss&quot;&gt;{{cite web|url=http://download.microsoft.com/download/5/D/6/5D6EAF2B-7DDF-476B-93DC-7CF0072878E6/NDIS_RSS.doc|title=Scalable Networking: Eliminating the Receive Processing Bottleneck—Introducing RSS|accessdate=2014-05-22|archiveurl=http://web.archive.org/web/20140522235610/http://download.microsoft.com/download/5/D/6/5D6EAF2B-7DDF-476B-93DC-7CF0072878E6/NDIS_RSS.doc|archivedate=22 May 2014|deadurl=no}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Hash functions]]
{{compu-prog-stub}}</text>
      <sha1>izdjms6bfmvihafuj8cyhwf8wghd9sz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>FM-index</title>
    <ns>0</ns>
    <id>22143928</id>
    <revision>
      <id>619093970</id>
      <parentid>611961269</parentid>
      <timestamp>2014-07-30T07:28:04Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>Replace unicode entity nbsp for character [NBSP] (or space)  per [[WP:NBSP]] + other fixes, replaced: →   (14) using [[Project:AWB|AWB]] (10331)</comment>
      <text xml:space="preserve" bytes="9798">In [[computer science]], an '''FM-index''' is a compressed full-text [[substring index]] based on the [[Burrows-Wheeler transform]], with some similarities to the [[suffix array]]. It was created by Paolo Ferragina and Giovanni Manzini,&lt;ref name=&quot;opportunistic_2000&quot;&gt;Paolo Ferragina and Giovanni Manzini (2000). &quot;Opportunistic Data Structures with Applications&quot;. Proceedings of the 41st Annual Symposium on Foundations of Computer Science. p.390.&lt;/ref&gt; who describe it as an opportunistic data structure as it allows compression of the input text while still permitting fast substring queries. The name stands for Full-text index in Minute space.&lt;ref&gt;Paolo Ferragina and Giovanni Manzini (2005). &quot;Indexing Compressed Text&quot;. Journal of the ACM (JACM), 52, 4 (Jul. 2005). p. 553&lt;/ref&gt;

It can be used to efficiently find the number of occurrences of a pattern within the compressed text, as well as locate the position of each occurrence. Both the query time and storage space requirements are sublinear with respect to the size of the input data.

The original authors have devised improvements to their original approach and dubbed it &quot;FM-Index version 2&quot;.&lt;ref&gt;[http://www.di.unipi.it/~ferragin/Libraries/fmindexV2/index.html Paolo Ferragina and Rossano Venturini &quot;FM-Index version 2&quot;]&lt;/ref&gt; A further improvement, the alphabet-friendly FM-index, combines the use of compression boosting and [[Wavelet Tree|wavelet trees]] &lt;ref name=&quot;FGMN04&quot;&gt;P. Ferragina, G. Manzini, V. Mäkinen and G. Navarro. An Alphabet-Friendly FM-index. ''In Proc. SPIRE'04'', pages 150-160. LNCS 3246.&lt;/ref&gt; to significantly reduce the space usage for large alphabets.

The FM-index has found use in, among other places, [[bioinformatics]].&lt;ref&gt;Simpson, Jared T. and Durbin, Richard (2010). &quot;Efficient construction of an assembly string graph using the FM-index&quot;. Bioinformatics, 26, 12 (Jun. 17). p. i367&lt;/ref&gt;

== Background ==

Using an index is a common strategy to efficiently search a large body of text. When the text is larger than what reasonably fits within a computer's main memory, there is a need to compress not only the text but also the index. When the FM-index was introduced, there were several suggested solutions that were based on traditional compression methods and tried to solve the compressed matching problem. In contrast, the FM-index is a compressed self-index, which means that it compresses the data and indexes it at the same time.

== FM-index data structure ==

An FM-index is created by first taking the [[Burrows-Wheeler transform]] (BWT) of the input text. For example, the BWT of the string {{math|T {{=}} }}&quot;abracadabra&quot; is &quot;ard$rcaaaabb&quot;, and here it is represented by the matrix {{math|M}} where each row is a rotation of the text that has been sorted. The transform corresponds to the last column labeled {{math|L}}.

{| cellpadding=&quot;0&quot; border=&quot;0&quot; align=&quot;center&quot;
|-
! {{math|F}} !!  !! {{math|L}}
|-
| &lt;tt&gt;$&lt;/tt&gt; || &lt;tt&gt;abracadabr&lt;/tt&gt; || &lt;tt&gt;a&lt;/tt&gt;
|-
| &lt;tt&gt;a&lt;/tt&gt; || &lt;tt&gt;$abracadab&lt;/tt&gt; || &lt;tt&gt;r&lt;/tt&gt;
|-
| &lt;tt&gt;a&lt;/tt&gt; || &lt;tt&gt;bra$abraca&lt;/tt&gt; || &lt;tt&gt;d&lt;/tt&gt;
|-
| &lt;tt&gt;a&lt;/tt&gt; || &lt;tt&gt;bracadabra&lt;/tt&gt; || &lt;tt&gt;$&lt;/tt&gt;
|-
| &lt;tt&gt;a&lt;/tt&gt; || &lt;tt&gt;cadabra$ab&lt;/tt&gt; || &lt;tt&gt;r&lt;/tt&gt;
|-
| &lt;tt&gt;a&lt;/tt&gt; || &lt;tt&gt;dabra$abra&lt;/tt&gt; || &lt;tt&gt;c&lt;/tt&gt;
|-
| &lt;tt&gt;b&lt;/tt&gt; || &lt;tt&gt;ra$abracad&lt;/tt&gt; || &lt;tt&gt;a&lt;/tt&gt;
|-
| &lt;tt&gt;b&lt;/tt&gt; || &lt;tt&gt;racadabra$&lt;/tt&gt; || &lt;tt&gt;a&lt;/tt&gt;
|-
| &lt;tt&gt;c&lt;/tt&gt; || &lt;tt&gt;adabra$abr&lt;/tt&gt; || &lt;tt&gt;a&lt;/tt&gt;
|-
| &lt;tt&gt;d&lt;/tt&gt; || &lt;tt&gt;abra$abrac&lt;/tt&gt; || &lt;tt&gt;a&lt;/tt&gt;
|-
| &lt;tt&gt;r&lt;/tt&gt; || &lt;tt&gt;a$abracada&lt;/tt&gt; || &lt;tt&gt;b&lt;/tt&gt;
|-
| &lt;tt&gt;r&lt;/tt&gt; || &lt;tt&gt;acadabra$a&lt;/tt&gt; || &lt;tt&gt;b&lt;/tt&gt;
|}

The BWT in itself allows for some compression with, for instance, [[move to front]] and [[Huffman encoding]], but the transform has even more uses. The rows in the matrix are essentially the sorted suffixes of the text and the first column F of the matrix shares similarities with [[suffix array]]s. How the suffix array relates to the BWT lies at the heart of the FM-index.

{| 
|
It is possible to make a last-to-first column mapping {{math|LF(i)}} from a character {{math|L[i]}} to {{math|F[j]}}, with the help of a table {{math|C[c]}} and a function {{math|Occ(c, k)}}. {{math|C[c]}} is a table that, for each character {{math|c}} in the alphabet, contains the number of occurrences of lexically smaller characters in the text. The function {{math|Occ(c, k)}} is the number of occurrences of character {{math|c}} in the prefix {{math|L[1..k]}}. Ferragina and Manzini showed&lt;ref name=&quot;opportunistic_2000&quot; /&gt; that it is possible to compute {{math|Occ(c, k)}} in constant time.
|
{| class=&quot;wikitable&quot; border=&quot;1&quot; style=&quot;text-align: center; float: right&quot;
|+ {{math|C[c]}} of &quot;ard$rcaaaabb&quot;
! {{math|c}}
| $ || a || b || c || d || r
|-
! {{math|C[c]}}
| 0 || 1 || 6 || 8 || 9 || 10
|}
|}

{| 
|
The last-to-first mapping can now be defined as {{math|LF(i) {{=}} C[L[i]] + Occ(L[i], i)}}. For instance, on row 9, {{math|L}} is {{math|a}} and the same {{math|a}} can be found on row 5 in the first column {{math|F}}, so {{math|LF(9)}} should be 5 and {{math|LF(9) {{=}} C[a] + Occ(a, 9) {{=}} 5}}. For any row {{math|i}} of the matrix, the character in the last column {{math|L[i]}} precedes the character in the first column {{math|F[i]}} also in T. Finally, if {{math|L[i] {{=}} T[k]}}, then {{math|L[LF(i)] {{=}} T[k - 1]}}, and using the equality it is possible to extract a string of {{math|T}} from {{math|L}}.

The FM-index itself is a compression of the string {{math|L}} together with {{math|C}} and {{math|Occ}} in some form, as well as information that maps a selection of indices in {{math|L}} to positions in the original string {{math|T}}.

|
{| class=&quot;wikitable&quot; border=&quot;1&quot; style=&quot;text-align: center; float: right&quot;
|+ {{math|Occ(c, k)}} of &quot;ard$rcaaaabb&quot;
!     !! a  !! r !! d !! $ !! r !! c !! a !! a !! a !! a  !! b  !! b  
|-
!     !! 1  !! 2 !! 3 !! 4 !! 5 !! 6 !! 7 !! 8 !! 9 !! 10 !! 11 !! 12 
|-
! $   
| 0  || 0 || 0 || 1 || 1 || 1 || 1 || 1 || 1 || 1  || 1  || 1  
|-
! a   
| 1  || 1 || 1 || 1 || 1 || 1 || 2 || 3 || 4 || 5  || 5  || 5  
|-
! b   
| 0  || 0 || 0 || 0 || 0 || 0 || 0 || 0 || 0 || 0  || 1  || 2  
|-
! c   
| 0  || 0 || 0 || 0 || 0 || 1 || 1 || 1 || 1 || 1  || 1  || 1  
|-
! d   
| 0  || 0 || 1 || 1 || 1 || 1 || 1 || 1 || 1 || 1  || 1  || 1  
|-
! r   
| 0  || 1 || 1 || 1 || 2 || 2 || 2 || 2 || 2 || 2  || 2  || 2
|}
|}

== Count ==

The operation ''count'' takes a pattern {{math|P[1..p]}} and returns the number of occurrences of that pattern in the original text {{math|T}}. Since the rows of matrix {{math|M}} are sorted, and it contains every suffix of {{math|T}}, the occurrences of pattern {{math|P}} will be next to each other in a single continuous range. The operation iterates backwards over the pattern. For every character in the pattern, the range that has the character as a suffix is found. For example, the count of the pattern &quot;bra&quot; in &quot;abracadabra&quot; follows these steps:
# The first character we look for is {{math|a}}, the last character in the pattern. The initial range is set to {{math|[C[a] + 1..C[a+1] {{=}} [2..6]}}. This range over {{math|L}} represents every character of {{math|T}} that has a suffix beginning with ''a''.
# The next character to look for is {{math|r}}. The new range is {{math|[C[r] + Occ(r, start-1) + 1..C[r] + Occ(r, end)] {{=}}}} {{math|[10 + 0 + 1..10 + 2] {{=}}}} {{math|[11..12]}}, if {{math|start}} is the index of the beginning of the range and {{math|end}} is the end. This range over {{math|L}} is all the characters of {{math|T}} that have suffixes beginning with ''ra''.
# The last character to look at is {{math|b}}. The new range is {{math|[C[b] + Occ(b, start-1) + 1..C[b] + Occ(b, end)] {{=}}}} {{math|[6 + 0 + 1..6 + 2] {{=}}}} {{math|[7..8]}}. This range over {{math|L}} is all the characters that have a suffix that begins with ''bra''. Now that the whole pattern has been processed, the count is the same as the size of the range: {{math|8 - 7 + 1 {{=}} 2}}.

If the range at becomes empty or the range boundaries cross each other before the whole pattern has been looked up, the pattern does not occur in {{math|T}}. Because {{math|Occ(c, k)}} can be performed in constant time, count can complete in linear time in the length of the pattern: {{math|O(p)}} time.

== Locate ==

The operation ''locate'' takes as input an index of a character in {{math|L}} and returns its position {{math|i}} in {{math|T}}. For instance {{math|locate(7) {{=}} 8}}. To locate every occurrence of a pattern, first the range of character is found whose suffix is the pattern in the same way  the ''count'' operation found the range. Then the position of every character in the range can be located.

To map an index in {{math|L}} to one in {{math|T}}, a subset of the indices in {{math|L}} are associated with a position in {{math|T}}. If {{math|L[j]}} has a position associated with it, {{math|locate(j)}} is trivial. If it's not associated, the string is followed with {{math|LF(i)}} until an associated index is found. By associating a suitable number of indices, an upper bound can be found. ''Locate'' can be implemented to find ''occ'' occurrences of a pattern {{math|P[1..p]}} in a text {{math|T[1..u]}} in {{math|O(p + ''occ'' log&lt;sup&gt;&amp;epsilon;&lt;/sup&gt; u)}} time with &lt;math&gt;O(H_k(T) + {{loglog u}\over{log^\epsilon u}})&lt;/math&gt; bits per input symbol for any {{math|k &amp;ge; 0}}.&lt;ref name=&quot;opportunistic_2000&quot; /&gt;

== Applications ==

=== DNA read mapping ===

FM index with Backtracking has been successfully (&gt;2000 citations) applied to approximate string matching/sequence alignment, See Bowtie http://bowtie-bio.sourceforge.net/index.shtml

== See also ==
[[Burrows–Wheeler transform]]

[[Suffix array]]

[[Compressed suffix array]]

[[Sequence alignment]]

== References==
{{reflist}}

[[Category:Substring indices]]
[[Category:String data structures]]</text>
      <sha1>cmvn4a844s41d3qcr1qm4w9czpbn8gn</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Generalized suffix tree</title>
    <ns>0</ns>
    <id>3142847</id>
    <revision>
      <id>575514348</id>
      <parentid>564662072</parentid>
      <timestamp>2013-10-03T00:30:05Z</timestamp>
      <contributor>
        <username>Nodirt</username>
        <id>19848312</id>
      </contributor>
      <minor/>
      <comment>Fixed a typo: generalised -&gt; generalized</comment>
      <text xml:space="preserve" bytes="3240">[[Image:Suffix tree ABAB BABA.svg|thumb|300px|right|Suffix tree for the strings &lt;code&gt;ABAB&lt;/code&gt; and &lt;code&gt;BABA&lt;/code&gt;. [[Suffix_tree#Description|Suffix links]] not shown.]]
In [[computer science]], a '''generalized suffix tree''' is a [[suffix tree]] for a set of [[String (computer science)|strings]]. Given the set of strings &lt;math&gt;D=S_1,S_2,\dots,S_d&lt;/math&gt; of total length &lt;math&gt;n&lt;/math&gt;, it is a [[Patricia tree]] containing all &lt;math&gt;n&lt;/math&gt; [[suffix (computer science)|suffixes]] of the strings. It is mostly used in [[bioinformatics]].{{ref|BRCR}}

== Functionality ==
It can be built in &lt;math&gt;\Theta(n)&lt;/math&gt; time and space, and can be used to find all &lt;math&gt;z&lt;/math&gt; occurrences of a string &lt;math&gt;P&lt;/math&gt; of length &lt;math&gt;m&lt;/math&gt; in &lt;math&gt;O(m + z)&lt;/math&gt; time, which is [[asymptotically optimal]] (assuming the size of the alphabet is constant, see {{ref|Gus97}} page 119).  

When constructing such a tree, each string should be padded with a unique out-of-alphabet marker symbol (or string) to ensure no suffix is a substring of another, guaranteeing each suffix is represented by a unique leaf node.

Algorithms for constructing a GST include [[Ukkonen's algorithm]] (1995) and [[McCreight's algorithm]] (1976).

== Example ==
A suffix tree for the strings &lt;code&gt;ABAB&lt;/code&gt; and &lt;code&gt;BABA&lt;/code&gt; is shown in a figure above. They are padded with the unique terminator strings &lt;code&gt;$0&lt;/code&gt; and &lt;code&gt;$1&lt;/code&gt;. The numbers in the leaf nodes are string number and starting position. Notice how a left to right traversal of the leaf nodes corresponds to the sorted order of the suffixes. The terminators might be strings or unique single symbols. Edges on &lt;code&gt;$&lt;/code&gt; from the root are left out in this example.

== Alternatives ==
An alternative to building a generalised suffix tree is to concatenate the strings, and build a regular suffix tree or [[suffix array]] for the resulting string. When hits are evaluated after a search, global positions are mapped into documents and local positions with some algorithm and/or data structure, such as a binary search in the starting/ending positions of the documents.

==References==

* {{note|Hui92}} {{cite conference
 | author=Lucas Chi Kwong Hui
 | title=Color Set Size Problem with Applications to String Matching
 | booktitle=Combinatorial Pattern Matching, Lecture Notes in Computer Science, 644.
 | year=1992
 | pages=230&amp;ndash;243
 | url=http://www.springerlink.com/content/y565487707522555/}}
* {{note|BRCR}} {{cite conference
 | author=Paul Bieganski, John Riedl, John Carlis, and Ernest F. Retzel
 | title=Generalized Suffix Trees for Biological Sequence Data
 | booktitle=Biotechnology Computing, Proceedings of the Twenty-Seventh Hawaii International Conference on.
 | year=1994
 | pages=35&amp;ndash;44
 | url=http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=323593}}
* {{note|Gus97}} {{cite book
 | last = Gusfield
 | first = Dan
 | origyear = 1997
 | year = 1999
 | title = Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology
 | publisher = Cambridge University Press
 | location = USA
 | isbn = 0-521-58519-8
}}

[[Category:Trees (data structures)]]
[[Category:Substring indices]]
[[Category:String data structures]]</text>
      <sha1>qr0vw2uo47lx42ioqqo6vh2gu0i6o5t</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Radix tree</title>
    <ns>0</ns>
    <id>1481659</id>
    <revision>
      <id>625908589</id>
      <parentid>625908468</parentid>
      <timestamp>2014-09-17T05:27:44Z</timestamp>
      <contributor>
        <username>Dsimic</username>
        <id>6479630</id>
      </contributor>
      <comment>Filled in bare references with [[User:Zhaofeng Li/Reflinks]]</comment>
      <text xml:space="preserve" bytes="15365">[[File:Patricia trie.svg|right]]

In [[computer science]], a '''[[radix]] tree''' (also '''patricia [[trie]]''' or '''radix trie''' or '''compact [[prefix tree]]''') is a space-optimized [[trie]] [[data structure]] where each node with only one child is merged with its parent. The result is that every internal node has up to the number of children of the radix {{mvar|r}} of the radix trie, where {{mvar|r}} is a positive integer and a power {{mvar|x}} of 2, having {{mvar|x}} ≥ 1. Unlike in regular tries, edges can be labeled with sequences of elements as well as single elements. This makes them much more efficient for small sets (especially if the strings are long) and for sets of strings that share long prefixes.

Unlike regular trees (where whole keys are compared ''en masse'' from their beginning up to the point of inequality), the key at each node is compared chunk-of-bits by chunk-of-bits, where the quantity of bits in that chunk at that node is the radix {{mvar|r}} of the radix trie.  When the {{mvar|r}} is 2, the radix trie is binary (i.e., compare that node's 1-bit portion of the key), which minimizes sparseness at the expense of maximizing trie depth—i.e., maximizing up to conflation of nondiverging bit-strings in the key.  When {{mvar|r}} is an integer power of 2 greater or equal to 4, then the radix trie is an {{mvar|r}}-ary trie, which lessens the depth of the radix trie at the expense of potential sparseness.

As an optimization, edge labels can be stored in constant size by using two pointers to a string (for the first and last elements).&lt;ref&gt;{{cite web|last=Morin|first=Patrick|title=Data Structures for Strings|url=http://cg.scs.carleton.ca/~morin/teaching/5408/notes/strings.pdf|accessdate=15 April 2012}}&lt;/ref&gt;

Note that although the examples in this article show strings as sequences of characters, the type of the string elements can be chosen arbitrarily; for example, as a bit or byte of the string representation when using [[multibyte character]] encodings or [[Unicode]].

== Applications ==

Radix trees are useful for constructing [[associative array]]s with keys that can be expressed as strings. They find particular application in the area of [[Internet Protocol|IP]] [[routing]], where the ability to contain large ranges of values with a few exceptions is particularly suited to the hierarchical organization of [[IP address]]es.&lt;ref&gt;Knizhnik, Konstantin. [http://www.ddj.com/architect/208800854  &quot;Patricia Tries: A Better Index For Prefix Searches&quot;], ''Dr. Dobb's Journal'', June, 2008.&lt;/ref&gt; They are also used for [[inverted index]]es of text documents in [[information retrieval]].

== Operations ==

Radix trees support insertion, deletion, and searching operations. Insertion adds a new string to the trie while trying to minimize the amount of data stored. Deletion removes a string from the trie. Searching operations include (but are not necessarily limited to) exact lookup, find predecessor, find successor, and find all strings with a prefix. All of these operations are O(''k'') where k is the maximum length of all strings in the set, where length is measured in the quantity of bits equal to the radix of the radix trie.

=== Lookup ===
[[File:An example of how to find a string in a Patricia trie.png|Finding a string in a Patricia trie|thumb|right]] The lookup operation determines if a string exists in a trie. Most operations modify this approach in some way to handle their specific tasks. For instance, the node where a string terminates may be of importance. This operation is similar to tries except that some edges consume multiple elements.

The following pseudo code assumes that these classes exist.

'''Edge'''
* ''Node'' targetNode
* ''string'' label

'''Node'''
* ''Array of Edges'' edges
* ''function'' isLeaf()

 '''function''' lookup(''string'' x)
 {
   ''// Begin at the root with no elements found''
   '''Node''' traverseNode := ''root'';
   '''int''' elementsFound := 0;
   
   ''// Traverse until a leaf is found or it is not possible to continue''
   '''while''' (traverseNode != ''null'' &amp;&amp; !traverseNode.isLeaf() &amp;&amp; elementsFound &lt; x.length)
   {
     ''// Get the next edge to explore based on the elements not yet found in x''
     '''Edge''' nextEdge := '''select''' edge '''from''' traverseNode.edges '''where''' edge.label ''is a prefix of'' x.suffix(elementsFound)
       ''// x.suffix(elementsFound) returns the last (x.length - elementsFound) elements of x''
   
     ''// Was an edge found?''
     '''if''' (nextEdge != ''null'')
     {
       ''// Set the next node to explore''
       traverseNode := nextEdge.targetNode;
     
       ''// Increment elements found based on the label stored at the edge''
       elementsFound += nextEdge.label.length;
     }
     '''else'''
     {
       ''// Terminate loop''
       traverseNode := ''null'';
     }
   }
   
   ''// A match is found if we arrive at a leaf node and have used up exactly x.length elements''
   '''return''' (traverseNode != ''null'' &amp;&amp; traverseNode.isLeaf() &amp;&amp; elementsFound == x.length);
 }

=== Insertion ===

To insert a string, we search the tree until we can make no further progress. At this point we either add a new outgoing edge labeled with all remaining elements in the input string, or if there is already an outgoing edge sharing a prefix with the remaining input string, we split it into two edges (the first labeled with the common prefix) and proceed. This splitting step ensures that no node has more children than there are possible string elements.

Several cases of insertion are shown below, though more may exist. Note that r simply represents the root. It is assumed that edges can be labelled with empty strings to terminate strings where necessary and that the root has no incoming edge. (The lookup algorithm described above will not work when using empty-string edges.)

&lt;gallery&gt;
File:Inserting the string 'water' into a Patricia trie.png|Insert 'water' at the root
File:Insert 'slower' with a null node into a Patricia trie.png|Insert 'slower' while keeping 'slow'
File:Insert 'test' into a Patricia trie when 'tester' exists.png|Insert 'test' which is a prefix of 'tester'
File:Inserting the word 'team' into a Patricia trie with a split.png|Insert 'team' while splitting 'test' and creating a new edge label 'st'
File:Insert 'toast' into a Patricia trie with a split and a move.png|Insert 'toast' while splitting 'te' and moving previous strings a level lower
&lt;/gallery&gt;

=== Deletion ===

To delete a string x from a tree, we first locate the leaf representing x. Then, assuming x exists, we remove the corresponding leaf node. If the parent of our leaf node has only one other child, then that child's incoming label is appended to the parent's incoming label and the child is removed.

=== Additional operations ===
* Find all strings with common prefix: Returns an array of strings which begin with the same prefix.
* Find predecessor: Locates the largest string less than a given string, by lexicographic order.
* Find successor: Locates the smallest string greater than a given string, by lexicographic order.

== History ==

Donald R. Morrison first described what he called &quot;Patricia trees&quot; in 1968;&lt;ref&gt;Morrison, Donald R. [http://portal.acm.org/citation.cfm?id=321481 Practical Algorithm to Retrieve Information Coded in Alphanumeric]&lt;/ref&gt; the name comes from the [[acronym]] '''PATRICIA''', which stands for &quot;''Practical Algorithm To Retrieve Information Coded In Alphanumeric''&quot;. Gernot Gwehenberger independently invented and described the data structure at about the same time.&lt;ref&gt;G. Gwehenberger,  [http://cr.yp.to/bib/1968/gwehenberger.html Anwendung einer binären Verweiskettenmethode beim Aufbau von Listen.] Elektronische Rechenanlagen 10 (1968), pp. 223–226&lt;/ref&gt;  PATRICIA tries are radix tries with radix equals 2, which means that each bit of the key is compared individually and each node is a two-way (i.e., left versus right) branch.

== Comparison to other data structures ==

(In the following comparisons, it is assumed that the keys are of length ''k'' and the data structure contains ''n'' members.)

Unlike [[balanced trees]], radix trees permit lookup, insertion, and deletion in O(''k'') time rather than O(log ''n''). This doesn't seem like an advantage, since normally ''k'' ≥ log ''n'', but in a balanced tree every comparison is a string comparison requiring O(''k'') worst-case time, many of which are slow in practice due to long common prefixes (in the case where comparisons begin at the start of the string). In a trie, all comparisons require constant time, but it takes ''m'' comparisons to look up a string of length ''m''. Radix trees can perform these operations with fewer comparisons, and require many fewer nodes.

Radix trees also share the disadvantages of tries, however: as they can only be applied to strings of elements or elements with an efficiently reversible mapping to strings, they lack the full generality of balanced search trees, which apply to any data type with a [[total ordering]]. A reversible mapping to strings can be used to produce the required total ordering for balanced search trees, but not the other way around. This can also be problematic if a data type only [[Interface (computer science)|provides]] a comparison operation, but not a (de)[[serialization]] operation.

[[Hash table]]s are commonly said to have expected O(1) insertion and deletion times, but this is only true when considering computation of the hash of the key to be a constant time operation. When hashing the key is taken into account, hash tables have expected O(''k'') insertion and deletion times, but may take longer in the worst-case depending on how collisions are handled. Radix trees have worst-case O(''k'') insertion and deletion. The successor/predecessor operations of radix trees are also not implemented by hash tables.

==Variants==

A common extension of radix trees uses two colors of nodes, 'black' and 'white'. To check if a given string is stored in the tree, the search starts from the top and follows the edges of the input string until no further progress can be made. If the search-string is consumed and the final node is a black node, the search has failed; if it is white, the search has succeeded. This enables us to add a large range of strings with a common prefix to the tree, using white nodes, then remove a small set of &quot;exceptions&quot; in a space-efficient manner by ''inserting'' them using black nodes.

The '''[[HAT-trie]]''' is a radix tree based cache-conscious data structure that offers efficient string storage and retrieval, and ordered iterations.  Performance, with respect to both time and space, is 
comparable to the cache-conscious [[hashtable]].&lt;ref&gt;{{Cite book | title=HAT-trie: A Cache-conscious Trie-based Data Structure for Strings | first1=Nikolas | last1=Askitis | first2=Ranjan | last2=Sinha | year=2007 | url=http://portal.acm.org/citation.cfm?id=1273749.1273761&amp;coll=GUIDE&amp;dl= | isbn=1-920682-43-0 | pages=97–105 | volume=62 | journal=Proceedings of the 30th Australasian Conference on Computer science | postscript=&lt;!--None--&gt;}}&lt;/ref&gt;&lt;ref&gt;
  {{Cite journal
  | title=Engineering scalable, cache and space efficient tries for strings
  | first1=Nikolas
  | last1=Askitis
  | first2=Ranjan
  | last2=Sinha
  | date=October 2010
  | issn=1066-8888
  | id=ISSN 0949-877X (0nline)
  | doi=10.1007/s00778-010-0183-9
  | url=http://www.springerlink.com/content/86574173183j6565/
  | journal=The VLDB Journal
  | issue=5
  | volume=19
  | pages=633–660
  }}
&lt;/ref&gt; See HAT trie implementation notes at [http://code.google.com/p/hat-trie]

The '''adaptive radix tree''' is a radix tree variant that integrates adaptive node sizes to the radix tree.  One major drawbacks of the usual radix trees is the use of space, because it uses a constant node size in every level. The major difference between the radix tree and the adaptive radix tree is its variable size for each node based on the number of child-elements, which grows while adding new entries. Hence, the adaptive radix tree leads to a better use of space without reducing its speed.&lt;ref&gt;
  {{Cite book
  | title=Datenbanksysteme, Eine Einführung
  | first1=Alfons
  | last1=Kemper
  | first2=André
  | last2=Eickler
  | date=2013
  | isbn=978-3-486-72139-3
  | volume=9
  | pages=604–605
  }}
&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://github.com/armon/libart|title=armon/libart · GitHub|work=GitHub|accessdate=17 September 2014}}&lt;/ref&gt;&lt;ref&gt;http://www-db.in.tum.de/~leis/papers/ART.pdf&lt;/ref&gt;

== See also ==
{{Portal|Computer programming}}

{{Div col||22em}}
* [[Prefix tree]] (also known as a Trie)
* [[Directed acyclic word graph]] (aka DAWG)
* [[Ternary search tries]]
* [[Acyclic deterministic finite automata]]
* [[Hash trie]]
* [[Deterministic finite automata]]
* [[Judy array]]
* [[Search algorithm]]
* [[Extendible hashing]]
* [[Hash array mapped trie]]
* [[Prefix Hash Tree]]
* [[Burstsort]]
* [[Luleå algorithm]]
* [[Huffman coding]]
{{Div col end}}

==References==
{{Reflist|30em}}

== External links ==
* [http://www.csse.monash.edu.au/~lloyd/tildeAlgDS/Tree/PATRICIA/ Algorithms and Data Structures Research &amp; Reference Material: PATRICIA], by Lloyd Allison, [[Monash University]]
* [http://www.nist.gov/dads/HTML/patriciatree.html Patricia Tree], [[NIST Dictionary of Algorithms and Data Structures]]
*[http://cr.yp.to/critbit.html  Crit-bit trees], by [[Daniel J. Bernstein]]
* [http://lwn.net/Articles/175432/ Radix Tree API in the Linux Kernel], by Jonathan Corbet
* [http://code.dogmap.org/kart/ Kart (key alteration radix tree)], by Paul Jarc

=== Implementations ===
* [http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/lib/radix-tree.c Linux Kernel Implementation], used for the page cache, among other things.
* [http://gcc.gnu.org/onlinedocs/libstdc++/ext/pb_ds/trie_based_containers.html GNU C++ Standard library has a trie implementation]
* [http://code.google.com/p/concurrent-trees/ Java implementation of Concurrent Radix Tree], by Niall Gallagher
* [http://paratechnical.blogspot.com/2011/03/radix-tree-implementation-in-c.html C# implementation of a Radix Tree]
* [http://code.google.com/p/patl/ Practical Algorithm Template Library], a C++ library on PATRICIA tries (VC++ &gt;=2003, GCC G++ 3.x), by Roman S. Klyujkov
* [http://www.codeproject.com/KB/string/PatriciaTrieTemplateClass.aspx Patricia Trie C++ template class implementation], by Radu Gruian
* [http://hackage.haskell.org/packages/archive/containers/latest/doc/html/Data-IntMap.html Haskell standard library implementation] &quot;based on big-endian patricia trees&quot;. Web-browsable [http://hackage.haskell.org/packages/archive/containers/latest/doc/html/src/Data-IntMap.html source code].
* [http://code.google.com/p/patricia-trie/ Patricia Trie implementation in Java], by Roger Kapsi and Sam Berlin
* [http://github.com/agl/critbit Crit-bit trees] forked from C code by Daniel J. Bernstein
* [http://cprops.sourceforge.net/gen/docs/trie_8c-source.html Patricia Trie implementation in C], in [http://cprops.sourceforge.net libcprops]
*[http://www.lri.fr/~filliatr/ftp/ocaml/ds Patricia Trees : efficient sets and maps over integers in] [[:fr:Objective Caml|OCaml]], by Jean-Christophe Filliâtre
{{CS-Trees}}

[[Category:Trees (data structures)]]
[[Category:String data structures]]</text>
      <sha1>535ip3wflknmb23kw4mmqz9dg75kb9o</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Substring index</title>
    <ns>0</ns>
    <id>3125005</id>
    <revision>
      <id>589231850</id>
      <parentid>571579622</parentid>
      <timestamp>2014-01-05T02:52:13Z</timestamp>
      <contributor>
        <ip>69.201.190.50</ip>
      </contributor>
      <comment>I removed the phrase &quot;o(n) means less than O(n)&quot; as it isn't accurate.</comment>
      <text xml:space="preserve" bytes="1276">In [[computer science]], a '''substring index''' is a [[data structure]] which gives [[substring]] search in a text or text collection in [[sublinear]] time. If you have a document &lt;math&gt;S&lt;/math&gt; of length &lt;math&gt;n&lt;/math&gt;, or a set of documents &lt;math&gt;D=\{S^1,S^2, \dots, S^d\}&lt;/math&gt; of total length &lt;math&gt;n&lt;/math&gt;, you can locate all occurrences of a pattern &lt;math&gt;P&lt;/math&gt; in &lt;math&gt;o(n)&lt;/math&gt; time. (See [[Big O notation]].)

The phrase '''full-text index''' is also often used for an index of all substrings of a text. But is ambiguous, as it is also used for regular word indexes such as [[inverted file]]s and [[document retrieval]]. See [[full text search]].

Substring indexes include:

* [[Suffix tree]]
* [[Suffix array]]
* N-gram index, an [[inverted file]] for all [[N-gram]]s of the text
* [[Compressed suffix array]]&lt;ref&gt;R. Grossi and J. S. Vitter, [http://www.di.unipi.it/~grossi/PAPERS/sicomp05.pdf Compressed Suffix Arrays and Suffix Trees, with Applications to Text Indexing and String Matching], ''SIAM Journal on Computing,'' 35(2), 2005, 378-407.&lt;/ref&gt;
* [[FM-index]]
* [[LZ-index]]

== References ==
{{reflist}}

[[Category:Algorithms on strings]]
[[Category:String data structures]]
[[Category:Database index techniques]]
[[Category:Substring indices| ]]</text>
      <sha1>l5rb6mizboimnpftcvsafvhz8of18z1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Compressed suffix array</title>
    <ns>0</ns>
    <id>25296445</id>
    <revision>
      <id>622236160</id>
      <parentid>619821134</parentid>
      <timestamp>2014-08-21T19:17:17Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>Tagging + other fixes, removed underlinked tag using [[Project:AWB|AWB]] (10392)</comment>
      <text xml:space="preserve" bytes="5490">In [[computer science]], a '''compressed suffix array'''&lt;ref name=&quot;GV00&quot;&gt;R. Grossi and J. S. Vitter, Compressed Suffix Arrays and Suffix Trees, with Applications to Text Indexing and String Matching, ''SIAM Journal on Computing,'' 35(2), 2005, 378-407.  An earlier version appeared in ''Proceedings of the 32nd ACM Symposium on Theory of Computing, May 2000, 397-406.&lt;/ref&gt;&lt;ref name=&quot;opportunistic_2000&quot;&gt;Paolo Ferragina and Giovanni Manzini (2000). &quot;Opportunistic Data Structures with Applications&quot;. Proceedings of the 41st Annual Symposium on Foundations of Computer Science. p.390.&lt;/ref&gt;&lt;ref name=&quot;GGV03&quot;&gt;R. Grossi, A. Gupta, and J. S. Vitter, High-Order Entropy-Compressed Text Indexes, ''Proceedings of the 14th Annual SIAM/ACM Symposium on Discrete Algorithms,'' January 2003, 841-850.&lt;/ref&gt; is a [[compressed data structure]] for [[pattern matching]].  Compressed suffix arrays are a general class of [[data structure]] that improve on the [[suffix array]].&lt;ref name=&quot;GV00&quot;/&gt;&lt;ref name=&quot;opportunistic_2000&quot;/&gt; These data structures enable quick search for an arbitrary [[String (computer science)|string]] with a comparatively small index.

Given a text ''T'' of ''n'' characters from an alphabet Σ, a compressed suffix array supports searching for arbitrary patterns in ''T''.  For an input pattern ''P'' of ''m'' characters, the search time is typically O(''m'') or O(''m'' + ''log(n)''). The space used is typically &lt;math&gt;O(n H_k(T)) + o(n)&lt;/math&gt;, where &lt;math&gt;H_k(T)&lt;/math&gt; is the [[Entropy (information theory)|k-th order empirical entropy]] of the text ''T''. The time and space to construct a compressed suffix array are normally ''O(n)''.
 
The original instantiation of the compressed suffix array&lt;ref name=&quot;GV00&quot;/&gt; solved a long-standing open problem by showing that fast pattern matching was possible using only a linear-space data structure, namely, one proportional to the size of the text ''T'', which takes &lt;math&gt;O(n \, {\log |\Sigma|})&lt;/math&gt; bits.  The conventional suffix array and suffix tree use &lt;math&gt;\Omega(n \, {\log n})&lt;/math&gt; bits, which is substantially larger.  The basis for the data structure is a recursive decomposition using the &quot;neighbor function,&quot; which allows a suffix array to be represented by one of half its length.  The construction is repeated multiple times until the resulting suffix array uses a linear number of bits.  Following work showed that the actual storage space was related to the zeroth-order entropy and that the index supports self-indexing.&lt;ref&gt;K. Sadakane, Compressed Text Databases with Efficient Query Algorithms Based on the Compressed Suffix Arrays, ''Proceedings of the International Symposium on Algorithms and Computation'', Lecture Notes in Computer Science, vol. 1969, Springer, December 2000, 410-421.&lt;/ref&gt;  The space bound was further improved achieving the ultimate goal of higher-order entropy; the compression is obtained by partitioning the neighbor function by high-order contexts, and compressing each partition with a [[Wavelet Tree|wavelet tree]].&lt;ref name=&quot;GGV03&quot;/&gt;  The space usage is extremely competitive in practice with other state-of-the-art compressors,&lt;ref&gt;L. Foschini, R. Grossi, A. Gupta, and J. S. Vitter, Indexing Equals Compression: Experiments on Suffix Arrays and Trees'', ACM Transactions on Algorithms'', 2(4), 2006, 611-639.&lt;/ref&gt; and it also supports fast pattern matching.
 
The memory accesses made by compressed suffix arrays and other compressed data structures for pattern matching are typically not localized, and thus these data structures have been notoriously hard to design efficiently for use in [[external memory]].  Recent progress using geometric duality takes advantage of the block access provided by disks to speed up the I/O time significantly&lt;ref&gt;W.-K. Hon, R. Shah, S. V. Thankachan, and J. S. Vitter, On Entropy-Compressed Text Indexing in External Memory, ''Proceedings of the Conference on String Processing and Information Retrieval'', August 2009.&lt;/ref&gt; In addition, potentially practical search performance for a compressed suffix array in external-memory has been demonstrated.&lt;ref&gt;M. P. Ferguson, ''FEMTO: fast search of large sequence collections'', ''Proceedings of the 23rd Annual Conference on Combinatorial Pattern Matching'', July 2012&lt;/ref&gt;

==Open Source Implementations==
There are several open source implementations of compressed suffix arrays available (see [[#External Links|External Links]] below). Bowtie and Bowtie2 are open-source compressed suffix array implementations of [[Sequence alignment|read alignment]] for use in [[bioinformatics]]. The Succinct Data Structure Library (SDSL) is a library containing a variety of compressed data structures including compressed suffix arrays. FEMTO is an implementation of compressed suffix arrays for external memory. In addition, a variety of implementations, including the original [[FM-index]] implementations, are available from the Pizza &amp; Chili Website.

==See also==
[[FM-index]]

[[Suffix Array]]

==References==
&lt;references/&gt;

==External links==
Implementations:
* [http://bowtie-bio.sourceforge.net/index.shtml Bowtie] and [http://bowtie-bio.sourceforge.net/bowtie2/index.shtml Bowtie2]
* [https://github.com/simongog/sdsl-lite Succinct Data Structure Library (SDSL)]
* [https://github.com/femto-dev/femto FEMTO]
* [http://pizzachili.di.unipi.it/indexes.html Pizza&amp;Chili website].

[[Category:String data structures]]
[[Category:Database index techniques]]
[[Category:Substring indices]]</text>
      <sha1>74d5k6yzu2m2q250j18b4jr0ntqsm91</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Rope (data structure)</title>
    <ns>0</ns>
    <id>1194858</id>
    <revision>
      <id>621322491</id>
      <parentid>621322468</parentid>
      <timestamp>2014-08-15T08:12:24Z</timestamp>
      <contributor>
        <ip>65.26.252.225</ip>
      </contributor>
      <comment>Undid revision 621322468 by [[Special:Contributions/183.83.53.190|183.83.53.190]] ([[User talk:183.83.53.190|talk]])</comment>
      <text xml:space="preserve" bytes="11165">{{one source|date = September 2011}}

[[File:Vector Rope example.svg|right|x200px|thumb|A simple rope built on the string of &quot;Hello_my_name_is_Simon&quot;.]]

In [[computer programming]] a '''rope''', or '''cord''', is a [[data structure]] composed of smaller [[String (computer science)|strings]] that is used for efficiently storing and manipulating a very long string. For example, a text editing program may use a rope to represent the text being edited, so that operations such as insertion, deletion, and random access can be done efficiently.&lt;ref name=&quot;Boehm&quot;&gt;
{{cite journal
  | last = Boehm
  | first = Hans-J
  | coauthors = Atkinson, Russ; and Plass, Michael
  | title = Ropes: an Alternative to Strings
  | journal = Software—Practice &amp; Experience
  | volume = 25
  | issue = 12
  | pages = 1315–1330
  | publisher = John Wiley &amp; Sons, Inc.
  | location = New York, NY, USA
  | date = December 1995
  | url = http://citeseer.ist.psu.edu/viewdoc/download?doi=10.1.1.14.9450&amp;rep=rep1&amp;type=pdf
  | format = [[PDF]]
  | doi = 10.1002/spe.4380251203
}}&lt;/ref&gt;

==Description==
A rope is a [[binary tree]] having leaf nodes that contain a short string. Each node has a weight value equal to the length of its string plus the sum of all the weights in its left [[subtree]]. Thus a node with two children divides the whole string into two parts: the left subtree stores the first part of the string. The right subtree stores the second part and its weight is the sum of the left child's weight and the length of its contained string.

The binary tree can be seen as several levels of nodes. The bottom level contains all the nodes that contain a string. Higher levels have fewer and fewer nodes. The top level consists of a single &quot;root&quot; node. The rope is built by putting the nodes with short strings in the bottom level, then attaching a random half of the nodes to parent nodes in the next level.

==Operations==
In the following definitions, ''N'' is the length of the rope.

=== Index ===

[[File:Vector Rope index.svg|right|x200px|thumb|Figure 2.1: Example of index lookup on a rope.]]

: ''Definition:'' &lt;code&gt;Index(i)&lt;/code&gt;: return the character at position ''i''
: ''Time complexity:'' ''O(log N)''

To retrieve the ''i''-th character, we begin a [[Recursion|recursive]] search from the root node:

&lt;syntaxhighlight lang=pascal&gt;
 // Note: Assumes 1-based indexing.
 function index(RopeNode node, integer i)
     if node.weight &lt; i then
         return index(node.right, i - node.weight)
     else
         if exists(node.left) then
             return index(node.left, i)
         else
             return node.string[i]
         endif
     endif
 end
&lt;/syntaxhighlight &gt;

For example, to find the character at ''i=10'' in Figure 2.1 shown on the right, start at the root node (A), find that 22 is greater than 10 and there is a left child, so go to the left child (B). 9 is less than 10, so subtract 9 from 10 (leaving ''i=1'') and go to the right child (D). Then because 6 is greater than 1 and there's a left child, go to the left child (G). 2 is greater than 1 and there's a left child, so go to the left child again (J).  Finally 2 is greater than 1 but there is no left child, so the character at index 1 of the short string &quot;na&quot;, is the answer.

=== Concat ===
[[File:Vector Rope concat.svg|right|x200px|thumb|Figure 2.2: Concatenating two child ropes into a single rope.]]
: ''Definition:'' &lt;code&gt;Concat(S1, S2)&lt;/code&gt;: concatenate two ropes, ''S&lt;sub&gt;1&lt;/sub&gt;'' and ''S&lt;sub&gt;2&lt;/sub&gt;'', into a single rope.
: ''Time complexity:'' ''O(1)'' (or O(log N) time to compute the root weight)

A concatenation can be performed simply by creating a new root node with ''left = S&lt;sub&gt;1&lt;/sub&gt;'' and ''right = S&lt;sub&gt;2&lt;/sub&gt;'', which is constant time. The weight of the parent node is set to the length of the left child ''S1'', which would take O(log N) time, if the tree is balanced.

As most rope operations require balanced trees, the tree may need to be re-balanced after concatenation.

=== Split ===
[[File:Vector Rope split.svg|right|x600px|thumb|Figure 2.3: Splitting a rope in half.]]

: ''Definition:'' &lt;code&gt;Split (i, S)&lt;/code&gt;: split the string ''S'' into two new strings ''S&lt;sub&gt;1&lt;/sub&gt;'' and ''S&lt;sub&gt;2&lt;/sub&gt;'', ''S&lt;sub&gt;1&lt;/sub&gt; = C&lt;sub&gt;1&lt;/sub&gt;, …, C&lt;sub&gt;i&lt;/sub&gt;'' and ''S&lt;sub&gt;2&lt;/sub&gt; = {{nowrap|C&lt;sub&gt;i + 1&lt;/sub&gt;}}, …, C&lt;sub&gt;m&lt;/sub&gt;''.
: ''Time complexity:'' ''O(log N)''

There are two cases that must be dealt with:
# The split point is at the end of a string (i.e. after the last character of a leaf node)
# The split point is in the middle of a string.

The second case reduces to the first by splitting the string at the split point to create two new leaf nodes, then creating a new node that is the parent of the two component strings.

For example, to split the 22-character rope pictured in Figure 2.3 into two equal component ropes of length 11, query the 12th character to locate the node '''K''' at the bottom level. Remove the link between '''K''' and the right child of '''G'''. Go to the parent '''G''' and subtract the weight of '''K''' from the weight of '''G'''. Travel up the tree and remove any right links, subtracting the weight of '''K''' from these nodes (only node '''D''', in this case). Finally, build up the newly orphaned nodes '''K''' and '''H''' by concatenating them together and creating a new parent '''P''' with weight equal to the length of the left node '''K'''.

As most rope operations require balanced trees, the tree may need to be re-balanced after splitting.

=== Insert ===
: ''Definition:'' &lt;code&gt;Insert(i, S’)&lt;/code&gt;: insert the string ''S’'' beginning at position ''i'' in the string ''s'', to form a new string ''C&lt;sub&gt;1&lt;/sub&gt;, …, C&lt;sub&gt;i&lt;/sub&gt;, S’, {{nowrap|C&lt;sub&gt;i + 1&lt;/sub&gt;}}, …, C&lt;sub&gt;m&lt;/sub&gt;''.
: ''Time complexity:'' ''O(log N)''.

This operation can be done by a &lt;code&gt;Split()&lt;/code&gt; and two &lt;code&gt;Concat()&lt;/code&gt; operations. The cost is the sum of the three.

=== Delete ===
: ''Definition:'' &lt;code&gt;Delete(i, j)&lt;/code&gt;: delete the substring ''C&lt;sub&gt;i&lt;/sub&gt;, …, {{nowrap|C&lt;sub&gt;i + j − 1&lt;/sub&gt;}}'', from ''s'' to form a new string ''C&lt;sub&gt;1&lt;/sub&gt;, …, {{nowrap|C&lt;sub&gt;i − 1&lt;/sub&gt;}}, {{nowrap|C&lt;sub&gt;i + j&lt;/sub&gt;}}, …, C&lt;sub&gt;m&lt;/sub&gt;''.
: ''Time complexity:'' ''O(log N)''.

This operation can be done by two &lt;code&gt;Split()&lt;/code&gt; and one &lt;code&gt;Concat()&lt;/code&gt; operation. First, split the rope in three, divided by ''i''-th and ''i+j''-th character respectively, which extracts the string to delete in a separate node. Then concatenate the other two nodes.

=== Report ===
: ''Definition:'' &lt;code&gt;Report(i, j)&lt;/code&gt;: output the string ''C&lt;sub&gt;i&lt;/sub&gt;, …, {{nowrap|C&lt;sub&gt;i + j − 1&lt;/sub&gt;}}''.
: ''Time complexity:'' ''O(j + log N)''

To report the string ''C&lt;sub&gt;i&lt;/sub&gt;, …, {{nowrap|C&lt;sub&gt;i + j − 1&lt;/sub&gt;}}'', find the node ''u'' that contains ''C&lt;sub&gt;i&lt;/sub&gt;'' and ''weight(u) &gt;= j'', and then traverse ''T'' starting at node ''u''. Output ''C&lt;sub&gt;i&lt;/sub&gt;, …, {{nowrap|C&lt;sub&gt;i + j − 1&lt;/sub&gt;}}'' by doing an [[Tree traversal#In-order|in-order traversal]] of ''T'' starting at node ''u''.

==Comparison with monolithic arrays==
{| class=&quot;wikitable floatright&quot;
|+ Performance{{citation needed|date=October 2010}}
 ! Operation                           !! Rope                  !! String
 |- align=&quot;middle&quot;
 | Index&lt;ref name=&quot;Boehm&quot; /&gt;           || {{okay|O(log n)}}     || {{yes|O(1)}}
 |- align=&quot;middle&quot;
 | Split&lt;ref name=&quot;Boehm&quot; /&gt;           || {{okay|O(log n)}}     || {{yes|O(1)}} 
 |- align=&quot;middle&quot;
 | Concatenate (destructive)      || {{yes|O(log n)}}      || {{bad|O(n)}}
 |- align=&quot;middle&quot;
 | Concatenate (nondestructive)  || {{yes|O(n)}}      || {{yes|O(n)}}
 |- align=&quot;middle&quot;
 | Iterate over each character&lt;ref name=&quot;Boehm&quot; /&gt;
                                       |  {{yes|O(n)}}          || {{yes|O(n)}}
 |- align=&quot;middle&quot;
 | Insert                              || {{yes|O(log n)}}      || {{bad|O(n)}}
 |- align=&quot;middle&quot;
 | Append                              || {{okay|O(log n)}}      || {{okay|O(1) amortized, O(n) worst case}}
 |- align=&quot;middle&quot;
 | Delete                              || {{yes|O(log n)}}      || {{bad|O(n)}}
 |- align=&quot;middle&quot;
 | Report                              || {{okay|O(j + log n)}} || {{yes|O(j)}}
 |- align=&quot;middle&quot;
 | Build                               || {{yes|O(n)}}          || {{yes|O(n)}}
 |}

Advantages:
* Ropes enable much faster insertion and deletion of text than monolithic string arrays, on which operations have time complexity O(n).
* Ropes don't require O(n) extra memory when operated upon (arrays need that for copying operations)
* Ropes don't require large contiguous memory spaces.
* If only nondestructive versions of operations are used, rope is a [[persistent data structure]]. For the text editing program example, this leads to an easy support for multiple [[undo]] levels.

Disadvantages:
* Greater overall space usage when not being operated on, mainly to store parent nodes. There is a trade-off between how much of the total memory is such overhead and how long pieces of data are being processed as strings; note that the strings in example figures above are unrealistically short for modern architectures. The overhead is always O(n), but the constant can be made arbitrarily small.
* Increase in time to manage the extra storage
* Increased complexity of source code; greater risk for bugs

This table compares the ''algorithmic'' characteristics of string and rope implementations, not their &quot;raw speed&quot;.  Array-based strings have smaller overhead, so (for example) concatenation and split operations are faster on small datasets. However, when array-based strings are used for longer strings, time complexity and memory usage for insertion and deletion of characters become unacceptably large. A rope data structure, on the other hand, has stable performance regardless of data size. Moreover, the space complexity for ropes and arrays are both O(n). In summary, ropes are better suited when the data is large and frequently modified.

==See also==
* The [[Cedar (programming language)|Cedar]] programming environment, which used ropes &quot;almost since its inception&quot;&lt;ref name=&quot;Boehm&quot;/&gt;
* The [[Enfilade (Xanadu)|Model T enfilade]], a similar data structure from the early 1970s.
* [[Gap buffer]], a data structure commonly used in text editors that allows efficient insertion and deletion operations clustered near the same location

==References==
&lt;references/&gt;

==External links==
* [http://www.sgi.com/tech/stl/Rope.html SGI's implementation of ropes for C++]
* [https://github.com/ivmai/bdwgc/ &quot;C cords&quot; implementation of ropes within the Boehm Garbage Collector library]
* [http://gcc.gnu.org/onlinedocs/libstdc++/libstdc++-html-USERS-4.3/a00223.html libstdc++ support for ropes]
* [http://ahmadsoft.org/ropes/ Ropes for Java]
* [http://rope.forge.ocamlcore.org/doc/Rope.html Ropes] for [[OCaml]]
* [https://github.com/Ramarren/ropes ropes] for [[Common Lisp]]
* [http://sourceforge.net/projects/pyropes/files/?source=navbar pyropes] for [[Python (programming language)|Python]]

{{DEFAULTSORT:Rope (Computer Science)}}
[[Category:Binary trees]]
[[Category:String data structures]]</text>
      <sha1>mcdkesiogp3y0z1aiuwpz92vk4efigz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Null-terminated string</title>
    <ns>0</ns>
    <id>338167</id>
    <revision>
      <id>624448055</id>
      <parentid>624362562</parentid>
      <timestamp>2014-09-06T19:04:15Z</timestamp>
      <contributor>
        <username>BIL</username>
        <id>661572</id>
      </contributor>
      <comment>/* Character encodings */</comment>
      <text xml:space="preserve" bytes="8027">In [[computer programming]], a '''null-terminated string''' is a [[character string]] stored as an [[Array data structure|array]] containing the characters and terminated with a [[null character]] (&lt;code&gt;'\0'&lt;/code&gt;, called NUL in [[ASCII]]). Alternative names are '''[[C string]]''', which refers to the [[C (programming language)|C programming language]] and '''ASCIIZ''' (note that C strings do not imply the use of ASCII).

The length of a C string is found by searching for the (first) NUL byte. This can be slow as it takes O(''n'') ([[linear time]]) with respect to the string length. It also means that a NUL cannot be inside the string, as the only NUL is the one marking the end.

== History ==
Null-terminated strings were produced by the &lt;code&gt;.ASCIZ&lt;/code&gt; directive of the [[PDP-11]] [[assembly language]]s and the &lt;code&gt;ASCIZ&lt;/code&gt; directive of the [[MACRO-10]] macro assembly language for the [[PDP-10]]. These predate the development of the C programming language, but other forms of strings were often used.

At the time C (and the languages that it was derived from) was developed, memory was extremely limited, so using only one byte of overhead to store the length of a string was attractive. The only popular alternative at that time, usually called a &quot;Pascal string&quot; (though also used by early versions of [[Microsoft BASIC|BASIC]]), used a leading byte to store the length of the string. This allows the string to contain NUL and made finding the length need only one memory access (O(1) [[constant time|(constant) time]]). However, C designer [[Dennis Ritchie]] chose to follow the convention of NUL-termination, already established in [[BCPL]],

:to avoid the limitation on the length of a string caused by holding the count in an 8- or 9-bit slot, and partly because maintaining the count seemed, in our experience, less convenient than using a terminator.&lt;ref&gt;Dennis M. Ritchie (1993). [The development of the C language]. Proc. 2nd History of Programming Languages Conf.&lt;/ref&gt;

This had some influence on CPU [[instruction set]] design. Some CPUs in the 1970s and 1980s, such as the [[Zilog Z80]] and the [[Digital Equipment Corporation|DEC]] [[VAX]], had dedicated instructions for handling length-prefixed strings. However, as the NUL-terminated string gained traction, CPU designers began to take it into account, as seen for example in IBM's decision to add the &quot;Logical String Assist&quot; instructions to the [[IBM ES/9000 family|ES/9000]] 520 in 1992.

[[FreeBSD]] developer [[Poul-Henning Kamp]], writing in ''[[ACM Queue]]'', would later refer to the victory of the C string over use of a ''2-byte'' (not 1-byte) length as &quot;the most expensive one-byte mistake&quot; ever.&lt;ref&gt;{{citation |last=Kamp |first=Poul-Henning |date=25 July 2011 |title=The Most Expensive One-byte Mistake |journal=ACM Queue |volume=9 |number=7 |issn=1542-7730 |accessdate=2 August 2011 |url=http://queue.acm.org/detail.cfm?id=2010365 }}&lt;/ref&gt;

== Implementations ==
{{expand section|date=November 2011}}
[[C (programming language)|C programming language]] supports null-terminated strings as the primary string type.&lt;ref&gt;{{cite web |title=The Development of the C Language |url=http://cm.bell-labs.com/cm/cs/who/dmr/chist.html |accessdate=9 November 2011 |first=Dennis |last=Richie |year=2003 }}&lt;/ref&gt; There are a lot of [[C string handling|functions for string handling]] in the [[C standard library]].

== Limitations ==
While simple to implement, this representation has been prone to errors and performance problems.

The NUL termination has historically created [[computer insecurity|security problems]].&lt;ref&gt;{{cite journal|url= http://artofhacking.com/files/phrack/phrack55/P55-07.TXT |author=Rain Forest Puppy |title=Perl CGI problems |work=Phrack Magazine |publisher=artofhacking.com |date=9 September 1999 |volume=9 |issue=55 |page=7 |accessdate=6 January 2012}}&lt;/ref&gt; A NUL byte inserted into the middle of a string will truncate it unexpectedly. A common bug was to not allocate the additional space for the NUL, so it was written over adjacent memory. Another was to not write the NUL at all, which was often not detected during testing because a NUL was already there by chance from previous use of the same block of memory. Due to the expense of finding the length, many programs did not bother before copying a string to a fixed-size buffer, causing a [[buffer overflow]] if it was too long.

The inability to store a NUL requires that string data and binary data be kept distinct and handled by different functions (with the latter requiring the length of the data to also be supplied). This can lead to code redundancy and errors when the wrong function is used.

The speed problems with finding the length can usually be mitigated by combining it with another operation that is O(''n'') anyway, such as in &lt;code&gt;[[strlcpy]]&lt;/code&gt;. However, this does not always result in an intuitive [[API]].

== Character encodings ==
Null-terminated strings require of the encoding that it does not use the zero code anywhere.

It is not possible to store every possible [[ASCII]] or [[UTF-8]] string in a null-terminated string, as the encoding of the [[Null character|NUL character]] is a zero byte.&lt;ref&gt;{{cite web|title=UTF-8, a transformation format of ISO 10646|url=http://tools.ietf.org/html/rfc3629#section-3|accessdate=19 September 2013}}&lt;/ref&gt;&lt;ref&gt;&lt;!-- This is the encoding table provided as a resource by the Unicode consortium: http://www.unicode.org/resources/utf8.html --&gt;{{cite web|title=Unicode/UTF-8-character table|url=http://www.utf8-chartable.de/|accessdate=13 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Kuhn|first=Markus|title=UTF-8 and Unicode FAQ|url=http://www.cl.cam.ac.uk/~mgk25/unicode.html#utf-8|accessdate=13 September 2013}}&lt;/ref&gt; However, it is common to store the subset of ASCII or UTF-8 not containing the NUL character in null-terminated strings. Some systems use &quot;modified UTF-8&quot; which encodes the NUL character as two non-zero bytes (0xC0, 0x80) and thus allow all possible strings to be stored. (this is not allowed by the UTF-8 standard as it is a security risk. A C0,80 NUL might be seen as a string terminator in security validation and as a character when used)

[[UTF-16]] uses 2-byte integers and since either byte may be zero, cannot be stored in a null-terminated byte string. However a null-terminated string of 16-bit words can be used and some languages implement this (again the NUL character, which encodes as a single zero code unit, cannot be stored).

== Improvements ==
Many attempts have been made to make C string handling less error prone. One strategy is to add safer and more useful functions such as &lt;code&gt;[[strdup]]&lt;/code&gt; and &lt;code&gt;[[strlcpy]]&lt;/code&gt;, while [[C standard library#Buffer overflow vulnerabilities | deprecating the use of unsafe functions]] such as &lt;code&gt;[[gets() | gets]]&lt;/code&gt;. Another is to add an object-oriented wrapper around C strings so that only safe calls can be done.

On modern systems memory usage is less of a concern, so a multi-byte length is acceptable (if you have so many small strings that the space used by this length is a concern, you will have enough duplicates that a [[hash table]] will use even less memory). Most replacements for C strings use a 32-bit or larger length value. Examples include the [[C++]] [[Standard Template Library]] &lt;code&gt;[[String (C++)|std::string]]&lt;/code&gt;, the [[Qt (toolkit)|Qt]] &lt;code&gt;QString&lt;/code&gt;, the [[Microsoft Foundation Class Library|MFC]] &lt;code&gt;CString&lt;/code&gt;, and the C-based implementation &lt;code&gt;CFString&lt;/code&gt; from [[Core Foundation]] as well as its [[Objective-C]] sibling &lt;code&gt;NSString&lt;/code&gt; from [[Foundation Kit|Foundation]], both by Apple. More complex structures may also be used to store strings such as the [[rope (computer science)|rope]].

==References==
{{reflist}}

{{CProLang}}
{{Data types}}
{{Use dmy dates|date=January 2011}}

[[Category:String data structures]]

[[es:C string]]
[[ko:C 문자열]]
[[ru:Нуль-терминированная строка]]</text>
      <sha1>3lk2b32guflx0c9m8jp51io63rhvch9</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Suffix tree</title>
    <ns>0</ns>
    <id>794679</id>
    <revision>
      <id>624023420</id>
      <parentid>623480582</parentid>
      <timestamp>2014-09-03T15:30:35Z</timestamp>
      <contributor>
        <ip>140.180.248.69</ip>
      </contributor>
      <text xml:space="preserve" bytes="22508">[[Image:Suffix tree BANANA.svg|thumb|250px|right|Suffix tree for the text &lt;code&gt;BANANA&lt;/code&gt;. Each substring is terminated with special character &lt;code&gt;$&lt;/code&gt;. The six paths from the root to a leaf (shown as boxes) correspond to the six suffixes &lt;code&gt;A$&lt;/code&gt;, &lt;code&gt;NA$&lt;/code&gt;, &lt;code&gt;ANA$&lt;/code&gt;, &lt;code&gt;NANA$&lt;/code&gt;, &lt;code&gt;ANANA$&lt;/code&gt; and &lt;code&gt;BANANA$&lt;/code&gt;. The numbers in the leaves give the start position of the corresponding suffix. Suffix links, drawn dashed, are used during construction.]]

In [[computer science]], a '''suffix tree''' (also called '''PAT tree''' or, in an earlier form, '''position tree''') is a compressed [[trie]] containing all the [[suffix (computer science)|suffixes]] of the given text as their keys and positions in the text as their values. Suffix trees allow particularly fast implementations of many important string operations.

The construction of such a tree for the string &lt;math&gt;S&lt;/math&gt; takes time and space linear in the length of &lt;math&gt;S&lt;/math&gt;. Once constructed, several operations can be performed quickly, for instance locating a [[substring]] in &lt;math&gt;S&lt;/math&gt;, locating a substring if a certain number of mistakes are allowed, locating matches for a [[regular expression]] pattern etc. Suffix trees also provide one of the first linear-time solutions for the [[longest common substring problem]]. These speedups come at a cost: storing a string's suffix tree typically requires significantly more space than storing the string itself.

==History==
The concept was first introduced by {{harvtxt|Weiner|1973}}, which [[Donald Knuth]] subsequently characterized as &quot;Algorithm of the Year 1973&quot;.  The construction was greatly simplified by {{harvtxt|McCreight|1976}}
, and also by {{harvtxt|Ukkonen|1995}}.{{sfnp|Giegerich|Kurtz|1997}}  Ukkonen provided the first online-construction of suffix trees, now known as [[Ukkonen's algorithm]], with running time that matched the then fastest algorithms.
These algorithms are all linear-time for a constant-size alphabet, and have worst-case running time of &lt;math&gt;O(n\log n)&lt;/math&gt; in general.

{{harvtxt|Farach|1997}} gave the first suffix tree construction algorithm that is optimal for all alphabets.  In particular, this is the first linear-time algorithm 
for strings drawn from an alphabet of integers in a polynomial range.  Farach's algorithm has become the basis for new algorithms for constructing both suffix trees and [[suffix array]]s, for example, in external memory, compressed, succinct, etc.

==Definition==
The suffix tree for the string &lt;math&gt;S&lt;/math&gt; of length &lt;math&gt;n&lt;/math&gt; is defined as a tree such that:&lt;ref&gt;http://www.cs.uoi.gr/~kblekas/courses/bioinformatics/Suffix_Trees1.pdf&lt;/ref&gt;&lt;!-- ref&gt;{{harvtxt|Gusfield|1999}}, p.90.&lt;/ref --&gt;
* The tree has exactly n leaves numbered from 1 to n.
* Except for the root, every internal node has at least two children.
* Each edge is labeled with a non-empty substring of S.
* No two edges starting out of a node can have string-labels beginning with the same character.
* The string obtained by concatenating all the string-labels found on the path from the root to leaf i spells out suffix S[i..n], for i from 1 to n.

Since such a tree does not exist for all strings, &lt;math&gt;S&lt;/math&gt; is padded with a terminal symbol not seen in the string (usually denoted &lt;code&gt;$&lt;/code&gt;). This ensures that no suffix is a prefix of another, and that there will be &lt;math&gt;n&lt;/math&gt; leaf nodes, one for each of the &lt;math&gt;n&lt;/math&gt; suffixes of &lt;math&gt;S&lt;/math&gt;. Since all internal non-root nodes are branching, there can be at most ''n''&amp;nbsp;&amp;minus;&amp;nbsp; 1 such nodes, and ''n''&amp;nbsp;+&amp;nbsp;(''n''&amp;nbsp;&amp;minus;&amp;nbsp;1)&amp;nbsp;+&amp;nbsp;1&amp;nbsp;=&amp;nbsp;2''n'' nodes in total (''n'' leaves, ''n''&amp;nbsp;&amp;minus;&amp;nbsp;1 internal non-root nodes, 1 root).

'''Suffix links''' are a key feature for older linear-time construction algorithms, although most newer algorithms, which are based on [[Farach's algorithm]], dispense with suffix links. In a complete suffix tree, all internal non-root nodes have a suffix link to another internal node. If the path from the root to a node spells the string &lt;math&gt;\chi\alpha&lt;/math&gt;, where &lt;math&gt;\chi&lt;/math&gt; is a single character and &lt;math&gt;\alpha&lt;/math&gt; is a string (possibly empty), it has a suffix link to the internal node representing &lt;math&gt;\alpha&lt;/math&gt;. See for example the suffix link from the node for &lt;code&gt;ANA&lt;/code&gt; to the node for &lt;code&gt;NA&lt;/code&gt; in the figure above. Suffix links are also used in some algorithms running on the tree.

==Generalized suffix tree==
A [[generalized suffix tree]] is a suffix tree made for a set of words instead of only for a single word. It represents all suffixes from this set of words. Each word must be terminated by a different termination symbol or word.

==Functionality==

A suffix tree for a string &lt;math&gt;S&lt;/math&gt; of length &lt;math&gt;n&lt;/math&gt; can be built in &lt;math&gt;\Theta(n)&lt;/math&gt; time, if the letters come from an alphabet of integers in a polynomial range (in particular, this is true for constant-sized alphabets).{{sfnp|Farach|1997}}
For larger alphabets, the running time is dominated by first [[Sorting algorithm|sorting]] the letters to bring them into a range of size &lt;math&gt;O(n)&lt;/math&gt;; in general, this takes &lt;math&gt;O(n\log n)&lt;/math&gt; time.
The costs below are given under the assumption that the alphabet is constant.

Assume that a suffix tree has been built for the string &lt;math&gt;S&lt;/math&gt; of length &lt;math&gt;n&lt;/math&gt;, or that a [[generalised suffix tree]] has been built for the set of strings &lt;math&gt;D=\{S_1,S_2,\dots,S_K\}&lt;/math&gt; of total length &lt;math&gt;n=|n_1|+|n_2|+\cdots+|n_K|&lt;/math&gt;.
You can:

* Search for strings:
** Check if a string &lt;math&gt;P&lt;/math&gt; of length &lt;math&gt;m&lt;/math&gt; is a substring in &lt;math&gt;O(m)&lt;/math&gt; time.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, p.92.&lt;/ref&gt;
** Find the first occurrence of the patterns &lt;math&gt;P_1,\dots,P_q&lt;/math&gt; of total length &lt;math&gt;m&lt;/math&gt; as substrings in &lt;math&gt;O(m)&lt;/math&gt; time.
** Find all &lt;math&gt;z&lt;/math&gt; occurrences of the patterns &lt;math&gt;P_1,\dots,P_q&lt;/math&gt; of total length &lt;math&gt;m&lt;/math&gt; as substrings in &lt;math&gt;O(m + z)&lt;/math&gt; time.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, p.123.&lt;/ref&gt;
&lt;!-- ** Search for a pattern &lt;math&gt;P&lt;/math&gt; of length &lt;math&gt;m&lt;/math&gt; with at most &lt;math&gt;k&lt;/math&gt; mismatches in &lt;math&gt;O(m \sum_{r=0}^k * {m \choose r} (|\Sigma| - 1)^k + z)&lt;/math&gt; time.--&gt;
** Search for a [[regular expression]] ''P'' in time expected [[Sublinear time algorithm|sublinear]] in &lt;math&gt;n&lt;/math&gt;.{{sfnp|Baeza-Yates|Gonnet|1996}}
** Find for each suffix of a pattern &lt;math&gt;P&lt;/math&gt;, the length of the longest match between a prefix of &lt;math&gt;P[i\dots m]&lt;/math&gt; and a substring in &lt;math&gt;D&lt;/math&gt; in &lt;math&gt;\Theta(m)&lt;/math&gt; time.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, p.132.&lt;/ref&gt; This is termed the '''matching statistics''' for &lt;math&gt;P&lt;/math&gt;.
* Find properties of the strings:
** Find the [[longest common substring problem|longest common substrings]] of the string &lt;math&gt;S_i&lt;/math&gt; and &lt;math&gt;S_j&lt;/math&gt; in &lt;math&gt;\Theta(n_i + n_j)&lt;/math&gt; time.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, p.125.&lt;/ref&gt;
** Find all [[maximal pair]]s, maximal repeats or supermaximal repeats in &lt;math&gt;\Theta(n + z)&lt;/math&gt; time.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, p.144.&lt;/ref&gt;
** Find the [[Lempel&amp;ndash;Ziv]] decomposition in &lt;math&gt;\Theta(n)&lt;/math&gt; time.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, p.166.&lt;/ref&gt;
** Find the [[longest repeated substring problem|longest repeated substrings]] in &lt;math&gt;\Theta(n)&lt;/math&gt; time.
** Find the most frequently occurring substrings of a minimum length in &lt;math&gt;\Theta(n)&lt;/math&gt; time.
** Find the shortest strings from &lt;math&gt;\Sigma&lt;/math&gt; that do not occur in &lt;math&gt;D&lt;/math&gt;, in &lt;math&gt;O(n + z)&lt;/math&gt; time, if there are &lt;math&gt;z&lt;/math&gt; such strings.
** Find the shortest substrings occurring only once in &lt;math&gt;\Theta(n)&lt;/math&gt; time.
** Find, for each &lt;math&gt;i&lt;/math&gt;, the shortest substrings of &lt;math&gt;S_i&lt;/math&gt; not occurring elsewhere in &lt;math&gt;D&lt;/math&gt; in &lt;math&gt;\Theta(n)&lt;/math&gt; time.

The suffix tree can be prepared for constant time [[lowest common ancestor]] retrieval between nodes in &lt;math&gt;\Theta(n)&lt;/math&gt; time.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, Chapter 8.&lt;/ref&gt; One can then also:
* Find the longest common prefix between the suffixes &lt;math&gt;S_i[p..n_i]&lt;/math&gt; and &lt;math&gt;S_j[q..n_j]&lt;/math&gt; in &lt;math&gt;\Theta(1)&lt;/math&gt;.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, p.196.&lt;/ref&gt;
* Search for a pattern ''P'' of length ''m'' with at most ''k'' mismatches in &lt;math&gt;O(k n + z)&lt;/math&gt; time, where ''z'' is the number of hits.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, p.200.&lt;/ref&gt;
* Find all &lt;math&gt;z&lt;/math&gt; maximal [[palindrome]]s in &lt;math&gt;\Theta(n)&lt;/math&gt;,&lt;ref&gt;{{harvtxt|Gusfield|1999}}, p.198.&lt;/ref&gt; or &lt;math&gt;\Theta(g n)&lt;/math&gt; time if gaps of length &lt;math&gt;g&lt;/math&gt; are allowed, or &lt;math&gt;\Theta(k n)&lt;/math&gt; if &lt;math&gt;k&lt;/math&gt; mismatches are allowed.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, p.201.&lt;/ref&gt;
* Find all &lt;math&gt;z&lt;/math&gt; [[tandem repeats]] in &lt;math&gt;O(n \log n + z)&lt;/math&gt;, and ''k''-mismatch tandem repeats in &lt;math&gt;O(k n \log (n/k) + z)&lt;/math&gt;.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, p.204.&lt;/ref&gt;
* Find the [[longest common substring problem|longest common substrings]] to at least &lt;math&gt;k&lt;/math&gt; strings in &lt;math&gt;D&lt;/math&gt; for &lt;math&gt;k=2,\dots,K&lt;/math&gt; in &lt;math&gt;\Theta(n)&lt;/math&gt; time.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, p.205.&lt;/ref&gt;
* Find the [[longest palindromic substring]] of a given string (using the suffix trees of both the string and its reverse) in linear time.&lt;ref&gt;{{harvtxt|Gusfield|1999}}, pp.197–199.&lt;/ref&gt;

== Applications ==
Suffix trees can be used to solve a large number of string problems that occur in text-editing, free-text search, computational biology and other application areas.&lt;ref name=&quot;allisons&quot;/&gt; Primary applications include:&lt;ref name=&quot;allisons&quot;&gt;{{cite web|url=http://www.allisons.org/ll/AlgDS/Tree/Suffix/|title=Suffix Trees|last=Allison|first=L. |accessdate=2008-10-14}}&lt;/ref&gt;
* [[String search#Index methods|String search]], in ''O(m)'' complexity, where ''m'' is the length of the sub-string (but with initial ''O(n)'' time required to build the suffix tree for the string)
* Finding the longest repeated substring
* Finding the longest common substring
* Finding the longest [[palindrome]] in a string

Suffix trees are often used in [[bioinformatics]] applications, searching for patterns in [[DNA]] or [[protein]] sequences (which can be viewed as long strings of characters). The ability to search efficiently with mismatches might be considered their greatest strength. Suffix trees are also used in [[data compression]]; they can be used to find repeated data, and can be used for the sorting stage of the [[Burrows&amp;ndash;Wheeler transform]]. Variants of the [[LZW]] compression schemes use suffix trees ([[LZSS]]). A suffix tree is also used in [[suffix tree clustering]], a [[data clustering]] algorithm used in some search engines.&lt;ref&gt;First introduced by {{harvtxt|Zamir|Etzioni|1998}}.&lt;/ref&gt;

==Implementation==
If each node and edge can be represented in &lt;math&gt;\Theta(1)&lt;/math&gt; space, the entire tree can be represented in &lt;math&gt;\Theta(n)&lt;/math&gt; space. The total length of all the strings on all of the edges in the tree is &lt;math&gt;O(n^2)&lt;/math&gt;, but each edge can be stored as the position and length of a substring of ''S'', giving a total space usage of &lt;math&gt;\Theta(n)&lt;/math&gt; computer words. The worst-case space usage of a suffix tree is seen with a [[fibonacci word]], giving the full &lt;math&gt;2n&lt;/math&gt; nodes.

An important choice when making a suffix tree implementation is the parent-child relationships between nodes. The most common is using [[linked list]]s called '''sibling lists'''. Each node has a pointer to its first child, and to the next node in the child list it is a part of. Other implementations with efficient running time properties use [[hash map]]s, sorted or unsorted [[array data structure|array]]s (with [[dynamic array|array doubling]]), or [[Self-balancing binary search tree|balanced search tree]]s. We are interested in:
* The cost of finding the child on a given character.
* The cost of inserting a child.
* The cost of enlisting all children of a node (divided by the number of children in the table below).

Let &lt;math&gt;\sigma&lt;/math&gt; be the size of the alphabet. Then you have the following costs:

{|
!
! Lookup
! Insertion
! Traversal
|-
! Sibling lists / unsorted arrays
| &lt;math&gt;O(\sigma)&lt;/math&gt;
| &lt;math&gt;\Theta(1)&lt;/math&gt;
| &lt;math&gt;\Theta(1)&lt;/math&gt;
|-
! Bitwise sibling trees
| &lt;math&gt;O(\log \sigma)&lt;/math&gt;
| &lt;math&gt;\Theta(1)&lt;/math&gt;
| &lt;math&gt;\Theta(1)&lt;/math&gt;
|-
! Hash maps
| &lt;math&gt;\Theta(1)&lt;/math&gt;
| &lt;math&gt;\Theta(1)&lt;/math&gt;
| &lt;math&gt;O(\sigma)&lt;/math&gt;
|-
! Balanced search tree
| &lt;math&gt;O(\log \sigma)&lt;/math&gt;
| &lt;math&gt;O(\log \sigma)&lt;/math&gt;
| &lt;math&gt;O(1)&lt;/math&gt;
|-
! Sorted arrays
| &lt;math&gt;O(\log \sigma)&lt;/math&gt;
| &lt;math&gt;O(\sigma)&lt;/math&gt;
| &lt;math&gt;O(1)&lt;/math&gt;
|-
! Hash maps + sibling lists
| &lt;math&gt;O(1)&lt;/math&gt;
| &lt;math&gt;O(1)&lt;/math&gt;
| &lt;math&gt;O(1)&lt;/math&gt;
|}

Note that the insertion cost is amortised, and that the costs for hashing are given for perfect hashing.

The large amount of information in each edge and node makes the suffix tree very expensive, consuming about 10 to 20 times the memory size of the source text in good implementations. The [[suffix array]] reduces this requirement to a factor of 8 (for array including [[LCP array|LCP]] values built within 32-bit address space and 8-bit characters.) This factor depends on the properties and may reach 2 with usage of 4-byte wide characters (needed to contain any symbol in some UNIX-like systems, see [[wchar t]]) on 32-bit systems. Researchers have continued to find smaller indexing structures.

==External construction==

Though linear, the memory usage of a suffix tree is significantly higher
than the actual size of the sequence collection.  For a large text,
construction may require external memory approaches.

There are theoretical results for constructing suffix trees in external
memory.
The algorithm by {{harvtxt|Farach-Colton|Ferragina|Muthukrishnan|2000}}
is theoretically optimal, with an I/O complexity equal to that of sorting.
However the overall intricacy of this algorithm has prevented, so far, its
practical implementation.{{sfnp|Smyth|2003}}

On the other hand, there have been practical works for constructing
disk-based suffix trees
which scale to (few) GB/hours.
The state of the art methods are TDD,&lt;ref name=&quot;tdd&quot;&gt;{{harvtxt|Tata|Hankins|Patel|2003}}.&lt;/ref&gt;
TRELLIS,&lt;ref name=&quot;trellis&quot;&gt;{{harvtxt|Phoophakdee|Zaki|2007}}.&lt;/ref&gt;
DiGeST,&lt;ref name=&quot;digest&quot;&gt;{{harvtxt|Barsky|Stege|Thomo|Upton|2008}}.&lt;/ref&gt;
and
B&lt;sup&gt;2&lt;/sup&gt;ST.&lt;ref name=&quot;b2st&quot;&gt;{{harvtxt|Barsky|Stege|Thomo|Upton|2009}}.&lt;/ref&gt;

TDD and TRELLIS scale up to the entire human genome &amp;ndash; approximately 3GB &amp;ndash; resulting in a disk-based suffix tree of a size in the tens of gigabytes.&lt;ref name=&quot;tdd&quot; /&gt;&lt;ref name=&quot;trellis&quot; /&gt; However, these methods cannot handle efficiently collections of sequences exceeding 3GB.&lt;ref name=&quot;digest&quot; /&gt;  DiGeST performs significantly better and is able to handle collections of sequences in the order of 6GB in about 6 hours.&lt;ref name=&quot;digest&quot; /&gt;
.
All these methods can efficiently build suffix trees for the case when the
tree does not fit in main memory,
but the input does.
The most recent method, B&lt;sup&gt;2&lt;/sup&gt;ST,&lt;ref name=&quot;b2st&quot; /&gt; scales to handle
inputs that do not fit in main memory. ERA  is a recent parallel suffix tree construction method that is significantly faster. ERA can index the entire human genome in 19 minutes on an 8-core desktop computer with 16GB RAM. On a simple Linux cluster with 16 nodes (4GB RAM per node), ERA can index the entire human genome in less than 9 minutes.{{sfnp|Mansour|Allam|Skiadopoulos|Kalnis|2011}}

==See also==
* [[Suffix array]]
* [[Generalised suffix tree]]
* [[Trie]]

==Notes==
{{reflist|colwidth=30em}}

==References==
*{{citation
 | last1 = Baeza-Yates | first1 = Ricardo A. | author1-link = Ricardo Baeza-Yates
 | last2 = Gonnet | first2 = Gaston H. | author2-link = Gaston Gonnet
 | doi = 10.1145/235809.235810
 | issue = 6
 | journal = [[Journal of the ACM]]
 | pages = 915–936
 | title = Fast text searching for regular expressions or automaton searching on tries
 | volume = 43
 | year = 1996}}.
*{{citation
 | last1 = Barsky | first1 = Marina
 | last2 = Stege | first2 = Ulrike
 | last3 = Thomo | first3 = Alex
 | last4 = Upton | first4 = Chris
 | contribution = A new method for indexing genomes using on-disk suffix trees
 | location = New York, NY, USA
 | pages = 649–658
 | publisher = ACM
 | title = CIKM '08: Proceedings of the 17th ACM Conference on Information and Knowledge Management
 | year = 2008}}.
*{{citation
 | last1 = Barsky | first1 = Marina
 | last2 = Stege | first2 = Ulrike
 | last3 = Thomo | first3 = Alex
 | last4 = Upton | first4 = Chris
 | contribution = Suffix trees for very large genomic sequences
 | location = New York, NY, USA
 | publisher = ACM
 | title = CIKM '09: Proceedings of the 18th ACM Conference on Information and Knowledge Management
 | year = 2009}}.
*{{citation
 | last = Farach | first = Martin
 | title = [[Symposium on Foundations of Computer Science|38th IEEE Symposium on Foundations of Computer Science (FOCS '97)]]
 | pages = 137–143
 | contribution = Optimal Suffix Tree Construction with Large Alphabets
 | url = http://www.cs.rutgers.edu/~farach/pubs/Suffix.pdf
 | year = 1997}}.
*{{citation
 | last1 = Farach-Colton | first1 = Martin
 | last2 = Ferragina | first2 = Paolo
 | last3 = Muthukrishnan | first3 = S.
 | doi = 10.1145/355541.355547
 | issue = 6
 | journal = Journal of the ACM
 | pages = 987–1011
 | title = On the sorting-complexity of suffix tree construction.
 | volume = 47
 | year = 2000}}.
*{{citation
 | last1 = Giegerich | first1 = R.
 | last2 = Kurtz | first2 = S.
 | doi = 10.1007/PL00009177
 | issue = 3
 | journal = [[Algorithmica]]
 | pages = 331–353
 | title = From Ukkonen to McCreight and Weiner: A Unifying View of Linear-Time Suffix Tree Construction
 | url = http://europa.zbh.uni-hamburg.de/pubs/pdf/GieKur1997.pdf
 | volume = 19
 | year = 1997}}.
*{{citation
 | last = Gusfield | first = Dan
 | isbn = 0-521-58519-8
 | publisher = Cambridge University Press
 | title = Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology
 | year = 1999}}.
*{{citation
 | last1 = Mansour | first1 = Essam
 | last2 = Allam | first2 = Amin
 | last3 = Skiadopoulos | first3 = Spiros
 | last4 = Kalnis | first4 = Panos
 | issue = 1
 | journal = PVLDB
 | pages = 49–60
 | title = ERA: Efficient Serial and Parallel Suffix Tree Construction for Very Long Strings
 | url = http://www.vldb.org/pvldb/vol5/p049_essammansour_vldb2012.pdf
 | volume = 5
 | year = 2011}}.
*{{citation
 | last = McCreight | first = Edward M. | authorlink = Edward M. McCreight
 | doi = 10.1145/321941.321946
 | issue = 2
 | journal = [[Journal of the ACM]]
 | pages = 262–272
 | title = A Space-Economical Suffix Tree Construction Algorithm
 | volume = 23
 | year = 1976}}.
*{{citation
 | last1 = Phoophakdee | first1 = Benjarath
 | last2 = Zaki | first2 = Mohammed J.
 | contribution = Genome-scale disk-based suffix tree indexing
 | location = New York, NY, USA
 | pages = 833–844
 | publisher = ACM
 | title = SIGMOD '07: Proceedings of the ACM SIGMOD International Conference on Management of Data
 | year = 2007}}.
*{{citation
 | last = Smyth | first = William
 | publisher = [[Addison-Wesley]]
 | title = Computing Patterns in Strings
 | year = 2003}}.
*{{citation
 | last1 = Tata | first1 = Sandeep
 | last2 = Hankins | first2 = Richard A.
 | last3 = Patel | first3 = Jignesh M.
 | contribution = Practical Suffix Tree Construction
 | pages = 36–47
 | publisher = Morgan Kaufmann
 | title = VLDB '03: Proceedings of the 30th International Conference on Very Large Data Bases
 | year = 2003}}.
*{{citation
 | last = Ukkonen | first = E.
 | doi = 10.1007/BF01206331
 | issue = 3
 | journal = [[Algorithmica]]
 | pages = 249–260
 | title = On-line construction of suffix trees
 | url = http://www.cs.helsinki.fi/u/ukkonen/SuffixT1withFigs.pdf
 | volume = 14
 | year = 1995}}.
*{{citation
 | last = Weiner | first = P.
 | contribution = Linear pattern matching algorithms
 | doi = 10.1109/SWAT.1973.13
 | pages = 1–11
 | title = [[Symposium on Foundations of Computer Science|14th Annual IEEE Symposium on Switching and Automata Theory]]
 | year = 1973}}.
*{{citation
 | last1 = Zamir | first1 = Oren
 | last2 = Etzioni | first2 = Oren
 | authorlink2=Oren Etzioni
 | contribution = Web document clustering: a feasibility demonstration
 | location = New York, NY, USA
 | pages = 46–54
 | publisher = ACM
 | title = SIGIR '98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval
 | year = 1998}}.

== External links ==
{{External links|date=August 2010}}
* [http://www.cise.ufl.edu/~sahni/dsaaj/enrich/c16/suffix.htm Suffix Trees] by [[Sartaj Sahni]]
* [http://www.allisons.org/ll/AlgDS/Tree/Suffix/ Suffix Trees] by Lloyd Allison
* [http://www.nist.gov/dads/HTML/suffixtree.html NIST's Dictionary of Algorithms and Data Structures: Suffix Tree]
* [http://mila.cs.technion.ac.il/~yona/suffix_tree/ suffix_tree] ANSI C implementation of a Suffix Tree
* [http://www.cl.cam.ac.uk/~cpk25/libstree/ libstree], a generic suffix tree library written in C
* [https://metacpan.org/module/Tree::Suffix Tree::Suffix], a Perl binding to libstree
* [http://www.cs.ucdavis.edu/~gusfield/strmat.html Strmat] a faster generic suffix tree library written in C (uses arrays instead of linked lists)
* [http://hkn.eecs.berkeley.edu/~dyoo/python/suffix_trees/ SuffixTree] a Python binding to Strmat
* [http://www.balkenhol.net/papers/t1043.pdf.gz Universal Data Compression Based on the Burrows-Wheeler Transformation: Theory and Practice], application of suffix trees in the BWT
* [http://www.cs.helsinki.fi/group/suds/ Theory and Practice of Succinct Data Structures], C++ implementation of a compressed suffix tree
* [http://code.google.com/p/patl/ Practical Algorithm Template Library], a C++ library with suffix tree implementation on PATRICIA trie, by Roman S. Klyujkov
* [http://en.literateprograms.org/Suffix_tree_(Java) A Java implementation]
* [http://code.google.com/p/concurrent-trees/ A Java implementation of Concurrent Suffix Tree]
* [http://code.google.com/p/text-indexing/ Text-Indexing project]  (linear-time construction of suffix trees, suffix arrays, LCP array and Burrows-Wheeler Transform)
{{CS-Trees}}

{{DEFAULTSORT:Suffix Tree}}
[[Category:Trees (data structures)]]
[[Category:Substring indices]]
[[Category:String data structures]]</text>
      <sha1>5gj9zqoexvwrj841dorwjpfjzmn8mtw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hollerith constant</title>
    <ns>0</ns>
    <id>5219725</id>
    <revision>
      <id>616633855</id>
      <parentid>575187004</parentid>
      <timestamp>2014-07-12T09:24:48Z</timestamp>
      <contributor>
        <username>Vadmium</username>
        <id>53292</id>
      </contributor>
      <comment>/* References */ [[Category:String data structures]]</comment>
      <text xml:space="preserve" bytes="4831">{{unreferenced|date=September 2013}}
'''Hollerith constants''', named in honor of [[Herman Hollerith]], were used in early [[FORTRAN]] programs to allow manipulation of character data.

Early FORTRAN had no &lt;code&gt;CHARACTER&lt;/code&gt; [[data type]], only numeric types.  In order to perform character manipulation, characters needed to be placed into numeric variables via Hollerith constants.  For example the constant &lt;code&gt;3HABC&lt;/code&gt; specified a three-character string 'ABC'.  These constants were ''[[typeless]]'', so that there were no [[type conversion]] issues.  If the constant specified fewer characters than was possible to hold in a data item, the characters were then stored in the item ''left-justified'' and ''blank-filled''.

== Mechanics ==
By the [[FORTRAN 66]] Standard, Hollerith syntax was allowed in the following uses:

* As constants in &lt;code&gt;DATA&lt;/code&gt; statements
* As constant actual arguments in subroutine &lt;code&gt;CALL&lt;/code&gt; statements
* As edit descriptors in &lt;code&gt;FORMAT&lt;/code&gt; statements

Portability was problematic with Hollerith constants.  First, [[Word (data type)|word]] sizes varied on different computer systems, so the number of characters that could be placed in each data item likewise varied.  Implementations varied from as few as two to as many as ten characters per word.  Second, it was difficult to manipulate individual characters within a word in a portable fashion.  This led to a great deal of ''shifting and masking'' code using non-standard, vendor-specific, features.  The fact that character sets varied between machines also complicated the issue.

Some authors were of the opinion that for best portability, only a single character should be used per data item.  However considering the small memory sizes of machines of the day, this technique was considered extremely wasteful.

== Technological obsolescence ==
One of the major features of FORTRAN 77 was the &lt;code&gt;CHARACTER&lt;/code&gt; string data type.  Use of this data type dramatically simplified character manipulation in Fortran programs - rendering almost all uses of the Hollerith constant technique obsolete.

Hollerith constants were deleted from the FORTRAN 77 Standard, though still described in an appendix for those wishing to continue support.  Hollerith edit descriptors were allowed through Fortran 90, and were deleted from the Fortran 95 Standard.

== Examples ==

The following is a FORTRAN 66 [[hello world]] program using Hollerith constants.  It assumes that at least four characters per word are supported by the implementation:
&lt;source lang=&quot;fortran&quot;&gt;
 C     PROGRAM HELLO1
 C
       INTEGER IHWSTR(3)
       DATA IHWSTR/4HHELL,4HO WO,3HRLD/
 C
       WRITE (6,100) IHWSTR
       STOP
   100 FORMAT (3A4)
       END
&lt;/source&gt;
Besides &lt;code&gt;DATA&lt;/code&gt; statements, Hollerith constants were also allowed as actual arguments in subroutine calls.  However there was no way that the callee could know how many characters were passed in.  The programmer had to pass the information explicitly.  The [[hello world]] program could be written as follows - on a machine where four characters are stored in a word:

 C     '''PROGRAM''' HELLO2
       '''CALL''' WRTOUT (11HHELLO WORLD, 11)
       '''STOP'''
       '''END'''
 C
       '''SUBROUTINE''' WRTOUT (IARRAY, NCHRS)
 C
       INTEGER IARRAY(1)&lt;ref group=&quot;notes&quot;&gt;FORTRAN 66 did not have a way to indicate a variable-sized array.  So a '1' was typically used to indicate that the size is unknown.&lt;/ref&gt;
       INTEGER NCHRS
 C
       INTEGER ICPW
       DATA ICPW/4/&lt;ref group=&quot;notes&quot;&gt;Four characters per word.&lt;/ref&gt;
       INTEGER I, NWRDS
 C
       NWRDS = (NCHRS + ICPW - 1) /ICPW
       WRITE (6,100) (IARRAY(I), I=1,NWRDS)
       '''RETURN'''
   100 FORMAT (100A4)&lt;ref group=&quot;notes&quot;&gt;A count of 100 is a 'large enough' value that any reasonable number of characters can be written.  Also note that four characters per word is hard-coded here too.&lt;/ref&gt;
       '''END'''

Although technically not a Hollerith constant, the same Hollerith syntax was allowed as an ''edit descriptor'' in &lt;code&gt;FORMAT&lt;/code&gt; statements.  The [[hello world]] program could also be written as:
&lt;source lang=&quot;fortran&quot;&gt;
 C     PROGRAM HELLO3
       WRITE (6,100)
       STOP
   100 FORMAT (11HHELLO WORLD)
       END
&lt;/source&gt;
One of the most surprising features was the behaviour of Hollerith edit descriptors when used for input.  The following program would change at run time &lt;code&gt;HELLO WORLD&lt;/code&gt; to whatever would happen to be the next eleven characters in the input stream and print that input:
&lt;source lang=&quot;fortran&quot;&gt;
 C     PROGRAM WHAT1
       READ (5,100)
       WRITE (6,100)
       STOP
   100 FORMAT (11HHELLO WORLD)
       END
&lt;/source&gt;

== Notes==
{{reflist|group=notes}}

==References==
{{reflist}}

[[Category:Fortran]]
[[Category:String data structures]]</text>
      <sha1>cgroiv9sj1h91zl6jzqoc3x83h1wkgh</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Netstring</title>
    <ns>0</ns>
    <id>16553961</id>
    <revision>
      <id>616634935</id>
      <parentid>578644278</parentid>
      <timestamp>2014-07-12T09:41:31Z</timestamp>
      <contributor>
        <username>Vadmium</username>
        <id>53292</id>
      </contributor>
      <comment>/* External links */ [[Category:String data structures]]</comment>
      <text xml:space="preserve" bytes="3695">In [[computer programming]], a '''netstring''' is a formatting method for [[String (computer science)|byte strings]] that uses a declarative notation to indicate the size of the string.&lt;ref&gt;defined in a document by D. J. Bernstein.&lt;/ref&gt;&lt;ref&gt;See e.g. Python Web Programming
By Steve Holden, David M. Beazley
Published by Sams Publishing, 2002
ISBN 0-7357-1090-2, 978-0-7357-1090-0
691 pages, page 202.&lt;/ref&gt;

Netstrings store the byte length of the data that follows, making it easier to unambiguously pass text and byte data between programs that could be sensitive to values that could be interpreted as [[delimiter]]s or terminators (such as a [[null character]]).

The format consists of the string's length written using ASCII digits, followed by a colon, the byte data, and a comma. &quot;Length&quot; in this context means &quot;number of 8-bit units&quot;, so if the string is, for example, encoded using [[UTF-8]], this may or may not be identical to the number of textual characters that are present in the string.

For example, the text &quot;hello world!&quot; encodes as:
 &lt;31 32 3a 68 65 6c 6c 6f 20 77 6f 72 6c 64 21 2c&gt;
i.e.
 12:hello world!,
And an empty string as:
 &lt;30 3a 2c&gt;
i.e.
 0:,
The comma makes it slightly simpler for humans to read netstrings that are used as adjacent records, and provides weak verification of correct parsing.
Note that without the comma, the format mirrors how [[Bencode]] encodes strings.

Since the format is easy to generate and to [[Parsing|parse]], it is easy to support by programs written in different programming languages. In practice, netstrings are often used to simplify exchange of bytestrings, or lists of bytestrings.
For example, see its use in the [[Simple Common Gateway Interface]] (SCGI) and the [[Quick Mail Queuing Protocol]] (QMQP) .

Netstrings avoid complications that arise in trying to embed arbitrary data in delimited formats. For example, XML may not contain certain byte values and requires a nontrivial combination of [[Escape character|escaping]] and [[Delimiter|delimiting]], while generating [[MIME#Multipart messages|multipart MIME messages]] involves choosing a delimiter that must not clash with the content of the data.

Netstrings can be stored recursively.  The result of encoding a sequence of strings is a single string.  Rewriting the above &quot;hello world!&quot; example to instead be a sequence of two netstrings, itself encoded as a single netstring, gives the following:
 17:5:hello,6:world!,,
Parsing such a nested netstring is an example of [[duck typing]], since the contained string (&quot;5:hello,6:world!,&quot;) is both a string and a sequence of netstrings.  Its effective type is determined by how the application chooses to interpret it, not by any explicit type declaration required by the netstring specification.  However, an application could use a [[tagged union]] convention to describe the types of nested netstrings, thereby establishing a [[self-describing]] hierarchical format.

Note that since netstrings pose no limitations on the contents of the data they store, netstrings can not be embedded verbatim in most delimited formats without the possibility of interfering with the delimiting of the containing format.

In the context of network programming it is potentially useful that the receiving program is informed of the size of the data that follows, as it can [[Memory allocation|allocate]] exactly enough memory and avoid the need for reallocation to accommodate more data.

==See also==
* [[Hollerith constant]]

== Notes and references ==
{{reflist}}

== External links ==
* http://cr.yp.to/proto/netstrings.txt
* http://wiki.tcl.tk/15074

[[Category:Data serialization formats]]
[[Category:String data structures]]</text>
      <sha1>g043drhv1r505t9gofirkkkbqy66blv</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Ukkonen's algorithm</title>
    <ns>0</ns>
    <id>4067031</id>
    <revision>
      <id>580381391</id>
      <parentid>580339991</parentid>
      <timestamp>2013-11-06T00:04:57Z</timestamp>
      <contributor>
        <ip>203.206.223.208</ip>
      </contributor>
      <comment>/* External links */</comment>
      <text xml:space="preserve" bytes="2582">In [[computer science]], '''Ukkonen's algorithm''' is a linear-time, [[online algorithm]] for constructing [[suffix tree]]s, proposed by [[Esko Ukkonen]] in 1995.&lt;ref&gt;{{cite doi|10.1007/BF01206331}}&lt;/ref&gt;

The algorithm begins with an implicit suffix tree containing the first character of the string. Then it steps through the string adding successive characters until the tree is complete. This order addition of characters gives Ukkonen's algorithm its &quot;on-line&quot; property. Earlier algorithms proceeded backward from the last character to the first one, let it be from the longest to the shortest suffix &lt;ref&gt;{{cite doi|10.1145/321941.321946}}&lt;/ref&gt; or from the shortest to the longest suffix.&lt;ref&gt;{{cite doi|10.1109/SWAT.1973.13}}&lt;/ref&gt; The naive implementation for generating a suffix tree requires [[Big O notation|''O''(''n''&lt;sup&gt;2&lt;/sup&gt;)]] or even [[Big O notation|''O''(''n''&lt;sup&gt;3&lt;/sup&gt;)]] time, where ''n'' is the length of the string.  By exploiting a number of algorithmic techniques, Ukkonen reduced this to ''O''(''n'') (linear) time, for constant-size alphabets, and ''O''(''n'' log ''n'') in general.

==References==
&lt;references/&gt;

==External links==
* Original Ukkonen's paper [http://www.cs.helsinki.fi/u/ukkonen/SuffixT1.pdf PDF] | [http://www.cs.helsinki.fi/u/ukkonen/SuffixT1withFigs.pdf PDF with figures]
* McCreight's paper in [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.8022&amp;rep=rep1&amp;type=pdf PDF]
* Weiner's paper in [http://airelles.i3s.unice.fr/files/Weiner.pdf PDF]
* [http://stackoverflow.com/a/9513423/414272 Detailed explanation in plain English]
* [http://marknelson.us/1996/08/01/suffix-trees/ Fast String Searching With Suffix Trees]  Mark Nelson's tutorial.  Has an implementation example written with C++.
* [https://github.com/maxgarfinkel/suffixTree Implementation in Java]
* [https://gist.github.com/2373868 Implementation in C#]
* [http://programmerspatch.blogspot.com.au/2013/02/ukkonens-suffix-tree-algorithm.html Implementation in C with detailed explanation]
* [http://www.cs.cmu.edu/~guyb/realworld/slidesF07/suffix.ppt Lecture slides by Guy Blelloch]
* [http://www.cs.helsinki.fi/u/ukkonen/  Ukkonen's homepage]
* [http://stackoverflow.com/questions/9452701/ukkonens-suffix-tree-algorithm-in-plain-english  Stackoverflow: Ukkonen's suffix tree algorithm in plain English?]
* [http://code.google.com/p/text-indexing/ Text-Indexing project]  (Ukkonen's linear-time construction of suffix trees)

[[Category:Bioinformatics algorithms]]
[[Category:Algorithms on strings]]
[[Category:Substring indices]]


{{comp-sci-stub}}</text>
      <sha1>9o5ul9i3jr88dckgdz5g3jmrx12dekh</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Inverted index</title>
    <ns>0</ns>
    <id>3125116</id>
    <revision>
      <id>623456093</id>
      <parentid>623455056</parentid>
      <timestamp>2014-08-30T15:15:30Z</timestamp>
      <contributor>
        <username>Flyer22</username>
        <id>4293477</id>
      </contributor>
      <minor/>
      <comment>Reverted 1 edit by [[Special:Contributions/101.63.235.236|101.63.235.236]] identified as test/vandalism using [[WP:STiki|STiki]]</comment>
      <text xml:space="preserve" bytes="8244">In [[computer science]], an '''inverted index''' (also referred to as '''postings file''' or '''inverted file''') is an [[index (database)|index data structure]] storing a mapping from content, such as words or numbers, to its locations in a [[Table (database)|database file]], or in a document or a set of documents.  The purpose of an inverted index is to allow fast [[full text search]]es, at a cost of increased processing when a document is added to the database.  The inverted file may be the database file itself, rather than its [[Index (database)|index]]. It is the most popular data structure used in [[document retrieval]] systems,&lt;ref&gt;{{Harvnb |Zobel|Moffat|Ramamohanarao|1998| Ref=none }}&lt;/ref&gt; used on a large scale for example in [[search engine]]s.   Several significant general-purpose [[Mainframe computer|mainframe]]-based [[database management systems]] have used inverted list architectures, including [[ADABAS]], [[DATACOM/DB]], and [[Model 204]].

There are two main variants of inverted indexes: A '''record level inverted index''' (or '''inverted file index''' or just '''inverted file''') contains a list of references to documents for each word. A '''word level inverted index''' (or '''full inverted index''' or '''inverted list''') additionally contains the positions of each word within a document.&lt;ref name=&quot;isbn0-201-39829-X-p192&quot;&gt;{{Harvnb |Baeza-Yates|Ribeiro-Neto|1999| p=192 | Ref=BYR99 }}&lt;/ref&gt; The latter form offers more functionality (like [[phrase search]]es), but needs more time and space to be created.

==Example==

Given the texts

 T[0] = &quot;it is what it is&quot;
 T[1] = &quot;what is it&quot;
 T[2] = &quot;it is a banana&quot;

we have the following inverted file index (where the integers in the set notation brackets refer to the indexes (or keys) of the text symbols, &lt;code&gt;T[0]&lt;/code&gt;, &lt;code&gt;T[1]&lt;/code&gt; etc.):

 &quot;a&quot;:      {2}
 &quot;banana&quot;: {2}
 &quot;is&quot;:     {0, 1, 2}
 &quot;it&quot;:     {0, 1, 2}
 &quot;what&quot;:   {0, 1}

A term search for the terms
&lt;code&gt;&quot;what&quot;&lt;/code&gt;, &lt;code&gt;&quot;is&quot;&lt;/code&gt; and &lt;code&gt;&quot;it&quot;&lt;/code&gt; would give the set
&lt;math&gt;\{0,1\} \cap \{0,1,2\} \cap \{0,1,2\} = \{0,1\}&lt;/math&gt;.

With the same texts, we get the following full inverted index, where the pairs are document numbers and local word numbers. Like the document numbers, local word numbers also begin with zero. So, &lt;code&gt;&quot;banana&quot;: {(2, 3)}&lt;/code&gt; means the word &quot;banana&quot; is in the third document (&lt;code&gt;T[2]&lt;/code&gt;), and it is the fourth word in that document (position 3).

 &quot;a&quot;:      {(2, 2)}
 &quot;banana&quot;: {(2, 3)}
 &quot;is&quot;:     {(0, 1), (0, 4), '''(1, 1)''', (2, 1)}
 &quot;it&quot;:     {(0, 0), (0, 3), '''(1, 2)''', (2, 0)} 
 &quot;what&quot;:   {(0, 2), '''(1, 0)'''}

If we run a phrase search for &lt;code&gt;&quot;what is it&quot;&lt;/code&gt; we get hits for all the words in both document 0 and 1. But the terms occur consecutively only in document 1.

==Applications==

The inverted index [[data structure]] is a central component of a typical [[Index (search engine)|search engine indexing algorithm]]. A goal of a search engine implementation is to optimize the speed of the query: find the documents where word X occurs. Once a [[Search engine indexing#The forward index|forward index]] is developed, which stores lists of words per document, it is next inverted to develop an inverted index. Querying the forward index would require sequential iteration through each document and to each word to verify a matching document. The time, memory, and processing resources to perform such a query are not always technically realistic.  Instead of listing the words per document in the forward index, the inverted index data structure is developed which lists the documents per word.

With the inverted index created, the query can now be resolved by jumping to the word id (via [[random access]]) in the inverted index.

In pre-computer times, [[Concordance (publishing)|concordances]] to important books were manually assembled.  These were effectively inverted indexes with a small amount of accompanying commentary that required a tremendous amount of effort to produce.

In bioinformatics, inverted indexes are very important in the [[sequence assembly]] of short fragments of sequenced DNA. One way to find the source of a fragment is to search for it against a reference DNA sequence. A small number of mismatches (due to differences between the sequenced DNA and reference DNA, or errors) can be accounted for by dividing the fragment into smaller fragments—at least one subfragment is likely to match the reference DNA sequence. The matching requires constructing an inverted index of all substrings of a certain length from the reference DNA sequence. Since the human DNA contains more than 3 billion base pairs, and we need to store a DNA substring for every index, and a 32-bit integer for index itself, the storage requirement for such an inverted index would probably be in the tens of gigabytes.

==See also==
* [[Index (search engine)]]
* [[Reverse index]]
* [[Vector space model]]

== Bibliography ==
*{{cite book |last= Knuth |first= D. E. |authorlink= Donald Knuth |title= [[The Art of Computer Programming]] |publisher= [[Addison-Wesley]] |edition= Third |year= 1997 |origyear= 1973 |location= [[Reading, Massachusetts]] |isbn= 0-201-89685-0 |ref= Knu97 |chapter= 6.5. Retrieval on Secondary Keys}}
*{{cite journal|last= Zobel |first= Justin |coauthors= Moffat, Alistair; Ramamohanarao, Kotagiri |date=December 1998 |title= Inverted files versus signature files for text indexing |journal= ACM Transactions on Database Systems |volume= 23 |issue= 4 |pages=pp. 453–490 |publisher= [[Association for Computing Machinery]] |location= New York |doi= 10.1145/296854.277632 |url= |accessdate= }}
*{{cite journal|last= Zobel |first= Justin RMIT University, Australia |coauthors= Moffat, Alistair The University of Melbourne, Australia |date=July 2006 |title= Inverted Files for Text Search Engines |journal= ACM Computing Surveys |volume= 38 |issue= 2 |pages= 6|publisher= [[Association for Computing Machinery]] |location= New York |doi= 10.1145/1132956.1132959 |url= |accessdate= }}
*{{cite book |last= Baeza-Yates | first = Ricardo |authorlink=Ricardo Baeza-Yates |author2=Ribeiro-Neto, Berthier |title= Modern information retrieval |publisher= Addison-Wesley Longman |location= [[Reading, Massachusetts]] |year= 1999 |isbn= 0-201-39829-X |oclc= |doi= |ref= BYR99 |page= 192 }}
*{{cite journal |last= Luk | first = Robert |author2=W. Lam | title=Efficient in-memory extensible inverted file | journal = Information Systems | volume = 32 | issue = 5 | pages = 733–754 | year = 2007 | doi = 10.1016/j.is.2006.06.001}}
*{{cite journal |last= Salton | first = Gerard |author2=Fox, Edward A. |author3=Wu, Harry  |title= Extended Boolean information retrieval |publisher= ACM |year= 1983
|journal = Commun. ACM |volume = 26 |issue = 11 |doi= 10.1145/182.358466 |pages= 1022 }}
*{{cite book |title=Information Retrieval: Implementing and Evaluating Search Engines  |url=http://www.ir.uwaterloo.ca/book/ |publisher=MIT Press |year=2010 |location=Cambridge, Massachusetts |isbn= 978-0-262-02651-2 |author8=Stefan B&amp;uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack}}

==References==
* {{Harvnb |Knuth|1997| pp=560&amp;ndash;563 of section 6.5: ''Retrieval on Secondary Keys'' | Ref= Knu97 }}
{{Reflist}}

==External links==
*[http://www.nist.gov/dads/HTML/invertedIndex.html NIST's Dictionary of Algorithms and Data Structures: inverted index]
*[http://mg4j.dsi.unimi.it Managing Gigabytes for Java] a free full-text search engine for large document collections written in Java.
*[http://lucene.apache.org/java/docs/ Lucene] - Apache Lucene is a full-featured text search engine library written in Java.
*[http://sphinxsearch.com/ Sphinx Search] - Open source high-performance, full-featured text search engine library used by craigslist and others employing an inverted index.
*[http://rosettacode.org/wiki/Inverted_Index Example implementations] on [[Rosetta Code]]
* [http://www.vision.caltech.edu/malaa/software/research/image-search/ Caltech Large Scale Image Search Toolbox]: a Matlab toolbox implementing Inverted File Bag-of-Words image search.

[[Category:Data management]]
[[Category:Search algorithms]]
[[Category:Database index techniques]]
[[Category:Substring indices]]</text>
      <sha1>58cthkvksk113m0oonn5tfolcat9n3q</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Document retrieval</title>
    <ns>0</ns>
    <id>731640</id>
    <revision>
      <id>619306508</id>
      <parentid>605588769</parentid>
      <timestamp>2014-07-31T16:06:34Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* Example: PubMed */Journal cites, added 1 DOI, added 1 PMC using [[Project:AWB|AWB]] (10331)</comment>
      <text xml:space="preserve" bytes="5743">'''Document retrieval''' is defined as the matching of some stated user query against a set of [[free-text]] records. These records could be any type of mainly [[natural language|unstructured text]], such as [[newspaper article]]s, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.

Document retrieval is sometimes referred to as, or as a branch of, '''Text Retrieval'''. Text retrieval is a branch of [[information retrieval]] where the information is stored primarily in the form of [[natural language|text]]. Text databases became decentralized thanks to the [[personal computer]] and the [[CD-ROM]]. Text retrieval is a critical area of study today, since it is the fundamental basis of all [[internet]] [[search engine]]s.

==Description==
Document retrieval systems find information to given criteria by matching text records (''documents'') against user queries, as opposed to [[expert system]]s that answer questions by [[Inference|inferring]] over a logical [[knowledge base|knowledge database]]. A document retrieval system consists of a database of documents, a [[classification algorithm]] to build a full text index, and a user interface to access the database.

A document retrieval system has two main tasks:
# Find relevant documents to user queries
# Evaluate the matching results and sort them according to relevance, using algorithms such as [[PageRank]].

Internet [[search engines]] are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using [[statistical]] or [[natural language processing]] techniques.

==Variations==
There are two main classes of indexing schemata for document retrieval systems: ''form based'' (or ''word based''), and ''content based'' indexing. The document classification scheme (or [[Search engine indexing|indexing algorithm]]) in use determines the nature of the document retrieval system.

===Form based===
Form based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A [[suffix tree]] algorithm is an example for form based indexing.

===Content based===
The content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an [[inverted index]] algorithm.

A ''signature file'' is a technique that creates a ''quick and dirty'' filter, for example a [[Bloom filter]], that will keep all the documents that match to the query and ''hopefully'' a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to [[inverted file]]s in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.

==Example: PubMed==
The [[PubMed]]&lt;ref&gt;{{cite journal |author=Kim W, Aronson AR, Wilbur WJ |title=Automatic MeSH term assignment and quality assessment |journal=Proc AMIA Symp |pages=319–23 |year=2001 |pmid=11825203 |pmc=2243528 }}
&lt;/ref&gt; form interface features the &quot;related articles&quot; search which works through a comparison of words from the documents' title, abstract, and [[Medical Subject Headings|MeSH]] terms using a word-weighted algorithm.&lt;ref&gt;{{cite web|url=https://www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.Computation_of_Related_Citati|title=Computation of Related Citations}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|journal=BMC Bioinformatics|date=Oct 30, 2007|volume=8|pages=423|pmid=17971238|title=PubMed related articles: a probabilistic topic-based model for content similarity|author=Lin J1, Wilbur WJ|doi=10.1186/1471-2105-8-423|pmc=2212667}}&lt;/ref&gt;

== See also ==

* [[Compound term processing]]
* [[Document classification]]
* [[Enterprise search]]
* [[Information retrieval]]
* [[Latent semantic indexing]]
* [[Search engine]]

== References ==

&lt;references/&gt;

==Further reading==
* {{cite journal|first1=Christos|last1=Faloutsos|first2=Stavros|last2=Christodoulakis|title=Signature files: An access method for documents and its analytical performance evaluation|journal=ACM Transactions on Information Systems (TOIS)|volume=2|issue=4|year=1984|pages=267–288|doi=10.1145/2275.357411}}
* {{cite journal|author=Justin Zobel, Alistair Moffat and Kotagiri Ramamohanarao|title=Inverted files versus signature files for text indexing|journal=ACM Transactions on Database Systems (TODS)|volume=23|issue=4|year=1998|pages= 453–490|url=http://www.cs.columbia.edu/~gravano/Qual/Papers/19%20-%20Inverted%20files%20versus%20signature%20files%20for%20text%20indexing.pdf|doi=10.1145/296854.277632}}
* {{cite journal|author=Ben Carterette and Fazli Can|title=Comparing inverted files and signature files for searching a large lexicon|journal=Information Processing and Management|volume= 41|issue=3|year=2005|pages= 613–633|url=http://www.users.miamioh.edu/canf/papers/ipm04b.pdf|doi=10.1016/j.ipm.2003.12.003}}

== External links ==
* [http://cir.dcs.uni-pannon.hu/cikkek/FINAL_DOMINICH.pdf Formal Foundation of Information Retrieval], Buckinghamshire Chilterns University College

[[Category:Information retrieval]]
[[Category:Electronic documents]]
[[Category:Substring indices]]

[[zh:文本信息检索]]</text>
      <sha1>lkocqxug0ux5ablwm7c7tpoxywz9s2u</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Factor oracle</title>
    <ns>0</ns>
    <id>33736410</id>
    <revision>
      <id>575677881</id>
      <parentid>466297883</parentid>
      <timestamp>2013-10-04T03:53:24Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>remove bogus defaultsort, wikify, fix categories, and stub sort</comment>
      <text xml:space="preserve" bytes="1639">A '''factor oracle''' is a [[finite state automaton]] that can efficiently search for factors ([[substring]]s) in a body of text.  Older techniques, such as [[suffix tree]]s, were time-efficient but required significant amounts of memory.  Factor oracles, by contrast, can be constructed in linear time and space in an incremental fashion.&lt;ref&gt;Allauzen C., Crochemore M., Raffinot M., ''[http://www.lsi.upc.edu/~marias/teaching/bom.pdf Factor oracle: a new structure for pattern matching]; Proceedings of SOFSEM’99; Theory and Practice of Informatics.''&lt;/ref&gt;

== Overview ==

Older techniques for matching strings include: [[suffix array]]s, [[suffix tree]]s, suffix [[Automata theory|automata]] or [[Directed acyclic word graph]]s, and factor automata (Allauzen, Crochemore, Raffinot, 1999).  In 1999, Allauzen, Crochemore, and Raffinot, presented the factor oracle algorithm as a memory efficient improvement upon these older techniques for string matching and compression.  Starting in the mid-2000s, factor oracles have found application in computer music, as well.&lt;ref&gt;Assayag G., Dubnov S., ''Using Factor Oracles for Machine Improvisation.''  Soft Computing - A Fusion of Foundations, Methodologies and Applications.  2004-09-01.  Springer Berlin / Heidelberg&lt;/ref&gt;

== Implementations ==
The [http://cosmal.ucsd.edu/cal/projects/CATbox/catboxdownload.htm Computer Audition Laboratory] provides a Matlab implementation of the factor oracle algorithm.

== See also ==

* [[Suffix array]]
* [[Generalised suffix tree]]

==References==
&lt;references /&gt;

[[Category:Automata theory]]
[[Category:Substring indices]]


{{algorithm-stub}}</text>
      <sha1>4k2q9yftil093kcmz1y0feqvcl0nrz1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>(a,b)-tree</title>
    <ns>0</ns>
    <id>4436119</id>
    <revision>
      <id>575757318</id>
      <parentid>566033012</parentid>
      <timestamp>2013-10-04T17:35:11Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>use defaultsort so that it is sorted better in the stub category as well as its real category</comment>
      <text xml:space="preserve" bytes="1408">In [[computer science]], an '''(a,b) tree''' is a specific kind of [[Tree traversal|search tree]].  

An (a,b) tree has all of its [[leaf node|leaves]] at the same depth, and all internal [[Node (computer science)|nodes]] except for the root have between &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; [[Child node|children]], where &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; are integers such that &lt;math&gt;2 \le a \le (b+1)/2&lt;/math&gt;. The root has, if it is not a leaf, between 2 and b children.

== Definition ==
Let &lt;math&gt;a, b \in \mathbb{N}&lt;/math&gt; such that &lt;math&gt;a \leq b&lt;/math&gt;. Then a tree ''T'' is an '''(a,b) tree''' when:
* Every inner node except the root has at least &lt;math&gt;a&lt;/math&gt; and maximally &lt;math&gt;b&lt;/math&gt; child nodes.
* Root has maximally &lt;math&gt;b&lt;/math&gt; child nodes.
* All paths from the root to the leaves are of the same length.

== Inner node representation ==
Every inner node &lt;math&gt;v&lt;/math&gt; has the following representation:
* Let &lt;math&gt;\rho_v&lt;/math&gt; be the number of child nodes of node v.
* Let &lt;math&gt;S_v[1 \dots \rho_v]&lt;/math&gt; be pointers to child nodes.
* Let &lt;math&gt;H_v[1 \dots \rho_v - 1]&lt;/math&gt; be an array of keys such that &lt;math&gt;H_v[i]&lt;/math&gt; equals the largest key in the subtree pointed to by &lt;math&gt;S_v[i]&lt;/math&gt;.

==See also==
*[[B-tree]]
*[[2-3 tree]]

==References==
* {{DADS|(a,b)-tree|abtree}}

{{CS-Trees}}

{{DEFAULTSORT:A,b tree}}
[[Category:Trees (data structures)]]

{{datastructure-stub}}</text>
      <sha1>i951663mn8d6jdr3etvovx6ig7unjlr</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>AA tree</title>
    <ns>0</ns>
    <id>1665969</id>
    <revision>
      <id>582467170</id>
      <parentid>551735616</parentid>
      <timestamp>2013-11-20T02:30:22Z</timestamp>
      <contributor>
        <username>Lurlock</username>
        <id>1176200</id>
      </contributor>
      <comment>/* Deletion */ Second skew() step is unnecessary (and may crash depending on language) if right tree is nil.</comment>
      <text xml:space="preserve" bytes="11283">{{refimprove|date=June 2011}}

An '''AA tree''' in [[computer science]] is a form of [[Self-balancing binary search tree|balanced tree]] used for storing and retrieving ordered data efficiently. AA trees are named for [[Arne Andersson (computer science)|Arne Andersson]], their inventor.

AA trees are a variation of the [[red-black tree]], which in turn is an enhancement to the [[binary search tree]]. Unlike red-black trees, red nodes on an AA tree can only be added as a right subchild. In other words, no red node can be a left sub-child.  This results in the simulation of a [[2-3 tree]] instead of a [[2-3-4 tree]], which greatly simplifies the maintenance operations. The maintenance algorithms for a red-black tree need to consider seven different shapes to properly balance the tree:

[[Image:Red Black Shape Cases.svg]]

An AA tree on the other hand only needs to consider two shapes due to the strict requirement that only right links can be red:

[[Image:AA Tree Shape Cases.svg]]

== Balancing rotations ==

Whereas red-black trees require one bit of balancing metadata per node (the color), AA trees require O(log(N)) bits of metadata per node, in the form of an integer &quot;level&quot;.  The following invariants hold for AA trees:

# The level of every leaf node is one.
# The level of every left child is exactly one less than that of its parent.
# The level of every right child is equal to or one less than that of its parent.
# The level of every right grandchild is strictly less than that of its grandparent.
# Every node of level greater than one has two children.

A link where the child's level is equal to that of its parent is called a ''horizontal'' link, and is analogous to a red link in the red-black tree.  Individual right horizontal links are allowed, but consecutive ones are forbidden; all left horizontal links are forbidden.  These are more restrictive constraints than the analogous ones on red-black trees, with the result that re-balancing an AA tree is procedurally much simpler than re-balancing a red-black tree.

Insertions and deletions may transiently cause an AA tree to become unbalanced (that is, to violate the AA tree invariants).  Only two distinct operations are needed for restoring balance: &quot;skew&quot; and &quot;split&quot;. Skew is a right rotation to replace a subtree containing a left horizontal link with one containing a right horizontal link instead. Split is a left rotation and level increase to replace a subtree containing two or more consecutive right horizontal links with one containing two fewer consecutive right horizontal links.  Implementation of balance-preserving insertion and deletion is simplified by relying on the skew and split operations to modify the tree only if needed, instead of making their callers decide whether to skew or split.

 '''function''' skew '''is'''
     '''input:''' T, a node representing an AA tree that needs to be rebalanced.
     '''output:''' Another node representing the rebalanced AA tree.
 
     '''if''' nil(T) '''then'''
         '''return''' Nil
     '''else if''' nil(left(T)) '''then'''
         '''return''' T
     '''else if''' level(left(T)) == level(T) '''then'''
         ''Swap the pointers of horizontal left links.''
         L = left(T)
         left(T) := right(L)
         right(L) := T
         '''return''' L
     '''else'''
         '''return''' T
     '''end if'''
 '''end function'''

Skew: [[Image:AA Tree Skew2.svg]]

 '''function''' split '''is'''
     '''input:''' T, a node representing an AA tree that needs to be rebalanced.
     '''output:''' Another node representing the rebalanced AA tree.
 
     '''if''' nil(T) '''then'''
         '''return''' Nil
     '''else if''' nil(right(T)) '''or ''' nil(right(right(T))) '''then'''
         '''return''' T
     '''else if''' level(T) == level(right(right(T))) '''then'''
         ''We have two horizontal right links.  Take the middle node, elevate it, and return it.''
         R = right(T)
         right(T) := left(R)
         left(R) := T
         level(R) := level(R) + 1
         '''return''' R
     '''else'''
         '''return''' T
     '''end if'''
 '''end function'''

Split: [[Image:AA Tree Split2.svg]]

== Insertion ==

Insertion begins with the normal binary tree search and insertion procedure.  Then, as the call stack unwinds (assuming a recursive implementation of the search), it's easy to check the validity of the tree and perform any rotations as necessary.  If a horizontal left link arises, a skew will be performed, and if two horizontal right links arise, a split will be performed, possibly incrementing the level of the new root node of the current subtree.  Note, in the code as given above, the increment of level(T).  This makes it necessary to continue checking the validity of the tree as the modifications bubble up from the leaves.

 '''function''' insert '''is'''
     '''input:''' X, the value to be inserted, and T, the root of the tree to insert it into.
     '''output:''' A balanced version T including X.
 
     ''Do the normal binary tree insertion procedure. Set the result of the''
     ''recursive call to the correct child in case a new node was created or the''
     ''root of the subtree changes.''
     '''if''' nil(T) '''then'''
         ''Create a new leaf node with X.''
         '''return''' node(X, 1, Nil, Nil)
     '''else if''' X &lt; value(T) '''then'''
         left(T) := insert(X, left(T))
     '''else if''' X &gt; value(T) '''then'''
         right(T) := insert(X, right(T))
     '''end if'''
     ''Note that the case of X == value(T) is unspecified. As given, an insert''
     ''will have no effect. The implementor may desire different behavior.''
 
     ''Perform skew and then split. The conditionals that determine whether or
     ''not a rotation will occur or not are inside of the procedures, as given''
     ''above.''
     T := skew(T)
     T := split(T)
 
     '''return T'''
 '''end function'''

== Deletion ==

As in most balanced binary trees, the deletion of an internal node can be turned into the deletion of a leaf node by swapping the internal node with either its closest predecessor or successor, depending on which are in the tree or on the implementor's whims.  Retrieving a predecessor is simply a matter of following one left link and then all of the remaining right links.  Similarly, the successor can be found by going right once and left until a null pointer is found.  Because of the AA property of all nodes of level greater than one having two children, the successor or predecessor node will be in level 1, making their removal trivial.

To re-balance a tree, there are a few approaches.  The one described by Andersson in his [http://user.it.uu.se/~arnea/abs/simp.html original paper] is the simplest, and it is described here, although actual implementations may opt for a more optimized approach.  After a removal, the first step to maintaining tree validity is to lower the level of any nodes whose children are two levels below them, or who are missing children.  Then, the entire level must be skewed and split.  This approach was favored, because when laid down conceptually, it has three easily understood separate steps:

# Decrease the level, if appropriate.
# Skew the level.
# Split the level.

However, we have to skew and split the entire level this time instead of just a node, complicating our code.

 '''function''' delete '''is'''
     '''input:''' X, the value to delete, and T, the root of the tree from which it should be deleted.
     '''output:''' T, balanced, without the value X.
    
     '''if''' nil(T) '''then'''
         return T
     '''else if''' X &gt; value(T) '''then'''
         right(T) := delete(X, right(T))
     '''else if''' X &lt; value(T) '''then'''
         left(T) := delete(X, left(T))
     '''else'''
         ''If we're a leaf, easy, otherwise reduce to leaf case.'' 
         '''if''' leaf(T) '''then'''
             return Nil
         '''else if''' nil(left(T)) '''then'''
             L := successor(T)
             right(T) := delete(value(L), right(T))
             value(T) := value(L)
         '''else'''
             L := predecessor(T)
             left(T) := delete(value(L), left(T))
             value(T) := value(L)
         '''end if'''
     '''end if'''
 
     ''Rebalance the tree. Decrease the level of all nodes in this level if
     necessary, and then skew and split all nodes in the new level.''
     T := decrease_level(T)
     T := skew(T)
     right(T) := skew(right(T))
     '''if not''' nil(right(T))
         right(right(T)) := skew(right(right(T)))
     '''end if'''
     T := split(T)
     right(T) := split(right(T))
     return T
 '''end function'''

 '''function''' decrease_level '''is'''
     '''input:''' T, a tree for which we want to remove links that skip levels.
     '''output:''' T with its level decreased.
 
     should_be = min(level(left(T)), level(right(T))) + 1
     '''if''' should_be &lt; level(T) '''then'''
         level(T) := should_be
         '''if''' should_be &lt; level(right(T)) '''then'''
             level(right(T)) := should_be
         '''end if'''
     '''end if'''
     return T
 '''end function'''

A good example of deletion by this algorithm is present in the [http://user.it.uu.se/~arnea/abs/simp.html Andersson paper].

== Performance ==

The performance of an AA tree is equivalent to the performance of a red-black tree. While an AA tree makes more rotations than a red-black tree, the simpler algorithms tend to be faster, and all of this balances out to result in similar performance. A red-black tree is more consistent in its performance than an AA tree, but an AA tree tends to be flatter, which results in slightly faster search times.&lt;ref&gt;{{cite web|url=http://www.cepis.org/upgrade/files/full-2004-V.pdf|title=A Disquisition on The Performance Behavior of Binary Search Tree Data Structures (pages 67-75)}}&lt;/ref&gt; 

== See also ==
* [[Red-black tree]]
* [[B-tree]]
* [[AVL tree]]
* [[Scapegoat tree]]

== References ==
{{Reflist}}

==External links==
*[http://user.it.uu.se/~arnea/abs/simp.html A. Andersson. Balanced search trees made simple]
*[http://user.it.uu.se/~arnea/abs/searchproc.html A. Andersson. A note on searching in a binary search tree]
*[http://people.ksp.sk/~kuko/bak/index.html AA-Tree Applet] by Kubo Kovac
*[http://bitbucket.org/trijezdci/bstlib/src/ BSTlib] - Open source AA tree library for C by trijezdci
*[http://www.softpedia.com/get/Others/Home-Education/AA-Visual-2007.shtml AA Visual 2007 1.5 - OpenSource Delphi program for educating AA tree structures]
*[http://www.eternallyconfuzzled.com/tuts/datastructures/jsw_tut_andersson.aspx Thorough tutorial] Julienne Walker with lots of code, including a practical implementation
*[http://www.cs.fiu.edu/~weiss/dsaa_c++3/code/ Object Oriented implementation with tests] 
*[http://www.cepis.org/upgrade/files/full-2004-V.pdf A Disquisition on The Performance Behavior of Binary Search Tree Data Structures (pages 67-75)] - Comparison of AA trees, red-black trees, treaps, skip lists, and radix trees
*[http://www.rational.co.za/aatree.c An example C implementation]
*[http://code.google.com/p/objc-aatree An Objective-C implementation]

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Aa Tree}}
[[Category:Trees (data structures)]]</text>
      <sha1>7i07i6uxitrfqzwi1xbtr034jj0687b</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Abstract syntax tree</title>
    <ns>0</ns>
    <id>75629</id>
    <revision>
      <id>621051205</id>
      <parentid>621050263</parentid>
      <timestamp>2014-08-13T12:15:33Z</timestamp>
      <contributor>
        <ip>141.52.45.117</ip>
      </contributor>
      <text xml:space="preserve" bytes="10507">{{For|the trees used in linguistics|Concrete syntax tree}}
{{no footnotes|date=February 2013}}
[[File:Abstract syntax tree for Euclidean algorithm.svg|thumb|400px|An abstract syntax tree for the following code for the [[Euclidean algorithm]]:&lt;br/&gt;
&lt;dl&gt;
&lt;dd&gt;'''while''' b ≠ 0&lt;dl&gt;
&lt;dd&gt;'''if''' a &gt; b&lt;dl&gt;
&lt;dd&gt;a := a − b&lt;/dd&gt;&lt;/dl&gt;&lt;/dd&gt;
&lt;dd&gt;'''else'''&lt;dl&gt;
&lt;dd&gt;b := b − a&lt;/dd&gt;&lt;/dl&gt;&lt;/dd&gt;&lt;/dl&gt;&lt;/dd&gt;
&lt;dd&gt;'''return''' a&lt;/dd&gt;
&lt;/dl&gt;
]]
In [[computer science]], an '''abstract syntax tree''' ('''AST'''), or just '''syntax tree''', is a [[directed tree|tree]] representation of the [[abstract syntax|abstract syntactic]] structure of [[source code]] written in a [[programming language]]. Each node of the tree denotes a construct occurring in the source code. The syntax is &quot;abstract&quot; in not representing every detail appearing in the real syntax. For instance, grouping [[Bracket#Parentheses|parentheses]] are implicit in the tree structure, and a syntactic construct like an if-condition-then expression may be denoted by means of a single node with three branches.

This distinguishes abstract syntax trees from [[concrete syntax tree]]s, traditionally designated [[parse tree]]s, which are often built by a [[parser]] during the source code translation and [[compiler|compiling]] process. Once built, additional information is added to the AST by means of subsequent processing, e.g., [[Semantic analysis (compilers)|contextual analysis]].

Abstract syntax trees are also used in program analysis and [[program transformation]] systems.

== Application in compilers ==

Abstract syntax trees are [[data structures]] widely used in [[compilers]], due to their property of representing the structure of program code. An AST is usually the result of the [[syntax analysis]] phase of a compiler. It often serves as an intermediate representation of the program through several stages that the compiler requires, and has a strong impact on the final output of the compiler.

=== Motivation ===

Being the product of the [[syntax analysis]] phase of a compiler, the AST has several properties that are invaluable to the further steps of the compilation process.
* Compared to the [[source code]], an AST does not include certain elements, such as inessential punctuation and delimiters (braces, semicolons, parentheses, etc.).
* A more important difference is that the AST can be edited and enhanced with properties and annotations for every element it contains. Such editing and annotation is impossible with the source code of a program, since it would imply changing it.
* At the same time, an AST usually contains extra information about the program, due to the consecutive stages of analysis by the compiler. A simple example of the additional information present in an AST is the position of an element in the source code. This information is used in case of an error in the code, to notify the user of the location of the error.

ASTs are needed because of the inherent nature of programming languages and their documentation. Languages are often ambiguous by nature. In order to avoid this ambiguity, programming languages are often specified as a [[context free grammar]] (CFG). However, there are often aspects of programming languages that a CFG can't express, but are part of the language and are documented in its specification. These are details that require a context to determine their validity and behaviour. For example, if a language allows new types to be declared, a CFG cannot predict the names of such types nor the way in which they should be used. Even if a language has a predefined set of types, enforcing proper usage usually requires some context. Another example is [[duck typing]], where the type of an element can change depending on context. [[Operator overloading]] is yet another case where correct usage and final function are determined based on the context. Java provides an excellent example, where the '+' operator is both numerical addition and concatenation of strings.

Although there are other [[data structure]]s involved in the inner workings of a compiler, the AST performs a unique function. During the first stage, the [[syntax analysis]] stage, a compiler produces a parse tree. This parse tree can be used to perform almost all functions of a compiler by means of syntax-directed translation. Although this method can lead to a more efficient compiler, it goes against the software engineering principles of writing and maintaining programs. Another advantage that the AST has over a parse tree is the size, particularly the smaller height of the AST and the smaller number of elements.

=== Design ===

The design of an AST is often closely linked with the design of a compiler and its expected features.

Core requirements include the following:

* Variable types must be preserved, as well as the location of each declaration in source code.
* The order of executable statements must be explicitly represented and well defined.
* Left and right components of binary operations must be stored and correctly identified.
* Identifiers and their assigned values must be stored for assignment statements.

These requirements can be used to design the data structure for the AST.

Some operations will always require two elements, such as the two terms for addition. However, some language constructs require an arbitrarily large number of children, such as argument lists passed to programs from the [[command shell]]. As a result, an AST has to also be flexible enough to allow for quick addition of an unknown quantity of children.

Another major design requirement for an AST is that it should be possible to unparse an AST into source code form. The source code produced should be sufficiently similar to the original in appearance and identical in execution, upon recompilation.

=== Design patterns ===

Due to the complexity of the requirements for an AST and the overall complexity of a compiler, it is beneficial to apply sound software development principles. One of these is to use proven design patterns to enhance modularity and ease of development.

Different operations don't necessarily have different types, so it is important to have a sound node class hierarchy. This is crucial in the creation and the modification of the AST as the compiler progresses.

Because the compiler traverses the tree several times to determine syntactic correctness, it is important to make traversing the tree a simple operation. The compiler executes a specific set of operations, depending on the type of each node, upon reaching it, so it makes sense to use the [[Visitor pattern]].

=== Usage ===

The AST is used intensively during [[Semantic analysis (compilers)|semantic analysis]], where the compiler checks for correct usage of the elements of the program and the language. The compiler also generates symbol tables based on the AST during semantic analysis. A complete traversal of the tree allows verification of the correctness of the program.

After verifying correctness, the AST serves as the base for code generation. The AST is often used to generate the 'intermediate representation' '(IR)', sometimes called an [[intermediate language]], for the code generation.

== See also ==
* [[Abstract semantic graph]] (ASG)
* [[Composite pattern]]
* [[Document Object Model]] (DOM)
* [[Extended Backus–Naur Form]]
* [[Lisp (programming language)|Lisp]], a family of languages written in trees, with macros to manipulate code trees at compile time
* [[Semantic resolution tree]] (RST)
* [[Shunting yard algorithm]]
* [[Symbol table]]
* [[TreeDL]]
* [[Term graph]]

== References ==
* {{FOLDOC}}

== External links ==
* [http://www.eclipse.org/jdt/ui/astview/index.php AST View]: an [[Eclipse (software)|Eclipse]] plugin to [[Scientific visualization|visualize]] a [[Java (programming language)|Java]] abstract syntax tree
* {{cite web|url=http://www.eclipse.org/articles/Article-JavaCodeManipulation_AST/index.html|title=Good information about the Eclipse AST and Java Code Manipulation|work=eclipse.org}}
* {{SourceForge|pmd|PMD}}: uses AST representation to control code source quality
* {{cite web|url=http://www.cs.utah.edu/flux/flick/current/doc/guts/gutsch6.html|title=CAST representation|work=cs.utah.edu}}
* [http://eli-project.sourceforge.net/elionline/idem_3.html eli project]: Abstract Syntax Tree [[Unparsing]]
; Papers
* {{cite paper | url = http://www.hillside.net/plop/plop2003/Papers/Jones-ImplementingASTs.pdf | title = Abstract Syntax Tree Implementation Idioms | first = Joel | last = Jones }} (overview of AST implementation in various language families)
* {{cite paper | title = Understanding source code evolution using abstract syntax tree matching | id = {{citeseerx|10.1.1.88.5815}} | first1 = Iulian | last1 = Neamtiu | authorlink1 = Iulian Neamtiu | first2 = Jeffrey S. | last2 = Foster | authorlink2 = Jeffrey S. Foster | first3 = Michael | last3 = Hicks }}
* {{cite paper | url = http://www.semanticdesigns.com/Company/Publications/ICSM98.pdf | title = Clone Detection Using Abstract Syntax Trees | first = Ira D. | last = Baxter | author2 = et al. }}
* {{cite paper | url = http://seal.ifi.uzh.ch/fileadmin/User_Filemount/Publications/fluri-changedistilling.pdf | title = Change Distilling: Tree Differencing for Fine-Grained Source Code Change Extraction | first1 = Beat | last1 = Fluri | authorlink1 = Beat Fluri | first2 = Michael | last2 = Würsch | authorlink2 = Michael Würsch | first3 = Martin | last3 = Pinzger | authorlink3 = Martin Pinzger | first4 = Harald C. | last4 = Gall | authorlink4 = Harald C. Gall }}
* {{cite thesis | degree = Diploma | url = http://seal.ifi.unizh.ch/137/ | title = Improving Abstract Syntax Tree based Source Code Change Detection | first = Michael | last = Würsch | authorlink = Michael Würsch }}
* {{cite web | url = http://blogs.msdn.com/vcblog/archive/2006/08/16/702823.aspx | title = Thoughts on the Visual C++ Abstract Syntax Tree (AST) | first = Jason | last = Lucas | authorlink = Jason Lucas }}
; Tutorial
* {{cite web | url = http://www.omg.org/news/meetings/workshops/ADM_2005_Proceedings_FINAL/T-3_Newcomb.pdf | title = Abstract Syntax Tree Metamodel Standard }}
; Standard(s)
* {{cite web | url = http://www.omg.org/spec/ASTM/ | title = Architecture‑Driven Modernization — ADM: Abstract Syntax Tree Metmodel — ASTM }} ([[Object Management Group|OMG]] standard).

{{DEFAULTSORT:Abstract Syntax Tree}}
[[Category:Trees (data structures)]]
[[Category:Formal languages]]</text>
      <sha1>sffziwqzs0unmvlzd8n6mhz9swybw4p</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Adaptive k-d tree</title>
    <ns>0</ns>
    <id>4436211</id>
    <revision>
      <id>577755646</id>
      <parentid>471784172</parentid>
      <timestamp>2013-10-18T19:48:54Z</timestamp>
      <contributor>
        <ip>152.7.224.6</ip>
      </contributor>
      <comment>Change google books reference to English &quot;.com&quot; domain, instead of &quot;.dk&quot;</comment>
      <text xml:space="preserve" bytes="553">An '''adaptive [[k-d tree]]''' is a [[Tree (data structure)|tree]] for multidimensional points where successive levels may be split along different dimensions.

== References ==
{{Refbegin}}
* {{cite book|last=Samet|first=Hanan|title=Foundations of multidimensional and metric data structures|year=2006|publisher=Morgan Kaufmann|isbn=978-0-12-369446-1|url=http://books.google.com/books?id=KrQdmLjTSaQC}}
{{Refend}}


{{DADS|Adaptive k-d tree|adaptkdtree}}

[[Category:Trees (data structures)]]
[[Category:Geometric data structures]]

{{compu-prog-stub}}</text>
      <sha1>9gvn1qx0y8wn0k9tj1tj148d4dnzwhk</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>B-tree</title>
    <ns>0</ns>
    <id>4674</id>
    <revision>
      <id>625791739</id>
      <parentid>625726516</parentid>
      <timestamp>2014-09-16T10:09:09Z</timestamp>
      <contributor>
        <username>CiaPan</username>
        <id>258338</id>
      </contributor>
      <minor/>
      <comment>/* Variants */ link to [[B*]]</comment>
      <text xml:space="preserve" bytes="41694">{{distinguish|Binary tree}}
{{Infobox data structure
|name=B-tree
|type=tree
|invented_by=[[Rudolf Bayer]], [[Edward M. McCreight]]
|invented_year=1972
|
|space_avg= O(''n'')
|space_worst= O(''n'')
|search_avg= O(log ''n'')
|search_worst= O(log ''n'')
|insert_avg= O(log ''n'')
|insert_worst= O(log ''n'')
|delete_avg= O(log ''n'')
|delete_worst= O(log ''n'')
}}

In [[computer science]], a '''B-tree''' is a [[tree data structure]] that keeps data sorted and allows searches, sequential access, insertions, and deletions in [[logarithmic time]]. The B-tree is a generalization of a [[binary search tree]] in that a node can have more than two children {{Harv|Comer|1979|p=123}}. Unlike [[self-balancing binary search tree]]s, the B-tree is optimized for systems that read and write large blocks of data. It is commonly used in [[database]]s and [[filesystem]]s.

==Overview==
[[File:B-tree.svg|thumb|400px|right|A B-tree of order 2 {{Harv|Bayer|McCreight|1972}} or order 5 {{Harv|Knuth|1998}}.]]
In B-trees, internal ([[Leaf node|non-leaf]]) nodes can have a variable number of child nodes within some pre-defined range. When data is inserted or removed from a node, its number of child nodes changes. In order to maintain the pre-defined range, internal nodes may be joined or split. Because a range of child nodes is permitted, B-trees do not need re-balancing as frequently as other self-balancing search trees, but may waste some space, since nodes are not entirely full. The lower and upper bounds on the number of child nodes are typically fixed for a particular implementation. For example, in a [[2-3 tree|2-3 B-tree]] (often simply referred to as a '''2-3 tree'''), each internal node may have only 2 or 3 child nodes.

Each internal node of a B-tree will contain a number of [[Unique key|keys]]. The keys act as separation values which divide its [[subtree]]s. For example, if an internal node has 3 child nodes (or subtrees) then it must have 2 keys: ''a''&lt;sub&gt;1&lt;/sub&gt; and ''a''&lt;sub&gt;2&lt;/sub&gt;. All values in the leftmost subtree will be less than ''a''&lt;sub&gt;1&lt;/sub&gt;, all values in the middle subtree will be between ''a''&lt;sub&gt;1&lt;/sub&gt; and ''a''&lt;sub&gt;2&lt;/sub&gt;, and all values in the rightmost subtree will be greater than ''a''&lt;sub&gt;2&lt;/sub&gt;.

Usually, the number of keys is chosen to vary between &lt;math&gt;d&lt;/math&gt; and &lt;math&gt;2d&lt;/math&gt;, where &lt;math&gt;d&lt;/math&gt; is the minimum number of keys, and &lt;math&gt;d+1&lt;/math&gt; is the minimum [[Outdegree#Indegree and outdegree|degree]] or [[branching factor]] of the tree. In practice, the keys take up the most space in a node. The factor of 2 will guarantee that nodes can be split or combined. If an internal node has &lt;math&gt;2d&lt;/math&gt; keys, then adding a key to that node can be accomplished by splitting the &lt;math&gt;2d&lt;/math&gt; key node into two &lt;math&gt;d&lt;/math&gt; key nodes and adding the key to the parent node. Each split node has the required minimum number of keys. Similarly, if an internal node and its neighbor each have &lt;math&gt;d&lt;/math&gt; keys, then a key may be deleted from the internal node by combining with its neighbor. Deleting the key would make the internal node have &lt;math&gt;d-1&lt;/math&gt; keys; joining the neighbor would add &lt;math&gt;d&lt;/math&gt; keys plus one more key brought down from the neighbor's parent. The result is an entirely full node of &lt;math&gt;2d&lt;/math&gt; keys.

The number of branches (or child nodes) from a node will be one more than the number of keys stored in the node. In a 2-3 B-tree, the internal nodes will store either one key (with two child nodes) or two keys (with three child nodes). A B-tree is sometimes described with the parameters &lt;math&gt;(d+1)&lt;/math&gt; — &lt;math&gt;(2d+1)&lt;/math&gt; or simply with the highest branching order, &lt;math&gt;(2d+1)&lt;/math&gt;.

A B-tree is kept balanced by requiring that all leaf nodes be at the same depth. This depth will increase slowly as elements are added to the tree, but an increase in the overall depth is infrequent, and results in all leaf nodes being one more node farther away from the root.

B-trees have substantial advantages over alternative implementations when otherwise the time to access the data of a node greatly exceeds the time spent processing that data, because then the cost of accessing the node may be amortized over multiple operations within the node. This usually occurs when the node data are in [[secondary storage]] such as [[hard drive|disk drives]]. By maximizing the number of keys within each [[internal node]], the height of the tree decreases and the number of expensive node accesses is reduced. In addition, rebalancing of the tree occurs less often. The maximum number of child nodes depends on the information that must be stored for each child node and the size of a full [[Block (data storage)|disk block]] or an analogous size in secondary storage. While 2-3 B-trees are easier to explain, practical B-trees using secondary storage need a large number of child nodes to improve performance.

===Variants===
The term '''B-tree''' may refer to a specific design or it may refer to a general class of designs. In the narrow sense, a B-tree stores keys in its internal nodes but need not store those keys in the records at the leaves. The general class includes variations such as the [[B+ tree]] and the [[B*]].
* In the B+ tree, copies of the keys are stored in the internal nodes; the keys and records are stored in leaves; in addition, a leaf node may include a pointer to the next leaf node to speed sequential access {{Harv|Comer|1979|p=129}}.
* The B&lt;sup&gt;*&lt;/sup&gt;-tree balances more neighboring internal nodes to keep the internal nodes more densely packed {{Harv|Comer|1979|p=129}}. This variant requires non-root nodes to be at least 2/3 full instead of 1/2 {{Harv|Knuth|1998|p=488}}. To maintain this, instead of immediately splitting up a node when it gets full, its keys are shared with a node next to it. When both nodes are full, then the two nodes are split into three.  Deleting nodes is somewhat more complex than inserting however.
* B-trees can be turned into [[order statistic tree]]s to allow rapid searches for the Nth record in key order, or counting the number of records between any two records, and various other related operations.&lt;ref&gt;[http://www.chiark.greenend.org.uk/~sgtatham/algorithms/cbtree.html Counted B-Trees], retrieved 2010-01-25&lt;/ref&gt;

===Etymology===
[[Rudolf Bayer]] and [[Edward M. McCreight|Ed McCreight]] invented the B-tree while working at [[Boeing|Boeing Research Labs]] in 1971 {{Harv|Bayer|McCreight|1972}}, but they did not explain what, if anything, the ''B'' stands for. [[Douglas Comer]] explains:
&lt;blockquote&gt;The origin of &quot;B-tree&quot; has never been explained by the authors. As we shall see, &quot;balanced,&quot; &quot;broad,&quot; or &quot;bushy&quot; might apply. Others suggest that the &quot;B&quot; stands for Boeing. Because of his contributions, however, it seems appropriate to think of B-trees as &quot;Bayer&quot;-trees. {{Harv|Comer|1979|p=123 footnote 1}}&lt;/blockquote&gt;

[[Donald Knuth]] speculates on the etymology of B-trees in his May, 1980 lecture on the topic &quot;CS144C classroom lecture about disk storage and B-trees&quot;, suggesting the &quot;B&quot; may have originated from Boeing or from Bayer's name.&lt;ref&gt;[http://scpd.stanford.edu/knuth/index.jsp Knuth's video lectures from Stanford]&lt;/ref&gt;

After a talk at CPM 2013 (24th Annual Symposium on Combinatorial Pattern Matching, Bad Herrenalb, Germany, June 17–19, 2013), [[Edward M. McCreight|Ed McCreight]] answered a question on B-tree's name by Martin Farach-Colton saying: &quot;Bayer and I were in a lunch time where we get to think a name.  And we were, so, B, we were thinking… B is, you know… We were working for Boeing at the time, we couldn't use the name without talking to lawyers.  So, there is a B.  It has to do with balance, another B.  Bayer was the senior author, who did have several years older than I am and had many more publications than I did. So there is another B.  And so, at the lunch table we never did resolve whether there was one of those that made more sense than the rest.  What really lives to say is: the more you think about what the B in B-trees means, the better you understand B-trees.&quot;&lt;ref&gt;[http://vimeo.com/channels/574784 Talk's video], retrieved 2014-01-17&lt;/ref&gt;

==The database problem==
This section describes a problem faced by database designers, outlines a series of increasingly effective solutions to the problem, and ends by describing how the B-tree solves the problem completely.

===Time to search a sorted file===
Usually, sorting and searching algorithms have been characterized by the number of comparison operations that must be performed using [[Big O notation|order notation]]. A [[binary search]] of a sorted table with &lt;math&gt;N&lt;/math&gt; records, for example, can be done in roughly &lt;math&gt;\lceil \log_2 N \rceil&lt;/math&gt; comparisons. If the table had 1,000,000 records, then a specific record could be located with at most 20 comparisons: &lt;math&gt;\lceil \log_2 (1,000,000) \rceil = 20 &lt;/math&gt;.

Large databases have historically been kept on disk drives. The time to read a record on a disk drive far exceeds the time needed to compare keys once the record is available. The time to read a record from a disk drive involves a [[seek time]] and a rotational delay. The seek time may be 0 to 20 or more milliseconds, and the rotational delay averages about half the rotation period. For a 7200 RPM drive, the rotation period is 8.33 milliseconds. For a drive such as the Seagate ST3500320NS, the track-to-track seek time is 0.8 milliseconds and the average reading seek time is 8.5 milliseconds.&lt;ref&gt;Seagate Technology LLC, Product Manual: Barracuda ES.2 Serial ATA, Rev. F., publication 100468393, 2008 [http://www.seagate.com/staticfiles/support/disc/manuals/NL35%20Series%20&amp;%20BC%20ES%20Series/Barracuda%20ES.2%20Series/100468393f.pdf], page 6&lt;/ref&gt; For simplicity, assume reading from disk takes about 10 milliseconds.

Naively, then, the time to locate one record out of a million would take 20 disk reads times 10 milliseconds per disk read, which is 0.2 seconds.

The time won't be that bad because individual records are grouped together in a disk '''block'''. A disk block might be 16 kilobytes. If each record is 160 bytes, then 100 records could be stored in each block. The disk read time above was actually for an entire block. Once the disk head is in position, one or more disk blocks can be read with little delay. With 100 records per block, the last 6 or so comparisons don't need to do any disk reads—the comparisons are all within the last disk block read.

To speed the search further, the first 13 to 14 comparisons (which each required a disk access) must be sped up.

===An index speeds the search===
A significant improvement can be made with an [[Index (database)|index]]. In the example above, initial disk reads narrowed the search range by a factor of two. That can be improved substantially by creating an auxiliary index that contains the first record in each disk block (sometimes called a [[Index (database)#Sparse index|sparse index]]). This auxiliary index would be 1% of the size of the original database, but it can be searched more quickly. Finding an entry in the auxiliary index would tell us which block to search in the main database; after searching the auxiliary index, we would have to search only that one block of the main database—at a cost of one more disk read. The index would hold 10,000 entries, so it would take at most 14 comparisons. Like the main database, the last 6 or so comparisons in the aux index would be on the same disk block. The index could be searched in about 8 disk reads, and the desired record could be accessed in 9 disk reads.

The trick of creating an auxiliary index can be repeated to make an auxiliary index to the auxiliary index. That would make an aux-aux index that would need only 100 entries and would fit in one disk block.

Instead of reading 14 disk blocks to find the desired record, we only need to read 3 blocks. Reading and searching the first (and only) block of the aux-aux index identifies the relevant block in aux-index. Reading and searching that aux-index block identifies the relevant block in the main database. Instead of 150 milliseconds, we need only 30 milliseconds to get the record.

The auxiliary indices have turned the search problem from a binary search requiring roughly &lt;math&gt;\log_2 N&lt;/math&gt; disk reads to one requiring only &lt;math&gt;\log_b N&lt;/math&gt; disk reads where &lt;math&gt;b&lt;/math&gt; is the blocking factor (the number of entries per block: &lt;math&gt;b = 100&lt;/math&gt; entries per block; &lt;math&gt;\log_b 1,000,000 = 3&lt;/math&gt; reads).

In practice, if the main database is being frequently searched, the aux-aux index and much of the aux index may reside in a [[page cache|disk cache]], so they would not incur a disk read.

===Insertions and deletions cause trouble===
If the database does not change, then compiling the index is simple to do, and the index need never be changed. If there are changes, then managing the database and its index becomes more complicated.

Deleting records from a database doesn't cause much trouble. The index can stay the same, and the record can just be marked as deleted. The database stays in sorted order. If there is a large number of deletions, then the searching and storage become less efficient.

Insertions can be very slow in a sorted sequential file because room for the inserted record must be made. Inserting a record before the first record in the file requires shifting all of the records down one. Such an operation is just too expensive to be practical. A trick is to leave some space lying around to be used for insertions. Instead of densely storing all the records in a block, the block can have some free space to allow for subsequent insertions. Those records would be marked as if they were &quot;deleted&quot; records.

Both insertions and deletions are fast as long as space is available on a block. If an insertion won't fit on the block, then some free space on some nearby block must be found and the auxiliary indices adjusted. The hope is that enough space is nearby such that a lot of blocks do not need to be reorganized. Alternatively, some out-of-sequence disk blocks may be used.

===The B-tree uses all those ideas===
The B-tree uses all of the ideas described above.  In particular, a B-tree:
* keeps keys in sorted order for sequential traversing
* uses a hierarchical index to minimize the number of disk reads
* uses partially full blocks to speed insertions and deletions
* keeps the index balanced with an elegant recursive algorithm

In addition, a B-tree minimizes waste by making sure the interior nodes are at least half full. A B-tree can handle an arbitrary number of insertions and deletions.

==Technical description==

===Terminology===
Unfortunately, the literature on B-trees is not uniform in its terminology {{Harv|Folk|Zoellick|1992|p=362}}.

{{Harvtxt|Bayer|McCreight|1972}}, {{Harvtxt|Comer|1979}}, and others define the '''order''' of B-tree as the minimum number of keys in a non-root node. {{Harvtxt|Folk|Zoellick|1992}} points out that terminology is ambiguous because the maximum number of keys is not clear. An order 3 B-tree might hold a maximum of 6 keys or a maximum of 7 keys. {{Harvtxt|Knuth|1998|p=483}} avoids the problem by defining the '''order''' to be maximum number of children (which is one more than the maximum number of keys).

The term '''leaf''' is also inconsistent. {{Harvtxt|Bayer|McCreight|1972}} considered the leaf level to be the lowest level of keys, but Knuth considered the leaf level to be one level below the lowest keys {{Harv|Folk|Zoellick|1992|p=363}}. There are many possible implementation choices. In some designs, the leaves may hold the entire data record; in other designs, the leaves may only hold pointers to the data record. Those choices are not fundamental to the idea of a B-tree.&lt;ref&gt;{{Harvtxt|Bayer|McCreight|1972}} avoided the issue by saying an index element is a (physically adjacent) pair of (''x'',&amp;nbsp;''a'') where ''x'' is the key, and ''a'' is some associated information. The associated information might be a pointer to a record or records in a random access, but what it was didn't really matter. {{Harvtxt|Bayer|McCreight|1972}} states, &quot;For this paper the associated information is of no further interest.&quot;&lt;/ref&gt;

There are also unfortunate choices like using the variable ''k'' to represent the number of children when ''k'' could be confused with the number of keys.

For simplicity, most authors assume there are a fixed number of keys that fit in a node. The basic assumption is the key size is fixed and the node size is fixed. In practice, variable length keys may be employed {{Harv|Folk|Zoellick|1992|p=379}}.

===Definition===
According to Knuth's definition, a B-tree of order ''m'' is a tree which satisfies the following properties:

# Every node has at most ''m'' children.
# Every non-leaf node (except root) has at least &amp;lceil;{{frac|''m''|2}}&amp;rceil; children.
# The root has at least two children if it is not a leaf node.
# A non-leaf node with ''k'' children contains ''k''−1 keys.
# All leaves appear in the same level, and internal vertices carry no information.

Each internal node’s keys act as separation values which divide its subtrees. For example, if an internal node has 3 child nodes (or subtrees) then it must have 2 keys: ''a''&lt;sub&gt;1&lt;/sub&gt; and ''a''&lt;sub&gt;2&lt;/sub&gt;. All values in the leftmost subtree will be less than ''a''&lt;sub&gt;1&lt;/sub&gt;, all values in the middle subtree will be between ''a''&lt;sub&gt;1&lt;/sub&gt; and ''a''&lt;sub&gt;2&lt;/sub&gt;, and all values in the rightmost subtree will be greater than ''a''&lt;sub&gt;2&lt;/sub&gt;.

;'''Internal nodes'''
: Internal nodes are all nodes except for leaf nodes and the root node. They are usually represented as an ordered set of elements and child pointers. Every internal node contains a '''maximum''' of ''U'' children and a '''minimum''' of ''L'' children. Thus, the number of elements is always 1 less than the number of child pointers (the number of elements is between ''L''−1 and ''U''−1). ''U'' must be either 2''L'' or 2''L''−1; therefore each internal node is at least half full. The relationship between ''U'' and ''L'' implies that two half-full nodes can be joined to make a legal node, and one full node can be split into two legal nodes (if there’s room to push one element up into the parent). These properties make it possible to delete and insert new values into a B-tree and adjust the tree to preserve the B-tree properties.

;'''The root node'''
: The root node’s number of children has the same upper limit as internal nodes, but has no lower limit. For example, when there are fewer than ''L''−1 elements in the entire tree, the root will be the only node in the tree, with no children at all.

;'''Leaf nodes'''
: Leaf nodes have the same restriction on the number of elements, but have no children, and no child pointers.

A B-tree of depth ''n''+1 can hold about ''U'' times as many items as a B-tree of depth ''n'', but the cost of search, insert, and delete operations grows with the depth of the tree. As with any balanced tree, the cost grows much more slowly than the number of elements.

Some balanced trees store values only at leaf nodes, and use different kinds of nodes for leaf nodes and internal nodes. B-trees keep values in every node in the tree, and may use the same structure for all nodes. However, since leaf nodes never have children, the B-trees benefit from improved performance if they use a specialized structure.

==Best case and worst case heights==
Let ''h'' be the height of the classic B-tree. Let ''n'' &gt; 0 be the number of entries in the tree.&lt;ref&gt;If ''n'' is zero, then no root node is needed, so the height of an empty tree is not well defined.&lt;/ref&gt; Let ''m'' be the maximum number of children a node can have. Each node can have at most ''m''−1 keys.

It can be shown (by induction for example) that a B-tree of height ''h'' with all its nodes completely filled has ''n''=''m''&lt;sup&gt;''h''&lt;/sup&gt;&amp;minus;1 entries. Hence, the best case height of a B-tree is:
: &lt;math&gt;\lceil \log_{m} (n+1) \rceil .&lt;/math&gt;

Let ''d'' be the minimum number of children an internal (non-root) node can have. For an ordinary B-tree, ''d''=⌈''m''/2⌉.

{{Harvtxt|Comer|1979|p=127}} and {{Harvtxt|Cormen|Leiserson|Rivest|Stein|2001|pp=383–384}} give the worst case height of a B-tree (where the root node is considered to have height 0) as
: &lt;math&gt;h \le \left\lfloor \log_{d}\left(\frac{n+1}{2}\right) \right\rfloor .&lt;/math&gt;

==Algorithms==
{{confusing|reason=the discussion below uses &quot;element&quot;, &quot;value&quot;, &quot;key&quot;, &quot;separator&quot;, and &quot;separation value&quot; to mean essentially the same thing. The terms are not clearly defined. There are some subtle issues at the root and leaves
|date=February 2012}}

===Search===
Searching is similar to searching a binary search tree. Starting at the root, the tree is recursively traversed from top to bottom. At each level, the search chooses the child pointer (subtree) whose separation values are on either side of the search value.

Binary search is typically (but not necessarily) used within nodes to find the separation values and child tree of interest.

===Insertion===
[[Image:B tree insertion example.png|thumb|A B Tree insertion example with each iteration. The nodes of this B tree have at most 3 children (Knuth order 3).]]

All insertions start at a leaf node. To insert a new element, search the tree to find the leaf node where the new element should be added. Insert the new element into that node with the following steps:

# If the node contains fewer than the maximum legal number of elements, then there is room for the new element. Insert the new element in the node, keeping the node's elements ordered.
# Otherwise the node is full, evenly split it into two nodes so:
## A single median is chosen from among the leaf's elements and the new element.
## Values less than the median are put in the new left node and values greater than the median are put in the new right node, with the median acting as a separation value.
## The separation value is inserted in the node's parent, which may cause it to be split, and so on. If the node has no parent (i.e., the node was the root), create a new root above this node (increasing the height of the tree).

If the splitting goes all the way up to the root, it creates a new root with a single separator value and two children, which is why the lower bound on the size of internal nodes does not apply to the root. The maximum number of elements per node is ''U''−1. When a node is split, one element moves to the parent, but one element is added. So, it must be possible to divide the maximum number ''U''−1 of elements into two legal nodes. If this number is odd, then ''U''=2''L'' and one of the new nodes contains (''U''−2)/2 = ''L''−1 elements, and hence is a legal node, and the other contains one more element, and hence it is legal too. If ''U''−1 is even, then ''U''=2''L''−1, so there are 2''L''−2 elements in the node. Half of this number is ''L''−1, which is the minimum number of elements allowed per node.

An improved algorithm {{Harv|Mond|Raz|1985}} supports a single pass down the tree from the root to the node where the insertion will take place, splitting any full nodes encountered on the way. This prevents the need to recall the parent nodes into memory, which may be expensive if the nodes are on secondary storage. However, to use this improved algorithm, we must be able to send one element to the parent and split the remaining ''U''−2 elements into two legal nodes, without adding a new element. This requires ''U'' = 2''L'' rather than ''U'' = 2''L''−1, which accounts for why some textbooks impose this requirement in defining B-trees.

===Deletion===
There are two popular strategies for deletion from a B-tree.

# Locate and delete the item, then restructure the tree to regain its invariants, '''OR'''
# Do a single pass down the tree, but before entering (visiting) a node, restructure the tree so that once the key to be deleted is encountered, it can be deleted without triggering the need for any further restructuring

The algorithm below uses the former strategy.

There are two special cases to consider when deleting an element:

# The element in an internal node is a separator for its child nodes
# Deleting an element may put its node under the minimum number of elements and children

The procedures for these cases are in order below.

====Deletion from a leaf node====
# Search for the value to delete.
# If the value is in a leaf node, simply delete it from the node.
# If underflow happens, rebalance the tree as described in section &quot;Rebalancing after deletion&quot; below.

====Deletion from an internal node====
Each element in an internal node acts as a separation value for two subtrees, therefore we need to find a replacement for separation. Note that the largest element in the left subtree is still less than the separator. Likewise, the smallest element in the right subtree is still greater than the separator. Both of those elements are in leaf nodes, and either one can be the new separator for the two subtrees. Algorithmically described below:

# Choose a new separator (either the largest element in the left subtree or the smallest element in the right subtree), remove it from the leaf node it is in, and replace the element to be deleted with the new separator.
# The previous step deleted an element (the new separator) from a leaf node. If that leaf node is now deficient (has fewer than the required number of nodes), then rebalance the tree starting from the leaf node.

====Rebalancing after deletion====
Rebalancing starts from a leaf and proceeds toward the root until the tree is balanced. If deleting an element from a node has brought it under the minimum size, then some elements must be redistributed to bring all nodes up to the minimum. Usually, the redistribution involves moving an element from a sibling node that has more than the minimum number of nodes. That redistribution operation is called a '''rotation'''. If no sibling can spare a node, then the deficient node must be '''merged''' with a sibling. The merge causes the parent to lose a separator element, so the parent may become deficient and need rebalancing. The merging and rebalancing may continue all the way to the root. Since the minimum element count doesn't apply to the root, making the root be the only deficient node is not a problem. The algorithm to rebalance the tree is as follows:{{Citation needed|date=July 2008}}

* If the deficient node's right sibling exists and has more than the minimum number of elements, then rotate left
*# Copy the separator from the parent to the end of the deficient node (the separator moves down; the deficient node now has the minimum number of elements)
*# Replace the separator in the parent with the first element of the right sibling (right sibling loses one node but still has at least the minimum number of elements)
*# The tree is now balanced
* Otherwise, if the deficient node's left sibling exists and has more than the minimum number of elements, then rotate right
*# Copy the separator from the parent to the start of the deficient node (the separator moves down; deficient node now has the minimum number of elements)
*# Replace the separator in the parent with the last element of the left sibling (left sibling loses one node but still has at least the minimum number of elements)
*# The tree is now balanced
* Otherwise, if both immediate siblings have only the minimum number of elements, then merge with a sibling sandwiching their separator taken off from their parent
*# Copy the separator to the end of the left node (the left node may be the deficient node or it may be the sibling with the minimum number of elements)
*# Move all elements from the right node to the left node (the left node now has the maximum number of elements, and the right node – empty)
*# Remove the separator from the parent along with its empty right child (the parent loses an element)
*#* If the parent is the root and now has no elements, then free it and make the merged node the new root (tree becomes shallower)
*#* Otherwise, if the parent has fewer than the required number of elements, then rebalance the parent

:&lt;small&gt;'''Note''': The rebalancing operations are different for B+ trees (e.g., rotation is different because parent has copy of the key) and B*-tree (e.g., three siblings are merged into two siblings).&lt;/small&gt;

===Sequential access===
While freshly loaded databases tend to have good sequential behavior, this behavior becomes increasingly difficult to maintain as a database grows, resulting in more random I/O and performance challenges.&lt;ref&gt;{{cite web |url=http://www.cs.sunysb.edu/~bender/pub/cache-oblivious-btree.ps |title=Cache Oblivious B-trees |publisher=State University of New York (SUNY) at Stony Brook |accessdate=2011-01-17}}&lt;/ref&gt;

===Initial construction===
In applications, it is frequently useful to build a B-tree to represent a large existing collection of data and then update it incrementally using standard B-tree operations. In this case, the most efficient way to construct the initial B-tree is not to insert every element in the initial collection successively, but instead to construct the initial set of leaf nodes directly from the input, then build the internal nodes from these. This approach to B-tree construction is called [[bulkloading]]. Initially, every leaf but the last one has one extra element, which will be used to build the internal nodes.{{Citation needed|date=July 2008}}

For example, if the leaf nodes have maximum size 4 and the initial collection is the integers 1 through 24, we would initially construct 4 leaf nodes containing 5 values each and 1 which contains 4 values:

{|
|-
|
{|class=&quot;wikitable&quot;
|-
|1||2||3||4||5
|}
||
{|class=&quot;wikitable&quot;
|-
|6||7||8||9||10
|}
||
{|class=&quot;wikitable&quot;
|-
|11||12||13||14||15
|}
||
{|class=&quot;wikitable&quot;
|-
|16||17||18||19||20
|}
||
{|class=&quot;wikitable&quot;
|-
|21||22||23||24
|}

|}

We build the next level up from the leaves by taking the last element from each leaf node except the last one. Again, each node except the last will contain one extra value. In the example, suppose the internal nodes contain at most 2 values (3 child pointers). Then the next level up of internal nodes would be:

{|
|-
| colspan=3 |
{|class=&quot;wikitable&quot; align=center
|-
|5||10||15
|}
| &amp;nbsp;
| colspan=2 |
{|class=&quot;wikitable&quot; align=center
|-
|20
|}
|-
|
{|class=&quot;wikitable&quot;
|-
|1||2||3||4
|}
||
{|class=&quot;wikitable&quot;
|-
|6||7||8||9
|}
||
{|class=&quot;wikitable&quot;
|-
|11||12||13||14
|}
| &amp;nbsp;
|
{|class=&quot;wikitable&quot;
|-
|16||17||18||19
|}
||
{|class=&quot;wikitable&quot;
|-
|21||22||23||24
|}

|}

This process is continued until we reach a level with only one node and it is not overfilled. In the example only the root level remains:

{|
|-
| colspan=6 |
{|class=&quot;wikitable&quot; align=center
|-
|15
|}
|-
| colspan=3 |
{|class=&quot;wikitable&quot; align=center
|-
|5||10
|}
| &amp;nbsp;
| colspan=2 |
{|class=&quot;wikitable&quot; align=center
|-
|20
|}
|-
|
{|class=&quot;wikitable&quot;
|-
|1||2||3||4
|}
||
{|class=&quot;wikitable&quot;
|-
|6||7||8||9
|}
||
{|class=&quot;wikitable&quot;
|-
|11||12||13||14
|}
| &amp;nbsp;
|
{|class=&quot;wikitable&quot;
|-
|16||17||18||19
|}
||
{|class=&quot;wikitable&quot;
|-
|21||22||23||24
|}

|}

==In filesystems==
In addition to its use in databases, the B-tree is also used in filesystems to allow quick random access to an arbitrary block in a particular file. The basic problem is turning the file block &lt;math&gt;i&lt;/math&gt; address into a disk block (or perhaps to a [[cylinder-head-sector]]) address.

Some operating systems require the user to allocate the maximum size of the file when the file is created. The file can then be allocated as contiguous disk blocks. Converting to a disk block: the operating system just adds the file block address to the starting disk block of the file. The scheme is simple, but the file cannot exceed its created size.

Other operating systems allow a file to grow. The resulting disk blocks may not be contiguous, so mapping logical blocks to physical blocks is more involved.

[[MS-DOS]], for example, used a simple [[File Allocation Table]] (FAT). The FAT has an entry for each disk block,&lt;ref group=&quot;note&quot;&gt;For FAT, what is called a &quot;disk block&quot; here is what the FAT documentation calls a &quot;cluster&quot;, which is fixed-size group of one or more contiguous whole physical disk [[Cylinder-head-sector|sectors]]. For the purposes of this discussion, a cluster has no significant difference from a physical sector.&lt;/ref&gt; and that entry identifies whether its block is used by a file and if so, which block (if any) is the next disk block of the same file. So, the allocation of each file is represented as a [[linked list]] in the table. In order to find the disk address of file block &lt;math&gt;i&lt;/math&gt;, the operating system (or disk utility) must sequentially follow the file's linked list in the FAT. Worse, to find a free disk block, it must sequentially scan the FAT. For MS-DOS, that was not a huge penalty because the disks and files were small and the FAT had few entries and relatively short file chains. In the [[FAT12]] filesystem (used on floppy disks and early hard disks), there were no more than 4,080 &lt;ref group=&quot;note&quot;&gt;Two of these were reserved for special purposes, so only 4078 could actually represent disk blocks (clusters).&lt;/ref&gt; entries, and the FAT would usually be resident in memory. As disks got bigger, the FAT architecture began to confront penalties. On a large disk using FAT, it may be necessary to perform disk reads to learn the disk location of a file block to be read or written.

[[TOPS-20]] (and possibly [[TOPS-20#TENEX|TENEX]]) used a 0 to 2 level tree that has similarities to a B-tree{{Citation needed|date=October 2009}}. A disk block was 512 36-bit words. If the file fit in a 512 (2&lt;sup&gt;9&lt;/sup&gt;) word block, then the file directory would point to that physical disk block. If the file fit in 2&lt;sup&gt;18&lt;/sup&gt; words, then the directory would point to an aux index; the 512 words of that index would either be NULL (the block isn't allocated) or point to the physical address of the block. If the file fit in 2&lt;sup&gt;27&lt;/sup&gt; words, then the directory would point to a block holding an aux-aux index; each entry would either be NULL or point to an aux index. Consequently, the physical disk block for a 2&lt;sup&gt;27&lt;/sup&gt; word file could be located in two disk reads and read on the third.

Apple's filesystem [[HFS+]], Microsoft's [[NTFS]],&lt;ref name=insidewin2kntfs&gt;{{cite web
 | url = http://msdn2.microsoft.com/en-us/library/ms995846.aspx
 | title = Inside Win2K NTFS, Part 1
 | author = [[Mark Russinovich]]
 | publisher = [[MSDN|Microsoft Developer Network]]
 | accessdate = 2008-04-18
| archiveurl= http://web.archive.org/web/20080413181940/http://msdn2.microsoft.com/en-us/library/ms995846.aspx| archivedate= 13 April 2008 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; AIX (jfs2) and some [[Linux]] filesystems, such as [[btrfs]] and [[Ext4]], use B-trees.

B*-trees are used in the [[Hierarchical File System|HFS]] and [[Reiser4]] [[file system]]s.

==Variations==

===Access concurrency===
Lehman and Yao&lt;ref&gt;{{cite web|url=http://portal.acm.org/citation.cfm?id=319663&amp;dl=GUIDE&amp;coll=GUIDE&amp;CFID=61777986&amp;CFTOKEN=74351190 |title=Efficient locking for concurrent operations on B-trees |doi=10.1145/319628.319663 |publisher=Portal.acm.org |date= |accessdate=2012-06-28}}&lt;/ref&gt; showed that all read locks could be avoided (and thus concurrent access greatly improved) by linking the tree blocks at each level together with a &quot;next&quot; pointer. This results in a tree structure where both insertion and search operations descend from the root to the leaf. Write locks are only required as a tree block is modified. This maximizes access concurrency by multiple users, an important consideration for databases and/or other B-tree based [[ISAM]] storage methods. The cost associated with this improvement is that empty pages cannot be removed from the btree during normal operations. (However, see &lt;ref&gt;http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA232287&amp;Location=U2&amp;doc=GetTRDoc.pdf&lt;/ref&gt; for various strategies to implement node merging, and source code at.&lt;ref&gt;{{cite web|url=https://github.com/malbrain/Btree-source-code|title=Downloads - high-concurrency-btree - High Concurrency B-Tree code in C - GitHub Project Hosting |date= |accessdate=2014-01-27}}&lt;/ref&gt;)

United States Patent 5283894, granted in 1994, appears to show a way to use a 'Meta Access Method' &lt;ref&gt;[http://www.freepatentsonline.com/5283894.html Lockless Concurrent B+Tree]&lt;/ref&gt; to allow concurrent B+ tree access and modification without locks. The technique accesses the tree 'upwards' for both searches and updates by means of additional in-memory indexes that point at the blocks in each level in the block cache. No reorganization for deletes is needed and there are no 'next' pointers in each block as in Lehman and Yao.

==See also==
* [[R-tree]]
* [[2–3 tree]]
* [[2–3–4 tree]]

==Notes==
{{reflist|30em|group=note}}

==References==
{{reflist|30em}}

;General
* {{Citation
 | last = Bayer
 | first = R.
 | author-link = Rudolf Bayer
 | last2 = McCreight
 | first2 = E.
 | author2-link = Edward M. McCreight
 | title = Organization and Maintenance of Large Ordered Indexes
 | journal = Acta Informatica
 | volume = 1
 | issue = 3
 | pages = 173–189
 | date =
 | year = 1972
 | url = http://www.minet.uni-jena.de/dbis/lehre/ws2005/dbs1/Bayer_hist.pdf
 | doi =10.1007/bf00288683
 | ref = harv}}
* {{Citation
 | last = Comer
 | first = Douglas
 | author-link = Douglas Comer
 | title = The Ubiquitous B-Tree
 | journal = Computing Surveys
 | volume = 11
 | issue = 2
 | pages = 123–137
 | date = June 1979
 | issn = 0360-0300
 | url =
 | doi = 10.1145/356770.356776
 | ref = harv}}.
* {{Citation
 | last = Cormen
 | first = Thomas
 | author-link = Thomas H. Cormen
 | last2 = Leiserson
 | first2 = Charles
 | author2-link = Charles E. Leiserson
 | last3 = Rivest
 | first3 = Ronald
 | author3-link = Ronald L. Rivest
 | last4 = Stein
 | first4 = Clifford
 | author4-link = Clifford Stein
 | title = [[Introduction to Algorithms]]
 | place =
 | publisher = MIT Press and McGraw-Hill
 | year = 2001
 | volume =
 | edition = Second
 | page =
 | pages = 434–454
 | url =
 | doi =
 | isbn = 0-262-03293-7}}. Chapter 18: B-Trees.
* {{Citation
 | last =Folk
 | first =Michael J.
 | author-link =
 | last2 =Zoellick
 | first2 =Bill
 | author2-link =
 | title =File Structures
 | place =
 | publisher =Addison-Wesley
 | year = 1992
 | volume =
 | edition =2nd
 | page =
 | pages =
 | url =
 | doi =
 | isbn = 0-201-55713-4
 | ref =harv}}
* {{Citation
 | last = Knuth
 | first = Donald
 | author-link = Donald Knuth
 | series = [[The Art of Computer Programming]]
 | title = Sorting and Searching
 | place =
 | publisher = Addison-Wesley
 | year = 1998
 | volume = Volume 3
 | edition = Second
 | page =
 | pages =
 | url =
 | doi =
 | isbn = 0-201-89685-0 }}. Section 6.2.4: Multiway Trees, pp.&amp;nbsp;481–491. Also, pp.&amp;nbsp;476–477 of section 6.2.3 (Balanced Trees) discusses 2-3 trees.
* {{Citation
 | last = Mond
 | first = Yehudit
 | author-link = Yehudit Mond
 | last2 = Raz
 | first2 = Yoav
 | author2-link = Yoav Raz
 | title = Concurrency Control in B+-Trees Databases Using Preparatory Operations
 | journal = VLDB'85, Proceedings of 11th International Conference on Very Large Data Bases
 | pages = 331–334
 | year = 1985
 | url = http://www.informatik.uni-trier.de/~ley/db/conf/vldb/MondR85.html
 | ref = harv}}.

===Original papers===
* {{Citation
 | last = Bayer
 | first = Rudolf
 | author-link = Rudolf Bayer
 | last2 = McCreight
 | first2 = E.
 | author2-link = Edward M. McCreight
 | title = Organization and Maintenance of Large Ordered Indices
 | place =
 | publisher = Boeing Scientific Research Laboratories
 | year = 1970
 | volume = Mathematical and Information Sciences Report No. 20
 | edition =
 | page =
 | pages =
 | url =
 | doi =
 | date = July 1970
 | isbn = }}.
* {{Citation
 | last = Bayer
 | first = Rudolf
 | author-link = Rudolf Bayer
 | contribution = Binary B-Trees for Virtual Memory
 | series = Proceedings of 1971 ACM-SIGFIDET Workshop on Data Description, Access and Control
 | year = 1971
 | pages =
 | place = San Diego, California
 | publisher =
 | url =
 | doi =
}}. November 11–12, 1971.

==External links==
* [http://www.youtube.com/watch?v=I22wEC1tTGo B-tree lecture] by David Scot Taylor, SJSU
* [http://slady.net/java/bt/view.php B-Tree animation applet] by slady
* [http://www.scholarpedia.org/article/B-tree_and_UB-tree B-tree and UB-tree on Scholarpedia] Curator: Dr Rudolf Bayer
* [http://www.bluerwhite.org/btree B-Trees: Balanced Tree Data Structures]
* [http://www.nist.gov/dads/HTML/btree.html NIST's Dictionary of Algorithms and Data Structures: B-tree]
* [http://cis.stvincent.edu/html/tutorials/swd/btree/btree.html B-Tree Tutorial]
* [http://www.boilerbay.com/infinitydb/TheDesignOfTheInfinityDatabaseEngine.htm The InfinityDB BTree implementation]
* [http://supertech.csail.mit.edu/cacheObliviousBTree.html Cache Oblivious B(+)-trees]
* [http://www.nist.gov/dads/HTML/bstartree.html Dictionary of Algorithms and Data Structures entry for B*-tree]
* [http://opendatastructures.org/versions/edition-0.1g/ods-python/14_2_B_Trees.html Open Data Structures - Section 14.2 - B-Trees]
* [http://www.chiark.greenend.org.uk/~sgtatham/algorithms/cbtree.html Counted B-Trees]

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:B-Tree}}
[[Category:Trees (data structures)]]
[[Category:1971 introductions]]
[[Category:B-tree]]
[[Category:Database index techniques]]</text>
      <sha1>ehdnpq5ur201h9js2913zv1muxliosz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>B-trie</title>
    <ns>0</ns>
    <id>18604065</id>
    <revision>
      <id>612576933</id>
      <parentid>610732423</parentid>
      <timestamp>2014-06-12T01:58:21Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor/>
      <comment>removed [[Template:Multiple issues]] &amp; [[WP:AWB/GF|general fixes]] using [[Project:AWB|AWB]] (10242)</comment>
      <text xml:space="preserve" bytes="959">{{orphan|date=February 2009}}

The '''B-trie''' is a [[trie]]-based [[data structure]] that can store and retrieve variable-length strings efficiently on disk.&lt;ref&gt;{{Citation | title=B-tries for Disk-based String Management | first1=Nikolas | last1=Askitis | first2=Justin | last2=Zobel | year=2008 | issn=1066-8888 | pages=1–26 | url=http://www.springerlink.com/content/x7545u2g85675u17/  | journal=VLDB Journal}}&lt;/ref&gt;

The B-trie was compared against several high-performance variants of [[B-tree]] that were
designed for string keys. It was shown to offer superior performance, particularly under skew access (i.e., many repeated searches). It is currently a leading choice for maintaining a string dictionary on disk, along with other disk-based tasks, such as maintaining an index to a string database or for accumulating the vocabulary of a large text collection.

==References==
{{Reflist}}

[[Category:Trees (data structures)]]


{{algorithm-stub}}</text>
      <sha1>lj9an6560r4x5nec2dwfyq8fhraucef</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>BK-tree</title>
    <ns>0</ns>
    <id>3417338</id>
    <revision>
      <id>624732553</id>
      <parentid>611405967</parentid>
      <timestamp>2014-09-08T23:06:50Z</timestamp>
      <contributor>
        <username>Loveoflearning1</username>
        <id>22286764</id>
      </contributor>
      <comment>Added link to another wiki page to give definition of &quot;approximate string matching&quot;</comment>
      <text xml:space="preserve" bytes="1982">{{Underlinked|date=December 2012}}

A '''BK-tree''' is a [[metric trees|metric tree]] suggested by Walter Austin Burkhard and Robert M. Keller{{ref|BK73}} specifically adapted to discrete [[metric space]]s.
For simplicity, let us consider '''integer''' discrete metric &lt;math&gt;d(x,y)&lt;/math&gt;. Then, BK-tree is defined in the following way. An arbitrary element ''a'' is selected as root node.  The root node may have zero or more subtrees.  The ''k-th'' subtree is recursively built of all elements ''b'' such that 
&lt;math&gt;d(a,b) = k&lt;/math&gt;. BK-trees can be used for [[Approximate_string_matching|approximate string matching]] in a dictionary {{ref|BN98}}.

== See also ==
* [[Levenshtein distance]] - the distance metric commonly used when building a BK-tree
* [[Damerau–Levenshtein distance]] - a modified form of Levenshtein distance that allows transpositions

== References ==
* {{note|BK73}} [http://doi.acm.org/10.1145/362003.362025 W. Burkhard and R. Keller. Some approaches to best-match file searching, CACM, 1973]
* {{note|BCMW04}} R. Baeza-Yates, W. Cunto, U. Manber, and S. Wu. Proximity matching using fixed queries trees. In M. Crochemore and D. Gusfield, editors, 5th Combinatorial   Pattern Matching, LNCS 807, pages 198-212, Asilomar, CA, June 1994.
* {{note|BN98}}  [http://reference.kfupm.edu.sa/content/f/a/fast_approximate_string_matching_in_a_di_269284.pdf Ricardo Baeza-Yates and Gonzalo Navarro. Fast Approximate String Matching in a Dictionary. Proc. SPIRE'98]

== External links ==
* A BK-tree implementation in [http://cliki.net/bk-tree Common Lisp] with test results and performance graphs.
* An explanation of BK-Trees and their relationship to metric spaces [http://blog.notdot.net/2007/4/Damn-Cool-Algorithms-Part-1-BK-Trees]
* An explanation of BK-Trees with an implementation in C#[http://nullwords.wordpress.com/2013/03/13/the-bk-tree-a-data-structure-for-spell-checking/]

{{CS-Trees}}

[[Category:Trees (data structures)]]


{{datastructure-stub}}</text>
      <sha1>icv8c9fmiflz9cakeglwh4ckai6b7u7</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Bounding interval hierarchy</title>
    <ns>0</ns>
    <id>6814241</id>
    <revision>
      <id>527510270</id>
      <parentid>497163626</parentid>
      <timestamp>2012-12-11T11:09:23Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>clean up, References after punctuation per [[WP:REFPUNC]] and [[WP:CITEFOOT]] using [[Project:AWB|AWB]] (8792)</comment>
      <text xml:space="preserve" bytes="7749">A '''bounding interval hierarchy (BIH)''' is a partitioning [[data structure]] similar to that of [[bounding volume hierarchy|bounding volume hierarchies]] or [[kd-tree]]s. Bounding interval hierarchies can be used in high performance (or real-time) [[Ray tracing (graphics)|ray tracing]] and may be especially useful for dynamic scenes.

The BIH was first presented under the name of SKD-Trees,&lt;ref name=&quot;skd&quot;&gt;Nam, Beomseok; Sussman, Alan. [http://ieeexplore.ieee.org/Xplore/login.jsp?url=/iel5/9176/29111/01311209.pdf A comparative study of spatial indexing techniques for multidimensional scientific datasets]&lt;/ref&gt; presented by Ooi et al., and BoxTrees,&lt;ref name=&quot;boxtree&quot;&gt;Zachmann, Gabriel. [http://zach.in.tu-clausthal.de/papers/vrst02.html Minimal Hierarchical Collision Detection]&lt;/ref&gt; independently invented by Zachmann.

== Overview ==
Bounding interval hierarchies (BIH) exhibit many of the properties of both [[bounding volume hierarchy|bounding volume hierarchies]] (BVH) and [[kd-tree]]s. Whereas the construction and storage of BIH is comparable to that of BVH, the traversal of BIH resemble that of [[kd-tree]]s. Furthermore, BIH are also [[binary tree]]s just like kd-trees (and in fact their superset, [[BSP tree]]s). Finally, BIH are axis-aligned as are its ancestors.
Although a more general non-axis-aligned implementation of the BIH should be possible (similar to the BSP-tree, which uses unaligned planes), it would almost certainly be less desirable due to decreased numerical stability and an increase in the complexity of ray traversal.

The key feature of the BIH is the storage of 2 planes per node (as opposed to 1 for the kd tree and 6 for an axis aligned [[bounding box]] hierarchy), which allows for overlapping children (just like a BVH), but at the same time featuring an order on the children along one dimension/axis (as it is the case for kd trees).

It is also possible to just use the BIH data structure for the construction phase but traverse the tree in a way a traditional axis aligned bounding box hierarchy does. This enables some simple speed up optimizations for large ray bundles &lt;ref name=&quot;WaldBVH&quot;&gt;Wald, Ingo;  Boulos, Solomon; Shirley, Peter (2007). 
[http://www.sci.utah.edu/~wald/Publications/2007///BVH/download//togbvh.pdf Ray Tracing Deformable Scenes using Dynamic Bounding Volume Hierarchies]&lt;/ref&gt; while keeping [[memory (computers)|memory]]/[[CPU cache|cache]] usage low.

Some general attributes of bounding interval hierarchies (and techniques related to BIH) as described by &lt;ref name=&quot;BIH&quot;&gt;Wächter, Carsten; Keller, Alexander (2006). [http://ainc.de/Research/BIH.pdf Instant Ray Tracing: The Bounding Interval Hierarchy]&lt;/ref&gt; are:
* Very fast construction times
* Low memory footprint
* Simple and fast traversal
* Very simple construction and traversal algorithms
* High numerical precision during construction and traversal
* Flatter tree structure (decreased tree depth) compared to kd-trees

== Operations ==

=== Construction ===
To construct any [[space partitioning]] structure some form of [[heuristic]] is commonly used. For this the [[surface area heuristic]], commonly used with many partitioning schemes, is a possible candidate. Another, more simplistic heuristic is the &quot;global&quot; heuristic described by &lt;ref name=&quot;BIH&quot;/&gt; which only requires an [[axis-aligned bounding box]], rather than the full set of primitives, making it much more suitable for a fast construction.

The general construction scheme for a BIH:
* calculate the scene bounding box
* use a heuristic to choose one axis and a split plane candidate perpendicular to this axis
* sort the objects to the left or right child (exclusively) depending on the bounding box of the object (note that objects intersecting the split plane may either be sorted by its overlap with the child volumes or any other heuristic)
* calculate the maximum bounding value of all objects on the left and the minimum bounding value of those on the right for that axis (can be combined with previous step for some heuristics)
* store these 2 values along with 2 bits encoding the split axis in a new node
* continue with step 2 for the children

Potential heuristics for the split plane candidate search:
* Classical: pick the longest axis and the middle of the node bounding box on that axis
* Classical: pick the longest axis and a split plane through the median of the objects (results in a leftist tree which is often unfortunate for ray tracing though)
* Global heuristic: pick the split plane based on a global criterion, in the form of a regular grid (avoids unnecessary splits and keeps node volumes as cubic as possible)
* Surface area heuristic: calculate the surface area and amount of objects for both children, over the set of all possible split plane candidates, then choose the one with the lowest costs (claimed to be optimal, though the cost function poses unusual demands to proof the formula, which can not be fulfilled in real life. also an exceptionally slow heuristic to evaluate)

=== Ray traversal ===
The traversal phase closely resembles a kd-tree traversal: One has to distinguish 4 simple cases, where the ray
* just intersects the left child
* just intersects the right child
* intersects both children
* intersects neither child (the only case not possible in a kd traversal)

For the third case, depending on the ray direction (negative or positive) of the component (x, y or z) equalling the split axis of the current node, the traversal continues first with the left (positive direction) or the right (negative direction) child and the other one is pushed onto a [[stack (data structure)|stack]].

Traversal continues until a leaf node is found. After intersecting the objects in the leaf, the next element is popped from the stack. If the stack is empty, the nearest intersection of all pierced leafs is returned.

It is also possible to add a 5th traversal case, but which also requires a slightly complicated construction phase. By swapping the meanings of the left and right plane of a node, it is possible to cut off empty space on both sides of a node.
This requires an additional bit that must be stored in the node to detect this special case during traversal. Handling this case during the traversal phase is simple, as the ray
* just intersects the only child of the current node or
* intersects nothing

== Properties ==

=== Numerical stability ===
All operations during the hierarchy construction/sorting of the triangles are min/max-operations and comparisons. Thus no triangle clipping has to be done as it is the case with kd-trees and which can become a problem for triangles that just slightly intersect a node. Even if the kd implementation is carefully written, numerical errors can result in a non-detected intersection and thus rendering errors (holes in the geometry) due to the missed ray-object intersection.

== Extensions ==
Instead of using two planes per node to separate geometry, it is also possible to use any number of planes to create a n-ary BIH or use multiple planes in a standard binary BIH (one and four planes per node were already proposed in &lt;ref name=&quot;BIH&quot;/&gt; and then properly evaluated in &lt;ref name=&quot;QMCLTSERT&quot;&gt;Wächter, Carsten (2008). [http://vts.uni-ulm.de/query/longview.meta.asp?document_id=6265 Quasi-Monte Carlo Light Transport Simulation by Efficient Ray Tracing]&lt;/ref&gt;) to achieve better object separation.

== See also ==
* [[Axis-aligned bounding box]]
* [[Bounding volume hierarchy]]
* [[kd-tree]]

==References==

===Papers===
&lt;references/&gt;

==External links==
* BIH implementations: [http://github.com/imbcmdth/jsBIH Javascript].

[[Category:Geometric data structures]]
[[Category:3D computer graphics]]
[[Category:Trees (data structures)]]</text>
      <sha1>n38l0ped2udor1e177xvclmq84r638j</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Branching factor</title>
    <ns>0</ns>
    <id>438471</id>
    <revision>
      <id>622933670</id>
      <parentid>611313320</parentid>
      <timestamp>2014-08-26T20:43:46Z</timestamp>
      <contributor>
        <ip>50.242.125.196</ip>
      </contributor>
      <text xml:space="preserve" bytes="2549">[[Image:Red-black tree example.svg|thumb|A [[red-black tree]] with branching factor 2.]] In [[computing]], [[tree data structure]]s, and [[game theory]], the '''branching factor''' is the number of [[child node|children]] at each [[node (computer science)|node]], the [[outdegree]]. If this value is not uniform, an ''average branching factor'' can be calculated.

For example, in [[chess]], if a &quot;node&quot; is considered to be a legal position, the average branching factor has been said to be about 35.&lt;ref name=&quot;wired&quot;/&gt;&lt;ref&gt;{{cite web | url=http://www.gamedev.net/page/resources/_/technical/artificial-intelligence/chess-programming-part-iv-basic-search-r1171 | title=Chess Programming Part IV: Basic Search | date= 6 August 2000 |first=François Dominic |last=Laramée | publisher=[[GameDev.net]] | accessdate=2007-05-01}}&lt;/ref&gt; This means that, on average, a player has about 35 legal moves at his disposal at each turn. By comparison, the branching factor for the game [[Go (game)|Go]] is 250.&lt;ref name=&quot;wired&quot;/&gt;

Higher branching factors make algorithms that follow every branch at every node, such as exhaustive [[brute-force search|brute force searches]], computationally more expensive due to the [[exponential growth|exponentially increasing]] number of nodes, leading to [[combinatorial explosion]].

For example, if the branching factor is 10, then there will be 10 nodes one level down from the current position, 10&lt;sup&gt;2&lt;/sup&gt; (or 100) nodes two levels down, 10&lt;sup&gt;3&lt;/sup&gt; (or 1,000) nodes three levels down, and so on. The higher the branching factor, the faster this &quot;explosion&quot; occurs. The branching factor can be cut down by a [[pruning (algorithm)|pruning algorithm]].

== See also ==
* [[Outdegree]]
* [[Hierarchy]]
* [[Hierarchical organization]]

== References ==
{{reflist|refs=
&lt;ref name=&quot;wired&quot;&gt;{{cite web| title=The Mystery of Go, the Ancient Game That Computers Still Can’t Win
|url= http://www.wired.com/2014/05/the-world-of-computer-go/
|first=Alan|last= Levinovitz  |date= 12 May 2014 |publisher= [[Wired (website)|Wired]]
|quote=The rate at which possible positions increase is directly related to a game’s “branching factor,” or the average number of moves available on any given turn. Chess’s branching factor is 35. Go’s is 250. Games with high branching factors make classic search algorithms like [[minimax]] extremely costly.
|accessdate= 2014-06-02}}&lt;/ref&gt;
}}

[[Category:Trees (data structures)]]
[[Category:Analysis of algorithms]]
[[Category:Combinatorial game theory]]
[[Category:Computer chess]]</text>
      <sha1>pe40jxyic86gxolv4xl3ghalxexr5l3</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Calkin–Wilf tree</title>
    <ns>0</ns>
    <id>22672164</id>
    <revision>
      <id>617177667</id>
      <parentid>615405947</parentid>
      <timestamp>2014-07-16T13:11:14Z</timestamp>
      <contributor>
        <ip>2A01:E34:ECE7:C370:98EE:6E67:FD:CABE</ip>
      </contributor>
      <comment>/* Stern's diatomic sequence */</comment>
      <text xml:space="preserve" bytes="10838">[[File:Calkin-Wilf tree.svg|thumb|400px|The Calkin–Wilf tree, drawn using an [[H tree]] layout.]]
In [[number theory]], the '''Calkin–Wilf tree''' is a [[tree (graph theory)|tree]] in which the vertices correspond [[bijection|1-for-1]] to the [[positive number|positive]] [[rational number]]s. The tree is rooted at the number 1, and any rational number expressed in simplest terms as the [[Fraction (mathematics)|fraction]] ''a''/''b'' has as its two children the numbers ''a''/(''a''&amp;nbsp;+&amp;nbsp;''b'') and (''a''&amp;nbsp;+&amp;nbsp;''b'')/''b''. Every positive rational number appears exactly once in the tree.

The sequence of rational numbers in a [[breadth-first search|breadth-first traversal]] of the Calkin–Wilf tree is known as the '''Calkin–Wilf sequence'''. Its sequence of numerators (or, offset by one, denominators) is '''Stern's diatomic series''', and can be computed by the '''fusc function'''.

The Calkin–Wilf tree is named after [[Neil Calkin]] and [[Herbert Wilf]], who considered it in their 2000 paper. The tree was introduced earlier by Jean Berstel and Aldo de Luca&lt;ref&gt;{{harvtxt|Berstel|de Luca|1997}}, Section 6.&lt;/ref&gt; as ''Raney tree'', since they drew some ideas from a paper by George N. Raney.&lt;ref&gt;{{harvtxt|Raney|1973}}.&lt;/ref&gt; Stern's diatomic series was formulated much earlier by [[Moritz Abraham Stern]], a 19th-century German mathematician who also invented the closely related [[Stern–Brocot tree]].

==Definition and structure==
The Calkin–Wilf tree may be defined as a directed graph in which each positive rational number ''a''/''b'' occurs as a vertex and has one outgoing edge to another vertex, its parent. We assume that ''a''/''b'' is in simplest terms; that is, the [[greatest common divisor]] of ''a'' and ''b'' is 1. If ''a''/''b''&amp;nbsp;&amp;lt;&amp;nbsp;1, the parent of ''a''/''b'' is ''a''/(''b''&amp;nbsp;&amp;minus;&amp;nbsp;''a''); if ''a''/''b'' is greater than one, the parent of ''a''/''b'' is (''a''&amp;nbsp;&amp;minus;&amp;nbsp;''b'')/''b''. Thus, in either case, the parent is a fraction with a smaller sum of numerator and denominator, so repeated reduction of this type must eventually reach the number 1. As a graph with one outgoing edge per vertex and one root reachable by all other vertices, the Calkin–Wilf tree must indeed be a tree.

The children of any vertex in the Calkin–Wilf tree may be computed by inverting the formula for the parents of a vertex. Each vertex ''a''/''b'' has one child whose value is less than 1, ''a''/(''a''&amp;nbsp;+&amp;nbsp;''b''), because this is the only value less than 1 whose parent formula leads back to ''a''/''b''. Similarly, each vertex ''a''/''b'' has one child whose value is greater than 1, (''a''&amp;nbsp;+&amp;nbsp;''b'')/''b''.&lt;ref&gt;The description here is dual to the original definition by Calkin and Wilf, which begins by defining the child relationship and derives the parent relationship as part of a proof that every rational appears once in the tree. As defined here, every rational appears once by definition, and instead the fact that the resulting structure is a tree requires a proof.&lt;/ref&gt;

Although it is a binary tree (each vertex has two children), the Calkin–Wilf tree is not a [[binary search tree]]: its inorder does not coincide with the sorted order of its vertices. However, it is closely related to a different binary search tree on the same set of vertices, the [[Stern–Brocot tree]]: the vertices at each level of the two trees coincide, and are related to each other by a [[bit-reversal permutation]].&lt;ref&gt;{{harvtxt|Gibbons|Lester|Bird|2006}}.&lt;/ref&gt;

==Breadth first traversal==
[[File:Calkin-Wilf spiral.svg|thumb|250px|The Calkin–Wilf sequence, depicted as the red spiral tracing through the Calkin–Wilf tree]]
The '''Calkin–Wilf sequence''' is the sequence of rational numbers generated by a breadth-first traversal of the Calkin–Wilf tree,
:1/1, 1/2, 2/1, 1/3, 3/2, 2/3, 3/1, 1/4, 4/3, 3/5, 5/2, 2/5, 5/3, 3/4, ….
Because the Calkin–Wilf tree contains every positive rational number exactly once, so does this sequence.&lt;ref&gt;{{harvtxt|Calkin|Wilf|2000}}: &quot;a list of all positive rational numbers, each appearing once and only once, can be made by writing down 1/1, then the fractions on the level just below the top of the tree, reading from left to right, then the fractions on the next level down, reading from left to right, etc.&quot; {{harvtxt|Gibbons|Lester|Bird|2006}} discuss efficient [[functional programming]] techniques for performing this breadth first traversal.&lt;/ref&gt; The denominator of each fraction equals the numerator of the next fraction in the sequence.
The Calkin–Wilf sequence can also be generated directly by the formula

:&lt;math&gt;q_{i+1} = \frac{1}{2\lfloor q_i\rfloor - q_i + 1}&lt;/math&gt;

where &lt;math&gt;q_i&lt;/math&gt; denotes the ''i''th number in the sequence, starting from &lt;math&gt;q_0 =1&lt;/math&gt;, and &lt;math&gt;\lfloor q_i \rfloor&lt;/math&gt; represents the [[Floor and ceiling functions|integral part]].&lt;ref&gt;{{harvtxt|Aigner|Ziegler|2004}} credit this formula to Moshe Newman.&lt;/ref&gt;

==Stern's diatomic sequence==
[[File:Fusc plot 4096.svg|thumb|250px|[[Scatter plot|Scatterplot]] of fusc(0...4096)]]
'''Stern's diatomic sequence''' is the [[integer sequence]]
:0, 1, 1, 2, 1, 3, 2, 3, 1, 4, 3, 5, 2, 5, 3, 4, … {{OEIS|id=A002487}}.
The ''n''th value in the sequence is the value fusc(''n'') of the '''fusc function''',&lt;ref&gt;The fusc name was given to it in 1976 by [[Edsger W. Dijkstra]]; see EWD570 and EWD578.&lt;/ref&gt;
defined by the [[recurrence relation]]s fusc(2''n'')&amp;nbsp;=&amp;nbsp;fusc(''n'') and fusc(2''n''&amp;nbsp;+&amp;nbsp;1)&amp;nbsp;=&amp;nbsp;fusc(''n'')&amp;nbsp;+&amp;nbsp;fusc(''n''&amp;nbsp;+&amp;nbsp;1), with the base cases fusc(0)&amp;nbsp;=&amp;nbsp;0 and fusc(1)&amp;nbsp;=&amp;nbsp;1.
The ''n''th rational number in a breadth-first traversal of the Calkin–Wilf tree is the number fusc(''n'')&amp;nbsp;/&amp;nbsp;fusc(''n''&amp;nbsp;+&amp;nbsp;1).&lt;ref&gt;{{harvtxt|Calkin|Wilf|2000}}, Theorem 1.&lt;/ref&gt; Thus, the diatomic sequence forms both the sequence of numerators and the sequence of denominators of the numbers in the Calkin–Wilf sequence.

The function fusc(''n''&amp;nbsp;+&amp;nbsp;1) is the number of odd [[binomial coefficient]]s of the form &lt;math&gt;\scriptstyle {n-r\choose r},\ 0\leq 2r&lt;n,&lt;/math&gt;&lt;ref&gt;{{harvtxt|Carlitz|1964}}.&lt;/ref&gt; and
also counts the number of ways of writing ''n'' as a sum of [[power of two|powers of two]] in which each power occurs at most twice. This can be seen from the recurrence defining fusc: the expressions as a sum of powers of two for an even number 2''n'' either have no 1's in them (in which case they are formed by doubling each term an expression for ''n'') or two 1's (in which case the rest of the expression is formed by doubling each term in an expression for ''n''&amp;nbsp;&amp;minus;&amp;nbsp;1), so the number of representations is the sum of the number of representations for ''n'' and for ''n''&amp;nbsp;&amp;minus;&amp;nbsp;1, matching the recurrence. Similarly, each representation for an odd number 2''n''&amp;nbsp;+&amp;nbsp;1 is formed by doubling a representation for ''n'' and adding 1, again matching the recurrence.&lt;ref&gt;The OEIS entry credits this fact to {{harvtxt|Carlitz|1964}} and to uncited work of Lind. However, Carlitz' paper describes a more restricted class of sums of powers of two, counted by fusc(''n'') instead of by fusc(''n''&amp;nbsp;+&amp;nbsp;1).&lt;/ref&gt; For instance,
:6&amp;nbsp;=&amp;nbsp;4&amp;nbsp;+&amp;nbsp;2&amp;nbsp;=&amp;nbsp;4&amp;nbsp;+&amp;nbsp;1&amp;nbsp;+&amp;nbsp;1&amp;nbsp;=&amp;nbsp;2&amp;nbsp;+&amp;nbsp;2&amp;nbsp;+&amp;nbsp;1&amp;nbsp;+&amp;nbsp;1
has three representations as a sum of powers of two with at most two copies of each power, so fusc(6&amp;nbsp;+&amp;nbsp;1)&amp;nbsp;=&amp;nbsp;3.

==See also==
* [[Stern–Brocot tree]]

==Notes==
{{reflist}}

==References==
*{{citation
 | last1 = Aigner | first1 = Martin| author1-link = Martin Aigner
| last2 = Ziegler | first2 = Günter M. | author2-link = Günter M. Ziegler
 | edition = 3rd
 | isbn = 978-3-540-40460-6
 | location = Berlin; New York
 | publisher = Springer
 | title = [[Proofs from THE BOOK]]
 | pages = 94–97
 | year = 2004}}
*{{citation
 | last1 = Berstel | first1 = Jean
 | last2 = de Luca | first2 = Aldo
 | year = 1997
 | title = Sturmian words, Lyndon words and trees
 | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]
 | volume = 178
 | pages = 171–203
 | doi = 10.1016/S0304-3975(96)00101-6}}
*{{citation
 | last1 = Calkin | first1 = Neil
 | last2 = Wilf | first2 = Herbert
 | issue = 4
 | journal = [[American Mathematical Monthly]]
| pages = 360–363
 | title = Recounting the rationals
 | url = http://www.math.upenn.edu/~wilf/website/recounting.pdf
 | volume = 107
 | year = 2000
 | doi = 10.2307/2589182
 | publisher = [[Mathematical Association of America]]
| jstor = 2589182}}.
*{{citation|first=L.|last=Carlitz | authorlink =Leonard Carlitz|title=A problem in partitions related to the [[Stirling numbers]]|journal=[[Bulletin of the American Mathematical Society]]|volume=70|issue=2|year=1964|pages=275–278 |mr=0157907|doi=10.1090/S0002-9904-1964-11118-6}}.
*{{citation
 |first=Edsger W.|last=Dijkstra|authorlink=Edsger W. Dijkstra
 |title=Selected Writings on Computing: A Personal Perspective
 |publisher=[[Springer-Verlag]]|year=1982|isbn=0-387-90652-5}}. [http://www.cs.utexas.edu/users/EWD/ewd05xx/EWD570.PDF EWD 570: An exercise for Dr.R.M.Burstall], pp.&amp;nbsp;215–216, and [http://www.cs.utexas.edu/users/EWD/ewd05xx/EWD578.PDF EWD 578: More about the function &quot;fusc&quot; (A sequel to EWD570)], pp.&amp;nbsp;230–232, reprints of notes originally written in 1976.
*{{citation
 | last1 = Gibbons | first1 = Jeremy | authorlink1 = Jeremy Gibbons
 | last2 = Lester | first2 = David
 | last3 = Bird | first3 = Richard | author3-link = Richard Bird (computer scientist)
 | doi = 10.1017/S0956796806005880
 | issue = 3
 | journal = [[Journal of Functional Programming]]
 | pages = 281–291
 | title = Functional pearl: Enumerating the rationals
 | volume = 16
 | year = 2006}}.
*{{citation
 | last = Raney | first = George N.
 | year = 1973
 | title = On continued fractions and finite automata
 | journal = [[Mathematische Annalen]]
 | volume = 206
 | pages = 265–283
 | doi = 10.1007/BF01355980}}
*{{citation
 | last = Stern | first = Moritz A. | author-link = Moritz Abraham Stern
 | journal = [[Crelle's Journal|Journal für die reine und angewandte Mathematik]]
 | pages = 193–220
 | title = Ueber eine zahlentheoretische Funktion
 | url = http://www.digizeitschriften.de/resolveppn/GDZPPN002150301
 | volume = 55
 | year = 1858}}.

==External links==
*{{mathworld|urlname=Calkin-WilfTree|title=Calkin–Wilf Tree}}
*{{mathworld|urlname=SternsDiatomicSeries|title=Stern's Diatomic Series}}
*{{citation|url=http://www.cut-the-knot.org/blue/Fusc.shtml|title=Fractions on a Binary Tree II|publisher=[[Cut-the-knot]]|first=Alexander|last=Bogomolny}}

{{DEFAULTSORT:Calkin-Wilf tree}}
[[Category:Integer sequences]]
[[Category:Trees (data structures)]]</text>
      <sha1>pypipumaylvnvgom9pqjr2c9xyfmxqm</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cover tree</title>
    <ns>0</ns>
    <id>10912009</id>
    <revision>
      <id>617564828</id>
      <parentid>586788494</parentid>
      <timestamp>2014-07-19T10:57:08Z</timestamp>
      <contributor>
        <username>Mack2</username>
        <id>1645842</id>
      </contributor>
      <comment>/* References */ fix format</comment>
      <text xml:space="preserve" bytes="3511">The '''cover tree''' is a type of [[data structure]] in [[computer science]] that is specifically designed to facilitate the speed-up of a [[nearest neighbor search]]. It is a refinement of the Navigating Net data structure, and related to a variety of other data structures developed for indexing intrinsically low-dimensional data.&lt;ref name=&quot;clarkson&quot;&gt;Kenneth Clarkson.  Nearest-neighbor searching and metric space dimensions.  In G. Shakhnarovich, T. Darrell, and P. Indyk, editors, Nearest-Neighbor Methods for Learning and Vision: Theory and Practice, pages 15--59. MIT Press, 2006.&lt;/ref&gt;

The tree can be thought of as a hierarchy of levels with the top level containing the root [[point (geometry)|point]] and the bottom level containing every point in the metric space.  Each level ''C'' is associated with an integer value ''i'' that decrements by one as the tree is descended. Each level ''C'' in the cover tree has three important properties:

*'''Nesting:''' &lt;math&gt;C_{i} \subseteq C_{i-1}&lt;/math&gt;
*'''Covering:''' For every point &lt;math&gt;p \in C_{i-1}&lt;/math&gt;, there exists a point &lt;math&gt;q \in C_{i} &lt;/math&gt; such that the distance from &lt;math&gt;p&lt;/math&gt; to &lt;math&gt;q&lt;/math&gt; is less than or equal to &lt;math&gt;2^{i}&lt;/math&gt; and exactly one such &lt;math&gt;q&lt;/math&gt; is a parent of &lt;math&gt;p&lt;/math&gt;.
*'''Separation:''' For all points &lt;math&gt;p,q \in C_i&lt;/math&gt;, the distance from &lt;math&gt;p&lt;/math&gt; to &lt;math&gt;q&lt;/math&gt; is greater than &lt;math&gt;2^{i}&lt;/math&gt;.

== Complexity ==

=== Find ===
Like other [[metric tree]]s the cover tree allows for nearest neighbor searches in &lt;math&gt;O(\eta*\log{n})&lt;/math&gt; where &lt;math&gt;\eta&lt;/math&gt; is a constant associated with the dimensionality of the dataset and n is the cardinality.  To compare, a basic linear search requires &lt;math&gt;O(n)&lt;/math&gt;, which is a much worse dependence on &lt;math&gt;n&lt;/math&gt;.  However, in high-dimensional [[metric space]]s the &lt;math&gt;\eta&lt;/math&gt; constant is non-trivial, which means it cannot be ignored in complexity analysis.  Unlike other metric trees, the cover tree has a theoretical bound on its constant that is based on the dataset's [[expansivity constant|expansion constant]] or doubling constant (in the case of approximate NN retrieval).  The bound on search time is &lt;math&gt;O(c^{12} \log{n})&lt;/math&gt; where &lt;math&gt;c&lt;/math&gt; is the expansion constant of the dataset.

=== Insert ===
Although cover trees provide faster searches than the naive approach, this advantage must be weighed with the additional cost of maintaining the data structure.  In a naive approach adding a new point to the dataset is trivial because order does not need to be preserved, but in a cover tree it can take &lt;math&gt;O(c^6 \log{n})&lt;/math&gt; time.  However, this is an upper-bound, and some techniques have been implemented that seem to improve the performance in practice.&lt;ref&gt;http://hunch.net/~jl/projects/cover_tree/cover_tree.html&lt;/ref&gt;

=== Space ===
The cover tree uses implicit representation to keep track of repeated points.  Thus, it only requires O(n) space.

==See also==
*[[Nearest neighbor search]]
*[[kd-tree]]

==References==
;Notes
{{reflist}}
;Bibliography
* Alina Beygelzimer, Sham Kakade, and John Langford.  Cover Trees for Nearest Neighbor.  In Proc. International Conference on Machine Learning (ICML), 2006.
*  [http://hunch.net/~jl/projects/cover_tree/cover_tree.html JL's Cover Tree page]. John Langford's page links to papers and code.
*  [https://github.com/DNCrane/Cover-Tree A C++ Cover Tree implementation on GitHub].

{{CS-Trees}}

[[Category:Trees (data structures)]]</text>
      <sha1>djwq045lecmg4a8vm3t2ijvb2979hrc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Dancing tree</title>
    <ns>0</ns>
    <id>1605712</id>
    <revision>
      <id>542563092</id>
      <parentid>496575352</parentid>
      <timestamp>2013-03-07T10:57:32Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 3 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q2078122]]</comment>
      <text xml:space="preserve" bytes="2533">In [[computer science]], a '''dancing tree''' is a [[tree data structure]] similar to [[B+ tree]]s. It was invented by [[Hans Reiser]], for use by the [[Reiser4]] file system.  As opposed to [[self-balancing binary search tree]]s that attempt to keep their nodes balanced at all times, dancing trees only balance their nodes when flushing data to a disk (either because of memory constraints or because a transaction has completed).&lt;ref&gt;{{cite web | url=http://www.namesys.com/v4/v4.html#dancing_tree  | title=Reiser4 release notes - Dancing Tree | author=Hans Reiser | accessdate=2009-07-22 | publisher = Archive.org, as Namesys.com is no longer accessible |archiveurl = http://web.archive.org/web/20071024001500/http://www.namesys.com/v4/v4.html#dancing_tree |archivedate = 2007-10-24}}&lt;/ref&gt;

The idea behind this is to speed up file system operations by delaying optimization of the tree and only writing to disk when necessary, as writing to disk is thousands of times slower than writing to memory.  Also, because this optimization is done less often than with other tree data structures, the optimization can be more extensive.

In some sense, this can be considered to be a self-balancing binary search tree that is optimized for storage on a slow medium, in that the on-disc form will always be balanced but will get no mid-transaction writes; doing so eases the difficulty (at the time) of adding and removing nodes, and instead performs these (slow) rebalancing operations at the same time as the (much slower) write to the storage medium.

However, a (negative) side effect of this behavior is witnessed in cases of unexpected shutdown, incomplete data writes, and other occurrences that may prevent the final (balanced) transaction from completing. In general, dancing trees will pose a greater difficulty for data recovery from incomplete transactions than a normal tree; though this can be addressed by either adding extra transaction logs or developing an algorithm to locate data on disk not previously present, then going through with the optimizations once more before continuing with any other pending operations/transactions.

== References ==
&lt;references/&gt;

== External links ==
*[http://www.namesys.com/v4/v4.html#dancing_tree ''Software Engineering Based Reiser4 Design Principles'']
*[http://nikitadanilov.blogspot.com/2006/03/reiser4-1-internal-tree.html Description of the Reiser4 internal tree]

{{CS-Trees}}

{{compu-storage-stub}}

[[Category:Computer file systems]]
[[Category:Trees (data structures)]]</text>
      <sha1>a4652jcrsjprgbptr1wcr2104kkyg5i</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Dendrogram</title>
    <ns>0</ns>
    <id>1409693</id>
    <revision>
      <id>619966983</id>
      <parentid>605990266</parentid>
      <timestamp>2014-08-05T14:56:21Z</timestamp>
      <contributor>
        <username>Maharris777</username>
        <id>5121726</id>
      </contributor>
      <minor/>
      <comment>/* Clustering example */  Small grammatical change.</comment>
      <text xml:space="preserve" bytes="1875">{{Noref|date=July 2013}}
A '''dendrogram''' (from [[Greek language|Greek]] ''dendron'' &quot;tree&quot; and ''gramma'' &quot;drawing&quot;) is a [[Tree (graph theory)|tree]] diagram frequently used to illustrate the arrangement of the clusters produced by [[hierarchical clustering]].  Dendrograms are often used in [[computational biology]] to illustrate the clustering of [[gene]]s or samples.
==Clustering example==
For a clustering example, suppose this data is to be clustered using [[Euclidean distance]] as the [[Metric (mathematics)|distance metric]].

[[Image:Clusters.svg|frame|none|Raw data]]

The [[hierarchical clustering]] dendrogram would be as such:

[[Image:Hierarchical clustering simple diagram.svg|frame|none|Traditional representation]]

The top row of nodes represents data (individual observations), and the remaining nodes represent the clusters to which the data belong, with the arrows representing the distance (dissimilarity).

The distance between merged clusters is monotone increasing with the level of the merger: the height of each node in the plot is proportional to the value of the intergroup dissimilarity between its two daughters (the top nodes representing individual observations are all plotted at zero height).

==See also==
* [[Cladogram]]
* [[Hierarchical clustering]]
* [[MEGA, Molecular Evolutionary Genetics Analysis|MEGA]], a free software for drawing dendrograms
* [[yEd]], a free software for drawing and automatically arranging dendrograms
* [http://www.applied-maths.com/bionumerics/modules/tree-and-network-inference-module/ BioNumerics] Commercial software for storing and analysis of biological data, including the creation of dendrograms.
* [[Distance matrices in phylogeny]]


[[Category:Trees (data structures)]]
[[Category:Statistical charts and diagrams]]
[[Category:Graph drawing]]
[[Category:Cluster analysis]]


{{statistics-stub}}</text>
      <sha1>5q1zit9l1d1ysrw0g07dqmc9m2arrr7</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Dialog tree</title>
    <ns>0</ns>
    <id>14675239</id>
    <revision>
      <id>607503179</id>
      <parentid>592299502</parentid>
      <timestamp>2014-05-07T17:02:03Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* Value and impact */Task 3: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <text xml:space="preserve" bytes="7462">[[File:Dialog tree example.svg|thumb|right|399px|Example of a simple dialog tree.]]
A '''dialog tree''' or '''conversation tree''' is a gameplay mechanic that is used throughout many [[adventure game]]s&lt;ref&gt;[http://pc.ign.com/articles/164/164528p1.html IGN: Escape From Monkey Island Review]&lt;/ref&gt;&lt;ref&gt;[http://www.godpatterns.com/2006/04/adventure-game-design-patterns.html Adventure Game Design Patterns at God Patterns]&lt;/ref&gt;&lt;ref&gt;[http://home.comcast.net/~ervind/sierrademos.html Strange Things in Sierra's Adventure Game Demos]&lt;/ref&gt;&lt;ref&gt;[http://games.yahoo.com/blogs/plugged-in/where-did-video-games-152.html Dreamfall: The Longest Journey Hands-On - Yahoo! Video Games]&lt;/ref&gt;&lt;ref&gt;[http://web.archive.org/web/20110526061613/http://www.gamedev.net/reference/articles/article247.asp GameDev.net - NPC Conversation Techniques]&lt;/ref&gt;&lt;ref&gt;[http://www.adventuregamers.com/article/id,508/ Adventure Gamers : Callahan's Crosstime Saloon]&lt;/ref&gt; (including [[action-adventure game]]s&lt;ref&gt;[http://web.archive.org/web/20071223122303/http://starcontrol.classicgaming.gamespy.com/history/fford.shtml The Pages of Now &amp; Forever - All About Star Control]&lt;/ref&gt;) and [[role-playing video game]]s.&lt;ref name=&quot;fundamentals&quot;/&gt; When interacting with a [[non-player character]], the player is given a choice of what to say and makes subsequent choices until the conversation ends.&lt;ref name=&quot;fundamentals&quot;/&gt; Certain [[video game genres]], such as [[visual novel]]s and [[dating sim]]s, revolve almost entirely around these character interactions and [[Nonlinear gameplay|branching dialogues]].&lt;ref name=&quot;Gamasutra&quot;/&gt;

== History ==
The concept of a dialog tree has existed long before the advent of [[video game]]s. The earliest known dialog tree is described in &quot;[[The Garden of Forking Paths]],&quot; a 1941 short story by [[Jorge Luis Borges]], in which the combination book of Ts'ui Pên allows all major outcomes from an event branch into their own chapters. Much like the game counterparts this story reconvenes as it progresses (as possible outcomes would approach n^m where n is the number of options at each fork and m is the depth of the tree).

The first computer dialogue system was featured in [[ELIZA]], a primitive [[natural language processing]] [[computer program]] written by [[Joseph Weizenbaum]] between 1964 and 1966. The program emulated interaction between the user and an [[Artificial intelligence|artificial]] therapist. With the advent of [[video game]]s, [[interactive entertainment]] have attempted to incorporate meaningful interactions with virtual characters. [[Nonlinear gameplay|Branching dialogues]] have since become a common feature in visual novels, [[dating sim]]s, [[adventure game]]s, and [[role-playing video game]]s.&lt;ref name=&quot;Gamasutra&quot;&gt;{{cite web|title=Defining Dialogue Systems|author=Brent Ellison|date=July 8, 2008|publisher=[[Gamasutra]]|url=http://www.gamasutra.com/view/feature/3719/defining_dialogue_systems.php|accessdate=2011-03-30}}&lt;/ref&gt;

== Game mechanics ==
[[File:TBS Conversation screenshot.jpg|thumb|upright=1.5|A dialog tree as implemented in the game ''[[The Banner Saga]]'': the query from the non-player character appears at the bottom, and three possible player responses at the upper left.]]
The player typically enters the gameplay mode by choosing to speak with a [[non-player character]] (or when a non-player character chooses to speak to them), and then choosing a line of pre-written dialog from a menu. Upon choosing what to say, the non-player character responds to the player, and the player is given another choice of what to say. This cycle continues until the conversation ends. The conversation may end when the player selects a farewell message, the non-player character has nothing more to add and ends the conversation, or when the player makes a bad choice (perhaps angering the non-player to leave the conversation).

Games often offer options to ask non-players to reiterate information about a topic, allowing players to replay parts of the conversation that they did not pay close enough attention to the first time.&lt;ref name=&quot;fundamentals&quot;/&gt; These conversations are said to be designed as a [[tree structure]], with players deciding between each branch of dialog to pursue. Unlike a branching story, players may return to earlier parts of a conversation tree and repeat them. Each branch point (or node) is essentially a different menu of choices, and each choice that the player makes triggers a response from the non-player character followed by a new menu of choices.

In some genres such as [[role-playing video game]]s, external factors such as charisma may influence the response of the non-player character or unlock options that would not be available to other characters.&lt;ref name=&quot;fundamentals&quot;/&gt; These conversations can have far-reaching consequences, such as deciding to disclose a valuable secret that has been entrusted to the player.&lt;ref name=&quot;fundamentals&quot;/&gt; However, these are usually not real [[tree data structure]] in programmers sense, because they contain cycles as can be seen on illustration on this page.

Certain game genres revolve almost entirely around character interactions, including [[visual novel]]s such as ''[[Ace Attorney]]'' and [[dating sim]]s such as ''[[Tokimeki Memorial]]'', usually featuring complex branching dialogues and often presenting the player's possible responses word-for-word as the [[player character]] would say them. Games revolving around relationship-building, including visual novels, dating sims such as ''Tokimeki Memorial'', and some role-playing games such as ''[[Shin Megami Tensei: Persona]]'', often give choices that have a different number of associated &quot;mood points&quot; which influence a player character's relationship and future conversations with a non-player character. These games often feature a [[Persistent world|day-night cycle]] with a time scheduling system that provides context and relevance to character interactions, allowing players to choose when and if to interact with certain characters, which in turn influences their responses during later conversations.&lt;ref name=&quot;Gamasutra&quot;/&gt;

Another variation of branching dialogues can be seen in the [[adventure game]] ''[[Culpa Innata]]'', where the player chooses a tactic at the beginning of a conversation, such as using either a formal, casual or accusatory manner, that affects the tone of the conversation and the information gleaned from the interviewee.&lt;ref name=&quot;Gamasutra&quot;/&gt;

== Value and impact ==

This mechanism allows game designers to provide interactive conversations with nonplayer characters without having to tackle the challenges of [[natural language processing]] in the field of [[artificial intelligence]].&lt;ref name=&quot;fundamentals&quot;&gt;{{cite book|last=Rollings|first=Andrew|authorlink=|author2=Ernest Adams|title=Fundamentals of Game Design|publisher=Prentice Hall|year=2006|location=|url=http://wps.prenhall.com/bp_gamedev_1/54/14053/3597646.cw/index.html}}&lt;/ref&gt;  In games such as ''[[Monkey Island (series)|Monkey Island]]'', these conversations can help demonstrate the personality of certain characters.&lt;ref name=&quot;fundamentals&quot;/&gt;

== References ==
{{Reflist|2}}

== See also ==

* [[Digital conversation]]
* [[Adventure game]]
* [[Action-adventure game]]
* [[Sierra Entertainment]]
* [[Dialog systems]]

[[Category:Adventure games]]
[[Category:Trees (data structures)]]
[[Category:Video game gameplay]]
[[Category:Role-playing game terminology]]</text>
      <sha1>1ocs6dp3uytlz4n77ay97in75k33snp</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Doubly chained tree</title>
    <ns>0</ns>
    <id>21678520</id>
    <revision>
      <id>471784266</id>
      <parentid>465867679</parentid>
      <timestamp>2012-01-17T00:59:57Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Moving category Trees (structure) to [[:Category:Trees (data structures)]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2012 January 12]].</comment>
      <text xml:space="preserve" bytes="540">In [[computer science]], a '''doubly chained tree''' is a [[Tree (data structure)|tree data structure]] in which each node has two [[pointer (computer programming)| pointers]]. Typically, each node has one pointer pointing to its [[child node|child]] and one pointing at the node to its right. Then the following node will have one pointing to its child and one to its parent.

== External links ==
*[http://www.dcs.gla.ac.uk/~iain/keith/index.htm Information Retrieval]

{{CS-Trees}}

[[Category:Trees (data structures)]]
{{Comp-sci-stub}}</text>
      <sha1>eqrdnaylk1sux0umcavwhx2xl4ky3ms</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Doubly logarithmic tree</title>
    <ns>0</ns>
    <id>25315112</id>
    <revision>
      <id>578041108</id>
      <parentid>567791221</parentid>
      <timestamp>2013-10-20T23:17:17Z</timestamp>
      <contributor>
        <username>ChrisGualtieri</username>
        <id>16333418</id>
      </contributor>
      <minor/>
      <comment>General Fixes, added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="1708">{{Orphan|date=October 2013}}

In [[computer science]] a '''doubly logarithmic tree''' is a [[Tree (data structure)|tree]] where each internal node of height 1, the tree layer above the leaves, has two children, and each internal node of height &lt;math&gt;h &gt; 1&lt;/math&gt; has &lt;math&gt;2^{2^{h-2}}&lt;/math&gt; children. Each child of the root contains &lt;math&gt;\sqrt{n} &lt;/math&gt; leaves. The number of children at a node as we go from leaf to root is 0,2,2,4,16, 256, 65536, ... {{OEIS|id=A001146}}

A similar tree called a k-merger is used in Prokop et al.'s [[cache oblivious]] [[Funnelsort]] to merge elements.

[[File:Double log tree.png]]

==Notes==
{{reflist}}

==References==
*{{citation|first1=Omer|last1=Berkman|first2=Baruch|last2=Schieber|first3=Uzi|last3=Vishkin|author3-link=Uzi Vishkin|title=Optimal doubly logarithmic parallel algorithms based on finding all nearest smaller values|journal=Journal of Algorithms|volume=14|pages=344–370|year=1993|issue=3|doi=10.1006/jagm.1993.1018}}.
*Harald Prokop. [http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.5650 Cache-Oblivious Algorithms]. Masters thesis, MIT. 1999.
*M. Frigo, C.E. Leiserson, H. Prokop, and S. Ramachandran. Cache-oblivious algorithms. In ''Proceedings of the 40th IEEE Symposium on Foundations of Computer Science'' (FOCS 99), p.&amp;nbsp;285-297. 1999. [http://ieeexplore.ieee.org/iel5/6604/17631/00814600.pdf?arnumber=814600 Extended abstract at IEEE], [http://citeseer.ist.psu.edu/307799.html at Citeseer].
*[[Erik Demaine]]. [http://courses.csail.mit.edu/6.897/spring03/scribe_notes/L17/lecture17.pdf Review of the Cache-Oblivious Sorting]. Notes for MIT Computer Science 6.897: Advanced Data Structures.

[[Category:Trees (data structures)]]</text>
      <sha1>3hey7tdgyyb0ya0fybthvi9nt0iwg5c</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Embedded Zerotrees of Wavelet transforms</title>
    <ns>0</ns>
    <id>1797175</id>
    <revision>
      <id>591852216</id>
      <parentid>591852076</parentid>
      <timestamp>2014-01-22T10:42:10Z</timestamp>
      <contributor>
        <ip>210.212.246.53</ip>
      </contributor>
      <text xml:space="preserve" bytes="5027">'''Embedded Zerotrees''' of '''Wavelet transforms''' ('''EZW''') is a lossy [[image compression]] [[algorithm]]. At low bit rates, i.e. high compression ratios, most of the coefficients produced by a [[Sub-band coding|subband transform]] (such as the [[wavelet transform]])
will be zero, or very close to zero. This occurs because &quot;real world&quot; images tend to contain mostly low frequency information (highly correlated). However where high frequency information does occur (such as edges in the image) this is particularly important in terms of human perception of the image quality, and thus must be represented accurately in any high quality coding scheme.

By considering the transformed coefficients as a [[Tree (graph theory)|tree]] (or trees) with the lowest frequency coefficients at the root node and with the children of each tree node being the spatially related coefficients in the next higher frequency subband, there is a high probability that one or more subtrees will consist entirely of coefficients which are zero or nearly zero, such subtrees are called '''zerotrees'''. Due to this, we use the terms node and coefficient interchangeably, and when we refer to the children of a coefficient, we mean the child coefficients of the node in the tree where that coefficient is located. We use '''children''' to refer to directly connected nodes lower in the tree and '''descendants''' to refer to all nodes which are below a particular node in the tree, even if not directly connected.

In zerotree based image compression scheme such as EZW and [[SPIHT]], the intent is to use the statistical properties of the trees in order to efficiently code the locations of the significant coefficients. Since most of the coefficients will be zero or close to zero, the spatial locations of the significant coefficients make up a large portion of the total size of a typical compressed image. A coefficient (likewise a tree) is considered '''significant''' if its magnitude (or magnitudes of a node and all its descendants in the case of a tree) is above a particular threshold. By starting with a threshold which is close to the maximum coefficient magnitudes and iteratively decreasing the threshold, it is possible to create a compressed representation of an image which progressively adds finer detail. Due to the structure of the trees, it is very likely that if a coefficient in a particular frequency band is insignificant, then all its descendants (the spatially related higher frequency band coefficients) will also be insignificant.

EZW uses four symbols to represent (a) a zerotree root, (b) an isolated zero (a coefficient which is insignificant, but which has significant descendants), (c) a significant positive coefficient and (d) a significant negative coefficient. The symbols may be thus represented by two binary bits. The compression algorithm consists
of a number of iterations through a '''dominant pass''' and a '''subordinate pass''', the threshold is updated (reduced by a factor of two) after each iteration. The dominant pass encodes the significance of the coefficients which have not yet been found significant in earlier iterations, by scanning the trees and emitting one of the four symbols. The children of a coefficient are only scanned if the coefficient was found to be significant, or if the coefficient was an isolated zero. The subordinate pass emits one bit (the most significant bit of each coefficient not so far emitted) for each coefficient which has been found significant in the previous significance passes. The subordinate pass is therefore similar to bit-plane coding.

There are several important features to note. Firstly, it is possible to stop the compression algorithm at any time and obtain an approximation of the original image, the greater the number of bits received, the better the image. Secondly, due to the way in which the compression algorithm is structured as a series of decisions, the same algorithm can be run at the decoder to reconstruct the coefficients, but with the decisions being taken according to the incoming bit stream. In practical implementations, it would be usual to use an entropy code such as [[Arithmetic coding|arithmetic code]] to further improve the performance of the dominant pass. Bits from the subordinate pass are usually random enough that entropy coding provides no further coding gain.

The coding performance of EZW has since been exceeded by [[SPIHT]] and its many derivatives.

==See also==
* [[Set partitioning in hierarchical trees]] (SPIHT)

==References==
*Shapiro, J. M., {{doi-inline|10.1109/78.258085|EMBEDDED IMAGE CODING USING ZEROTREES OF WAVELET COEFFICIENTS}}. [[IEEE]] Transactions on Signal Processing, Vol. 41, No. 12 (1993), p.&amp;nbsp;3445-3462.

==External links==
{{commons category|EZW}}
* http://perso.wanadoo.fr/polyvalens/clemens/ezw/ezw.html

{{Compression methods}}

[[Category:Image compression]]
[[Category:Lossless compression algorithms]]
[[Category:Trees (data structures)]]
[[Category:Wavelets]]</text>
      <sha1>lugh75xt5peovxox5mf38uvjkjb8tny</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Enfilade (Xanadu)</title>
    <ns>0</ns>
    <id>10032858</id>
    <revision>
      <id>610006589</id>
      <parentid>593810758</parentid>
      <timestamp>2014-05-24T23:57:52Z</timestamp>
      <contributor>
        <username>Nubok2</username>
        <id>21477115</id>
      </contributor>
      <comment>Fixed link</comment>
      <text xml:space="preserve" bytes="8697">'''Enfilades''' are a class of [[Tree (data structure)|tree data structures]] used in [[Project Xanadu]] &quot;Green&quot; designs of the 1970s and 1980s. Enfilades allow quick editing, versioning, retrieval and inter-comparison operations in a large, cross-linked hypertext database. The Xanadu &quot;Gold&quot; design starting in the 1990s used a related data structure called the '''Ent'''.

==Structure and properties==
Although the principles of enfilades can be applied to any tree data structure, the particular structure used in the Xanadu system was much like a [[B-Tree]]. What distinguishes enfilades is the use of '''dsps''' and '''wids''' in the indexing information within tree nodes.

Dsps are displacements, offsets or relative keys. A dsp is the difference in key between a containing node and that of a subtree or leaf. For instance, the leaf for a grid square in a map might have a certain longitude and latitude offset relative to the larger grid represented by the subtree the leaf is part of. The key of any leaf of an enfilade is found by combining all the dsps on the path down the tree to that leaf. Dsps can also be used for other context information that is imposed top-down on entire subtrees or ranges of content at once.

Wids are widths, ranges, or bounding boxes. A wid is relative to the key of a subtree or leaf, but specifies a range of addresses covering all items within the subtree. Wids identify the interesting parts of sparsely populated address spaces. In some enfilades, the wids of subtrees under a given node can overlap, and in any case, a search for data within a range of addresses must visit any subtrees whose wids intersect the search range. Wids are combined from the leaves of the tree, upward through all layers to the root (although they are maintained incrementally). Wids can also contain other summaries such as totals or maxima of data.

The relative nature of wids and dsps allows subtrees to be rearranged within an enfilade. By changing the dsp at the top of a subtree, the keys of all the data underneath are implicitly changed. Edit operations in enfilades are performed by &quot;cutting,&quot; or splitting the tree down relevant access paths, inserting, deleting or rearranging subtrees, and splicing the pieces back together. The cost of cutting and splicing operations is generally log-like in 1-D trees and between log-like and square-root-like in 2-D trees.

&lt;!-- Does this copying make an enfilade a [[persistent data structure]] &amp; [[purely functional]]? --&gt;
&lt;!-- There are different enfilades and implementations. Semantically, most enfilades are persistent but some are not. Internally, most use destructive operations like splitting, rebalancing and refcounts. I don't know
whether purely functional data structures are a semantic or underlying implementation concept. So, below, I'm adding persistent, and not mentioning purely functional. --SteveWitham --&gt;  
Subtrees can also be shared between trees, or be linked from multiple places within a tree. This makes the enfilade a fully [[persistent data structure]] with virtual copying and versioning of content. Each use of a subtree inherits a different context from the chain of dsps down to it. Changes to a copy create new nodes only along the cut paths, and leave the entire original in place. The overhead for a version is very small, a new version's tree is balanced and fast, and its storage cost is related only to changes from the original.

One-dimensional enfilades are intermediate between arrays' direct addressability and linked lists' ease of insertion, deletion and rearrangement. Multidimensional enfilades resemble loose, rearrangeable, versionable [[Quad tree]]s, [[Oct tree]]s or [[K-d tree|''k''-d trees]].

==Types of enfilades in Xanadu==
The '''Model-T''' enfilade, used in Xanadu designs before 1979, is a data structure very much like the [[Rope (computer science)|Rope]]. It stores linear sequences of characters, with easy insertion, deletion, rearrangement and versioning, but not with links or easy comparison between versions. Text is stored directly in the leaves of the enfilade.

Later Xanadu designs are more indirect: a growing pool of sharable content pieces, called the istream (invariant stream) is organized into the documents, links and versions&amp;mdash;all with virtual addresses&amp;mdash;that the users see and work on. A collection of enfilade types manages the bi-directional mapping between virtual and istream addresses. Tracing correspondences and links between documents is made possible by mapping from virtual, to invariant, and back to virtual addresses. Storing documents using shared content pieces that remember their identities and can trace back to all their appearances, is called [[Transclusion]].

The '''POOMfilade''' (permutation of order matrix) is a 2D enfilade representing a [[Permutation matrix]]. This maps virtual position in a document to istream positions in the pooled content that the document is built from. The POOM starts out an identity matrix, then each edit to the document slices and rearranges horizontal strips of the map. The POOM can be queried in the V-&gt;I or I-&gt;V directions by searching in squat, wide address ranges or tall, narrow ones.

The '''Spanfilade''' collects the union of all spans of istream content used by a document or set of documents. Taking the intersection of span-sets from two documents or versions of a document speeds up the comparison of documents. This same mechanism is used to find links from or to a document.

The '''Granfilade''' organizes the storage of all this information on disks and a network of servers.

==Trade Secret until 1999==
Enfilades (internal data structures) and istream addresses are not exposed to Xanadu's external interfaces. Enfilades were trade-secret information until the Xanadu code was made open-source in 1999, and were mentioned but not explained in some publications before that point, e.g.&lt;ref&gt;''[[Literary Machines]]: The report on, and of, Project Xanadu concerning word processing, electronic publishing, hypertext, thinkertoys, tomorrow's intellectual revolution, and certain other topics including knowledge, education and freedom'' (1981), Mindful Press, Sausalito, California.&lt;/ref&gt;

Client-server communications in the Xanadu system use vstream addresses in a format called [[Tumbler (Project Xanadu)|tumblers]].

Hence the term Enfilade is not mentioned explicitly in the [http://udanax.xanadu.com/green/febe/index.html FeBe] (Front end - Back end protocol) document, but is instead noted indirectly in [http://www.xanadu.com.au/ted/XUsurvey/xuDation.html Xanalogical Structure] and several other documents. In the aforementioned document, it is noted that xu88 was based on &quot;General Enfilade Theory&quot;.

==History==
In 1972, xu72 introduced the concept of the Enfilade. This was called the &quot;Model T Enfilade&quot;, and was used in a word-processing type interface. In 1976, xu76 implemented the &quot;tightly coupled enfilade&quot;. In 1980, the xu80 system introduced the &quot;ent&quot;, described as a versioning enfilade. In 1988, the xu88 system utilized the concept of &quot;General Enfilade Theory&quot; of [[Mark S. Miller]], [[Stuart Greene]] and [[Roger Gregory (programmer)|Roger Gregory]], described as &quot;generating data management trees with an upwardly propagating search property and simultaneously a downwardly imposable structural property&quot;. The xu88 also extended the concept of the Enfilade over a distributed network, introduced two-dimensional Enfilades, and implemented an algorithm for searching the entire [[docuverse]] for overlapping Enfilade spans. In 1992, xu92 implemented the modern concept of the ent.&lt;ref name=xudation&gt;[http://www.xanadu.com.au/ted/XUsurvey/xuDation.html Xanalogical Structure&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

==See also==
* [[Project Xanadu]]
* [[Hypertext]]
* [[Hypermedia]]
* [[Ted Nelson]]
* [[GiST|Generalized Search Tree]]

==References==
{{reflist}}

&lt;!--
* http://udanax.com/green/febe/addressing.html
* http://udanax.com/green/index.html
* http://udanax.com/green/febe/index.html
* http://udanax.com/green/febe/protocol.html
* http://www.xanadu.com.au/ted/XUsurvey/xuDation.html
--&gt;

==External links==
* [http://www.sunless-sea.net/wiki/EnfiladeTheory Introduction to Enfilades]
* [http://xanadu.com/tech Xanadu Technologies: an introduction]
* http://xanadu.com
* http://udanax.com
* http://hyperworlds.com
* [http://www.ylem.org/artists/mmosher/nelsonia.html The Cartoon History of Cyberspace]
* [http://www.sunless-sea.net/wiki/EntTheory Papers on the Ent]
* [http://www.mprove.de/diplom/ht/xuDation.html Xanalogical Structure] (mirror)

[[Category:Ted Nelson]]
[[Category:Hypertext]]
[[Category:Trees (data structures)]]</text>
      <sha1>cph7lfg9ipsm4cl8pptq23fjjwlvem4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Expectiminimax tree</title>
    <ns>0</ns>
    <id>1153192</id>
    <revision>
      <id>543881775</id>
      <parentid>516913181</parentid>
      <timestamp>2013-03-13T17:20:42Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q1384259]]</comment>
      <text xml:space="preserve" bytes="3674">An '''expectiminimax tree''' is a specialized variation of a [[minimax]] [[game tree]] for use in [[artificial intelligence]] systems that play two-player [[zero-sum]] games such as [[backgammon]], in which the outcome depends on a combination of the player's skill and [[games of chance|chance elements]] such as dice rolls. In addition to &quot;min&quot; and &quot;max&quot; nodes of the traditional minimax tree, this variant has &quot;chance&quot; (&quot;[[move by nature]]&quot;) nodes, which take the [[expected value]] of a random event occurring.&lt;ref name=&quot;RussellNorvig2009&quot;&gt;{{cite book|author1=Stuart J. Russell|author2=Peter Norvig|title=Artificial Intelligence: A Modern Approach|url=http://books.google.com/books?id=8jZBksh-bUMC&amp;pg=PA177|year=2009|publisher=Prentice Hall|isbn=978-0-13-604259-4|pages=177–178}}&lt;/ref&gt; In [[game theory]] terms, an expectiminimax tree is the game tree of an [[extensive-form game]] of [[perfect information|perfect]], but [[incomplete information]].

In the traditional [[minimax]] method, the levels of the tree alternate from max to min until the depth limit of the tree has been reached. In an expectiminimax tree, the &quot;chance&quot; nodes are interleaved with the max and min nodes. Instead of taking the max or min of the [[utility|utility values]] of their children, chance nodes take a weighted average, with the weight being the probability that that child is reached.&lt;ref name=&quot;RussellNorvig2009&quot;/&gt;

The interleaving depends on the game. Each &quot;turn&quot; of the game is evaluated as a &quot;max&quot; node (representing the AI player's turn), a &quot;min&quot; node (representing a potentially-optimal opponent's turn), or a &quot;chance&quot; node (representing a random effect or player).&lt;ref name=&quot;RussellNorvig2009&quot;/&gt;

For example, consider a game which, in each round, consists of a single dice throw, and then decisions made by first the AI player, and then another intelligent opponent. The order of nodes in this game would alternate between &quot;chance&quot;, &quot;max&quot; and then &quot;min&quot;.&lt;ref name=&quot;RussellNorvig2009&quot;/&gt;

==Pseudocode==

The expectiminimax algorithm is a variant of the [[minimax]] algorithm and was first proposed by [[Donald Michie]].&lt;ref&gt;[[Donald Michie|D. Michie]] (1966). Game-playing and game-learning automata. In L. Fox (ed.), Advances in Programming and Non-Numerical Computation, pp. 183-200.&lt;/ref&gt;
Its [[pseudocode]] is given below. 

 '''function''' expectiminimax(node, depth)
     '''if''' node is a terminal node '''or''' depth = 0
         '''return''' the heuristic value of node
     '''if''' the adversary is to play at node
         // Return value of minimum-valued child node
         '''let''' α := +∞
         '''foreach''' child of node
             α := min(α, expectiminimax(child, depth-1))
     '''else if''' we are to play at node
         // Return value of maximum-valued child node
         '''let''' α := -∞
         '''foreach''' child of node
             α := max(α, expectiminimax(child, depth-1))
     '''else if''' random event at node
         // Return weighted average of all child nodes' values
         '''let''' α := 0
         '''foreach''' child of node
             α := α + (Probability[child] * expectiminimax(child, depth-1))
     '''return''' α

Note that for random nodes, there must be a known probability of reaching each child. (For most games of chance, child nodes will be equally-weighted, which means the return value can simply be the average of all child values.)

==See also==
* [[Minimax]]
* [[Expected value]]

==References==
&lt;references/&gt;

{{DEFAULTSORT:Expectiminimax Tree}}
[[Category:Game theory]]
[[Category:Search algorithms]]
[[Category:Game artificial intelligence]]
[[Category:Trees (data structures)]]</text>
      <sha1>h7vqbdmgt2dwngvnr1c2kbf5jm7abid</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Exponential tree</title>
    <ns>0</ns>
    <id>572903</id>
    <revision>
      <id>612166784</id>
      <parentid>585941567</parentid>
      <timestamp>2014-06-09T04:30:11Z</timestamp>
      <contributor>
        <username>Samuel Marks</username>
        <id>14000026</id>
      </contributor>
      <comment>Linear space - http://user.it.uu.se/~arnea/ps/expTr.pdf</comment>
      <text xml:space="preserve" bytes="2830">{{Infobox data structure
|name=Exponential tree
|type=tree
|invented_by=[[Arne_Andersson_(computer_scientist)|Arne Andersson]]
|invented_year=1995
|space_avg=O(''n'')
|space_worst=O(''n'')
|search_avg=O(min(√log&amp;nbsp;''n'', log&amp;nbsp;''n''/log&amp;nbsp;''w''+ log&amp;nbsp;log&amp;nbsp;''n'', log&amp;nbsp;''w'' log&amp;nbsp;log&amp;nbsp;''n''))
|search_worst=O(min(√log&amp;nbsp;''n'', log&amp;nbsp;''n''/log&amp;nbsp;''w''+ log&amp;nbsp;log&amp;nbsp;''n'', log&amp;nbsp;''w'' log&amp;nbsp;log&amp;nbsp;''n''))
|insert_avg=O(min(√log&amp;nbsp;''n'', log&amp;nbsp;''n''/log&amp;nbsp;''w''+ log&amp;nbsp;log&amp;nbsp;''n'', log&amp;nbsp;''w'' log&amp;nbsp;log&amp;nbsp;''n''))
|insert_worst=O(min(√log&amp;nbsp;''n'', log&amp;nbsp;''n''/log&amp;nbsp;''w''+ log&amp;nbsp;log&amp;nbsp;''n'', log&amp;nbsp;''w'' log&amp;nbsp;log&amp;nbsp;''n''))
|delete_avg=O(min(√log&amp;nbsp;''n'', log&amp;nbsp;''n''/log&amp;nbsp;''w''+ log&amp;nbsp;log&amp;nbsp;''n'', log&amp;nbsp;''w'' log&amp;nbsp;log&amp;nbsp;''n''))
|delete_worst=O(min(√log&amp;nbsp;''n'', log&amp;nbsp;''n''/log&amp;nbsp;''w''+ log&amp;nbsp;log&amp;nbsp;''n'', log&amp;nbsp;''w'' log&amp;nbsp;log&amp;nbsp;''n''))
}}

An '''exponential tree''' is almost identical to a [[binary search tree]], with the exception that the dimension of the tree is not the same at all levels. In a normal binary search tree, each node has a dimension (''d'') of 1, and has 2&lt;sup&gt;''d''&lt;/sup&gt; children. In an exponential tree, the dimension equals the depth of the node, with the root node having a ''d''&amp;nbsp;=&amp;nbsp;1. So the second level can hold two nodes, the third can hold eight nodes, the fourth 64 nodes, and so on.

==Layout==

&quot;Exponential Tree&quot; can also refer to a method of laying out the nodes of a tree structure in n (typically 2) dimensional space. Nodes are placed closer to a baseline than their parent node, by a factor equal to the number of child nodes of that parent node (or by some sort of weighting), and scaled according to how close they are.  Thus, no matter how &quot;deep&quot; the tree may be, there is always room for more nodes, and the geometry of a subtree is unrelated to its position in the whole tree. The whole has a [[fractal]] structure.

In fact, this method of laying out a tree can be viewed as an application of the [[upper half-plane]] model of [[hyperbolic geometry]], with isometries limited to translations only.

&lt;!-- Deleted image removed: [[Image:ExponentialTree2.PNG]] --&gt;

==See also==
* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.7109 Faster deterministic sorting and searching in linear space] (Original paper from '95)
* [http://www2.parc.com/csl/groups/sda/publications/papers/Lamping-UIST94/for-web.pdf Laying out and Visualizing Large Trees Using a Hyperbolic Space]
* [http://www.ijcaonline.org/volume24/number3/pxc3873876.pdf Implementation and Performance Analysis of Exponential Tree Sorting]


{{CS-Trees}}

[[Category:Exponentials]]
[[Category:Trees (data structures)]]

{{datastructure-stub}}</text>
      <sha1>axz7hol4z0jjys4f9tbwvlb8q5s1wv1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Fenwick tree</title>
    <ns>0</ns>
    <id>22808985</id>
    <revision>
      <id>602152662</id>
      <parentid>602151693</parentid>
      <timestamp>2014-03-31T19:15:52Z</timestamp>
      <contributor>
        <username>XLinkBot</username>
        <id>6163802</id>
      </contributor>
      <comment>BOT--Reverting link addition(s) by [[:en:Special:Contributions/Devuy11|Devuy11]] to revision 597508180 (http://bitdevu.blogspot.in/ [\bblogspot\.])</comment>
      <text xml:space="preserve" bytes="4452">A '''Fenwick tree''' or '''binary indexed tree''' is a data structure providing efficient methods for calculation and manipulation of the [[prefix sum]]s of a table of values. It was proposed by [[Peter Fenwick (computer scientist)|Peter Fenwick]] in 1994.&lt;ref&gt;{{cite journal |author=Peter M. Fenwick |title=A new data structure for cumulative frequency tables |journal=Software: Practice and Experience |volume=24 |issue=3 |year=1994 |pages=327–336 |doi=10.1002/spe.4380240306 |url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.8917}}&lt;/ref&gt; Fenwick trees primarily solve the problem of balancing prefix sum calculation efficiency with element modification efficiency. The efficiency of these operations comes as a trade-off - greater efficiency in prefix sum calculation is achieved by pre-calculating values, but as more values are pre-calculated more must be re-calculated upon any modification to the underlying value table. Fenwick trees both calculate prefix sums and modify the table in &lt;math&gt;O(\log n)&lt;/math&gt; time, where &lt;math&gt;n&lt;/math&gt; is the size of the table.

==Description==
Given a table of elements, it is sometimes desirable to calculate the [[running total]] of values up to each index according to some [[Associative property|associative]] [[binary operation]] (addition on integers, for example). Fenwick trees provide a method to query the running total at any index, in addition to allowing changes to the underlying value table and having all further queries reflect those changes. Although Fenwick trees are [[Tree (data structure)|trees]] in concept, in practice they are implemented using a flat array analogous to implementations of a [[binary heap]]. Given an index in the array representing a vertex, the index of a vertex's parent or child is calculated through [[bitwise operation]]s on the [[Binary numeral system|binary]] representation of its index. Each index contains the pre-calculated sum of a section of the table, and by combining that sum with section sums encountered during an upward traversal to the root, the prefix sum is calculated. When a table value is modified, all section sums which contain the modified value are in turn modified during a similar traversal of the tree. The section sums are defined in such a way that queries and modifications to the table are executed in asymptotically equivalent time - &lt;math&gt;O(\log n)&lt;/math&gt; in the worst case.

The initial process of building the Fenwick tree over a table of values runs in &lt;math&gt;O(n \log n)&lt;/math&gt; time. Other efficient operations include locating the index of a value if all values are positive, or all indices with a given value if all values are non-negative. Also supported is the scaling of all values by a constant factor in &lt;math&gt;O(n)&lt;/math&gt; time. The sum of an arbitrary subarray can also be calculated by subtracting the prefix sum before its start point from the prefix sum at its end point.

==Applications==
Fenwick trees are used to implement the [[arithmetic coding]] algorithm. Development of operations it supports were primarily motivated by use in that case.

Using a Fenwick tree it is very easy to generate the cumulative sum table. From this cumulative sum table it is possible to calculate the summation of the frequencies in a certain range in order of &lt;math&gt;O(\log n)&lt;/math&gt;.

Fenwick tree can also be used to update and query ranges in multidimensional arrays with complexity &lt;math&gt;O(2^d* \log ^d n)&lt;/math&gt;, where d is number of dimensions and n is the number of elements along each dimension. &lt;ref&gt;{{cite journal |author=Pushkar Mishra |title=On Updating and Querying Sub-arrays of Multidimensional Arrays |year=2013 |pages=1-5 |url=http://arxiv.org/abs/1311.6093}}&lt;/ref&gt;
{{See also|Prefix sums#Applications|l1=Applications of Prefix sums}}

==See also==
* [[Prefix sums]]
* [[Segment tree]]

==References==
&lt;references/&gt;

==External links==
* [http://www.topcoder.com/tc?module=Static&amp;d1=tutorials&amp;d2=binaryIndexedTrees A tutorial on Fenwick Trees on TopCoder]
* [http://www.algorithmist.com/index.php/Fenwick_tree An article on Fenwick Trees on Algorithmist]
* [http://michaelnielsen.org/polymath1/index.php?title=Updating_partial_sums_with_Fenwick_tree An entry on Fenwick Trees on Polymath wiki]
* [http://pine.cs.yale.edu/pinewiki/OrderStatisticsTree Order-statistics tree, a related data structure]
* [http://cs.stackexchange.com/questions/10538/]
{{CS-Trees}}

[[Category:Trees (data structures)]]</text>
      <sha1>cx155zqynvr4qujwvqof1yx6ejeu4qi</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Figurative system of human knowledge</title>
    <ns>0</ns>
    <id>464119</id>
    <revision>
      <id>594609768</id>
      <parentid>594609711</parentid>
      <timestamp>2014-02-09T02:47:28Z</timestamp>
      <contributor>
        <username>I am One of Many</username>
        <id>18057899</id>
      </contributor>
      <minor/>
      <comment>Reverted 1 edit by [[Special:Contributions/70.44.187.98|70.44.187.98]] ([[User talk:70.44.187.98|talk]]) to last revision by ChrisGualtieri. ([[WP:TW|TW]])</comment>
      <text xml:space="preserve" bytes="12481">[[File:ENC SYSTEME FIGURE.jpeg|right|200px|thumb|The original &quot;figurative system of human knowledge&quot; tree, in French.]]

The '''&quot;figurative system of human knowledge&quot;''', sometimes known as '''the tree of Diderot and d'Alembert''', was a tree developed to represent the structure of [[knowledge]] itself, produced for the ''[[Encyclopédie]]'' by [[Jean le Rond d'Alembert]] and [[Denis Diderot]].

The tree was a [[Taxonomy (general)|taxonomy]] of human knowledge, inspired by [[Francis Bacon]]'s ''[[The Advancement of Learning]]''. The three main branches of knowledge in the tree are: &quot;Memory&quot;/[[History]], &quot;Reason&quot;/[[Philosophy]], and &quot;Imagination&quot;/[[Poetry]].

Notable is the fact that [[theology]] is ordered under 'Philosophy'. The historian [[Robert Darnton]] has argued that this categorization of [[religion]] as being subject to human reason, and not a source of knowledge in and of itself ([[revelation]]), was a significant factor in the controversy surrounding the work.&lt;ref&gt;Robert Darnton, &quot;Philosophers Trim the Tree of Knowledge: The Epistemological Strategy of the ''Encyclopedie'',&quot; ''The Great Cat Massacre and Other Episodes in French Cultural History'' (New York: Basic Books, Inc., 1984), 191-213.&lt;/ref&gt;  Additionally notice that 'Knowledge of God' is only a few nodes away from 'Divination' and 'Black Magic'.

The original version, in [[French (language)|French]], can be seen in the graphic on the right. An [http://www.hti.umich.edu/d/did/tree.html image of the diagram with English translations superimposed over the French text] is available. Another example of English translation of the tree is available in literature (see the reference by Schwab). Below is a version of it rendered in [[English (language)|English]] as a bulleted outline.

== ''The Tree of Diderot and d'Alembert'' ==
'''&quot;Detailed System of Human Knowledge&quot;'''
from the [[Encyclopédie]].
* [[Understanding]]
** [[Memory]].
:** [[History]].
**** [[Sacred history|Sacred]] (History of [[Prophet]]s).
**** [[History of Christianity|Ecclesiastical]].
**** [[Civilization#History|Civil]], [[Ancient history|Ancient]] and [[Modern history|Modern]].
:**** [[Civilization#History|Civil History]], properly said. ''(See also: [[Civil society#History|History of civil society]])''
:**** [[Literary History]].
::**** [[Memoirs]].
::**** [[Antiquities]]. ''(See also: [[Classical antiquity]])''
::**** Complete Histories.
**** [[Natural history|Natural]].
:**** Uniformity of Nature. ''(See: [[Uniformitarianism]])''
::**** [[Cosmology|Celestial History]].
::**** History...
******* of [[Meteoroid#History|Meteors]].
******* of the [[History of the Earth|Earth]] and the [[World Ocean|Sea]] ''(See also: [[Origin of water on Earth]])''
******* of [[Minerals]]. ''(See also:  [[Geological history of Earth]])''
******* of [[Vegetable]]s. ''(See also: [[History of agriculture]])''
******* of [[Animal]]s.  ''(See also: [[Evolutionary history of life]])''
******* of the [[Chemical element|Elements]]. ''(See also: [[Classical element]], [[History of alchemy]], and [[History of chemistry]])''
:**** Deviations of Nature.
::**** [[Celestial object|Celestial Wonders]].
::**** [[Meteoroid#Frequency of large meteors|Large Meteors]]. ''(See also: [[Asteroid]]s)''
::**** Wonders of Land and Sea. ''(See: [[Wonders of the World]])''
::**** [[Mineral#Other properties|Monstrous Mineral]]s.
::**** Monstrous Vegetables. ''(See: [[Largest organisms#Plants|Largest plants]], [[Poisonous plant]]s, and [[Carnivorous plant]]s)''
::**** Monstrous Animals. (See: ''[[Largest organisms#Animals|Largest animals]] and [[Predator]]s)''
::**** Wonders of the Elements. ''(See: [[Natural disaster]]s)''
:**** Uses of Nature (See ''[[Technology]] and [[Applied science]]s)''
::**** Arts, [[Craft]]s, Manufactures.
******* Work and Uses of [[Gold]] and [[Silver]].
:******* [[Mint (coin)|Minting]].
:******* [[Goldsmith]].
:******* Gold Spinning.
:******* Gold Drawing.
:******* [[Silversmith]]
:******* [[Planishing|Planisher]], etc.
******* Work and Uses of Precious Stones.
:******* [[Lapidary]].
:******* [[Diamond cutting]].
:******* [[Jewellery|Jeweler]], etc.
******* Work and Uses of [[Iron]].
:******* Large [[Forge]]s.
:******* [[Locksmithing|Locksmith]].
:******* Tool Making.
:******* Armorer.
:******* Gun Making, etc.
******* Work and Uses of [[Glass]].
:******* [[Glass]]making.
:******* [[Plate-Glass]]making.
:******* [[Mirror#Manufacture|Mirror Making]].
:******* [[Optician]].
:******* [[Glazier]], etc.
******* Work and Uses of Skin.
:******* [[Tanner (occupation)|Tanner]].
:******* [[Chamois leather|Chamois Maker]].
:******* Leather Merchant.
:******* [[Glove]] Making, etc.
******* Work and Uses of [[Stonemasonry|Stone]], [[Plaster#Uses|Plaster]], [[Slate#Uses|Slate]], etc.
:******* Practical [[Architecture]].
:******* Practical [[Sculpture]].
:******* [[Masonry|Mason]].
:******* [[Tiler]], etc.
******* Work and Uses of [[Silk#Uses|Silk]].
:******* Spinning.
:******* Milling.
:******* Work like.
:******* [[Velvet]].
:******* Brocaded Fabrics, etc.
******* Work and Uses of [[Wool]].
:******* Cloth-Making.
:******* Bonnet-Making, etc.
******* Working and Uses, etc.
** [[Reason]]
:** [[Philosophy]]
**** General [[Metaphysics]], or [[Ontology]], or Science of Being in General, of Possibility, of [[Existence]], of Duration, etc.
**** Science of [[God]].
:**** [[Natural Theology]].
:**** Revealed [[Theology]].
:**** Science of Good and Evil Spirits.
::**** [[Divination]].
::**** [[Black Magic]].
**** Science of Man.
:**** [[Pneumatology]] or Science of the [[Soul]].
::**** Reasonable.
::**** Sensible.
:**** [[Logic]].
::**** Art of [[Outline of thought|Thinking]].
******* [[Apprehension (understanding)|Apprehension]].
:******* Science of [[Idea]]s
******* [[Judgement]].
:******* Science of [[Proposition]]s.
******* [[Reasoning]].
:******* [[Inductive reasoning|Induction]].
******* [[Reasoning#Logical_reasoning_methods_and_argumentation|Method]].
:******* Demonstration.
::******* [[Analysis]].
::******* [[:wikt:-synthesis|Synthesis]].
::**** Art of Remembering.
******* [[Memory]].
:******* Natural.
:******* [[Art of memory|Artificial]].
::******* Prenotion.
::******* Emblem.
******* Supplement to Memory.
:******* [[Writing]].
:******* [[Printing]].
::******* [[Alphabet]].
::******* Cipher.
:::******* Arts of [[Writing]], Printing, [[Reading (process)|Reading]], Deciphering.
*********** [[Orthography]].
::**** Art of [[Communication]]
******* Science of the Instrument of [[Discourse]].
:******* [[Grammar]].
::******* [[Sign]]s.
:::******* [[Gesture]].
*********** [[Mime|Pantomime]].
*********** Declamation.
:::******* Characters.
*********** [[Ideogram]]s.
*********** [[Hieroglyphics]].
*********** [[Heraldry]] or Blazonry.
::******* [[Prosody (linguistics)|Prosody]].
::******* Construction.
::******* [[Syntax]].
::******* [[Philology]].
::******* Critique.
:******* [[Pedagogy]].
::******* [[Curriculum|Choice of Studies]].
::******* [[Teaching method|Manner of Teaching]].
******* Science of Qualities of [[Discourse]].
:******* [[Rhetoric]].
:******* Mechanics of [[Poetry]].
:**** [[Outline of ethics|Ethics]].
::**** [[Contemporary ethics|General]].
******* General Science of [[Good and evil|Good and Evil]], of duties in general, of [[Virtue]], of the necessity of being Virtuous, etc.
::**** [[Outline of ethics#Branches of ethics|Particular]].
******* Science of [[Law]]s or [[Jurisprudence]].
:******* [[Natural law|Natural]].
:******* [[Economic forces|Economic]]. ''(See also [[commercial law]])''
:******* [[Politics|Political]]. ''(See also [[political law]])''
::******* [[Domestic politics|Internal]] and [[International politics|External]]. ''(See also [[foreign policy]])''
::******* [[Commerce]] on Land and [[Maritime industry|Sea]].
**** [[Natural science|Science of Nature]]
:**** [[Metaphysics]] of Bodies or, General Physics, of Extent, of Impenetrability, of Movement, of Word, etc.
:**** [[Outline of mathematics|Mathematics]].
::**** [[Pure mathematics|Pure]].
******* [[Outline of arithmetic|Arithmetic]].
:******* [[Number|Numeric]].
:******* [[Algebra]].
::******* [[Elementary algebra|Elementary]].
::******* [[Infinitesimal]].
:::******* [[Differential algebra|Differential]].
:::******* [[Integral]].
******* [[Outline of geometry|Geometry]].
:******* Elementary (Military Architecture, Tactics).
:******* Transcendental (Theory of Courses).
::**** Mixed.
******* [[Mechanics]].
::******* [[Statics]].
:::******* Statics, properly said.
:::******* [[Hydrostatics]].
::******* [[Dynamics (mechanics)|Dynamics]].
:::******* Dynamics, properly said.
:::******* [[Ballistics]].
:::******* [[Hydrodynamics]].
*********** [[Hydraulics]].
*********** [[Navigation]], Naval Architecture.
******* Geometric [[Astronomy]].
:******* [[Cosmography]].
::******* [[Celestial cartography|Uranography]].
::******* [[Geography]].
::******* [[Hydrography]].
:******* [[Chronology]].
:******* [[Gnomon]]ics.
******* [[Optics]].
:******* Optics, properly said.
:******* [[Dioptrics]], Perspective.
:******* [[Catoptrics]].
******* [[Acoustics]].
******* [[Pneumatics]].
******* Art of Conjecture. [[probability|Analysis of Chance]].
::**** Physicomathematics.
:**** Particular Physics.
::**** [[Outline of zoology|Zoology]].
******* [[Anatomy]].
:******* Simple.
:******* [[Comparative anatomy|Comparative]].
******* [[Physiology]].
******* [[Outline of medicine|Medicine]].
:******* Hygiene.
::******* [[Hygiene]], properly said.
::******* Cosmetics (Orthopedics).
::******* Athletics (Gymnastics).
:******* Pathology.
:******* Semiotics.
:******* Treatment.
::******* Diete.
::******* [[Surgery]].
::******* Pharmacy.
******* [[Veterinary medicine|Veterinary Medicine]].
******* [[Horse care|Horse Management]].
******* [[Hunting]].
******* [[Outline of fishing|Fishing]].
******* [[Falconry]].
::**** Physical [[Astronomy]].
******* [[Astrology]].
:******* Judiciary Astrology.
:******* Physical Astrology.
::**** [[Meteorology]].
::**** [[Cosmology]].
******* Uranology.
******* [[Aerology]].
******* [[Geology]].
******* [[Hydrology]].
::**** [[Botany]].
******* [[Agriculture]].
******* [[Gardening]].
::**** [[Mineralogy]].
::**** [[Chemistry]].
******* Chemistry, properly said, ([[Pyrotechnics]], Dyeing, etc.).
******* [[Metallurgy]].
******* [[Alchemy]].
******* Natural Magic.
** Imagination.
:** [[Poetry]].
**** Profane.
:**** Narrative.
::**** [[Epic poetry|Epic Poem]]
::**** [[Madrigal (poetry)|Madrigal]]
::**** [[Epigram]]
::**** [[Novel]], etc.
:**** [[Drama]]tic
::**** [[Tragedy]]
::**** [[Comedy]]
::**** [[Pastoral]], etc.
:**** Parable
::**** [[Allegory]]
(NOTE: THIS NEXT BRANCH SEEMS TO BELONG TO BOTH THE NARRATIVE AND DRAMATIC TREE AS DEPICTED BY THE LINE DRAWN CONNECTING THE TWO.)
:**** [[Outline of music|Music]]
::**** [[Music theory|Theoretical]]
::**** Practical ''(see also [[musical technique]])''
::***** [[Instrumental]]
::***** [[vocal music|Vocal]]
:**** [[Outline of painting|Painting]]
:**** [[Outline of sculpture|Sculpture]]
:**** [[Engraving]]

==See also==
*[[Instauratio magna]]
*[[Propædia]]

==Notes==
&lt;references/&gt;

==References==
{{refbegin}}
* Robert Darnton, &quot;Epistemological angst: From encyclopedism to advertising,&quot; in Tore Frängsmyr, ed., ''The structure of knowledge: classifications of science and learning since the Renaissance'' (Berkeley, CA: Office for the History of Science and Technology, University of California, Berkeley, 2001).
* Adams, David (2006) 'The Système figuré des Connaissances humaines and the structure of Knowledge in the Encyclopédie',  in Ordering the World, ed. Diana Donald and Frank O'Gorman, London: Macmillan, p.&amp;nbsp;190-215. 
* ''Preliminary discourse to the Encyclopedia of Diderot'', Jean Le Rond d'Alembert, translated by Richard N. Schwab, 1995. ISBN 0-226-13476-8
{{refend}}

==External links==
* [http://artfl.uchicago.edu/cactus/ ESSAI D'UNE DISTRIBUTION GÉNÉALOGIQUE DES SCIENCES ET DES ARTS PRINCIPAUX, published as a fold-out frontspiece in volume 1 of Pierre Mouchon, ''Table analytique et raisonnée des matieres contenues dans les XXXIII volumes in-folio du Dictionnaire des sciences, des arts et des métiers, et dans son supplément'', Paris, Panckoucke 1780.]
* http://commons.wikimedia.org/wiki/File:System-der-kenntnisse-des-menschen.jpg

{{DEFAULTSORT:Figurative System Of Human Knowledge}}
[[Category:Taxonomy]]
[[Category:Age of Enlightenment]]
[[Category:Trees (data structures)]]
[[Category:Knowledge representation]]</text>
      <sha1>fweeo0u88ffioyf59db7l5b7vm8nwlt</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Finger tree</title>
    <ns>0</ns>
    <id>15262012</id>
    <revision>
      <id>624138842</id>
      <parentid>621267868</parentid>
      <timestamp>2014-09-04T09:57:22Z</timestamp>
      <contributor>
        <ip>176.199.213.69</ip>
      </contributor>
      <comment>/* External links */</comment>
      <text xml:space="preserve" bytes="4529">{{for|the binary search tree|finger search tree}}

A '''finger tree''' is a [[purely functional]] [[data structure]] used in efficiently implementing other functional data structures. &lt;!-- They were invented in 2006 by Ralf Hinze and Ross Paterson, and have been --&gt; A finger tree gives [[amortized constant time]] access to the &quot;fingers&quot; (leaves) of the tree, where data is stored, and also stores in each internal node the result of applying some [[monoid|associative operation]] to its descendants.  This &quot;summary&quot; data stored in the internal nodes can be used to provide the functionality of data structures other than trees. For example, a [[priority queue]] can be implemented by labeling the internal nodes by the minimum priority of its children in the tree, or an indexed list/array can be implemented with a labeling of nodes by the count of the leaves in their children. 

Finger trees can provide amortized O(1) pushing, reversing, popping, O(log n) append and split; and can be adapted to be indexed or ordered sequences. And like all functional data structures, it is inherently [[persistent data structure|persistent]]; that is, older versions of the tree are always preserved.

They have since been used in the [[Haskell (programming language)|Haskell]] core libraries (in the implementation of ''Data.Sequence''), and an implementation in [[OCaml]] exists&lt;ref&gt;[http://alan.petitepomme.net/cwn/2007.10.30.html#5 Caml Weekly News&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; which was derived from a proven-correct [[Coq]] specification;&lt;ref&gt;[http://mattam.org/research/russell/fingertrees.en.html Matthieu Sozeau :: Dependent Finger Trees in Coq&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; and a [http://dnovatchev.spaces.live.com/blog/cns!44B0A32C2CCF7488!582.entry C# implementation of finger trees] was published in 2008; the [[Yi (editor)|Yi]] [[text editor]] specializes finger trees to finger strings for efficient storage of buffer text. Finger trees can be implemented with or without&lt;ref&gt;{{citation|last1=Kaplan|first1=H.|last2=Tarjan|first2=R. E.|authorlink2=Robert Tarjan|year=1995|contribution=Persistent lists with catenation via recursive slow-down|title=Proceedings of the Twenty-Seventh Annual ACM Symposium on the Theory of Computing|pages=93–102}}.&lt;/ref&gt;[[lazy evaluation]], but laziness allows for simpler implementations.

They were first published in 1977 by [[Leonidas J. Guibas]],&lt;ref&gt;{{citation|last1=Guibas|first1=L. J.|authorlink1=Leonidas J. Guibas|last2=McCreight|first2=E. M.|last3=Plass|first3=M. F.|last4=Roberts|first4=J. R.|year=1977|contribution=A new representation for linear lists|title=Conference Record of the Ninth Annual ACM Symposium on Theory of Computing|pages=49–60}}.&lt;/ref&gt; and periodically refined since (e.g. a version using [[AVL trees]],&lt;ref&gt;{{citation|last=Tsakalidis|first=A. K.|year=1985|title=AVL-trees for localized search|journal=Information and Control|volume=67|issue=1-3|pages=173–194|doi=10.1016/S0019-9958(85)80034-6}}.&lt;/ref&gt; non-lazy finger trees, simpler 2-3 finger trees,&lt;ref&gt;{{citation|title=Finger Trees: A Simple General-purpose Data Structure|first1=Ralf|last1=Hinze|first2=Ross|last2=Paterson|journal=[[Journal of Functional Programming]]|volume=16|issue=2|year=2006|pages=197–217|url=http://www.soi.city.ac.uk/~ross/papers/FingerTree.pdf|doi=10.1017/S0956796805005769}}.&lt;/ref&gt; B-Trees and so on)

==See also==
* [[Monoid]]

==References==
{{reflist}}

==External links==
* http://www.soi.city.ac.uk/~ross/papers/FingerTree.html
* http://hackage.haskell.org/packages/archive/EdisonCore/1.2.1.1/doc/html/Data-Edison-Concrete-FingerTree.html
* [http://blogs.msdn.com/ericlippert/archive/2008/02/12/immutability-in-c-part-eleven-a-working-double-ended-queue.aspx Example of 2-3 trees in C#]
* [http://code.google.com/p/jfingertree/ Example of Hinze/Paterson Finger Trees in Java]
* [http://dnovatchev.wordpress.com/2008/07/20/the-swiss-army-knife-of-data-structures-in-c/ Example of Hinze/Paterson Finger Trees in C#]
* [http://apfelmus.nfshost.com/monoid-fingertree.html &quot;Monoids and Finger Trees in Haskell&quot;]
* [https://github.com/clojure/data.finger-tree &quot;Finger tree library for Clojure&quot;]
* [https://github.com/scalaz/scalaz/blob/4bacca9d9aba7c2f0f613c3e5e57d3442b652b21/core/src/main/scala/scalaz/FingerTree.scala &quot;Finger tree in Scalaz&quot;]
* [http://afp.sourceforge.net/entries/Finger-Trees.shtml &quot;Verified Finger Trees in Isabelle/HOL&quot;]

{{CS-Trees}}

[[Category:Trees (data structures)]]
[[Category:Functional data structures]]


{{datastructure-stub}}</text>
      <sha1>olxl2a6cnhp24cjp13hc8vngeaw192p</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Fusion tree</title>
    <ns>0</ns>
    <id>1051942</id>
    <revision>
      <id>585893722</id>
      <parentid>583816477</parentid>
      <timestamp>2013-12-13T11:52:09Z</timestamp>
      <contributor>
        <ip>46.237.218.145</ip>
      </contributor>
      <comment>/* Approximating the sketch */</comment>
      <text xml:space="preserve" bytes="11383">A '''fusion tree''' is a type of [[tree data structure]] that implements an [[associative array]] on ''w''-bit integers. It uses ''O''(''n'') space and performs searches in ''O''(log&lt;sub&gt;''w''&lt;/sub&gt; ''n'') time, which is asymptotically faster than a traditional [[self-balancing binary search tree]], and actually better than the [[van Emde Boas tree]] when ''w'' is large. It achieves this speed by exploiting certain constant-time operations that can be done on a [[machine word]]. Fusion trees were invented in 1990 by [[Michael Fredman]] and [[Dan Willard]].&lt;ref&gt;M. L. Fredman and D. E. Willard. BLASTING through the information theoretic barrier with FUSION TREES. Proceedings of the twenty-second annual ACM symposium on Theory of Computing, 1-7, 1990.&lt;/ref&gt;

Several advances have been made since Fredman and Willard's original 1990 paper. In 1999 &lt;ref&gt;A. Andersson, P. B. Miltersen, and M. Thorup. Fusion trees can be implemented with AC0 instructions only. Theoretical Computer Science, 215:337-344, 1999.&lt;/ref&gt; it was shown how to implement fusion trees under the AC&lt;sup&gt;0&lt;/sup&gt; model, in which multiplication no longer takes constant time. A dynamic version of fusion trees using [[Hash tables]] was proposed in 1996 &lt;ref&gt;R. Raman. Priority queues: Small, monotone, and trans-dichotomous. Algorithms - ESA ’96, 121-137, 1996.&lt;/ref&gt; which matched the ''O''(log&lt;sub&gt;''w''&lt;/sub&gt; ''n'') runtime in expectation. Another dynamic version using [[Exponential tree]] was proposed in 2007 &lt;ref&gt;A. Andersson and M. Thorup. Dynamic ordered sets with exponential search trees. Journal of the ACM, 54:3:13, 2007.&lt;/ref&gt; which yields worst-case runtimes of ''O''(log&lt;sub&gt;''w''&lt;/sub&gt; ''n'' + log log ''u'') per operation, where ''u'' is the size of the largest key. It remains open whether dynamic fusion trees can achieve ''O''(log&lt;sub&gt;''w''&lt;/sub&gt; ''n'') per operation with high probability.

== How it works ==
A fusion tree is essentially a [[B-tree]] with branching factor of ''w''&lt;sup&gt;1/5&lt;/sup&gt; (any small exponent is also possible), which gives it a height of ''O''(log&lt;sub&gt;''w''&lt;/sub&gt; ''n''). To achieve the desired runtimes for updates and queries, the fusion tree must be able to search a node containing up to ''w''&lt;sup&gt;1/5&lt;/sup&gt; keys in constant time. This is done by compressing (&quot;sketching&quot;) the keys so that all can fit into one machine word, which in turn allows comparisons to be done in parallel. The rest of this article will describe the operation of a static Fusion Tree; that is, only queries are supported.

== Sketching ==
Sketching is the method by which each ''w''-bit key at a node containing ''k'' keys is compressed into only ''k''-1 bits. Each key ''x'' may be thought of as a path in the full binary tree of height ''w'' starting at the root and ending at the leaf corresponding to ''x''. To distinguish two paths, it suffices to look at their branching point (the first bit where the two keys differ). All ''k'' paths together have ''k''-1 branching points, so at most ''k''-1 bits are needed to distinguish any two of the ''k'' keys.

[[File:FusionTreeSketch.gif|center|Visualization of the sketch function.]]

An important property of the sketch function is that it preserves the order of the keys. That is, &lt;code&gt;sketch&lt;/code&gt;(''x'') &amp;lt; &lt;code&gt;sketch&lt;/code&gt;(''y'') for any two keys ''x'' &amp;lt; ''y''.

== Approximating the sketch ==
If the locations of the sketch bits are ''b''&lt;sub&gt;1&lt;/sub&gt; &amp;lt; ''b''&lt;sub&gt;2&lt;/sub&gt; &amp;lt; &amp;middot;&amp;middot;&amp;middot; &amp;lt; ''b''&lt;sub&gt;''r''&lt;/sub&gt;, then the sketch of the key ''x''&lt;sub&gt;''w''-1&lt;/sub&gt;&amp;middot;&amp;middot;&amp;middot;''x''&lt;sub&gt;1&lt;/sub&gt;''x''&lt;sub&gt;0&lt;/sub&gt; is the ''r''-bit integer &lt;math&gt;x_{b_r}x_{b_{r-1}}\cdots x_{b_1}&lt;/math&gt;.

With only standard word operations, such as those of the [[C programming language]], it is difficult to directly compute the sketch of a key in constant time. Instead, the sketch bits can be packed into a range of size at most ''r''&lt;sup&gt;4&lt;/sup&gt;, using [[bitwise AND]] and multiplication. The bitwise AND operation serves to clear all non-sketch bits from the key, while the multiplication shifts the sketch bits into a small range. Like the &quot;perfect&quot; sketch, the approximate sketch preserves the order of the keys.

Some preprocessing is needed to determine the correct multiplication constant. Each sketch bit in location ''b''&lt;sub&gt;''i''&lt;/sub&gt; will get shifted to ''b''&lt;sub&gt;''i''&lt;/sub&gt; + ''m''&lt;sub&gt;''i''&lt;/sub&gt; via a multiplication by ''m'' = &lt;math&gt;\textstyle\sum_{i=1}^r&lt;/math&gt; 2&lt;sup&gt;''m''&lt;sub&gt;''i''&lt;/sub&gt;&lt;/sup&gt;. For the approximate sketch to work, the following three properties must hold:
# ''b''&lt;sub&gt;''i''&lt;/sub&gt; + ''m''&lt;sub&gt;''j''&lt;/sub&gt; are distinct for all pairs (''i'', ''j''). This will ensure that the sketch bits are uncorrupted by the multiplication.
# ''b''&lt;sub&gt;''i''&lt;/sub&gt; + ''m''&lt;sub&gt;''j''&lt;/sub&gt; is a strictly increasing function of ''i''. That is, the order of the sketch bits is preserved.
# (''b''&lt;sub&gt;''r''&lt;/sub&gt; + ''m''&lt;sub&gt;''r''&lt;/sub&gt;) - (''b''&lt;sub&gt;1&lt;/sub&gt; - ''m''&lt;sub&gt;1&lt;/sub&gt;) ≤ ''r''&lt;sup&gt;4&lt;/sup&gt;. That is, the sketch bits are packed into a range of size at most ''r''&lt;sup&gt;4&lt;/sup&gt;.

An inductive argument shows how the ''m''&lt;sub&gt;''i''&lt;/sub&gt; can be constructed. Let ''m''&lt;sub&gt;1&lt;/sub&gt; = ''w'' &amp;minus; ''b''&lt;sub&gt;1&lt;/sub&gt;. Suppose that 1 &amp;lt; ''t'' ≤ ''r'' and that ''m''&lt;sub&gt;1&lt;/sub&gt;, ''m''&lt;sub&gt;2&lt;/sub&gt;... ''m''&lt;sub&gt;''t-1''&lt;/sub&gt; have already been chosen. Then pick the smallest integer ''m''&lt;sub&gt;''t''&lt;/sub&gt; such that both properties (1) and (2) are satisfied. Property (1) requires that ''m''&lt;sub&gt;''t''&lt;/sub&gt; ≠ ''b''&lt;sub&gt;''i''&lt;/sub&gt; &amp;minus; ''b''&lt;sub&gt;''j''&lt;/sub&gt; + ''m''&lt;sub&gt;''l''&lt;/sub&gt; for all 1 ≤ ''i'', ''j'' ≤ ''r'' and 1 ≤ ''l'' ≤ ''t''-1. Thus, there are less than ''tr''&lt;sup&gt;2&lt;/sup&gt; ≤ ''r''&lt;sup&gt;3&lt;/sup&gt; values that ''m''&lt;sub&gt;''t''&lt;/sub&gt; must avoid. Since ''m''&lt;sub&gt;''t''&lt;/sub&gt; is chosen to be minimal, (''b''&lt;sub&gt;''t''&lt;/sub&gt; + ''m''&lt;sub&gt;''t''&lt;/sub&gt;) ≤ (''b''&lt;sub&gt;''t''-1&lt;/sub&gt; + ''m''&lt;sub&gt;''t''-1&lt;/sub&gt;) + ''r''&lt;sup&gt;3&lt;/sup&gt;. This implies Property (3).

The approximate sketch is thus computed as follows:
# Mask out all but the sketch bits with a bitwise AND.
# Multiply the key by the predetermined constant ''m''. This operation actually requires two machine words, but this can still by done in constant time.
# Mask out all but the shifted sketch bits. These are now contained in a contiguous block of at most ''r''&lt;sup&gt;4&lt;/sup&gt; &amp;lt; ''w''&lt;sup&gt;4/5&lt;/sup&gt; bits.

For the rest of this article, sketching will be taken to mean approximate sketching.

== Parallel comparison ==
The purpose of the compression achieved by sketching is to allow all of the keys to be stored in one ''w''-bit word. Let the ''node sketch'' of a node be the bit string

:1&lt;code&gt;sketch&lt;/code&gt;(''x''&lt;sub&gt;1&lt;/sub&gt;)1&lt;code&gt;sketch&lt;/code&gt;(''x''&lt;sub&gt;2&lt;/sub&gt;)...1&lt;code&gt;sketch&lt;/code&gt;(''x''&lt;sub&gt;''k''&lt;/sub&gt;)

We can assume that the sketch function uses exactly ''b'' ≤ ''r''&lt;sup&gt;4&lt;/sup&gt; bits. Then each block uses 1 + ''b'' ≤ ''w''&lt;sup&gt;4/5&lt;/sup&gt; bits, and since ''k'' ≤ ''w''&lt;sup&gt;1/5&lt;/sup&gt;, the total number of bits in the node sketch is at most ''w''.

A brief notational aside: for a bit string ''s'' and nonnegative integer ''m'', let ''s''&lt;sup&gt;''m''&lt;/sup&gt; denote the concatenation of ''s'' to itself ''m'' times. If ''t'' is also a bit string ''st'' denotes the concatenation of ''t'' to ''s''.

The node sketch makes it possible to search the keys for any ''b''-bit integer ''y''. Let ''z'' = (0''y'')&lt;sup&gt;''k''&lt;/sup&gt;, which can be computed in constant time (multiply ''y'' by the constant (0&lt;sup&gt;''b''&lt;/sup&gt;1)&lt;sup&gt;''k''&lt;/sup&gt;). Note that 1&lt;code&gt;sketch&lt;/code&gt;(''x''&lt;sub&gt;''i''&lt;/sub&gt;) - 0''y'' is always positive, but preserves its leading 1 iff &lt;code&gt;sketch&lt;/code&gt;(''x''&lt;sub&gt;''i''&lt;/sub&gt;) ≥ ''y''. We can thus compute the smallest index ''i'' such that &lt;code&gt;sketch&lt;/code&gt;(''x''&lt;sub&gt;''i''&lt;/sub&gt;) ≥ ''y'' as follows:

# Subtract ''z'' from the node sketch.
# Take the bitwise AND of the difference and the constant (10&lt;sup&gt;''b''&lt;/sup&gt;)&lt;sup&gt;''k''&lt;/sup&gt;. This clears all but the leading bit of each block.
# Find the [[most significant bit]] of the result.
# Compute ''i'', using the fact that the leading bit of the ''i''-th block has index ''i''(''b''+1).

== Desketching ==
For an arbitrary query ''q'', parallel comparison computes the index ''i'' such that
:&lt;code&gt;sketch&lt;/code&gt;(''x''&lt;sub&gt;''i''-1&lt;/sub&gt;) ≤ &lt;code&gt;sketch&lt;/code&gt;(''q'') ≤ &lt;code&gt;sketch&lt;/code&gt;(''x''&lt;sub&gt;''i''&lt;/sub&gt;)
Unfortunately, the sketch function is not in general order-preserving outside the set of keys, so it is not necessarily the case that ''x''&lt;sub&gt;''i''-1&lt;/sub&gt; ≤ ''q'' ≤ ''x''&lt;sub&gt;''i''&lt;/sub&gt;. What is true is that, among all of the keys, either ''x''&lt;sub&gt;''i''-1&lt;/sub&gt; or ''x''&lt;sub&gt;''i''&lt;/sub&gt; has the longest common prefix with ''q''. This is because any key ''y'' with a longer common prefix with ''q'' would also have more sketch bits in common with ''q'', and thus &lt;code&gt;sketch&lt;/code&gt;(''y'') would be closer to &lt;code&gt;sketch&lt;/code&gt;(''q'') than any &lt;code&gt;sketch&lt;/code&gt;(''x''&lt;sub&gt;''j''&lt;/sub&gt;).

The length longest common prefix between two ''w''-bit integers ''a'' and ''b'' can be computed in constant time by finding the most significant bit of the [[bitwise XOR]] between ''a'' and ''b''. This can then be used to mask out all but the longest common prefix.

Note that ''p'' identifies exactly where ''q'' branches off from the set of keys. If the next bit of ''q'' is 0, then the successor of ''q'' is contained in the ''p''1 subtree, and if the next bit of ''q'' is 1, then the predecessor of ''q'' is contained in the ''p''0 subtree. This suggests the following algorithm:

# Use parallel comparison to find the index ''i'' such that &lt;code&gt;sketch&lt;/code&gt;(''x''&lt;sub&gt;''i''-1&lt;/sub&gt;) ≤ &lt;code&gt;sketch&lt;/code&gt;(''q'') ≤ &lt;code&gt;sketch&lt;/code&gt;(''x''&lt;sub&gt;''i''&lt;/sub&gt;).
# Compute the longest common prefix ''p'' of ''q'' and either ''x''&lt;sub&gt;''i''-1&lt;/sub&gt; or ''x''&lt;sub&gt;''i''&lt;/sub&gt; (taking the longer of the two).
# Let ''l''-1 be the length of the longest common prefix ''p''.
## If the ''l''-th bit of ''q'' is 0, let ''e'' = ''p''10&lt;sup&gt;''w''-''l''&lt;/sup&gt;. Use parallel comparison to search for the successor of &lt;code&gt;sketch&lt;/code&gt;(''e''). This is the actual predecessor of ''q''.
## If the ''l''-th bit of ''q'' is 1, let ''e'' = ''p''01&lt;sup&gt;''w''-''l''&lt;/sup&gt;. Use parallel comparison to search for the predecessor of &lt;code&gt;sketch&lt;/code&gt;(''e''). This is the actual successor of ''q''.
# Once either the predecessor or successor of ''q'' is found, the exact position of ''q'' among the set of keys is determined.

== References ==
{{reflist}}
* [http://theory.csail.mit.edu/classes/6.897/spring03/scribe_notes/L4/lecture4.pdf MIT CS 6.897: Advanced Data Structures: Lecture 4, Fusion Trees], Prof. Erik Demaine (Spring 2003)
* [http://theory.csail.mit.edu/classes/6.897/spring03/scribe_notes/L5/lecture5.pdf MIT CS 6.897: Advanced Data Structures: Lecture 5, More fusion trees; self-organizing data structures, move-to-front, static optimality], Prof. Erik Demaine (Spring 2003)
* [http://courses.csail.mit.edu/6.851/spring07/scribe/lec13.pdf MIT CS 6.851: Advanced Data Structures: Lecture 13, Fusion Tree notes], Prof. Erik Demaine (Spring 2007)
* [http://courses.csail.mit.edu/6.851/spring12/scribe/lec12.pdf MIT CS 6.851: Advanced Data Structures: Lecture 12, Fusion Tree notes], Prof. Erik Demaine (Spring 2012)

{{CS-Trees}}

[[Category:Trees (data structures)]]
[[Category:Associative arrays]]</text>
      <sha1>grrm02xdpztdn52nc4h7sxk33r0w6u9</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>GiST</title>
    <ns>0</ns>
    <id>3003657</id>
    <revision>
      <id>604411658</id>
      <parentid>580059686</parentid>
      <timestamp>2014-04-16T06:32:11Z</timestamp>
      <contributor>
        <username>Edward</username>
        <id>4261</id>
      </contributor>
      <minor/>
      <comment>link [[lossy compression]] using [[User:Edward/Find link|Find link]]</comment>
      <text xml:space="preserve" bytes="5064">In computing, '''GiST''' or Generalized Search Tree, is a [[data structure]] and [[API]] that can be used to build a variety of disk-based [[Tree search algorithm|search trees]].  GiST is a generalization of the [[B+ tree]], providing a concurrent and recoverable height-balanced search tree infrastructure without making any assumptions about the type of data being stored, or the queries being serviced.  GiST can be used to easily implement a range of well-known indexes, including  [[B+ tree]]s, [[R-tree]]s, [[hB-tree]]s, [[RD-tree]]s, and many others; it also allows for easy development of specialized indexes for new data types. It cannot be used directly to implement non-height-balanced trees such as [[Quadtree|quad trees]] or [[trie|prefix trees]] (tries), though like prefix trees it does support  compression, including [[lossy compression]]. GiST can be used for any data type that can be naturally ordered into a hierarchy of [[superset]]s. Not only is it extensible in terms of data type support and tree layout, it allows the extension writer to support any query predicates that they choose.  The most widely used GiST implementation is in the [[PostgreSQL]] [[relational database]]; it was also implemented in the [[Informix]] Universal Server, and as a standalone library, libgist.

GiST is an example of software [[extensibility]] in the context of database systems: it allows the easy evolution of a database system to support new tree-based indexes.  It achieves this by factoring out its core system infrastructure from a narrow [[API]] that is sufficient to capture the application-specific aspects of a wide variety of index designs. The GiST infrastructure code manages the layout of the index pages on disk, the algorithms for searching indexes and deleting from indexes, and complex transactional details such as page-level locking for high concurrency and [[write-ahead logging]] for crash recovery. This allows authors of new tree-based indexes to focus on implementing the novel features of the new index type &amp;mdash; for example, the way in which subsets of the data should be described for search &amp;mdash; without becoming experts in database system internals.

Although originally designed for answering Boolean selection queries, GiST can also support [[Nearest neighbor search|nearest-neighbor search]], and various forms of statistical [[approximation]] over large data sets.

The PostgreSQL  GiST implementation includes support for variable length keys, composite keys, concurrency control and recovery; these features are inherited by all GiST extensions. There are several contributed modules developed using GiST and distributed with PostgreSQL. For example:

* rtree_gist, btree_gist - GiST implementation of R-Tree and B-Tree
* intarray - index support for one-dimensional array of int4's
* tsearch2 - a searchable (full text) data type with indexed access
* ltree - data types, indexed access methods and queries for data organized as a tree-like structures
* hstore - a storage for (key,value) data
* cube - data type, representing multidimensional cubes 

The PostgreSQL GiST implementation provides the indexing support for the [[PostGIS]] ([[geographic information system]]) and the BioPostgres [[bioinformatics]] system.

==References==
*[[Joseph M. Hellerstein]], [[Jeffrey F. Naughton]] and Avi Pfeffer. [http://db.cs.berkeley.edu/papers/vldb95-gist.pdf Generalized Search Trees for Database Systems]. Proc. 21st Int'l Conf. on Very Large Data Bases, Zürich, September 1995, 562-573.
*Marcel Kornacker, C. Mohan and Joseph M. Hellerstein. [http://db.cs.berkeley.edu/papers/sigmod97-gist.pdf Concurrency and Recovery in Generalized Search Trees]. Proc. ACM SIGMOD Conf. on Management of Data, Tucson, AZ, May 1997, 62-72.
*Paul M. Aoki. [http://db.cs.berkeley.edu/papers/icde98-search.pdf Generalizing &quot;Search&quot; in Generalized Search Trees]. Proc. 14th Int'l Conf. on Data Engineering, Orlando, FL, Feb. 1998, 380-389.
*Marcel Kornacker. [http://gist.cs.berkeley.edu/hiperf-gist.pdf High-Performance Generalized Search Trees], Proc. 24th Int'l Conf. on Very Large Data Bases, Edinburgh, Scotland, September 1999.
*Paul M. Aoki. [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.2507 How to Avoid Building DataBlades That Know the Value of Everything and the Cost of Nothing], Proc. 11th Int'l Conf. on Scientific and Statistical Database Management, Cleveland, OH, July 1999, 122-133.

==External links==
* [http://gist.cs.berkeley.edu/ GiST research project website]
* [http://www.sai.msu.su/~megera/postgres/gist/ PostgreSQL GiST Development]
* [http://www.postgresql.org/docs/current/interactive/gist.html Documentation] for the GiST support in PostgreSQL.
* [http://www.sai.msu.su/~megera/postgres/talks/gist_tutorial.html Developing PostgreSQL extension with GiST] {{ru icon}}
* [http://www.sai.msu.su/~megera/wiki/GiST GiST in PostgreSQL wiki]
* [http://postgis.refractions.net/ PostGIS] 
* [http://phenomics.cs.ucla.edu/ BioPostgres]

[[Category:Trees (data structures)]]
[[Category:PostgreSQL]]</text>
      <sha1>nfwumu9p1pa06krhk4zqede758s86u8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>H tree</title>
    <ns>0</ns>
    <id>11333082</id>
    <revision>
      <id>617601342</id>
      <parentid>617599320</parentid>
      <timestamp>2014-07-19T17:16:54Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>/* Applications */ refactor to put vlsi together</comment>
      <text xml:space="preserve" bytes="8433">{{distinguish2|[[Htree]], a Linux filesystem indexing structure}}
[[Image:H tree.svg|thumb|360px|The first ten levels of an H tree]]
In [[fractal geometry]], the '''H tree''' is a [[fractal]] tree structure constructed from [[perpendicular]] [[line segment]]s, each smaller by a factor of the [[square root of 2]] from the next larger adjacent segment. It is so called because its repeating pattern resembles the letter &quot;H&quot;. It has [[Hausdorff dimension]] 2, and comes arbitrarily close to every point in a [[rectangle]]. Its applications include [[VLSI]] design and microwave engineering.

==Construction==
An H tree can be constructed by starting with a [[line segment]] of arbitrary length, drawing two shorter segments at right angles to the first through its endpoints, and continuing in the same vein, reducing (dividing) the length of the line segments drawn at each stage by [[square root of 2|√2]].&lt;ref&gt;&quot;[http://demonstrations.wolfram.com/HFractal/ H-Fractal] by Sándor Kabai, [[The Wolfram Demonstrations Project]].&lt;/ref&gt;

An alternative process that generates the same fractal set is to begin with a rectangle with sides in the ratio 1:√2, known as a &quot;silver rectangle&quot;, and repeatedly bisect it into two smaller silver rectangles, at each stage connecting the two [[centroid]]s of the two smaller rectangles by a line segment. A similar process can be performed with rectangles of any other shape, but the silver rectangle leads to the line segment size decreasing uniformly by a √2 factor at each step while for other rectangles the length will decrease by different factors at odd and even levels of the recursive construction.

==Properties==
The '''H tree''' is a [[Self-similarity|self-similar]] [[fractal]]; its [[Hausdorff dimension]] is equal to 2.{{sfnp|Kaloshin|Saprykina|2012}}

The points of the H tree come arbitrarily close to every point in a [[rectangle]] (the same as the starting rectangle in the constructing by centroids of subdivided rectangles). However, it does not include all points of the rectangle; for instance, the perpendicular bisector of the initial line segment is not included.

==Applications==
In [[VLSI]] design, the H tree may be used as the layout for a [[complete binary tree]] using a total area that is proportional to the number of nodes of the tree.{{sfnp|Leiserson|1980}}
it is commonly used as a [[clock distribution network]] for routing [[clock signal|timing signals]] to all parts of a chip with equal propagation delays to each part,&lt;ref&gt;{{harvtxt|Ullman|1984}}; {{harvtxt|Burkis|1991}}.&lt;/ref&gt; and has also been used as an interconnection network for VLSI multiprocessors,&lt;ref&gt;{{harvtxt|Browning|1980}}. See especially Figure 1.1.5, page&amp;nbsp;15.&lt;/ref&gt;

For the same reason, the H tree is used in arrays of [[microstrip antenna]]s in order to get the radio signal to every individual microstrip antenna with equal propagation delay. Additionally, the H tree forms a space efficient layout for trees in [[graph drawing]],{{sfnp|Nguyen|Huang|2002}} and as part of a construction of a point set for which the sum of squared edge lengths of the [[traveling salesman problem|traveling salesman tour]] is large.{{sfnp|Bern|Eppstein|1993}}

The planar H tree can be generalized to the three-dimensional structure via adding line segments on the direction perpendicular to the H tree plane.&lt;ref name=Hou&amp;Wen&gt;{{harvtxt|Hou|Xie|Wen|Sheng|2008}}; {{harvtxt|Wen|Zhou|Li|Ge|2002}}.&lt;/ref&gt;  The resultant three-dimensional H tree has [[Hausdorff dimension]] equal to 3.  The planar H tree and its three-dimensional version have been found to constitute artificial electromagnetic atoms in [[photonic crystals]] and [[metamaterials]] and might have potential applications in microwave engineering.&lt;ref name=Hou&amp;Wen/&gt;

==Related sets==
The H tree is an example of a [[fractal canopy]], in which the angle between neighboring line segments is always 180 degrees. In its property of coming arbitrarily close to every point of its bounding rectangle, it also resembles a [[space-filling curve]], although it is not itself a curve.

[[Topology|Topologically]], an H tree has properties similar to those of a [[Dendroid (topology)|dendroid]]. However, they are not dendroids: dendroids must be [[closed set]]s, and H trees are not closed (their closure is the whole rectangle).

The Mandelbrot Tree is a very closely related fractal using rectangles instead of line segments, slightly offset from the H-tree positions, in order to produce a more naturalistic appearance. To compensate for the increased width of its components and avoid self-overlap, the scale factor by which the size of the components is reduced at each level must be slightly greater than √2.&lt;ref&gt;{{mathworld|title=Mandelbrot Tree|urlname=MandelbrotTree}}&lt;/ref&gt;

==Notes==
{{reflist|30em}}

==References==
*{{citation
 | last1 = Bern | first1 = Marshall
 | last2 = Eppstein | first2 = David | author2-link = David Eppstein
 | contribution = Worst-case bounds for subadditive geometric graphs
 | doi = 10.1145/160985.161018
 | pages = 183–188
 | publisher = [[Association for Computing Machinery]]
 | title = Proc. 9th Annual Symposium on Computational Geometry
 | url = http://www.ics.uci.edu/~eppstein/pubs/BerEpp-SCG-93.pdf
 | year = 1993}}.
*{{citation
 | last = Browning | first = Sally A.
 | publisher = California Institute of Technology
 | series = Ph.D. thesis
 | title = The Tree Machine: A Highly Concurrent Computing Environment
 | url = http://authors.library.caltech.edu/26932/
 | year = 1980}}.
*{{citation
 | last = Burkis | first = J.
 | contribution = Clock tree synthesis for high performance ASICs
 | doi = 10.1109/ASIC.1991.242921
 | pages = 9.8.1–9.8.4
 | title = IEEE International Conference on ASIC
 | year = 1991}}.
*{{citation
 | last1 = Hou | first1 = Bo
 | last2 = Xie | first2 = Hang
 | last3 = Wen | first3 = Weijia
 | last4 = Sheng | first4 = Ping
 | doi = 10.1103/PhysRevB.77.125113
 | journal = [[Physical Review B]]
 | page = 125113
 | title = Three-dimensional metallic fractals and their photonic crystal characteristics
 | volume = 77
 | year = 2008}}.
*{{citation
 | last1 = Kaloshin | first1 = Vadim
 | last2 = Saprykina | first2 = Maria
 | doi = 10.1007/s00220-012-1532-x
 | issue = 3
 | journal = Communications in Mathematical Physics
 | mr = 2981810
 | pages = 643–697
 | title = An example of a nearly integrable Hamiltonian system with a trajectory dense in a set of maximal Hausdorff dimension
 | volume = 315
 | year = 2012}}.
*{{citation
 | last = Leiserson | first = Charles E. | author-link = Charles E. Leiserson
 | contribution = Area-efficient graph layouts
 | doi = 10.1109/SFCS.1980.13
 | pages = 270–281
 | title = 21st Annual Symposium on Foundations of Computer Science (FOCS 1980)
 | year = 1980}}.
*{{citation
 | last1 = Nguyen | first1 = Quang Vinh
 | last2 = Huang | first2 = Mao Lin
 | contribution = A space-optimized tree visualization
 | doi = 10.1109/INFVIS.2002.1173152
 | pages = 85–92
 | title = IEEE Symposium on Information Visualization
 | year = 2002}}.
*{{citation
 | last = Ullman | first = Jeffrey D. | author-link = Jeffrey D. Ullman
 | publisher = Computer Science Press
 | title = Computational Aspects of VSLI
 | year = 1984}}.
*{{citation
 | last1 = Wen | first1 = Weijia
 | last2 = Zhou | first2 = Lei
 | last3 = Li | first3 = Jensen
 | last4 = Ge | first4 = Weikun
 | last5 = Chan | first5 = C. T.
 | last6 = Sheng | first6 = Ping
 | doi = 10.1103/PhysRevLett.89.223901
 | page = 223901
 | publisher = [[Physical Review Letters]]
 | title = Subwavelength photonic band gaps from planar fractals
 | volume = 89
 | year = 2002}}.

== Further reading ==
*{{citation
 | last = Kabai | first = S.
 | location = Püspökladány, Hungary
 | page = 231
 | publisher = Uniconstant
 | title = Mathematical Graphics I: Lessons in Computer Graphics Using Mathematica
 | year = 2002}}.
*{{citation
 | last = Lauwerier | first = H.
 | location = Princeton, NJ
 | pages = 1–2
 | publisher = Princeton University Press
 | title = Fractals: Endlessly Repeated Geometric Figures
 | year = 1991}}.

==External links==
*[http://library.thinkquest.org/26242/full/fm/fm13.html Famous Fractals - H-fractal]
*{{mathworld | urlname = H-Fractal | title = H-Fractal}}
*[http://www.josechu.com/moving_fractal/index.htm Moving H-fractal (including Java Applet)]

[[Category:Fractals]]
[[Category:Trees (data structures)]]
[[Category:Clock signal]]</text>
      <sha1>86alczuk3za18lxwoa66v55mdtkw7qo</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hash trie</title>
    <ns>0</ns>
    <id>3568843</id>
    <revision>
      <id>625034086</id>
      <parentid>571324753</parentid>
      <timestamp>2014-09-11T03:06:29Z</timestamp>
      <contributor>
        <username>Jonesey95</username>
        <id>9755426</id>
      </contributor>
      <minor/>
      <comment>Fixing [[Category:Pages containing cite templates with deprecated parameters|deprecated month parameter]] using [[WP:AutoEd|AutoEd]]</comment>
      <text xml:space="preserve" bytes="1458">In [[computer science]], '''hash trie''' can refer to:

* [[Hash tree (persistent data structure)]], a trie used to map hash values to keys
* A space-efficient implementation of a sparse [[trie]], in which the descendants of each node may be interleaved in memory. (The name is suggested by a similarity to a closed [[hash table]].) &lt;ref name=Liang1983/&gt; {{Verify source|I believe the source doesn't reference a &quot;hash trie&quot;, only &quot;packed tries&quot; and &quot;indexed tries&quot;|date=May 2009}}
* A data structure which &quot;combines features of hash tables and LC-tries in order to perform efficient lookups and updates&quot; &lt;ref name=Roshan2004/&gt;

== See also ==
* [[Hash array mapped trie]]
* [[Hash tree]]

==References==
&lt;references&gt;
&lt;ref name=Liang1983&gt;{{Cite
|last1=Liang
|first1=Frank
|first2=Mark
| date=June 1983 |title=Word hy-phen-a-tion by com-pu-ter
|publisher=Frank M. Liang, Ph.D. thesis, Stanford University.
|url=http://www.tug.org/docs/liang/
|url=http://www.tug.org/docs/liang/liang-thesis.pdf
|accessdate=2010-03-28
}}&lt;/ref&gt;
&lt;ref name=Roshan2004&gt;{{cite
|last1=Thomas
|first1=Roshan
|last2=Mark
|first2=Brian
|last3=Johnson
|first3=Tommy
|last4=Croall
|first4=James
|title=High-speed Legitimacy-based DDoS Packet Filtering with Network Processors: A Case Study and Implementation on the Intel IXP1200
|url=http://napl.gmu.edu/pubs/BookContrib/ThomasMarkJC-NPW04.pdf
|accessdate=2009-05-03
|year=2004
}}&lt;/ref&gt;
&lt;/references&gt;

[[Category:Trees (data structures)]]</text>
      <sha1>fecg731vu8q21le51ctocx9u4aww44b</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>HTree</title>
    <ns>0</ns>
    <id>19763183</id>
    <revision>
      <id>601564868</id>
      <parentid>588716281</parentid>
      <timestamp>2014-03-27T21:31:45Z</timestamp>
      <contributor>
        <username>Frap</username>
        <id>612852</id>
      </contributor>
      <comment>Added text</comment>
      <text xml:space="preserve" bytes="2871">{{distinguish2|[[H tree]], a family of fractal sets}}
{{primary sources|date=September 2013}}
An '''HTree''' is a specialized [[Tree (data structure)|tree data structure]] for directory indexing, similar to a [[B-tree]]. They are constant depth of either one or two levels, have a high fanout factor, use a [[hash table|hash]] of the [[filename]], and do not require [[balanced tree|balancing]].&lt;ref&gt;{{cite web|url=http://ext2.sourceforge.net/2005-ols/paper-html/node3.html|title=Directory indexing|author=Mingming Cao|work=Features found in Linux 2.6}}&lt;/ref&gt; The HTree algorithm is distinguished from standard B-tree methods by its treatment of [[hash collision]]s, which may overflow across multiple leaf and index blocks. HTree [[index (database)|index]]es are used in the [[ext3]] and [[ext4]] [[Linux]] [[filesystem]]s, and were incorporated into the [[Linux kernel]] around 2.5.40.&lt;ref&gt;{{cite web|url=http://lwn.net/Articles/11481/|title=Add ext3 indexed directory (htree) support|author=tytso@mit.edu}}&lt;/ref&gt; HTree indexing improved the scalability of [[Linux]] ext2 based filesystems from a practical limit of a few thousand files, into the range of tens of millions of files per directory.

PHTree is a derivation intended as a successor.&lt;ref&gt;http://phunq.net/pipermail/tux3/2013-January/000026.html&lt;/ref&gt; It fixes all the known issues with HTree except for write multiplication.

==History==
The HTree index data structure and algorithm were developed by Daniel Phillips in 2000 and implemented for the ext2 filesystem in February 2001. A port to the ext3 filesystem by Christopher Li and [[Andrew Morton (computer programmer)|Andrew Morton]] in 2002 during the 2.5 kernel series added [[journaling file system|journal]] based crash consistency. With minor improvements, HTree continues to be used in ext4 in the Linux 3.x.x kernel series.

==Use==
* [[ext2]] HTree indexes were originally developed for ext2 but the patch never made it to the official branch. The dir_index feature can be enabled when creating an ext2 filesystem, but the ext2 code won't act on it.
* [[ext3]] HTree indexes are available in ext3 when the dir_index feature is enabled.
* [[ext4]] HTree indexes are turned on by default in ext4. This feature is implemented in Linux kernel 2.6.23. HTree indexes is also used for file [[Extent (file systems)|extents]] when a file needs more than the 4 extents stored in the [[inode]].

==See also==
* [[Kernel (computing)|Kernel]]

==References==
{{reflist}}

==External links==
*[http://www.linuxshowcase.org/2001/full_papers/phillips/phillips_html/index.html A Directory Index for Ext2 (which describes the HTree data structure)]
*[http://ext2.sourceforge.net/2005-ols/paper-html/node3.html HTree]

{{CS-Trees}}

[[Category:Disk file systems]]
[[Category:Trees (data structures)]]
[[Category:B-tree]]
[[Category:Linux]]

{{datastructure-stub}}</text>
      <sha1>ra5n9idmnqkh5pi2zzu4w9o0t140hzc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hyperbolic tree</title>
    <ns>0</ns>
    <id>5566181</id>
    <revision>
      <id>561132862</id>
      <parentid>544418378</parentid>
      <timestamp>2013-06-23T00:09:47Z</timestamp>
      <contributor>
        <username>Myasuda</username>
        <id>1187538</id>
      </contributor>
      <minor/>
      <comment>avoid redirect</comment>
      <text xml:space="preserve" bytes="4516">In [[Web development]] [[jargon]] and [[Visualization (graphic)|information visualization]], a '''hyperbolic tree''' (often shortened as '''hypertree''') defines a [[graph drawing]] method inspired by [[hyperbolic geometry]].

[[Image:BasicTree.png|thumb|right|256px| A basic hyperbolic tree. Nodes in focus are placed in the center and given more room, while out-of-focus nodes are compressed near the boundaries.]]
[[Image:BasicTreeFocused.png|thumb|right|256px| Focusing on a different node brings it and its children to the center of the disk, while uninteresting portions of the tree are compressed.]]
Displaying hierarchical data as a [[Tree (data structure)|tree]] suffers from visual clutter as the number of nodes per level can grow exponentially.  For a simple binary tree, the maximum number of nodes at a level ''n'' is 2&lt;sup&gt;n&lt;/sup&gt;, while the number of nodes for larger trees grows much more quickly.  Drawing the tree as a node-link diagram thus requires exponential amounts of space to be displayed.

One approach is to use a ''hyperbolic tree'', first introduced by Lamping et al.&lt;ref&gt;http://sigchi.org/chi95/Electronic/documnts/papers/jl_bdy.htm&lt;/ref&gt;  Hyperbolic trees employ [[hyperbolic geometry|hyperbolic space]], which intrinsically has &quot;more room&quot; than Euclidean space.  For instance, linearly increasing the radius of a circle in Euclidean space increases its circumference linearly, while the same circle in hyperbolic space would have its circumference increase exponentially.  Exploiting this property allows laying out the tree in hyperbolic space in an uncluttered manner: placing a node far enough from its parent gives the node almost the same amount of space as its parent for laying out its own children.  

Displaying a hyperbolic tree commonly utilizes the [[Poincaré disk model]] of hyperbolic geometry, though the [[Klein model|Klein-Beltrami]] model can also be used.  Both display the entire hyperbolic plane within a unit disk, making the entire tree visible at once.  The unit disk gives a fish-eye lens view of the plane, giving more emphasis to nodes which are in focus and displaying nodes further out of focus closer to the boundary of the disk.  Traversing the hyperbolic tree requires [[Möbius transformation]]s of the space, bringing new nodes into focus and moving higher levels of the hierarchy out of view.  

Although hyperbolic trees have been patented in the U.S. by Xerox, various Java &amp; JavaScript implementations exist on the web as well as C++ &amp; OpenGL.&lt;ref&gt;http://vsxu.com&lt;/ref&gt;&lt;ref&gt;http://vsxu.com/shots/030.jpg&lt;/ref&gt;&lt;ref&gt;http://patft.uspto.gov/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F5590250&lt;/ref&gt;

==See also==
*[[Hyperbolic geometry]]
*[[Information visualization]]
*[[Tree (graph theory)]]
*[[Tree (data structure)]]
*[[Radial tree]] - is also circular, but uses linear geometry.

== External links ==
* http://sigchi.org/chi95/Electronic/documnts/papers/jl_bdy.htm
* [http://thejit.org JavaScript InfoVis Toolkit] has an interactive Hyperbolic Tree visualization.
* [http://ucjeps.berkeley.edu/map2.html The Green Tree of Life] - [[Tree of life]] - University of California at Berkeley and Jepson Herbaria
* [http://ocsigen.org/js_of_ocaml/files/hyperbolic/index.html Tree of life] Similar to the above, but with pictures
* http://xebece.sourceforge.net/screenshots
* http://hypergraph.sourceforge.net/ Applet, supports trees as well as generic graphs
* [http://www.visualcomplexity.com/vc/ VisualComplexity] Images of alternative visualizations
* [http://www.touchgraph.com/ TouchGraph] Live demonstration
* [http://treevis.net Comprehensive survey and bibliography] of Tree Visualization techniques

== References ==

&lt;div class=&quot;references-small&quot;&gt;
* {{Cite conference
  | last1 = Lamping | first1 = John 
  | last2 = Rao | first2 = Ramana 
  | last3 = Pirolli| first3 = Peter 
  | title = A Focus+Context Technique Based on Hyperbolic Geometry for Visualizing Large Hierarchies
  | booktitle = Proc. ACM Conf. Human Factors in Computing Systems, CHI
  | pages = 401&amp;ndash;408
  | publisher = [[Association for Computing Machinery|ACM]]
  | year = 1995
  | url = http://citeseer.ist.psu.edu/lamping95focuscontext.html
  | contribution-url = http://sigchi.org/chi95/Electronic/documnts/papers/jl_bdy.htm
}}
&lt;/div&gt;
&lt;references/&gt;

{{DEFAULTSORT:Hyperbolic Tree}}
[[Category:Hyperbolic geometry]]
[[Category:Graph drawing]]
[[Category:Trees (data structures)]]</text>
      <sha1>60g5zfkbf08qm6lazvcwot5tevv2wfe</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hypertree</title>
    <ns>0</ns>
    <id>19716804</id>
    <revision>
      <id>545462025</id>
      <parentid>490878678</parentid>
      <timestamp>2013-03-19T16:39:49Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q5958769]]</comment>
      <text xml:space="preserve" bytes="1340">A [[hypergraph]] ''H'' is called a '''hypertree''', if it admits a [[host graph]] ''T'' such that ''T'' is a [[tree (graph theory)|tree]], in other words if there exists a tree ''T'' such that every [[hyperedge]] of ''H'' [[induced subgraph|induces]] a [[subtree]] in ''T''.  &lt;ref name=voloshin&gt;V. I. Voloshin (2002) ''Coloring Mixed Hypergraphs'', ISBN 0-8218-2812-6 , [http://books.google.com/books?id=RYM_Qi5HR68C&amp;pg=PA26&amp;dq=hypertree+hypergraph&amp;ei=TX7vSJiuOZHEMaT4pA4&amp;sig=ACfU3U12X-utDYS4BRfgYOOvlCAUCFV-gQ#PPA23,M1 p. 23]&lt;/ref&gt;

Since a tree is a hypertree, hypertrees may be seen as a generalization of the notion of a tree for [[hypergraph]]s. Any hypertree is isomorphic to some family of subtrees of a tree. &lt;ref name=voloshin/&gt;

==Properties==

A hypertree has the  [[Helly property]] (2-Helly property), i.e., if any two hyperedges from a [[subset]] of its hyperedges have a common vertex, then all hyperedges of the subset have a common vertex. &lt;ref name=voloshin/&gt;

The [[line graph of a hypergraph|line graph]] of a hypertree is a [[chordal graph]].&lt;ref name=voloshin/&gt;

A hypergraph is a hypertree [[if and only if]] its [[dual hypergraph]] is [[conformal hypergraph|conformal]] and [[Chordal hypergraph|chordal]].&lt;ref name=voloshin/&gt;

==References==
{{reflist}}

[[Category:Hypergraphs]]
[[Category:Trees (data structures)]]</text>
      <sha1>80jsplmzsgn90yo6lixayligycugjpi</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Implicit k-d tree</title>
    <ns>0</ns>
    <id>12480975</id>
    <revision>
      <id>497261567</id>
      <parentid>497261114</parentid>
      <timestamp>2012-06-12T18:25:38Z</timestamp>
      <contributor>
        <username>Dicklyon</username>
        <id>869314</id>
      </contributor>
      <comment>/* Assigning attributes to implicit k-d tree-nodes */ take out spurious hyphen</comment>
      <text xml:space="preserve" bytes="9298">{{DISPLAYTITLE:Implicit ''k''-d tree}}
[[Image:Implicitmaxkdtree.gif|thumb|Construction and storage of a 2D implicit max ''k''d-tree using the grid median splitting-function. Each cell of the rectilinear grid has one scalar value from low (bright blue) to high (bright red) assigned to it. The grid's memory footprint is indicated in the lower line. The implicit max ''k''d-tree's predefined memory footprint needs one scalar value less than that. The storing of the node's max values is indicated in the upper line.|400px|right]]
An '''implicit ''k''-d tree''' is a [[k-d tree|''k''-d tree]] defined implicitly above a [[rectilinear grid]]. Its split [[Plane (mathematics)|plane]]s' positions and [[Orientation (geometry)|orientation]]s are not given explicitly but implicitly by some [[Recursion|recursive]] splitting-function defined on the [[hyperrectangle]]s belonging to the tree's [[node (computer science)|node]]s. Each inner node's split plane is positioned on a grid plane of the underlying grid, partitioning the node's grid into two subgrids.

==Nomenclature and references==

The terms &quot;[[min max kd tree|min/max ''k''-d tree]]&quot; and &quot;implicit ''k''-d tree&quot; are sometimes mixed up. This is because the first publication using the term &quot;implicit ''k''-d tree&quot; &lt;ref&gt;Ingo Wald, Heiko Friedrich, Gerd Marmitt, Philipp Slusallek and Hans-Peter Seidel &quot;Faster Isosurface Ray Tracing using Implicit KD-Trees&quot; IEEE Transactions on Visualization and Computer Graphics (2005)&lt;/ref&gt; did actually use explicit min/max ''k''-d trees but referred to them as  &quot;implicit ''k''-d trees&quot; to indicate that they may be used to ray trace implicitly given iso surfaces. Nevertheless this publication used also slim ''k''-d trees which are a subset of the implicit ''k''-d trees with the restriction that they can only be built over integer hyperrectangles with sidelengths that are powers of two. Implicit ''k''-d trees as defined here have recently been introduced, with applications in computer graphics.&lt;ref&gt;Matthias Groß, Carsten Lojewski, Martin Bertram and Hans Hagen &quot;Fast Implicit ''k''-d Trees: Accelerated Isosurface Ray Tracing and Maximum Intensity Projection for Large Scalar Fields&quot; CGIM07: Proceedings of Computer Graphics and Imaging (2007) 67-74&lt;/ref&gt;&lt;ref&gt;Matthias Groß (PhD, 2009) [http://kluedo.ub.uni-kl.de/volltexte/2009/2361/ Towards Scientific Applications for Interactive Ray Casting]&lt;/ref&gt; As it is possible to assign attributes to implicit ''k''-d tree nodes, one may refer to an implicit ''k''-d tree which has min/max values assigned to its nodes as an &quot;implicit min/max ''k''-d tree&quot;.

==Construction==

Implicit ''k''-d trees are in general not constructed explicitly. When accessing a node, its split plane orientation and position are evaluated using the specific splitting-function defining the tree. Different splitting-functions may result in different trees for the same underlying grid.

===Splitting-functions===

Splitting-functions may be adapted to special purposes. Underneath two specifications of special splitting-function classes.

* '''Non-degenerated splitting-functions''' do not allow the creation of degenerated nodes (nodes whose corresponding integer hyperrectangle's volume is equal zero). Their corresponding implicit ''k''-d trees are [[binary tree|full binary tree]]s, which have for ''n'' leaf nodes ''n - 1'' inner nodes. Their corresponding implicit ''k''-d trees are '''non-degenerated implicit ''k''-d trees'''.

* '''complete splitting-functions''' are non-degenerated splitting-functions whose corresponding implicit ''k''-d tree's leaf nodes are single grid cells such that they have one inner node less than the amount of gridcells given in the grid. The corresponding implicit ''k''-d trees are '''complete implicit ''k''-d trees'''.

A complete splitting function is for example the '''grid median splitting-function'''. It creates fairly balanced implicit ''k''-d trees by using ''k''-dimensional integer hyperrectangles ''hyprec[2][k]'' belonging to each node of the implicit ''k''-d tree. The hyperrectangles define which gridcells of the rectilinear grid belong to their corresponding node. If the volume of this hyperrectangle equals one, the corresponding node is a single grid cell and is therefore not further subdivided and marked as leaf node. Otherwise the hyperrectangle's longest extend is chosen as orientation ''o''. The corresponding split plane ''p'' is positioned onto the grid plane that is closest to the hyperrectangle's grid median along that orientation.

Split plane orientation ''o'':
 o = min{argmax(i = 1 ... ''k'': (hyprec[1][i] - hyprec[0][i]))}
Split plane position ''p'':
 p = roundDown((hyprec[0][o] + hyprec[1][o]) / 2)

===Assigning attributes to implicit ''k''-d tree nodes===

An obvious advantage of implicit ''k''-d trees is that their split plane's orientations and positions need not to be stored explicitly.

But some applications require besides the split plane's orientations and positions further attributes at the inner tree nodes. These attributes may be for example single bits or single scalar values, defining if the subgrids belonging to the nodes are of interest or not. For complete implicit ''k''-d trees it is possible to pre-allocate a correctly sized array of attributes and to assign each inner node of the tree to a unique element in that allocated array.

The amount of gridcells in the grid is equal the volume of the integer hyperrectangle belonging to the grid. As a complete implicit ''k''-d tree has one inner node less than grid cells, it is known in advance how many attributes need to be stored. The relation &quot;''Volume of integer hyperrectangle to inner nodes''&quot; defines together with the complete splitting-function a recursive formula assigning to each split plane a unique element in the allocated array. The corresponding algorithm is given in C-pseudo code underneath.

&lt;source lang=&quot;cpp&quot;&gt;
// Assigning attributes to inner nodes of a complete implicit k-d tree 

// create an integer help hyperrectangle hyprec (its volume vol(hyprec) is equal the amount of leaves)
int hyprec[2][k] = { { 0, ..., 0 }, { length_1, ..., length_k } };
// allocate once the array of attributes for the entire implicit k-d tree
attr *a = new attr[volume(hyprec) - 1];

attr implicitKdTreeAttributes(int hyprec[2][k], attr *a)
{
 if(vol(hyprec) &gt; 1) // the current node is an inner node
 {
   // evaluate the split plane's orientation o and its position p using the underlying complete split-function
   int o, p;
   completeSplittingFunction(hyprec, &amp;o, &amp;p);
   // evaluate the children's integer hyperrectangles hyprec_l and hyprec_r 
   int hyprec_l[2][k], hyprec_r[2][k];
   hyprec_l       = hyprec;
   hyprec_l[1][o] = p;
   hyprec_r       = hyprec;
   hyprec_r[0][o] = p;
   // evaluate the children's memory location a_l and a_r 
   attr* a_l = a + 1;
   attr* a_r = a + vol(hyprec_l);
   // evaluate recursively the children's attributes c_l and c_r 
   attr c_l = implicitKdTreeAttributes(hyprec_l, a_l);
   attr c_r = implicitKdTreeAttributes(hyprec_r, a_r);
   // merge the children's attributes to the current attribute c 
   attr c = merge(c_l, c_r);
   // store the current attribute and return it
   a[0] = c;
   return c;
 }
 // The current node is a leaf node. Return the attribute belonging to the corresponding gridcell
 return attribute(hyprec);
}
&lt;/source&gt;

It is worth mentioning that this algorithm works for all rectilinear grids. The corresponding integer hyperrectangle does not necessarily have to have sidelengths that are powers of two.

==Applications==

Implicit [[min max kd tree|max-''k''-d trees]] are used for [[ray casting]] [[isosurface]]s/MIP ([[maximum intensity projection]]). The attribute assigned to each inner node is the maximal scalar value given in the subgrid belonging to the node. Nodes are not traversed if their scalar values are smaller than the searched iso-value/current maximum intensity along the ray. The low storage requirements of the implicit max ''k''d-tree and the favorable visualization complexity of ray casting allow to ray cast (and even change the isosurface for) very large scalar fields at interactive framerates on commodity PCs. Similarly an implicit [[min/max kd-tree]] may be used to efficiently evaluate queries such as terrain [[Line of sight (gaming)|line of sight]].&lt;ref&gt;Bernardt Duvenhage &quot;Using An Implicit Min/Max KD-Tree for Doing Efficient Terrain Line of Sight Calculations&quot; in &quot;Proceedings of the 6th International Conference on Computer Graphics, Virtual Reality, Visualisation and Interaction in Africa&quot;, 2009.&lt;/ref&gt;

==Complexity==

Given an implicit ''k''-d tree spanned over an ''k''-dimensional grid with ''n'' gridcells.
* Assigning attributes to the nodes of the tree takes &lt;math&gt;\mathrm{O}(kn)&lt;/math&gt;'' time.
* Storing attributes to the nodes takes &lt;math&gt;\mathrm{O}(n)&lt;/math&gt; memory.
* Ray casting iso-surfaces/MIP an underlying scalar field using the corresponding implicit max ''k''-d tree takes roughly &lt;math&gt;\mathrm{O}(\log(n))&lt;/math&gt; time.

==See also==

* [[k-d tree|''k''-d tree]]
* [[min/max kd-tree|min/max ''k''-d tree]]

==References==

&lt;references/&gt;

{{CS-Trees}}

{{DEFAULTSORT:Implicit Kd-Tree}}
[[Category:Computer graphics data structures]]
[[Category:Trees (data structures)]]</text>
      <sha1>998fsss9uya8lts6aq08m3pud7ihpyy</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Infinite tree automaton</title>
    <ns>0</ns>
    <id>25436675</id>
    <revision>
      <id>564362252</id>
      <parentid>564362173</parentid>
      <timestamp>2013-07-15T13:59:41Z</timestamp>
      <contributor>
        <ip>129.67.150.74</ip>
      </contributor>
      <comment>/* Acceptance condition */ typo: Buchi -&gt; Büchi</comment>
      <text xml:space="preserve" bytes="5306">{{refimprove|date=December 2009}}
In [[computer science]] and [[mathematical logic]], an '''infinite tree automaton''' is a [[state machine]] that deals with infinite [[Tree (set theory)#Tree (automata theory)|tree structure]]. It can be viewed as an extension from a [[tree automaton|finite tree automaton]], which accepts only finite tree structures. It can also be viewed as an extension of some infinite word automatons such as the [[Büchi automaton]] and the [[Muller automaton]].

A finite automaton which runs on an infinite tree was first used by Rabin&lt;ref&gt;Rabin, M. O.: ''Decidability of second order theories and automata on infinite trees'',''[[Transactions of the American Mathematical Society]]'', vol. 141, pp. 1–35, 1969.&lt;/ref&gt; for proving decidability of monadic [[second order logic]]. It has been further observed that tree automaton and logical theories are closely connected and it allows decision problems in logic to be reduced into decision problems for automaton.

==Definition==
Infinite tree automaton runs of over a [[Tree (set theory)#Tree (automata theory)|&lt;math&gt;\Sigma&lt;/math&gt;-labeled tree]]. There are many slightly different formulations of tree automaton. Here one of the formulation is described. An '''infinite tree automaton''' is a tuple &lt;math&gt;A = (\Sigma, D, Q, \delta, q_0, F )&lt;/math&gt; where,
* &lt;math&gt;\Sigma&lt;/math&gt; is an alphabet.
* &lt;math&gt;D\subset \mathbb{N}&lt;/math&gt; is a finite set. Each element of &lt;math&gt;D&lt;/math&gt; is an allowed degree in input [[tree structure#Formal definition|tree]].
* &lt;math&gt;Q&lt;/math&gt; is a finite set of states.
* &lt;math&gt;\delta: Q \times \Sigma \times D \rightarrow 2^{Q^*}&lt;/math&gt; is a transition relation that maps an automaton state &lt;math&gt;q \in Q&lt;/math&gt;, an input letter &lt;math&gt;\sigma \in \Sigma &lt;/math&gt;, and a degree &lt;math&gt; d \in D &lt;/math&gt; to a set of d-tuple of states.
* &lt;math&gt; q_0 \in Q&lt;/math&gt; is an initial state of automaton.
* &lt;math&gt;F \subseteq \Sigma^{\omega}&lt;/math&gt; is an accepting condition.

==Run==
A ''run'' of tree automaton &lt;math&gt;A&lt;/math&gt; over a &lt;math&gt;\Sigma&lt;/math&gt;-labeled tree &lt;math&gt;(T,V )&lt;/math&gt; is a &lt;math&gt; Q&lt;/math&gt;-labeled tree &lt;math&gt;(T_r, r )&lt;/math&gt;. Lets suppose that the tree automaton is at state &lt;math&gt; q&lt;/math&gt; and it has reached to a node t labeled with &lt;math&gt; \sigma \in \Sigma&lt;/math&gt; of input tree. &lt;math&gt; d(t) &lt;/math&gt; is degree of node t. Then, the automaton proceeds by selecting a tuple &lt;math&gt;(q_1,...,q_{d(t)})&lt;/math&gt; from set &lt;math&gt;\delta( q, \sigma, d(t))&lt;/math&gt; and splitting into &lt;math&gt;d(t)&lt;/math&gt; copies of itself. For each &lt;math&gt; 0 &lt; i \leq d(t)&lt;/math&gt;, one copy enters into &lt;math&gt;q_i&lt;/math&gt; state and proceeds to the node &lt;math&gt;t.i&lt;/math&gt;. This process produces a run over a tree. 

Formally, a run &lt;math&gt;(T_r, r )&lt;/math&gt; on the input tree satisfy following two conditions:
# &lt;math&gt;r(\epsilon) =  q_0&lt;/math&gt;
# For every &lt;math&gt;t \in T_r &lt;/math&gt; with &lt;math&gt;r(t) = q&lt;/math&gt;, there exists a &lt;math&gt;(q_1,...,q_{d(t)}) \in \delta(q,V(t),d(t))&lt;/math&gt; such that for every &lt;math&gt; 0 &lt; i \leq d(t) &lt;/math&gt;, we have &lt;math&gt;t.i \in T_r &lt;/math&gt; and &lt;math&gt; r(t.i) = q_i &lt;/math&gt;

==Acceptance condition==
In a run &lt;math&gt;(T_r, r )&lt;/math&gt;, an infinite path is labeled by a sequence of states. This sequence of states form an infinite word over states. If all these infinite words belong to accepting condition &lt;math&gt;F&lt;/math&gt;, then the run is ''accepting''. The interesting accepting conditions are [[omega automaton#Acceptance conditions|Büchi]], [[Rabin automaton#Acceptance conditions|Rabin]], [[Streett automaton#Acceptance conditions|Streett]] and [[Muller automaton#Acceptance conditions|Muller]]. If for an input &lt;math&gt;\Sigma&lt;/math&gt;-labeled tree  &lt;math&gt;(T,V )&lt;/math&gt; there exist an accepting run then the input tree is ''accepted'' by the automaton. The set of all the  accepted &lt;math&gt;\Sigma&lt;/math&gt;-labeled trees is called tree language &lt;math&gt;\mathcal{L}(A)&lt;/math&gt; which is ''recognized'' by tree automaton &lt;math&gt;A&lt;/math&gt;.

==Remarks==

The set D may seem unusual to some people. Some times D is omitted from the definition when it is a singleton set that means input tree has fixed branching at each node. For example, if D = {2} then the input tree has to be a full binary tree.

Infinite tree automaton is ''deterministic'' if for some &lt;math&gt;q \in Q&lt;/math&gt;, &lt;math&gt;\sigma \in \Sigma &lt;/math&gt;, and &lt;math&gt; d \in D &lt;/math&gt; transition relation &lt;math&gt;\delta( q, \sigma, d)&lt;/math&gt; has exactly one element. Otherwise the automaton is ''non-deterministic''.

==Accepting tree languages==

Muller, parity, Rabin, and Streett accepting conditions in an infinite tree automaton recognize the same tree languages.

But, Büchi accepting condition is strictly weaker than other accepting conditions, i.e., there exists a tree language which can be recognized by Muller accepting condition in infinite tree automata but can't be recognized by any Büchi accepting condition in some infinite tree automaton.&lt;ref&gt;Rabin, M. O.: ''Weakly definable relations and special automata'',''Mathematical logic and foundation of set theory'', pp. 1–23, 1970.&lt;/ref&gt;

Tree languages which are recognized by Muller accepting conditions are closed under union, intersection, projection and complementation.

==Reference list==

&lt;references/&gt;

{{DEFAULTSORT:Infinite Tree Automaton}}
[[Category:Trees (data structures)]]
[[Category:Automata theory]]</text>
      <sha1>fyujc86f41yz7zdqj931jcwj40flm3e</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Interval tree</title>
    <ns>0</ns>
    <id>1533767</id>
    <revision>
      <id>621307593</id>
      <parentid>620470980</parentid>
      <timestamp>2014-08-15T05:00:54Z</timestamp>
      <contributor>
        <username>SchreiberBike</username>
        <id>5839411</id>
      </contributor>
      <comment>Correct ordinal 3nd→ 3rd based on ISBN</comment>
      <text xml:space="preserve" bytes="19113">In [[computer science]], an '''interval tree''' is an [[ordered tree data structure|ordered tree]] [[data structure]] to hold [[Interval (mathematics)|intervals]]. Specifically, it allows one to efficiently find all intervals that overlap with any given interval or point. It is often used for windowing queries, for instance, to find all roads on a computerized map inside a rectangular viewport, or to find all visible elements inside a three-dimensional scene. A similar data structure is the [[segment tree]].

The trivial solution is to visit each interval and test whether it intersects the given point or interval, which requires Θ(''n'') time, where ''n'' is the number of intervals in the collection. Since a query may return all intervals, for example if the query is a large interval intersecting all intervals in the collection, this is [[asymptotically optimal]]; however, we can do better by considering [[output-sensitive algorithm]]s, where the runtime is expressed in terms of ''m'', the number of intervals produced by the query. Interval trees are dynamic, i.e., they allow insertion and deletion of intervals. They obtain a query time of O(log ''n'') while the preprocessing time to construct the data structure is O(''n'' log ''n'') (but the space consumption is O(''n'')). If the endpoints of intervals are within a small integer range (e.g., in the range [1,...,O(''n'')]), faster data structures&lt;ref&gt;{{cite web|url=http://en.wikipedia.org/wiki/Range_Queries#Semigroup_Operator}}&lt;/ref&gt; exist with preprocessing time O(''n'') and query time O(1+''m'') for reporting ''m'' intervals containing a given query point.

==Naive approach==
In a simple case, the intervals do not overlap and they can be inserted into a simple [[binary search tree]] and queried in O(log ''n'') time. However, with arbitrarily overlapping intervals, there is no way to compare two intervals for insertion into the tree since orderings sorted by the beginning points or the ending points may be different. A naive approach might be to build two parallel trees, one ordered by the beginning point, and one ordered by the ending point of each interval. This allows discarding half of each tree in O(log ''n'') time, but the results must be merged, requiring O(''n'') time. This gives us queries in O(''n'' + log ''n'') = O(''n''), which is no better than brute-force.

Interval trees solve this problem. This article describes two alternative designs for an interval tree, dubbed the ''centered interval tree'' and the ''augmented tree''.

== Centered interval tree ==

Queries require O(log ''n'' + ''m'') time, with ''n'' being the total number of intervals and ''m'' being the number of reported results. Construction requires O(''n'' log ''n'') time, and storage requires O(''n'') space.

=== Construction ===

Given a set of ''n'' intervals on the number line, we want to construct a data structure so that we can efficiently retrieve all intervals overlapping another interval or point.

We start by taking the entire range of all the intervals and dividing it in half at ''x_center'' (in practice, ''x_center'' should be picked to keep the tree relatively balanced). This gives three sets of intervals, those completely to the left of ''x_center'' which we'll call ''S_left'', those completely to the right of ''x_center'' which we'll call ''S_right'', and those overlapping ''x_center'' which we'll call ''S_center''.

The intervals in ''S_left'' and ''S_right'' are recursively divided in the same manner until there are no intervals left.

The intervals in S_center that overlap the center point are stored in a separate data structure linked to the node in the interval tree. This data structure consists of two lists, one containing all the intervals sorted by their beginning points, and another containing all the intervals sorted by their ending points.

The result is a ternary tree with each node storing:
* A center point
* A pointer to another node containing all intervals completely to the left of the center point
* A pointer to another node containing all intervals completely to the right of the center point
* All intervals overlapping the center point sorted by their beginning point
* All intervals overlapping the center point sorted by their ending point

=== Intersecting ===

Given the data structure constructed above, we receive queries consisting of ranges or points, and return all the ranges in the original set overlapping this input.

==== With a Point ====

The task is to find all intervals in the tree that overlap a given point ''x''. The tree is walked with a similar recursive algorithm as would be used to traverse a traditional binary tree, but with extra affordance for the intervals overlapping the &quot;center&quot; point at each node.

For each tree node, ''x'' is compared to ''x_center'', the midpoint used in node construction above.  If ''x'' is less than ''x_center'', the leftmost set of intervals, ''S_left'', is considered. If ''x'' is greater than ''x_center'', the rightmost set of intervals, ''S_right'', is considered.

As each node is processed as we traverse the tree from the root to a leaf, the ranges in its ''S_center'' are processed. If ''x'' is less than ''x_center'', we know that all intervals in ''S_center'' end after ''x'', or they could not also overlap ''x_center''. Therefore, we need only find those intervals in ''S_center'' that begin before ''x''. We can consult the lists of ''S_center'' that have already been constructed. Since we only care about the interval beginnings in this scenario, we can consult the list sorted by beginnings. Suppose we find the closest number no greater than ''x'' in this list. All ranges from the beginning of the list to that found point overlap ''x'' because they begin before ''x'' and end after ''x'' (as we know because they overlap ''x_center'' which is larger than ''x''). Thus, we can simply start enumerating intervals in the list until the endpoint value exceeds ''x''.

Likewise, if ''x'' is greater than ''x_center'', we know that all intervals in ''S_center'' must begin before ''x'', so we find those intervals that end after ''x'' using the list sorted by interval endings.

If ''x'' exactly matches ''x_center'', all intervals in ''S_center'' can be added to the results without further processing and tree traversal can be stopped.

==== With an Interval ====

First, we can reduce the case where an interval ''R'' is given as input to the simpler case where a single point is given as input. We do this by first finding all ranges with beginning or end points inside the input interval ''R'' using a separately constructed tree. In the one-dimensional case, we can use a simple tree containing all the beginning and ending points in the interval set, each with a pointer to its corresponding interval.

A binary search in O(log ''n'') time for the beginning and end of R reveals the minimum and maximum points to consider. Each point within this range references an interval that overlaps our range and is added to the result list. Care must be taken to avoid duplicates, since an interval might both begin and end within ''R''. This can be done using a binary flag on each interval to mark whether or not it has been added to the result set.

The only intervals not yet considered are those overlapping ''R'' that do not have an endpoint inside ''R'', in other words, intervals that enclose it. To find these, we pick any point inside ''R'' and use the algorithm above to find all intervals intersecting that point (again, being careful to remove duplicates).

=== Higher Dimensions ===

The interval tree data structure can be generalized to a higher dimension ''N'' with identical query and construction time and O(''n'' log ''n'') space.

First, a [[range tree]] in ''N'' dimensions is constructed that allows efficient retrieval of all intervals with beginning and end points inside the query region ''R''. Once the corresponding ranges are found, the only thing that is left are those ranges that enclose the region in some dimension. To find these overlaps, N interval trees are created, and one axis intersecting ''R'' is queried for each. For example, in two dimensions, the bottom of the square ''R'' (or any other horizontal line intersecting R) would be queried against the interval tree constructed for the horizontal axis. Likewise, the left (or any other vertical line intersecting R) would be queried against the interval tree constructed on the vertical axis.

Each interval tree also needs an addition for higher dimensions. At each node we traverse in the tree, ''x'' is compared with ''S_center'' to find overlaps. Instead of two sorted lists of points as was used in the one-dimensional case, a range tree is constructed. This allows efficient retrieval of all points in ''S_center'' that overlap region ''R''.

=== Deletion ===

If after deleting an interval from the tree, the node containing that interval contains no more intervals, that node may be deleted from the tree. This is more complex than a normal binary tree deletion operation.

An interval may overlap the center point of several nodes in the tree. Since each node stores the intervals that overlap it, with all intervals completely to the left of its center point in the left subtree, similarly for the right subtree, it follows that each interval is stored in the node closest to the root from the set of nodes whose center point it overlaps.

Normal deletion operations in a binary tree (for the case where the node being deleted has two children) involve promoting a node further from the root to the position of the node being deleted (usually the leftmost child of the right subtree, or the rightmost child of the left subtree). As a result of this promotion, some nodes that were above the promoted node will become descendents of it; it is necessary to search these nodes for intervals that also overlap the promoted node, and move those intervals into the promoted node. As a consequence, this may result in new empty nodes, which must be deleted, following the same algorithm again.

=== Balancing ===

The same issues that affect deletion also affect rotation operations; rotation must preserve the invariant that intervals are stored as close to the root as possible.

== Augmented tree ==

Another way to represent intervals is described in {{harvtxt|Cormen|Leiserson|Rivest|Stein|2009|loc=Section 14.3: Interval trees, pp.&amp;nbsp;348&amp;ndash;354}}.

Both insertion and deletion require O(log ''n'') time, with ''n'' being the total number of intervals in the tree prior to the insertion or deletion operation.

Use a simple ordered tree, for example a [[binary search tree]] or [[self-balancing binary search tree]], where the tree is ordered by the 'low' values of the intervals, and an extra annotation is added to every node recording the maximum high value of both its subtrees. It is simple to maintain this attribute in only O(''h'') steps during each addition or removal of a node, where ''h'' is the height of the node added or removed in the tree, by updating all ancestors of the node from the bottom up. Additionally, the [[tree rotation]]s used during insertion and deletion may require updating the high value of the affected nodes.

Now, it is known that two intervals ''A'' and ''B'' overlap only when both ''A''.low ≤ ''B''.high and ''A''.high ≥ ''B''.low. When searching the trees for nodes overlapping with a given interval, you can immediately skip:
* all nodes to the right of nodes whose low value is past the end of the given interval.
* all nodes that have their maximum 'high' value below the start of the given interval.

A total order can be defined on the intervals by ordering them first by their 'low' value and finally by their
'high' value. This ordering can be used to prevent duplicate intervals from being inserted into the tree in O(log ''n'') time, versus the O(''k'' + log ''n'') time required to find duplicates if ''k'' intervals overlap a new interval.

===Java Example: Adding a new interval to the tree===

The key of each node is the interval itself and the value of each node is the end point of the interval:

&lt;source lang=java&gt;
 public void add(Interval i) {
     put(i, i.getEnd());
 }
&lt;/source&gt;

===Java Example: Searching a point or an interval in the tree===

To search for an interval, you walk the tree, omitting those branches which can't contain what you're looking for. The simple case is looking for a point:

&lt;source lang=java&gt;
 // Search for all intervals which contain &quot;p&quot;, starting with the
 // node &quot;n&quot; and adding matching intervals to the list &quot;result&quot;
 public void search(IntervalNode n, Point p, List&lt;Interval&gt; result) {
     // Don't search nodes that don't exist
     if (n == null)
         return;
 
     // If p is to the right of the rightmost point of any interval
     // in this node and all children, there won't be any matches.
     if (p.compareTo(n.getValue()) &gt; 0)
         return;
 
     // Search left children
     if (n.getLeft() != null)
         search(IntervalNode (n.getLeft()), p, result);
 
     // Check this node
     if (n.getKey().contains(p))
         result.add(n.getKey());
 
     // If p is to the left of the start of this interval,
     // then it can't be in any child to the right.
     if (p.compareTo(n.getKey().getStart()) &lt; 0)
         return;
 
     // Otherwise, search right children
     if (n.getRight() != null)
         search(IntervalNode (n.getRight()), p, result);
 }
&lt;/source&gt;

The code to search for an interval is similar, except for the check in the middle:
&lt;source lang=java&gt;
 // Check this node
 if (n.getKey().overlapsWith(i))
     result.add (n.getKey());
&lt;/source&gt;

&lt;tt&gt;overlapsWith()&lt;/tt&gt; is defined as:
&lt;source lang=java&gt;
 public boolean overlapsWith(Interval other) {
     return start.compareTo(other.getEnd()) &lt;= 0 &amp;&amp;
            end.compareTo(other.getStart()) &gt;= 0;
 }
&lt;/source&gt;

===Higher dimension===

This can be extended to higher dimensions by cycling through the dimensions at each level of the tree. For example, for two dimensions, the odd levels of the tree might contain ranges for the ''x''&amp;nbsp;coordinate, while the even levels contain ranges for the ''y''&amp;nbsp;coordinate. However, it is not quite obvious how the rotation logic will have to be extended for such cases to keep the tree balanced.

A much simpler solution is to use nested interval trees. First, create a tree using the ranges for the ''y''&amp;nbsp;coordinate. Now, for each node in the tree, add another interval tree on the ''x''&amp;nbsp;ranges, for all elements whose ''y''&amp;nbsp;range intersect that node's ''y''&amp;nbsp;range.

The advantage of this solution is that it can be extended to an arbitrary amount of dimensions using the same code base.

At first, the cost for the additional trees might seem prohibitive but that is usually not the case. As with the solution above, you need one node per ''x''&amp;nbsp;coordinate, so this cost is the same in both solutions. The only difference is that you need an additional tree structure per vertical interval. This structure is typically very small (a pointer to the root node plus maybe the number of nodes and the height of the tree).

==Medial/length oriented tree==
Similar to Augmented tree, but in a symmetrical way, where the [[Binary Search Tree]] is ordered by the Medial point of intervals. And there is a Maximum-oriented [[Binary Heap]] in every node, ordered by the length of interval (or half of the length). Also we store minimum possible value of the subtree in each node, additional to maximum possible value (this is how it is symmetrical).

===Overlap test===
Using only start and end values of two intervals &lt;math&gt;\left( a_{i}, b_i \right)&lt;/math&gt;, for &lt;math&gt;i=0,1&lt;/math&gt;, the overlap test can be performed like:

&lt;math&gt;a_0 \leqslant a_1 &lt; b_0&lt;/math&gt; &amp;nbsp;&amp;nbsp; OR &amp;nbsp;&amp;nbsp;
&lt;math&gt;a_0 &lt; b_1 \leqslant b_0&lt;/math&gt; &amp;nbsp;&amp;nbsp; OR &amp;nbsp;&amp;nbsp;
&lt;math&gt;a_1 \leqslant a_0 &lt; b_1&lt;/math&gt; &amp;nbsp;&amp;nbsp; OR &amp;nbsp;&amp;nbsp;
&lt;math&gt;a_1 &lt; b_0 \leqslant b_1&lt;/math&gt;

But with defining:

&lt;math&gt;m_i = \frac{a_i + b_i}{2}&lt;/math&gt;

&lt;math&gt;d_i = \frac{b_i - a_i}{2}&lt;/math&gt;

The overlap test is simpler:

&lt;math&gt;\left| m_1 - m_0 \right| &lt; d_0 + d_1&lt;/math&gt;

===Adding interval===
Adding new intervals to the tree is the same as BST, just we use medial value as the key, and when we found/created the node to put the interval. We should push &lt;math&gt;d_i&lt;/math&gt; to the [[Binary Heap]] associated to node. And update minimum and maximum possible values associated with all higher nodes.

===Searching for all overlapping intervals===
Let's use &lt;math&gt;a_q, b_q, m_q, d_q&lt;/math&gt; for the query interval, and &lt;math&gt;M_n&lt;/math&gt; for the key of a node (compared to &lt;math&gt;m_i&lt;/math&gt; of intervals)

Starting with root node, in each node, first we check if it is possible that our query interval overlaps with the node subtree using minimum and maximum values of node (if it is not, we don't continue for this node).

Then we calculate &lt;math&gt;\min \left\{ d_i \right\}&lt;/math&gt; for intervals inside this node (not its children) to overlap with query interval (knowing &lt;math&gt;m_i = M_n&lt;/math&gt;):

&lt;math&gt;\min \left\{ d_i \right\} = \left| m_q - M_n \right| - d_q&lt;/math&gt;

And perform a query on its [[binary heap]] for the &lt;math&gt;d_i&lt;/math&gt;'s bigger than &lt;math&gt;\min \left\{ d_i \right\}&lt;/math&gt;

Then we pass through both left and right children of node, doing the same thing.
In the worst-case, we have to scan all nodes of BST, but since [[Binary Heap]] query is optimum, there is not much worries (a 2- dimensional problem can not be optimum in both dimensions)

This algorithm is expected to be faster than traditional Interval Tree (Augmented tree) in search operation, adding is just a little bit slower (order of growth is the same).

== References ==

&lt;references /&gt;
* [[Mark de Berg]], [[Marc van Kreveld]], [[Mark Overmars]], and [[Otfried Schwarzkopf]]. ''Computational Geometry'', Second Revised Edition. Springer-Verlag 2000. Section 10.1: Interval Trees, pp.&amp;nbsp;212–217.
* {{citation
 | last=Cormen | first=Thomas H. | author-link =Thomas H. Cormen
 | first2=Charles E. | last2=Leiserson | author2-link=Charles E. Leiserson
 | first3=Ronald L.  | last3=Rivest | author3-link=Ronald L. Rivest
 | first4=Clifford   | last4=Stein  | author4-link=Clifford Stein
 | title=[[Introduction to Algorithms]]
 | edition=3rd
 | publisher=MIT Press and McGraw-Hill
 | year=2009 
 | isbn=978-0-262-03384-8
}}
* [[Franco P. Preparata]] and [[Michael Ian Shamos]]. ''Computational Geometry: An Introduction''. Springer-Verlag, 1985
* [[Jens M. Schmidt]]. ''Interval Stabbing Problems in Small Integer Ranges''. [http://dx.doi.org/10.1007/978-3-642-10631-6_18 DOI]. ISAAC'09, 2009

== External links ==
* [http://www.cgal.org/ CGAL : Computational Geometry Algorithms Library in C++] contains a robust implementation of Range Trees
* [http://code.google.com/p/intervaltree/ Interval Tree (an augmented self balancing avl tree implementation)]
* [https://github.com/gam3/interval-tree/ Interval Tree (a ruby implementation)]

{{CS-Trees}}

[[Category:Trees (data structures)]]</text>
      <sha1>26y12esv5oxid9ovswp0h5fzokussry</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>K-d tree</title>
    <ns>0</ns>
    <id>1676725</id>
    <revision>
      <id>614567688</id>
      <parentid>614514822</parentid>
      <timestamp>2014-06-26T23:24:15Z</timestamp>
      <contributor>
        <username>Mwtoews</username>
        <id>711150</id>
      </contributor>
      <comment>/* Range search */ text</comment>
      <text xml:space="preserve" bytes="27487">{{DISPLAYTITLE:''k''-d tree}}
{{Infobox data structure
|name= KD-tree
|type= Multidimensional [[Binary Search Tree|BST]]
|invented_by= [[Jon Bentley|Jon Louis Bentley]]
|invented_year= 1975
|
|space_avg= O(''n'')
|space_worst= O(''n'')
|search_avg= O(log ''n'')
|search_worst= O(''n'')
|insert_avg= O(log ''n'')
|insert_worst= O(''n'')
|delete_avg= O(log ''n'')
|delete_worst= O(''n'')
}}

[[File:3dtree.png|thumb|A 3-dimensional ''k''-d tree. The first split (red) cuts the root cell (white) into two subcells, each of which is then split (green) into two subcells. Finally, each of those four is split (blue) into two subcells. Since there is no more splitting, the final eight are called leaf cells.|250px|right]]

In [[computer science]], a '''''k''-d tree''' (short for ''k-dimensional [[tree data structure|tree]]'') is a [[space partitioning|space-partitioning]] [[data structure]] for organizing [[Point (geometry)|point]]s in a ''k''-dimensional [[Euclidean space|space]]. ''k''-d trees are a useful data structure for several applications, such as searches involving a multidimensional search key (e.g. [[range search]]es and [[nearest neighbor search]]es). ''k''-d trees are a special case of [[binary space partitioning]] trees.

==Informal description==

The ''k''-d tree is a [[binary tree]] in which every node is a k-dimensional point. Every non-leaf node can be thought of as implicitly generating a splitting [[hyperplane]] that divides the space into two parts, known as [[Half-space (geometry)|half-space]]s. Points to the left of this hyperplane are represented by the left subtree of that node and points right of the hyperplane are represented by the right subtree. The hyperplane direction is chosen in the following way: every node in the tree is associated with one of the k-dimensions, with the hyperplane perpendicular to that dimension's axis. So, for example, if for a particular split the &quot;x&quot; axis is chosen, all points in the subtree with a smaller &quot;x&quot; value than the node will appear in the left subtree and all points with larger &quot;x&quot; value will be in the right subtree. In such a case, the hyperplane would be set by the x-value of the point, and its [[Surface normal|normal]] would be the unit x-axis.&lt;ref&gt;{{cite doi|10.1145/361002.361007}}&lt;/ref&gt;

== Operations on ''k''-d trees ==

===Construction===

Since there are many possible ways to choose axis-aligned splitting planes, there are many different ways to construct ''k''-d trees. The canonical method of ''k''-d tree construction has the following constraints:&lt;ref name=&quot;compgeom&quot;/&gt;

* As one moves down the tree, one cycles through the axes used to select the splitting planes. (For example, in a 3-dimensional tree, the root would have an ''x''-aligned plane, the root's children would both have ''y''-aligned planes, the root's grandchildren would all have ''z''-aligned planes, the root's great-grandchildren would all have ''x''-aligned planes, the root's great-great-grandchildren would all have ''y''-aligned planes, and so on.)
* Points are inserted by selecting the [[median]] of the points being put into the [[Tree (data structure)|subtree]], with respect to their coordinates in the axis being used to create the splitting plane. (Note the assumption that we feed the entire set of ''n'' points into the algorithm up-front.)
This method leads to a [[balanced tree|balanced]] ''k''-d tree, in which each leaf node is about the same distance from the root. However, balanced trees are not necessarily optimal for all applications.

Note also that it is not ''required'' to select the median point. In that case, the result is simply that there is no guarantee that the tree will be balanced. A simple heuristic to avoid coding a complex linear-time [[Selection algorithm|median-finding]] [[algorithm]], or using an O(''n'' log ''n'') sort of all ''n'' points, is to use sort to find the median of a ''fixed'' number of ''randomly'' selected points to serve as the splitting plane. In practice, this technique often results in nicely balanced trees.

Given a list of ''n'' points, the following algorithm uses a median-finding sort to construct a balanced ''k''-d tree containing those points.

 '''function''' kdtree (''list of points'' pointList, ''int'' depth)
 {
     ''// Select axis based on depth so that axis cycles through all valid values''
     '''var''' ''int'' axis := depth '''mod''' k;
         
     ''// Sort point list and choose median as pivot element''
     '''[[Selection algorithm|select]]''' median '''by''' axis '''from''' pointList;
         
     ''// Create node and construct subtrees''
     '''var''' ''tree_node'' node;
     node.location := median;
     node.leftChild := kdtree(points '''in''' pointList '''before''' median, depth+1);
     node.rightChild := kdtree(points '''in''' pointList '''after''' median, depth+1);
     '''return''' node;
 }
It is common that points &quot;after&quot; the median include only the ones that are strictly greater than the median. For points that lie on the median, it is possible to define a &quot;superkey&quot; function that compares the points in all dimensions. In some cases, it is acceptable to let points equal to the median lie on one side of the median, for example, by splitting the points into a &quot;less than&quot; subset and a &quot;greater than or equal to&quot; subset.
{| align=&quot;right&quot; style=&quot;margin-left: 1em&quot;
|+ '''Example implementation''' 
| width=&quot;480px&quot; |
The above algorithm implemented in the [[Python (programming language)|Python programming language]] is as follows:

&lt;source lang=&quot;python&quot; enclose=&quot;div&quot;&gt;
from collections import namedtuple
from operator import itemgetter
from pprint import pformat

class Node(namedtuple('Node', 'location left_child right_child')):
    def __repr__(self):
        return pformat(tuple(self))

def kdtree(point_list, depth=0):
    try:
        k = len(point_list[0]) # assumes all points have the same dimension
    except IndexError as e: # if not point_list:
        return None
    # Select axis based on depth so that axis cycles through all valid values
    axis = depth % k
 
    # Sort point list and choose median as pivot element
    point_list.sort(key=itemgetter(axis))
    median = len(point_list) // 2 # choose median
 
    # Create node and construct subtrees
    return Node(
        location=point_list[median],
        left_child=kdtree(point_list[:median], depth + 1),
        right_child=kdtree(point_list[median + 1:], depth + 1)
    )

def main():
    &quot;&quot;&quot;Example usage&quot;&quot;&quot;
    point_list = [(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)]
    tree = kdtree(point_list)
    print(tree)

if __name__ == '__main__':
    main()
&lt;/source&gt;

Output would be:
&lt;source lang=&quot;text&quot; enclose=&quot;div&quot;&gt;
((7, 2),
 ((5, 4), ((2, 3), None, None), ((4, 7), None, None)),
 ((9, 6), ((8, 1), None, None), None))
&lt;/source&gt;
The generated tree is shown below.
[[File:Kdtree 2d.svg|center|thumb|300px|''k''-d tree decomposition for the point set &lt;code&gt;(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)&lt;/code&gt;.]]
[[File:Tree 0001.svg|center|thumb|300px|The resulting ''k''-d tree.]]
|}
This algorithm creates the [[invariant (computer science)|invariant]] that for any node, all the nodes in the left [[subtree]] are on one side of a splitting [[plane (mathematics)|plane]], and all the nodes in the right subtree are on the other side. Points that lie on the splitting plane may appear on either side. The splitting plane of a node goes through the point associated with that node (referred to in the code as ''node.location'').

A novel tree-building algorithm builds a balanced ''k''-d tree in {{math|O(''kn'' log ''n'')}} time by sorting ''n'' points in ''k'' dimensions independently and ''prior'' to building the {{nobr|''k''-d tree}}.&lt;ref&gt;{{cite doi|10.1109/RT.2006.280216}}&lt;/ref&gt;&lt;ref&gt;H. M. Kakde. [http://www.cs.fsu.edu/~lifeifei/cis5930/kdtree.pdf Range searching using kd tree. pp. 1-12, 2005][http://www.scribd.com/doc/26091526/Range-Searching-Using-Kd-Tree Range searching using kd tree. pp. 1-12, 2005]&lt;/ref&gt; A suitable sorting algorithm is [[Heapsort]] that creates a [[sorted array]] in {{math|O(''n'' log ''n'')}} time.  Application of Heapsort to ''n'' points in each of ''k'' dimensions requires {{math|O(''kn'' log ''n'')}} time, and produces ''k'' [[sorted array]]s of length ''n'' that contain references (or pointers) to the ''n'' points.  These arrays are numbered from 0 to {{math|''k''-1}}.  Each array represents the result of sorting the points in one of the ''k'' dimensions.  For example, the elements of array 0, from first to last, reference the ''n'' points in order of increasing ''x''-coordinate.  Similarly, the elements of arrays 1, 2, and 3, from first to last, reference the ''n'' points in order of increasing ''y''-, ''z''- and ''w''-coordinates, respectively.

In order to insert the first node into the ''k''-d tree, the median element of array 0 is chosen and stored in the tree node.  This median element splits array 0 into two subarrays.  One subarray lies above the median element, and the other subarray lies below it.  Also, the ''x''-coordinate of the point that this median element references defines an ''x''-aligned splitting plane that may be used to split each of the other ''k''-1 arrays into two subarrays.  The following procedure splits an array into two subarrays:

* Consider each element of the array in order from first to last.
* Test against the splitting plane the ''x''-coordinate of the point that is referenced by the array element, and assign that element to one of two subarrays, depending on which side of the splitting plane the point lies.
* Ignore the array element that references the same point that the median element of array 0 references, because this point defines the splitting plane.

This procedure splits the arrays into two sets of subarrays while preserving the original sorted order within each subarray.  These subarrays may then be used to insert nodes into the two [[Tree (data structure)|subtrees]] at the next level of the tree in a [[Recursion (computer science)|recursive]] manner.  However, if the subarrays comprise only one or two array elements, no further [[Recursion (computer science)|recursion]] is required because these cases may be solved trivially.

These guidelines will simplify creation of ''k''-d trees:

* Arrays should be split into subarrays that represent &quot;less than&quot; and &quot;greater than or equal to&quot; partitioning.  This convention requires that, after choosing the median element of array 0, the element of array 0 that lies immediately below the median element be examined to ensure that this adjacent element references a point whose ''x''-coordinate is less than and not equal to the ''x''-coordinate of the splitting plane.  If this adjacent element references a point whose ''x''-coordinate is equal to the ''x''-coordinate of the splitting plane, continue searching towards the beginning of array 0 until the first instance of an array element is found that references a point whose ''x''-coordinate is less than and not equal to the ''x''-coordinate of the splitting plane.  When this array element is found, the element that lies immediately above this element is the correct choice for the median element.  Apply this method of choosing the median element at each level of recursion.
* This procedure for producing subarrays guarantees that the two subarrays comprise one less array element than the array from which these subarrays were produced.  This characteristic permits re-use of the ''k'' arrays at each level of recursion as follows: (1) copy array 0 into a temporary array, (2) build the subarrays that are produced from array 1 in array 0, (3) build the subarrays that are produced from array 2 in array 1, (4) continue this pattern, and build the subarrays that are produced from array {{math|''k''-1}} in array {{math|''k''-2}}, and finally (4) copy the temporary array into array ''k''-1.  This method permutes the subarrays so that at successive levels of the ''k''-d tree, the median element is chosen from ''x''-, ''y''-, ''z''- ''w''-,... sorted arrays.
* The addresses of the first and last elements of the 2''k'' subarrays can be passed to the next level of recursion in order to designate where these subarrays lie within the ''k'' arrays.  Each of the two sets of ''k'' subarrays have identical addresses for their first and last elements.

This tree-building algorithm requires at most [[math|O([''k''-1]''n'')]] tests of coordinates against splitting planes to build each of the {{math|log ''n''}} levels of a balanced ''k''-d tree.  Hence, building the entire ''k''-d tree requires less than {{math|O([''k''-1]''n'' log ''n'')}} time, which is less than the {{math|O(''kn'' log ''n'')}} time that is required to sort the ''n'' points in ''k'' dimensions prior to building the ''k''-d tree.

=== Adding elements ===

{{Expand section|date=November 2008}}

One adds a new point to a ''k''-d tree in the same way as one adds an element to any other [[binary search tree|search tree]]. First, traverse the tree, starting from the root and moving to either the left or the right child depending on whether the point to be inserted is on the &quot;left&quot; or &quot;right&quot; side of the splitting plane. Once you get to the node under which the child should be located, add the new point as either the left or right child of the leaf node, again depending on which side of the node's splitting plane contains the new node.

Adding points in this manner can cause the tree to become unbalanced, leading to decreased tree performance. The rate of tree performance degradation is dependent upon the spatial distribution of tree points being added, and the number of points added in relation to the tree size. If a tree becomes too unbalanced, it may need to be re-balanced to restore the performance of queries that rely on the tree balancing, such as nearest neighbour searching.

===Removing elements===

{{Expand section|date=February 2011}}

To remove a point from an existing ''k''-d tree, without breaking the invariant, the easiest way is to form the set of all nodes and leaves from the children of the target node, and recreate that part of the tree.

Another approach is to find a replacement for the point removed.&lt;ref&gt;Chandran, Sharat. [http://www.cs.umd.edu/class/spring2002/cmsc420-0401/pbasic.pdf Introduction to kd-trees]. University of Maryland Department of Computer Science.&lt;/ref&gt; First, find the node R that contains the point to be removed. For the base case where R is a leaf node, no replacement is required. For the general case, find a replacement point, say p, from the subtree rooted at R. Replace the point stored at R with p. Then, recursively remove p.

For finding a replacement point, if R discriminates on x (say) and R has a right child, find the point with the minimum x value from the subtree rooted at the right child. Otherwise, find the point with the maximum x value from the subtree rooted at the left child.

===Balancing===

Balancing a ''k''-d tree requires care because ''k''-d trees are sorted in multiple dimensions so the [[tree rotation]] technique cannot be used to balance them as this may break the invariant.

Several variants of balanced ''k''-d trees exist. They include divided ''k''-d tree, pseudo ''k''-d tree, ''k''-d B-tree, hB-tree and Bkd-tree. Many of these variants are [[adaptive k-d tree]]s.

===Nearest neighbour search===

&lt;!--  Incomplete and wafty description of the KD-NN-algorithm --&gt;
[[File:KDTree-animation.gif|thumb|300px|Animation of NN searching with a ''k''-d tree in two dimensions]]

The [[nearest neighbour search]] (NN) algorithm aims to find the point in the tree that is nearest to a given input point. This search can be done efficiently by using the tree properties to quickly eliminate large portions of the search space.

Searching for a nearest neighbour in a ''k''-d tree proceeds as follows:

# Starting with the root node, the algorithm moves down the tree recursively, in the same way that it would if the search point were being inserted (i.e. it goes left or right depending on whether the point is less than or greater than the current node in the split dimension).
# Once the algorithm reaches a leaf node, it saves that node point as the &quot;current best&quot;
# The algorithm unwinds the recursion of the tree, performing the following steps at each node:
## If the current node is closer than the current best, then it becomes the current best.
## The algorithm checks whether there could be any points on the other side of the splitting plane that are closer to the search point than the current best. In concept, this is done by intersecting the splitting [[hyperplane]] with a [[hypersphere]] around the search point that has a radius equal to the current nearest distance. Since the hyperplanes are all axis-aligned this is implemented as a simple comparison to see whether the difference between the splitting coordinate of the search point and current node is less than the distance (overall coordinates) from the search point to the current best.
### If the hypersphere crosses the plane, there could be nearer points on the other side of the plane, so the algorithm must move down the other branch of the tree from the current node looking for closer points, following the same recursive process as the entire search.
### If the hypersphere doesn't intersect the splitting plane, then the algorithm continues walking up the tree, and the entire branch on the other side of that node is eliminated.
# When the algorithm finishes this process for the root node, then the search is complete.
Generally the algorithm uses squared distances for comparison to avoid computing square roots. Additionally, it can save computation by holding the squared current best distance in a variable for comparison.

Finding the nearest point is an O(log N) operation in the case of randomly distributed points, although analysis in general is tricky. However an algorithm has been given that claims guaranteed O(log N) complexity.&lt;ref name=&quot;Friedman:1977:AFB:355744.355745&quot;&gt;{{cite doi|10.1145/355744.355745}}&lt;/ref&gt;

In high-dimensional spaces, the [[curse of dimensionality]] causes the algorithm to need to visit many more branches than in lower-dimensional spaces. In particular, when the number of points is only slightly higher than the number of dimensions, the algorithm is only slightly better than a linear search of all of the points.

The algorithm can be extended in several ways by simple modifications. It can provide the ''k'' nearest neighbours to a point by maintaining ''k'' current bests instead of just one. A branch is only eliminated when ''k'' points have been found and the branch cannot have points closer than any of the ''k'' current bests.

It can also be converted to an approximation algorithm to run faster. For example, approximate nearest neighbour searching can be achieved by simply setting an upper bound on the number points to examine in the tree, or by interrupting the search process based upon a real time clock (which may be more appropriate in hardware implementations). Nearest neighbour for points that are in the tree already can be achieved by not updating the refinement for nodes that give zero distance as the result, this has the downside of discarding points that are not unique, but are co-located with the original search point.

Approximate nearest neighbour is useful in real-time applications such as robotics due to the significant speed increase gained by not searching for the best point exhaustively. One of its implementations is [[best-bin-first search]].

===Range search===
A range search searches for ranges of parameters. For example, if a tree is storing values corresponding to income and age, then a range search might be something like looking for all members of the tree which have an age between 20 and 50 years and an income between 50,000 and 80,000. Since k-d trees divide the range of a domain in half at each level of the tree, they are useful for performing range searches.

Analyses of binary search trees has found that the worst case time for range search in a k-dimensional KD tree containing N nodes is given by the following equation.&lt;ref name=Lee1977&gt;{{Cite doi|10.1007/BF00263763}}&lt;/ref&gt;

:&lt;math&gt;t_\text{worst} = O(k \cdot N^{1-\frac{1}{k}})&lt;/math&gt;

==High-dimensional data==

''k''-d trees are not suitable for efficiently finding the nearest neighbour in high-dimensional spaces. As a general rule, if the dimensionality is ''k'', the number of points in the data, ''N'', should be ''N &gt;&gt; 2&lt;sup&gt;k&lt;/sup&gt;''. Otherwise, when ''k''-d trees are used with high-dimensional data, most of the points in the tree will be evaluated and the efficiency is no better than exhaustive search,&lt;ref&gt;{{cite book|author=[[Jacob E. Goodman]], Joseph O'Rourke and Piotr Indyk (Ed.) | title=Handbook of Discrete and Computational Geometry|chapter=Chapter 39 : Nearest neighbours in high-dimensional spaces|publisher=CRC Press |edition=2nd|year=2004}}&lt;/ref&gt; and approximate nearest-neighbour methods should be used instead.

==Complexity==

* Building a static ''k''-d tree from ''n'' points takes:
** [[Big O notation|O]](''n'' log&lt;sup&gt;2&lt;/sup&gt; ''n'') time if an O(''n'' log ''n'') sort such as [[Heapsort]] is used to compute the median at each level;
** O(''n'' log ''n'') time if a complex linear-time [[Selection algorithm|median-finding]] algorithm such as the one described in Cormen ''et al.''&lt;ref&gt;{{Introduction to Algorithms}} Chapter 10.&lt;/ref&gt; is used;
** O(''kn'' log ''n'') plus O([''k''-1]''n'' log ''n'') time if ''n'' points are sorted in each of ''k'' dimensions using an O(''n'' log ''n'') sort ''prior'' to building the ''k''-d tree.

* Inserting a new point into a balanced ''k''-d tree takes O(log ''n'') time.
* Removing a point from a balanced ''k''-d tree takes O(log ''n'') time.
* Querying an axis-parallel range in a balanced ''k''-d tree takes O(''n''&lt;sup&gt;1-1/k&lt;/sup&gt; +''m'') time, where ''m'' is the number of the reported points, and ''k'' the dimension of the ''k''-d tree.
* Finding 1 nearest neighbour in a balanced ''k''-d tree with randomly distributed points takes O(log ''n'') time on average.

==Variations==

===Volumetric objects===

Instead of points, a ''k''-d tree can also contain [[rectangle]]s or hyperrectangles.&lt;ref&gt;{{cite doi|10.1109/TCAD.1985.1270098}}&lt;/ref&gt;&lt;ref&gt;{{cite doi|10.1007/BF01952830}}&lt;/ref&gt; Thus range search becomes the problem of returning all rectangles intersecting the search rectangle. The tree is constructed the usual way with all the rectangles at the leaves. In an [[orthogonal range search]], the ''opposite'' coordinate is used when comparing against the median. For example, if the current level is split along x&lt;sub&gt;high&lt;/sub&gt;, we check the x&lt;sub&gt;low&lt;/sub&gt; coordinate of the search rectangle. If the median is less than the x&lt;sub&gt;low&lt;/sub&gt; coordinate of the search rectangle, then no rectangle in the left branch can ever intersect with the search rectangle and so can be pruned. Otherwise both branches should be traversed. See also [[interval tree]], which is a 1-dimensional special case.

===Points only in leaves===

It is also possible to define a ''k''-d tree with points stored solely in leaves.&lt;ref name=&quot;compgeom&quot;&gt;{{cite doi|10.1007/978-3-540-77974-2_5}}&lt;/ref&gt; This form of ''k''-d tree allows a variety of split mechanics other than the standard median split. The midpoint splitting rule&lt;ref name=&quot;midpointsplit&quot;&gt;S. Maneewongvatana and [[David Mount|D. M. Mount]]. [http://www.cs.umd.edu/~mount/Papers/cgc99-smpack.pdf It's okay to be skinny, if your friends are fat]. 4th Annual CGC Workshop on Computational Geometry, 1999.&lt;/ref&gt; selects on the middle of the longest axis of the space being searched, regardless of the distribution of points. This guarantees that the aspect ratio will be at most 2:1, but the depth is dependent on the distribution of points. A variation, called sliding-midpoint, only splits on the middle if there are points on both sides of the split. Otherwise, it splits on point nearest to the middle. Maneewongvatana and Mount show that this offers &quot;good enough&quot; performance on common data sets.
Using sliding-midpoint, an [[Nearest neighbour search#Approximate nearest neighbour|approximate nearest neighbour]] query can be answered in &lt;math&gt;O \left ( \frac{ 1 }{ { \epsilon\ }^d } \log n \right )&lt;/math&gt;.
Approximate range counting can be answered in &lt;math&gt;O \left ( \log n + { \left ( \frac{1}{ \epsilon\ } \right ) }^d \right )&lt;/math&gt; with this method.

==See also==

* [[implicit kd-tree|implicit ''k''-d tree]], a ''k''-d tree defined by an implicit splitting function rather than an explicitly-stored set of splits
* [[min/max kd-tree|min/max ''k''-d tree]], a ''k''-d tree that associates a minimum and maximum value with each of its nodes
* [[Ntropy]], computer library for the rapid development of algorithms that uses a kd-tree for running on a parallel computer
* [[Octree]], a higher-dimensional generalization of a quadtree
* [[Quadtree]], a space-partitioning structure that splits at the geometric midpoint rather than the median coordinate
* [[R-tree]] and [[bounding interval hierarchy]], structure for partitioning objects rather than points, with overlapping regions
* [[Recursive partitioning]], a technique for constructing statistical decision trees that are similar to ''k''-d trees
* [[Klee's measure problem]], a problem of computing the area of a union of rectangles, solvable using ''k''-d trees
* [[Guillotine problem]], a problem of finding a ''k''-d tree whose cells are large enough to contain a given set of rectangles

==References==

{{reflist}}

== External links ==
* [http://libkdtree.alioth.debian.org libkdtree++], an open-source STL-like implementation of ''k''-d trees in C++.
* [http://www.autonlab.org/autonweb/14665/version/2/part/5/data/moore-tutorial.pdf?branch=main&amp;language=en A tutorial on KD Trees]
* [http://people.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN FLANN] and its fork [http://code.google.com/p/nanoflann/ nanoflann], efficient C++ implementations of ''k''-d tree algorithms.
* [http://spatial.sourceforge.net Spatial C++ Library], a generic implementation of ''k''-d tree as multi-dimensional containers, algorithms, in C++.
* [http://code.google.com/p/kdtree/ kdtree] A simple C library for working with KD-Trees
* [http://donar.umiacs.umd.edu/quadtree/points/kdtree.html K-D Tree Demo, Java applet]
* [http://www.cs.umd.edu/~mount/ANN/ libANN] Approximate Nearest Neighbour Library includes a ''k''-d tree implementation
* [http://www.vision.caltech.edu/malaa/software/research/image-search/ Caltech Large Scale Image Search Toolbox]: a Matlab toolbox implementing randomized ''k''-d tree for fast approximate nearest neighbour search, in addition to [[Locality sensitive hashing|LSH]], Hierarchical K-Means, and [[Inverted file|Inverted File]] search algorithms.
* [http://dcgi.felk.cvut.cz/home/havran/phdthesis.html Heuristic Ray Shooting Algorithms], pp.&amp;nbsp;11 and after
* [http://intopii.com/into/ Into] contains open source implementations of exact and approximate (k)NN search methods using ''k''-d trees in C++.
* [https://metacpan.org/module/Math::Vector::Real::kdTree Math::Vector::Real::kdTree] Perl implementation of ''k''-d trees.

{{CS-Trees}}

{{DEFAULTSORT:K-d tree}}
[[Category:Computer graphics data structures]]
[[Category:Trees (data structures)]]
[[Category:Geometric data structures]]
[[Category:Database index techniques]]
[[Category:Data types]]</text>
      <sha1>n78pipzpchew1xvqyo2lx7uczb9kc5y</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Left rotation</title>
    <ns>0</ns>
    <id>2754301</id>
    <revision>
      <id>544136435</id>
      <parentid>471784446</parentid>
      <timestamp>2013-03-14T16:51:30Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q6516995]]</comment>
      <text xml:space="preserve" bytes="1557">'''Left rotation''' refers to the following
* In an [[Array data structure|array]], moving all items to the next lower location. The first item is moved to the last location, which is now vacant.
* In a [[List (computing)|list]], removing the [[head]] and inserting it at the [[tail]].

== Tree Rotation ==
In a [[binary search tree]], a left rotation is the movement of a node, X,  down to the left. This rotation assumes that X has a right child (or subtree). X's right child, R, becomes X's parent node and R's left child becomes X's new right child. This rotation is done to balance the tree; specifically when the right subtree of node X has a significantly (depends on the type of tree) greather height than its left subtree.

Left rotations (and right) are ''order preserving'' in a [[binary search tree]]; it preserves the binary search tree property (an [[in-order traversal]] of the tree will yield the keys of the nodes in proper order). [[AVL]] trees and [[red-black tree]]s are two examples of binary search trees that use the left rotation.

A single left rotation is done in O(1) time but is often integrated within the node insertion and deletion of [[binary search trees]]. The rotations are done to keep the cost of other methods and tree height at a minimum.

== References ==
*[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]], 2001, ''Introduction to Algorithms'', second edition. McGraw-Hill, ISBN 0-07-013151-1. Chapter 13.

{{DEFAULTSORT:Left Rotation}}
[[Category:Trees (data structures)]]</text>
      <sha1>9jfzuy93bs255zpwrt2v5i1levsayj3</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Linear octree</title>
    <ns>0</ns>
    <id>31075291</id>
    <revision>
      <id>471784463</id>
      <parentid>449502348</parentid>
      <timestamp>2012-01-17T01:01:20Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Moving category Trees (structure) to [[:Category:Trees (data structures)]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2012 January 12]].</comment>
      <text xml:space="preserve" bytes="644">{{unreferenced|date=March 2011}}
A '''linear octree''' is an [[octree]] that is represented by a linear [[Array data structure|array]] instead of a [[tree data structure]]. 

To simplify implementation, a linear octree is usually complete (that is, every [[internal node]] has exactly 8 child nodes) and where the maximum permissible depth is fixed a priori (making it sufficient to store the complete list of [[leaf node]]s). That is, all the nodes of the octree can be generated from the list of its leaf nodes. [[Space filling curves]] are often used to represent linear octrees.

[[Category:Trees (data structures)]]

{{datastructure-stub}}</text>
      <sha1>f1dit6y8n31o320qulkal9j72vu8br0</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>M-tree</title>
    <ns>0</ns>
    <id>27481099</id>
    <revision>
      <id>618931782</id>
      <parentid>596136766</parentid>
      <timestamp>2014-07-29T06:47:47Z</timestamp>
      <contributor>
        <ip>188.98.192.242</ip>
      </contributor>
      <comment>More complex, but also realistic, example.</comment>
      <text xml:space="preserve" bytes="10673">{{Context|date=June 2010}}

'''M-trees''' are [[tree data structure]]s that are similar to [[R-tree]]s and [[B-tree]]s. It is constructed using a [[Metric (mathematics)|metric]] and relies on the [[triangle inequality]] for efficient range and [[k-nearest neighbor algorithm|k-NN]] queries.
While M-trees can perform well in many conditions, the tree can also have large overlap and there is no clear strategy on how to best avoid overlap. In addition, it can only be used for [[distance function]]s that satisfy the triangle inequality, while many advanced dissimilarity functions used in [[information retrieval]] do not satisfy this.&lt;ref name=&quot;p426&quot;&gt;{{cite conference
 | first = Paolo
 | last = Ciaccia
 | authorlink =
 | coauthors = Patella, Marco; Zezula, Pavel
 | title = M-tree An Efficient Access Method for Similarity Search in Metric Spaces
 | booktitle = Proceedings of the 23rd VLDB Conference Athens, Greece, 1997
 | pages = 426–435
 | publisher = Very Large Databases Endowment Inc.
 | year = 1997
 | location = IBM Almaden Research Center
 | url = http://www.vldb.org/conf/1997/P426.PDF
 | accessdate = 2010-09-07
 | id = p426
}}&lt;/ref&gt;

==Overview==
[[File:M-tree_built_with_MMRad_split.png|thumb|350px|2D M-Tree visualized using [[Environment for DeveLoping KDD-Applications Supported by Index-Structures|ELKI]]. Due to the axis scales, the spheres appear ellipsoidal. Every blue sphere (leaf) is contained in a red sphere (directory nodes). Leaves overlap, but not too much.]]

As in any Tree-based data structure, the M-Tree is composed of Nodes and Leaves.  In each node there is a data object that identifies it uniquely and a pointer to a sub-tree where its children reside.  Every leaf has several data objects.  For each node there is a radius &lt;math&gt;r&lt;/math&gt; that defines a Ball in the desired metric space. Thus, every node &lt;math&gt;n&lt;/math&gt; and leaf &lt;math&gt;l&lt;/math&gt; residing in a particular node &lt;math&gt;N&lt;/math&gt; is at most distance &lt;math&gt;r&lt;/math&gt; from &lt;math&gt;N&lt;/math&gt;, and every node &lt;math&gt;n&lt;/math&gt; and leaf &lt;math&gt;l&lt;/math&gt; with node parent &lt;math&gt;N&lt;/math&gt; keep the distance from it.

==M-Tree construction==

=== Components ===
An M-Tree has these components and sub-components:

# Non-leaf nodes
## A set of routing objects N&lt;sub&gt;''RO''&lt;/sub&gt;.
## Pointer to Node's parent object O&lt;sub&gt;''p''&lt;/sub&gt;.
# Leaf nodes
## A set of objects N&lt;sub&gt;''O''&lt;/sub&gt;.
## Pointer to Node's parent object O&lt;sub&gt;''p''&lt;/sub&gt;.
# Routing Object 
## (Feature value of) routing object O&lt;sub&gt;''r''&lt;/sub&gt;.
## Covering radius r(O&lt;sub&gt;''r''&lt;/sub&gt;).
## Pointer to covering tree T(O&lt;sub&gt;''r''&lt;/sub&gt;).
## Distance of O&lt;sub&gt;''r''&lt;/sub&gt; from its parent object d(O&lt;sub&gt;''r''&lt;/sub&gt;,P(O&lt;sub&gt;''r''&lt;/sub&gt;))
# Object
## (Feature value of the) object O&lt;sub&gt;''j''&lt;/sub&gt;.
## Object identifier oid(O&lt;sub&gt;''j''&lt;/sub&gt;).
## Distance of O&lt;sub&gt;''j''&lt;/sub&gt; from its parent object d(O&lt;sub&gt;''j''&lt;/sub&gt;,P(O&lt;sub&gt;''j''&lt;/sub&gt;))

=== Insert ===
The main idea is first to find a leaf node &lt;math&gt;N&lt;/math&gt; where the new object &lt;math&gt;O&lt;/math&gt; belongs. If &lt;math&gt;N&lt;/math&gt; is not full then just attach it to &lt;math&gt;N&lt;/math&gt;. If &lt;math&gt;N&lt;/math&gt; is full then invoke a method to split &lt;math&gt;N&lt;/math&gt;. The algorithm is as follows:

{{algorithm-begin|name=Insert}}
   Input: Node &lt;math&gt;N&lt;/math&gt;  of M-Tree &lt;math&gt;MT&lt;/math&gt;, Entry &lt;math&gt;O_{n}&lt;/math&gt;
   Output: A new instance of &lt;math&gt;MT&lt;/math&gt; containing all entries in original &lt;math&gt;MT&lt;/math&gt; plus &lt;math&gt;O_{n}&lt;/math&gt;

   &lt;math&gt;N_{e}&lt;/math&gt; ← &lt;math&gt;N&lt;/math&gt;'s routing objects or objects
   '''if''' &lt;math&gt;N&lt;/math&gt; is not a leaf '''then'''
   {
        /*Look for entries that the new object fits into*/
        let &lt;math&gt;N_{in}&lt;/math&gt; be routing objects from &lt;math&gt;N_{e}&lt;/math&gt;'s set of routing objects &lt;math&gt;N_{RO}&lt;/math&gt; such that &lt;math&gt;d(O_{r}, O_{n}) &lt;= r(O_{r})&lt;/math&gt;
        '''if''' &lt;math&gt;N_{in}&lt;/math&gt; is not empty '''then'''
        {
           /*If there are one or more entry, then look for an entry such that is closer to the new object*/
           &lt;math&gt;O_{r}^{*} = \min_{O_{r}\in N_{in}} d(O_{r}, O_{n})&lt;/math&gt;
        }
        '''else'''
        {
           /*If there are no such entry, then look for an object with minimal distance from */ 
           /*its covering radius's edge to the new object*/
           &lt;math&gt;O_{r}^{*} = \min_{O_{r}\in N_{in}} d(O_{r}, O_{n}) - r(O_{r})&lt;/math&gt;
           /*Upgrade the new radii of the entry*/
           &lt;math&gt;r(O_{r}^{*})&lt;/math&gt; = &lt;math&gt;d(O_{r}^{*}, O_{n})&lt;/math&gt;
        }
        /*Continue inserting in the next level*/
        return insert(&lt;math&gt;T(O_{r}^{*})&lt;/math&gt;, &lt;math&gt;O_{n}&lt;/math&gt;);
   '''else'''
   {
        /*If the node has capacity then just insert the new object*/
        '''if''' &lt;math&gt;N&lt;/math&gt; is not full '''then'''
        {  store(&lt;math&gt;N&lt;/math&gt;, &lt;math&gt;O_{n}&lt;/math&gt;)   }
        /*The node is at full capacity, then it is needed to do a new split in this level*/
        '''else'''
        {  split(&lt;math&gt;N&lt;/math&gt;, &lt;math&gt;O_{n}&lt;/math&gt;) }
   }
{{algorithm-end}}

=== Split ===
If the split method arrives to the root of the tree, then it choose two routing objects from &lt;math&gt;N&lt;/math&gt;, and creates two new nodes containing all the objects in original &lt;math&gt;N&lt;/math&gt;, and store them into the new root. If split methods arrives to a node &lt;math&gt;N&lt;/math&gt; that is not the root of the tree, the method choose two new routing objects from &lt;math&gt;N&lt;/math&gt;, re-arrange every routing object in &lt;math&gt;N&lt;/math&gt; in two new nodes &lt;math&gt;N_{1}&lt;/math&gt; and &lt;math&gt;N_{2}&lt;/math&gt;, and store this new nodes in the parent node &lt;math&gt;N_{p}&lt;/math&gt; of original &lt;math&gt;N&lt;/math&gt;.  The split must be repeated if &lt;math&gt;N_{p}&lt;/math&gt; has not enough capacity to store &lt;math&gt;N_{2}&lt;/math&gt;. The algorithm is as follow:

{{algorithm-begin|name=Split}}
   Input: Node &lt;math&gt;N&lt;/math&gt;  of M-Tree &lt;math&gt;MT&lt;/math&gt;, Entry &lt;math&gt;O_{n}&lt;/math&gt;
   Output: A new instance of &lt;math&gt;MT&lt;/math&gt; containing a new partition.

   /*The new routing objects are now all those in the node plus the new routing object*/
   let be &lt;math&gt;NN&lt;/math&gt; entries of &lt;math&gt;N \cup O&lt;/math&gt;
   '''if''' &lt;math&gt;N&lt;/math&gt; is not the root '''then'''
   {
      /*Get the parent node and the parent routing object*/
      let &lt;math&gt;O_{p}&lt;/math&gt; be the parent routing object of &lt;math&gt;N&lt;/math&gt;
      let &lt;math&gt;N_{p}&lt;/math&gt; be the parent node of &lt;math&gt;N&lt;/math&gt;
   }
   /*This node will contain part of the objects of the node to be split*/
   Create a new node &lt;math&gt;N'&lt;/math&gt;
   /*Promote two routing objects from the node to be split, to be new routing objects*/
   Create new objects &lt;math&gt;O_{p1}&lt;/math&gt; and &lt;math&gt;O_{p2}&lt;/math&gt;.
   Promote(&lt;math&gt;N&lt;/math&gt;, &lt;math&gt;O_{p1}&lt;/math&gt;, &lt;math&gt;O_{p2}&lt;/math&gt;)
   /*Choose which objects from the node being split will act as new routing objects*/
   Partition(&lt;math&gt;N&lt;/math&gt;, &lt;math&gt;O_{p1}&lt;/math&gt;, &lt;math&gt;O_{p2}&lt;/math&gt;, &lt;math&gt;N_{1}&lt;/math&gt;, &lt;math&gt;N_{2}&lt;/math&gt;)
   /*Store entries in each new routing object*/
   Store &lt;math&gt;N_{1}&lt;/math&gt;'s entries in &lt;math&gt;N&lt;/math&gt; and &lt;math&gt;N_{2}&lt;/math&gt;'s entries in &lt;math&gt;N'&lt;/math&gt;
   '''if''' &lt;math&gt;N&lt;/math&gt; is the current root '''then'''
   {
       /*Create a new node and set it as new root and store the new routing objects*/
       Create a new root node &lt;math&gt;N_{p}&lt;/math&gt;
       Store &lt;math&gt;O_{p1}&lt;/math&gt; and &lt;math&gt;O_{p2}&lt;/math&gt; in &lt;math&gt;N_{p}&lt;/math&gt;
   }
   '''else'''
   {
       /*Now use the parent rouing object to store one of the new objects*/
       Replace entry &lt;math&gt;O_{p}&lt;/math&gt; with entry &lt;math&gt;O_{p1}&lt;/math&gt; in &lt;math&gt;N_{p}&lt;/math&gt;
       '''if''' &lt;math&gt;N_{p}&lt;/math&gt; is no full '''then'''
       {
           /*The second routinb object is stored in the parent only if it has free capacity*/
           Store &lt;math&gt;O_{p2}&lt;/math&gt; in &lt;math&gt;N_{p}&lt;/math&gt;
       }
       '''else'''
       {
            /*If there is no free capacity then split the level up*/
            split(&lt;math&gt;N_{p}&lt;/math&gt;, &lt;math&gt;O_{p2}&lt;/math&gt;)
       }
   }
{{algorithm-end}}

== M-Tree Queries ==

=== Range Query ===
A range query is where a minimum similarity/maximum distance value is speciﬁed. 
For a given query object Q ∈ D and a maximum search distance
r(Q), the range query '''range'''(Q, r(Q)) selects all the indexed objects Oj such that d(Oj, Q) ≤ r(Q).&lt;ref name=&quot;Univ Bologna Range&quot;&gt;{{cite web|title=Indexing Metric Spaces with M-tree|url=http://www-db.deis.unibo.it/research/papers/SEBD97.pdf|work=Department of Computer Science and Engineering|publisher=University of Bologna|accessdate=19 November 2013|author=P. Ciaccia, M. Patella, F. Rabitti, P. Zezula|page=3}}&lt;/ref&gt;

Algorithm RangeSearch starts from the root node and recursively traverses all the paths which cannot be excluded from leading to qualifying objects.
{{algorithm-begin|name=Insert}}
Input: Node &lt;math&gt;N&lt;/math&gt;  of M-Tree &lt;math&gt;MT&lt;/math&gt;,  &lt;math&gt;Q&lt;/math&gt;: query object, &lt;math&gt;r(Q)&lt;/math&gt;: search radius

Output: all the DB objects such that &lt;math&gt;d(Oj, Q)&lt;/math&gt; ≤ &lt;math&gt;r(Q)&lt;/math&gt;

{ let &lt;math&gt;O_{p}&lt;/math&gt; be the parent object of node &lt;math&gt;N&lt;/math&gt;;

'''if''' &lt;math&gt;N&lt;/math&gt; is not a leaf
'''then''' { for each &lt;math&gt;entry(O_{r})&lt;/math&gt; in N do:
           '''if''' | &lt;math&gt;d(O_{p}, Q)&lt;/math&gt; − &lt;math&gt;d(O_{r}, O_{p})&lt;/math&gt; | ≤ &lt;math&gt;r(Q) +r(O_{r})&lt;/math&gt;
           '''then''' { Compute &lt;math&gt;d(O_{r}, Q)&lt;/math&gt;;
                       '''if''' &lt;math&gt;d(O_{r}, Q)&lt;/math&gt; ≤ &lt;math&gt;r(Q) +r(O_{r})&lt;/math&gt;
                       '''then''' &lt;math&gt;RangeSearch(*ptr(T(O_{r})),Q,r(Q))&lt;/math&gt;; }}
'''else''' { for each &lt;math&gt;entry(O_{j})&lt;/math&gt; in &lt;math&gt;N&lt;/math&gt; do:
           '''if''' | &lt;math&gt;d(O_{p}, Q)&lt;/math&gt; − &lt;math&gt;d(O_{j}, O_{p})&lt;/math&gt; | ≤ &lt;math&gt;r(Q)&lt;/math&gt;
           '''then''' { Compute &lt;math&gt;d(O_{j}, Q)&lt;/math&gt;;
                      '''if''' &lt;math&gt;d(O_{j}, Q)&lt;/math&gt; ≤ &lt;math&gt;r(Q)&lt;/math&gt;
                      '''then''' add &lt;math&gt;oid(O_{j})&lt;/math&gt; to the result; }}}
{{algorithm-end}}

&lt;math&gt;oid(O_{j})&lt;/math&gt; is the identiﬁer of the object which resides on a separate data ﬁle.

&lt;math&gt;T(O_{r})&lt;/math&gt; is a sub-tree – the covering tree of &lt;math&gt;O_{r}&lt;/math&gt;

=== k-NN queries ===
K Nearest Neighbor (k-NN) query takes the cardinality of the input set as an input perimeter. For a given query object Q ∈ D and an
integer k ≥ 1, the k-NN query NN(Q, k) selects the k indexed objects which have the shortest distance from Q, according to the distance function d.
&lt;ref name=&quot;Univ Bologna Range&quot;/&gt; 
{{Empty section|date=January 2011}}

==See also==

* [[Segment tree]]
* [[Interval tree]] - A degenerate R-Tree  for 1 dimension (usually time).
* [[Bounding volume hierarchy]]
* [[Spatial index]]
* [[GiST]]

==References==
{{reflist}}

{{CS-Trees}}

{{DEFAULTSORT:M-Tree}}
[[Category:Trees (data structures)]]
[[Category:Database index techniques]]
[[Category:Geometric data structures]]</text>
      <sha1>klqiib2vpov9qfluewxj5blpnzh60af</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Metric tree</title>
    <ns>0</ns>
    <id>3417216</id>
    <revision>
      <id>612582130</id>
      <parentid>600316916</parentid>
      <timestamp>2014-06-12T03:03:05Z</timestamp>
      <contributor>
        <ip>98.207.2.205</ip>
      </contributor>
      <comment>Added url for citation.</comment>
      <text xml:space="preserve" bytes="4098">{{About|the data structure|the type of metric space|Real tree}}

A '''metric tree''' is any [[tree (data structure)|tree]] [[data structure]] specialized to index data in [[metric space]]s. Metric trees exploit properties of metric spaces such as the [[triangle inequality]] to make accesses to the data more efficient. Examples include the [[M-tree]], [[vp-tree]]s, [[cover tree]]s, [[MVP Tree]]s, and [[bk tree]]s.&lt;ref name=&quot;Samet&quot;&gt;{{cite book|last=Samet|first=Hanan|title=Foundations of multidimensional and metric data structures|year=2006|publisher=Morgan Kaufmann|isbn=978-0-12-369446-1|url=http://books.google.com/books/about/Foundations_of_Multidimensional_and_Metr.html?id=vO-NRRKHG84C}}&lt;/ref&gt;
&lt;!-- should have a list and summary of metric trees, with links to the main articles. --&gt;

==Multidimensional search==

Most algorithms and data structures for searching a dataset are based on the classical [[binary search]] algorithm, and generalizations such as the [[k-d tree]] or [[range tree]] work by interleaving the [[binary search algorithm]] over the separate coordinates and treating each spatial coordinate as an independent search constraint. These data structures are well-suited for [[range query]] problems asking for every point &lt;math&gt;(x,y)&lt;/math&gt; that  satisfies &lt;math&gt;\mbox{min}_x \leq x \leq \mbox{max}_x&lt;/math&gt; and &lt;math&gt;\mbox{min}_y \leq y \leq \mbox{max}_y&lt;/math&gt;.

A limitation of these multidimensional search structures is that they are only defined for searching over objects that can be treated as vectors. They aren't applicable for the more general case in which the algorithm is given only a collection of objects and a function for measuring the distance or similarity between two objects. If, for example, someone were to create a function that returns a value indicating how similar one image is to another, a natural algorithmic problem would be to take a dataset of images and find the ones that are similar according to the function to a given query image.

==Metric data structures==

If there is no structure to the similarity measure then a [[brute force search]] requiring the comparison of the query image to every image in the dataset is the best that can be done. If, however, the similarity function satisfies the [[triangle inequality]] then it is possible to use the result of each comparison to prune the set of candidates to be examined.

The first article on metric trees, as well as the first use of the term &quot;metric tree&quot;, published in the open literature was by [[Jeffrey Uhlmann]] in 1991.&lt;ref&gt;{{cite journal |last=Uhlmann |first=Jeffrey |title=Satisfying General Proximity/Similarity Queries with Metric Trees |journal=Information Processing Letters |volume=40 |number=4 |year=1991}}&lt;/ref&gt; Other researchers were working independently on similar data structures. In particular, Peter Yianilos claimed to have independently discovered the same method, which he called a [[Vantage-point tree|vantage point tree]] (VP-tree).&lt;ref name=&quot;pny93&quot;&gt;{{cite conference
 | first = Peter N.
 | last = Yianilos
 | authorlink = 
 | coauthors = 
 | title = Data structures and algorithms for nearest neighbor search in general metric spaces
 | booktitle = Proceedings of the fourth annual ACM-SIAM Symposium on Discrete algorithms
 | pages = 311–321
 | publisher = Society for Industrial and Applied Mathematics Philadelphia, PA, USA
 | year = 1993
 | location = 
 | url = http://pnylab.com/pny/papers/vptree/vptree/
 | accessdate = 2008-08-22
 | id = pny93
}}&lt;/ref&gt;
The research on metric tree data structures blossomed in the late 1990s and included an examination by Google co-founder [[Sergey Brin]] of their use for very large databases.&lt;ref&gt;{{cite conference |last=Brin |first=Sergey |title=Near Neighbor Search in Large Metric Spaces |booktitle=21st International Conference on Very Large Data Bases (VLDB) |year=1995 | url=http://www.vldb.org/dblp/db/conf/vldb/Brin95.html}}&lt;/ref&gt; The first textbook on metric data structures was published in 2006.&lt;ref name=&quot;Samet&quot;/&gt;

==References==
{{Reflist}}

{{CS-Trees}}

[[Category:Trees (data structures)]]</text>
      <sha1>2uo1i7r5iv71oiry3f7mrhky5b59osc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Newick format</title>
    <ns>0</ns>
    <id>7857636</id>
    <revision>
      <id>594210502</id>
      <parentid>593633824</parentid>
      <timestamp>2014-02-06T13:53:18Z</timestamp>
      <contributor>
        <ip>216.66.5.43</ip>
      </contributor>
      <comment>/* The grammar nodes */ clarify distinction between leaf and internal nodes</comment>
      <text xml:space="preserve" bytes="5290">In mathematics, '''Newick tree format''' (or '''Newick notation''' or '''New Hampshire tree format''') is a way of representing [[graph-theoretical tree]]s with edge lengths using parentheses and commas.  It was adopted by James Archie, William H. E. Day, [[Joseph Felsenstein]], [[Wayne Maddison]], Christopher Meacham, F. James Rohlf, and David Swofford, at two meetings in 1986, the second of which was at Newick's restaurant in [[Dover, New Hampshire|Dover]], New Hampshire, US. The adopted format is a generalization of the format developed by Meacham in 1984 for the first tree-drawing programs in Felsenstein's [[PHYLIP]] package.&lt;ref&gt;[http://evolution.genetics.washington.edu/phylip/newicktree.html The Newick tree format.]&lt;/ref&gt;

==Examples==

The following tree:

[[Image:NewickExample.svg]]

could be represented in Newick format in several ways 

 ({{Not a typo|,,}}(,));                               ''no nodes are named''
 (A,B,(C,D));                           ''leaf nodes are named''
 (A,B,(C,D)E)F;                         ''all nodes are named''
 (:0.1,:0.2,(:0.3,:0.4):0.5);           ''all but root node have a distance to parent''
 (:0.1,:0.2,(:0.3,:0.4):0.5):0.0;       ''all have a distance to parent''
 (A:0.1,B:0.2,(C:0.3,D:0.4):0.5);       ''distances and leaf names'' '''(popular)'''
 (A:0.1,B:0.2,(C:0.3,D:0.4)E:0.5)F;     ''distances and all names''
 ((B:0.2,(C:0.3,D:0.4)E:0.5)F:0.1)A;    ''a tree rooted on a leaf node'' '''(rare)'''

Newick format is typically used for tools like [[PHYLIP]] and is a minimal definition for a [[phylogenetic tree]].

==Rooted, unrooted, and binary trees==
When an ''unrooted'' tree is represented in Newick notation, an arbitrary node is chosen as its root.  Whether rooted or unrooted, typically a tree's representation is rooted on an internal node and it is rare (but legal) to root a tree on a leaf node.  

A ''rooted binary'' tree that is rooted on an internal node has exactly two immediate descendant nodes for each internal node.
An ''unrooted binary'' tree that is rooted on an arbitrary internal node has exactly three immediate descendant nodes for the root node, and each other internal node has exactly two immediate descendant nodes.
A ''binary tree rooted from a leaf'' has at most one immediate descendant node for the root node, and each internal node has exactly two immediate descendant nodes.

==Grammar==
A grammar for parsing the Newick format:

=== The grammar nodes ===

    '''Tree''': The full input Newick Format for a single tree
    '''Subtree''': an internal node (and its descendants) or a leaf node
    '''Leaf''': a node with no descendants
    '''Internal''': a node and its one or more descendants
    '''BranchSet''': a set of one or more Branches
    '''Branch''': a tree edge and its descendant subtree.
    '''Name''': the name of a node
    '''Length''': the length of a tree edge.

=== The grammar rules ===
Note, &quot;|&quot; separates alternatives.

    '''Tree''' --&gt; '''Subtree''' &quot;;&quot; | '''Branch''' &quot;;&quot;
    '''Subtree''' --&gt; '''Leaf''' | '''Internal'''
    '''Leaf''' --&gt; '''Name'''
    '''Internal''' --&gt; &quot;(&quot; '''BranchSet''' &quot;)&quot; '''Name'''
    '''BranchSet''' --&gt; '''Branch''' | '''BranchSet''' &quot;,&quot; '''Branch'''
    '''Branch''' --&gt; '''Subtree''' '''Length'''
    '''Name''' --&gt; ''empty'' | ''string''
    '''Length''' --&gt; ''empty'' | &quot;:&quot; ''number''

Whitespace (spaces, tabs, carriage returns, and linefeeds) within ''number'' is prohibited.  Whitespace within ''string'' is often prohibited.  Whitespace elsewhere is ignored.  Sometimes the '''Name''' ''string'' must be of a specified fixed length; otherwise the punctuation characters from the grammar (semicolon, parentheses, comma, and colon) are prohibited.  The '''Tree''' --&gt; '''Branch''' &quot;;&quot; production makes the entire tree descendant from nowhere, which can be nonsensical, and is sometimes prohibited.

Note that when a tree having more than one leaf is rooted from one of its leaves, a representation that is rarely seen in practice, the root leaf is characterized as an '''Internal''' node by the above grammar.  Generally, a ''root node'' labeled as '''Internal''' should be construed as a leaf if and only if it has exactly one '''Branch''' in its '''BranchSet'''.  One can make a grammar that formalizes this distinction by replacing the above '''Tree''' production rule with

    '''Tree''' --&gt; '''RootLeaf''' &quot;;&quot; | '''RootInternal''' &quot;;&quot; | '''Branch''' &quot;;&quot;
    '''RootLeaf''' --&gt; '''Name''' | &quot;(&quot; '''Branch''' &quot;)&quot; '''Name'''
    '''RootInternal''' --&gt; &quot;(&quot; '''BranchSet''' &quot;,&quot; '''Branch''' &quot;)&quot; '''Name'''

The first '''RootLeaf''' production is for a tree with exactly one leaf.  The second '''RootLeaf''' production is for rooting a tree from one of its two or more leaves.

== See also ==
* [[DOT language]]
* Gary Olsen's Interpretation of the &quot;Newick's 8:45&quot; Tree Format Standard [http://evolution.genetics.washington.edu/phylip/newick_doc.html]
*[[phyloXML]]
*[http://phylogram.org/ Miyamoto and Goodman's Phylogram of Eutherian Mammals] An example of a large phylogram with its Newick format representation.
*[[T-REX (Webserver)]] allows handling phylogenetic trees and networks in the Newick format.

== References ==
{{reflist}}

[[Category:Trees (data structures)]]
[[Category:Graph description languages]]</text>
      <sha1>hxp7gzxhx6l4x2rfnmzplini1c68cf5</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Octree</title>
    <ns>0</ns>
    <id>675699</id>
    <revision>
      <id>615184571</id>
      <parentid>615184221</parentid>
      <timestamp>2014-07-01T18:02:52Z</timestamp>
      <contributor>
        <username>Alanf777</username>
        <id>651119</id>
      </contributor>
      <comment>/* Octrees for spatial representation */ grammar</comment>
      <text xml:space="preserve" bytes="6956">[[File:Octree2.svg|thumb|right|400px|Left: Recursive subdivision of a cube into [[octant (solid geometry)|octant]]s. Right: The corresponding octree.]]

An '''octree''' is a [[tree data structure]] in which each [[internal node]] has exactly eight [[child node|children]]. Octrees are most often used to partition a three dimensional space by recursively subdividing it into eight octants. Octrees are the three-dimensional analog of [[quadtree]]s. The name is formed from ''oct'' + ''tree'', but note that it is normally written &quot;''octree''&quot; with only one &quot;t&quot;. Octrees are often used in [[3D graphics]] and 3D [[game engine]]s.

==Octrees for spatial representation==

Each node in an octree subdivides the space it represents into eight [[octant (solid geometry)|octant]]s. In a point region (PR) octree, the node stores an explicit 3-dimensional point, which is the &quot;center&quot; of the subdivision for that node; the point defines one of the corners for each of the eight children. In a matrix based (MX) octree, the subdivision point is implicitly the center of the space the node represents. The root node of a PR octree can represent infinite space; the root node of an MX octree must represent a finite bounded space so that the implicit centers are well-defined. Note that Octrees are not the same as [[k-d tree|''k''-d trees]]: ''k''-d trees split along a dimension and octrees split around a point. Also ''k''-d trees are always binary, which is not the case for octrees.
By using a [[depth-first search]] the nodes are to be traversed and only required surfaces are to be viewed.

== History ==
The use of octrees for [[3D computer graphics]] was pioneered by Donald Meagher at [[Rensselaer Polytechnic Institute]], described in a 1980 report &quot;Octree Encoding: A New Technique for the Representation, Manipulation and Display of Arbitrary 3-D Objects by Computer&quot;,&lt;ref&gt;{{cite journal|last=Meagher|first=Donald|title=Octree Encoding: A New Technique for the Representation, Manipulation and Display of Arbitrary 3-D Objects by Computer|journal=Rensselaer Polytechnic Institute|date=October 1980|issue=Technical Report IPL-TR-80-111}}&lt;/ref&gt;  for which he holds a 1995 patent (with a 1984 [[priority right|priority date]]) &quot;High-speed image generation of complex solid objects using octree encoding&quot; &lt;ref&gt;{{cite web|last=Meagher|first=Donald|title=High-speed image generation of complex solid objects using octree encoding|url=http://www.google.com/patents/EP0152741B1?cl=en|publisher=USPO|accessdate=20 September 2012}}&lt;/ref&gt;

==Common uses of octrees==
* [[3D computer graphics]]
* [[Spatial index]]ing
* [[Nearest neighbor search]]
* Efficient [[collision detection]] in three dimensions
* [[View frustum culling]]
* [[Fast Multipole Method]]
* [[Unstructured grid]]
* [[Finite element analysis]]
* [[Sparse voxel octree]]
* [[State estimation]]&lt;ref&gt;[http://isas.uka.de/Publikationen/Fusion10_EberhardtKlumpp.pdf Henning Eberhardt, Vesa Klumpp, Uwe D. Hanebeck, ''Density Trees for Efficient Nonlinear State Estimation'', Proceedings of the 13th International Conference on Information Fusion, Edinburgh, United Kingdom, July, 2010.]&lt;/ref&gt;
* [[Set estimation]]&lt;ref&gt;
[http://www.ensta-bretagne.fr/jaulin/paper_drevelle_nolcos_2013.pdf V. Drevelle, L. Jaulin and B. Zerr, ''Guaranteed Characterization of the Explored Space of a Mobile Robot by using Subpavings'', NOLCOS 2013.]&lt;/ref&gt;

==Application to color quantization==

The octree [[color quantization]] algorithm, invented by Gervautz and Purgathofer in 1988, encodes image color data as an octree up to nine levels deep. Octrees are used because &lt;math&gt;2^3 = 8&lt;/math&gt; and there are three color components in the [[RGB]] system. The node index to branch out from at the top level is determined by a formula that uses the most significant bits of the red, green, and blue color components, e.g. 4r + 2g + b. The next lower level uses the next bit significance, and so on. Less significant bits are sometimes ignored to reduce the tree size.

The algorithm is highly memory efficient because the tree's size can be limited. The bottom level of the octree consists of leaf nodes that accrue color data not represented in the tree; these nodes initially contain single bits. If much more than the desired number of palette colors are entered into the octree, its size can be continually reduced by seeking out a bottom-level node and averaging its bit data up into a leaf node, pruning part of the tree. Once sampling is complete, exploring all routes in the tree down to the leaf nodes, taking note of the bits along the way, will yield approximately the required number of colors.

==See also==
* [[Binary space partitioning]]
* [[K-d tree]]
* [[Quadtree]]
* [[Subpaving]]
* [[Bounding Interval Hierarchy]]
* [[Klee's measure problem]]
* [[Linear octrees]]
* [[Cube 2: Sauerbraten|Cube 2]], a 3D game engine in which geometry is almost entirely based on octrees
* [[OGRE]], has an Octree Scene Manager Implementation
* [[Irrlicht Engine]], supports octree scene nodes
* [[id Tech 6]] an in-development 3D game engine that utilizes voxels stored in octrees
* [[Voxel]]

==References==
&lt;references/&gt;

==External links==
{{commons category|Octrees}}
*[http://www.microsoft.com/msj/archive/S3F1.aspx Octree Quantization in Microsoft Systems Journal]
*[http://www.ddj.com/184409805 Color Quantization using Octrees in Dr. Dobb's]
*[ftp://ftp.drdobbs.com/sourcecode/ddj/1996/9601.zip Color Quantization using Octrees in Dr. Dobb's Source Code]
*[http://web.cs.wpi.edu/~matt/courses/cs563/talks/color_quant/CQoctree.html Octree Color Quantization Overview]
*[http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=727419 Parallel implementation of octtree generation algorithm, P. Sojan Lal, A Unnikrishnan, K Poulose Jacob, ICIP 1997, IEEE Digital Library]
*[http://dblp.uni-trier.de/db/conf/viip/viip2001.html#LalUJ01 Generation of Octrees from Raster Scan with Reduced Information Loss, P. Sojan Lal, A Unnikrishnan, K Poulose Jacob, IASTED International conference VIIP 2001]  [http://www.actapress.com/catalogue2009/proc_series13.html#viip2001]
*[http://nomis80.org/code/octree.html C++ implementation (GPL license)]
*[http://sc07.supercomputing.org/schedule/pdf/pap117.pdf Parallel Octrees for Finite Element Applications]
*[http://www.sauerbraten.org/ Cube 2: Sauerbraten - a game written in the octree-heavy Cube 2 engine]
*[http://www.ogre3d.org Ogre - A 3d Object-oriented Graphics Rendering Engine with a Octree Scene Manager Implementation (MIT license)]
*[http://www.cc.gatech.edu/csela/dendro Dendro: parallel multigrid for octree meshes (MPI/C++ implementation)]
*[http://www.youtube.com/watch?v=Jw4VAgcWruY '''Video''': Use of an octree in state estimation]
*[https://github.com/toki78/RayCL Source code of an OpenCL raytracer applet using an Octree]

{{CS-Trees}}

[[Category:Trees (data structures)]]
[[Category:Computer graphics data structures]]
[[Category:Database index techniques]]</text>
      <sha1>eck49540staosjknnuw0ti17rpitf7f</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Parse tree</title>
    <ns>0</ns>
    <id>118404</id>
    <revision>
      <id>614676911</id>
      <parentid>614676770</parentid>
      <timestamp>2014-06-27T19:56:36Z</timestamp>
      <contributor>
        <username>Bollyjeff</username>
        <id>12037174</id>
      </contributor>
      <comment>Spelling/grammar correction</comment>
      <text xml:space="preserve" bytes="7109">{{Not to be confused with |Abstract syntax tree}}

A '''concrete syntax tree''' or '''parse tree''' or '''parsing tree'''&lt;ref&gt;See Chiswell and Hodges 2007: 34.&lt;/ref&gt; or '''derivation tree''' is an ordered, rooted [[tree (data structure)|tree]] that represents the [[syntax|syntactic]] structure of a [[string (computer science)|string]] according to some [[context-free grammar]]. Parse trees are usually constructed according to one of two competing relations, either in terms of the constituency relation of constituency grammars (= [[phrase structure grammar]]s) or in terms of the dependency relation of [[dependency grammar]]s. Parse trees are distinct from [[abstract syntax tree]]s (also known simply as syntax trees), in that their structure and elements more concretely reflect the syntax of the input language. Parse trees may be generated for [[sentence (linguistics)|sentence]]s in [[natural language]]s (''see'' [[natural language processing]]), as well as during [[compiler|processing]] of computer languages, such as [[programming language]]s.

==Notes on terminology==
The term ''parse tree'' itself is used primarily within the field of [[computational linguistics]]. Theoretical syntax tends to prefer the term ''syntax tree'' over ''parse tree''. When diagramming sentences in [[grammar school]], one refers to [[sentence diagram]]s. The sentence diagrams that one learns in middle school ([[Reed-Kellogg diagram]]s) are, however, much different from the parse trees of computational linguistics and syntax trees of theoretical linguistics.

==Constituency-based parse trees==
The constituency-based parse trees of constituency grammars (= [[phrase structure grammar]]s) distinguish between terminal and non-terminal nodes. The [[interior node]]s are labeled by [[nonterminal|non-terminal]] categories of the grammar, while the [[leaf node]]s are labeled by [[terminal symbol|terminal]] categories. The image below represents a constituency-based parse tree; it shows the syntactic structure of the [[English language|English]] sentence ''John hit the ball'':

:::[[File:Parse tree 1.jpg|Parse tree PSG]]

The parse tree is the entire structure, starting from S and ending in each of the leaf nodes (''John'', ''hit'', ''the'', ''ball''). The following abbreviations are used in the tree:

::* S for [[sentence (linguistics)|sentence]], the top-level structure in this example

::* NP for [[noun phrase]]. The first (leftmost) NP, a single noun &quot;John&quot;, serves as the [[subject (grammar)|subject]] of the sentence. The second one is the [[object (grammar)|object]] of the sentence.

::* VP for [[verb phrase]], which serves as the [[predicate (grammar)|predicate]]

::* V for [[verb]]. In this case, it's a [[transitive verb]] ''hit''.

::* D for [[determiner (class)|determiner]], in this instance the [[article (grammar)|definite article]] &quot;the&quot;

::* N for [[noun]]

Each node in the tree is either a ''root'' node, a ''branch'' node, or a ''leaf'' node.&lt;ref&gt;See Carnie (2013:118ff.) for an introduction to the basic concepts of syntax trees (e.g. root node, terminal node, non-terminal node, etc.).&lt;/ref&gt; A root node is a node that doesn't have any branches on top of it. Within a sentence, there is only ever one root node. A branch node is a mother node that connects to two or more daughter nodes. A leaf node, however; is a terminal node that does not dominate other nodes in the tree. S is the root node, NP and VP are branch nodes, and ''John'' (N), ''hit'' (V), ''the'' (D), and ''ball'' (N) are all leaf nodes. The leaves are the lexical tokens of the sentence.&lt;ref&gt;See Alfred et al. 2007.&lt;/ref&gt; A node can also be referred to as ''parent'' node or a ''child'' node. A parent node is one that has at least one other node linked by a branch under it. In the example, S is a parent of both N and VP. A child node is one that has at least one node directly above it to which it is linked by a branch of a tree. From the example, ''hit'' is a child node of V.  The terms ''mother'' and ''daughter'' are also sometimes used for this relationship.

==Dependency-based parse trees==
The dependency-based parse trees of [[dependency grammar]]s&lt;ref&gt;See for example Ágel et al. 2003/2006.&lt;/ref&gt; see all nodes as terminal, which means they do not acknowledge the distinction between terminal and non-terminal categories. They are simpler on average than constituency-based parse trees because they contain many fewer nodes. The dependency-based parse tree for the example sentence above is as follows:

:::[[File:Parse2.jpg|Parse tree DG]]

This parse tree lacks the phrasal categories (S, VP, and NP) seen in the constituency-based counterpart above. Like the constituency-based tree however, [[constituent (linguistics)|constituent]] structure is acknowledged. Any complete sub-tree of the tree is a constituent. Thus this dependency-based parse tree acknowledges the subject noun ''John'' and the object noun phrase ''the ball'' as constituents just like the constituency-based parse tree does. 

The constituency vs. dependency distinction is far-reaching. Whether the additional syntactic structure associated with constituency-based parse trees is necessary or beneficial is a matter of debate.

==Notes==
{{reflist|2}}

==See also==
{{div col|cols=3}}
*[[Constituent (linguistics)]]
*[[Dependency grammar]]
*[[Computational linguistics]]
*[[Terminal and non-terminal functions]]
*[[Parsing]]
*[[Phrase structure grammar]]
*[[Sentence diagram]]
*[[Verb phrase]]
*[[Parse Thicket]] 
{{div col end}}

==References==
{{div col|cols=2}}
*[[Vilmos Ágel|Ágel, V.]], Ludwig Eichinger, Hans-Werner Eroms, Peter Hellwig, Hans Heringer, and Hennig Lobin (eds.) 2003/6. Dependency and valency: An international handbook of contemporary research. Berlin: Walter de Gruyter.
*Carnie, A. 2013. Syntax: A generative introduction, 3rd edition. Malden, MA: Wiley-Blackwell.
*Chiswell, Ian and Wilfrid Hodges 2007. Mathematical logic. Oxford: Oxford University Press.
*Aho, Alfred et al. 2007. Compilers: Principles, techniques, &amp; tools. Boston: Pearson/Addison Wesley.
{{div col end}}

==External links==
* [http://www.ductape.net/~eppie/tree/ Syntax Tree Editor]
* [http://ltc.sourceforge.net/ Linguistic Tree Constructor]
* [http://www.ironcreek.net/phpsyntaxtree/ phpSyntaxTree] – Online parse tree drawing site
* [http://lrv.bplaced.net/syntaxtree/ phpSyntaxTree (Unicode)] – Online parse tree drawing site (improved version that supports Unicode)
* [http://www.ling.upenn.edu/advice/latex/qtree/ Qtree] – [[LaTeX]] package for drawing parse trees
* [http://www.ece.ubc.ca/~donaldd/treeform.htm TreeForm Syntax Tree Drawing Software]
* [http://yohasebe.com/rsyntaxtree/ rSyntaxTree] Enhanced version of phpSyntaxTree in Ruby with Unicode and Vectorized graphics
* [http://trimc-nlp.blogspot.com/2013/05/phrase-structure-and-dependency-parsing.html Visual Introduction to Parse Trees] Introduction and Transformation 
* [http://www.youtube.com/watch?v=UTnHwzVAIOo OpenCourseOnline] Dependency Parse Introduction (Christoper Manning)

[[Category:Syntax]]
[[Category:Trees (data structures)]]</text>
      <sha1>3dbqwi2nweyqx98w1dfkmtpvcdkt1yi</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Pebble automaton</title>
    <ns>0</ns>
    <id>16025028</id>
    <revision>
      <id>471784874</id>
      <parentid>436643966</parentid>
      <timestamp>2012-01-17T01:04:21Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Moving category Trees (structure) to [[:Category:Trees (data structures)]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2012 January 12]].</comment>
      <text xml:space="preserve" bytes="3009">In [[computer science]], a '''pebble automaton''' is an extension of [[tree walking automaton|tree walking automata]] which allows the automaton to use a finite amount of &quot;pebbles&quot;, used for marking tree node{{fact|date=January 2011}}. The result is a model stronger than ordinary tree walking automata, but still strictly weaker than [[tree automaton|branching automata]].

==Definitions==

A pebble automaton is a tree walking automaton with an additional finite set of fixed size containing pebbles, identified with &lt;math&gt;\{ 1, 2, \dots, n \}&lt;/math&gt;. Besides ordinary actions, an automaton can put a pebble at a currently visited node, lift a pebble from the currently visited node and perform a test &quot;is the i-th pebble present at the current node?&quot;. There is an important stack restriction on the order in which pebbles can be put or lifted - the i+1-th pebble can be put only if the pebbles from 1st to i-th are already on the tree, and the i+1-th pebble can be lifted only if pebbles from i+2-th to n-th are not on the tree. Without this restriction, the automaton has undecidable emptiness and expressive power beyond regular tree languages.

The class of languages recognized by deterministic (resp. nondeterministic) pebble automata with n pebbles is denoted &lt;math&gt;DPA_{n}&lt;/math&gt; (resp. &lt;math&gt;PA_{n}&lt;/math&gt;). We also define &lt;math&gt;DPA = \bigcup_{n} DPA_{n}&lt;/math&gt; and likewise &lt;math&gt;PA = \bigcup_{n} PA_{n}&lt;/math&gt;.

&lt;!-- ==Examples==

(to be added soon) --&gt;
==Properties==

*there exists a language recognized by a pebble automaton with 1 pebble, but not by any tree walking automaton; this implies that either &lt;math&gt;TWA \subsetneq DPA&lt;/math&gt; or these classes are incomparable, which is an open problem
*&lt;math&gt;PA \subsetneq REG&lt;/math&gt;, i.e. pebble automata are strictly weaker than [[tree automaton|branching automata]]
*it is not known whether &lt;math&gt;DPA = PA&lt;/math&gt;, i.e. whether pebble automata can be determinized
*it is not known whether pebble automata are closed under complementation
*the pebble hierarchy is strict, for every n &lt;math&gt;PA_{n} \subsetneq PA_{n+1}&lt;/math&gt; and &lt;math&gt;DPA_{n} \subsetneq DPA_{n+1}&lt;/math&gt; 

&lt;!-- ==Invisible pebbles==

(to be added soon)
 --&gt;
==Automata and logic==

Pebble automata admit an interesting logical characterization. Let &lt;math&gt;FO+TC&lt;/math&gt; denote the set of tree properties describable in [[transitive closure logic|transitive closure first-order logic]], and &lt;math&gt;FO+\text{pos}\,TC&lt;/math&gt; the same for positive transitive closure logic, i.e. a logic where the transitive closure operator is not used under the scope of negation. Then it can be proved that &lt;math&gt;PA \subseteq FO+TC&lt;/math&gt; and, in fact, &lt;math&gt;PA = FO+\text{pos}\,TC&lt;/math&gt; - the languages recognized by pebble automata are exactly those expressible in positive transitive closure logic.

==See also==

*[[Tree walking automaton|Tree walking automata]]
*[[Tree automaton|Branching automata]]
*[[Transitive closure logic]]

[[Category:Trees (data structures)]]
[[Category:Automata theory]]</text>
      <sha1>8wp8ps5iww90z7sxtknncfwp6ov9mzm</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Phylogenetic tree</title>
    <ns>0</ns>
    <id>149326</id>
    <revision>
      <id>620971626</id>
      <parentid>620971554</parentid>
      <timestamp>2014-08-12T21:31:52Z</timestamp>
      <contributor>
        <username>TranquilHope</username>
        <id>21846838</id>
      </contributor>
      <minor/>
      <comment>Reverted 1 edit by [[Special:Contributions/96.56.185.10|96.56.185.10]] ([[User talk:96.56.185.10|talk]]) to last revision by Nicolas Perrault III. ([[WP:TW|TW]])</comment>
      <text xml:space="preserve" bytes="16994">{{PhylomapB|caption=A speculatively rooted tree for [[rRNA]] [[gene]]s, showing major branches [[Bacteria]], [[Archaea]], and [[Eucaryota]]}}
{{Evolutionary biology}}

A '''phylogenetic tree''' or '''evolutionary tree''' is a branching [[diagram]] or &quot;[[tree (graph theory)|tree]]&quot; showing the inferred [[evolution]]ary relationships among various biological [[species]] or other entities—their [[phylogeny]]—based upon similarities and differences in their physical or genetic characteristics. The taxa joined together in the tree are implied to have descended from a [[common descent|common ancestor]].

In a {{em|rooted}} phylogenetic tree, each node with descendants represents the inferred [[most recent common ancestor]] of the descendants, and the edge lengths in some trees may be interpreted as [[time]] estimates. Each node is called a taxonomic unit. Internal nodes are generally called hypothetical taxonomic units, as they cannot be directly observed. Trees are useful in fields of biology such as [[bioinformatics]], [[systematics]], and [[comparative phylogenetics]].

==History==
The idea of a &quot;[[tree of life (science)|tree of life]]&quot; arose from ancient notions of a ladder-like progression from lower to higher forms of [[life]] (such as in the [[Great Chain of Being]]). Early representations of &quot;branching&quot; phylogenetic trees include a &quot;paleontological chart&quot; showing the geological relationships among plants and animals in the book ''Elementary Geology'', by Edward Hitchcock (first edition: 1840).

[[Charles Darwin]] (1859) also produced one of the first illustrations and crucially popularized the notion of an [[Natural selection|evolutionary &quot;tree&quot;]] in his seminal book ''[[The Origin of Species]]''. Over a century later, [[Evolutionary biology|evolutionary biologist]]s still use [[Tree structure|tree diagram]]s to depict [[evolution]] because such diagrams effectively convey the concept that [[speciation]] occurs through the [[Adaptation|adaptive]] and semi[[random]] splitting of lineages. Over time, species classification has become less static and more dynamic.

==Types==

===Rooted tree===
[[Image:Neomuratree.svg|thumb|220px|A phylogenetic tree, showing how Eukaryota and Archaea are more closely related to each other than to [[Bacteria]], based on [[Cavalier-Smith]]'s theory of bacterial evolution. (Cf. [[Last universal ancestor|LUCA]], [[Neomura]].)]]

A rooted phylogenetic tree is a [[directed graph|directed]] [[tree (data structure)|tree]] with a unique node corresponding to the (usually [[imputation (statistics)|imputed]]) most recent common ancestor of all the entities at the [[leaf node|leaves]] of the tree. The most common method for rooting trees is the use of an uncontroversial [[outgroup (cladistics)|outgroup]]—close enough to allow inference from sequence or trait data, but far enough to be a clear outgroup.

===Unrooted tree===
[[Image:MyosinUnrootedTree.jpg|thumb|340px|Fig. 2: Unrooted tree of the myosin supergene family&lt;ref name=Hodge_2000&gt;{{cite journal|author=Hodge T, Cope M |title=A myosin family tree|journal=J Cell Sci |volume=113|issue=19|pages=3353–4 |date=1 October 2000|url=http://jcs.biologists.org/cgi/content/full/113/19/3353 |pmid=10984423}}&lt;/ref&gt;]]

Unrooted trees illustrate the relatedness of the leaf nodes without making assumptions about ancestry. They do not require the ancestral root to be known or inferred.&lt;ref&gt;http://www.ncbi.nlm.nih.gov/Class/NAWBIS/Modules/Phylogenetics/phylo9.html&lt;/ref&gt; Unrooted trees can always be generated from rooted ones by simply omitting the root. By contrast, inferring the root of an unrooted tree requires some means of identifying ancestry. This is normally done by including an outgroup in the input data so that the root is necessarily between the outgroup and the rest of the taxa in the tree, or by introducing additional assumptions about the relative rates of evolution on each branch, such as an application of the [[molecular clock]] [[hypothesis]]. Figure 2 depicts an unrooted phylogenetic tree for [[myosin]], a [[gene family|superfamily]] of [[protein]]s.&lt;ref name=Maher_2002&gt;{{cite journal|author=Maher BA|title=Uprooting the Tree of Life|journal=The Scientist|volume=16|pages=18 |year=2002|url=http://www.the-scientist.com/yr2002/sep/research1_020916.html}}&lt;/ref&gt;

===Bifurcating tree===
Both rooted and unrooted phylogenetic trees can be either [[bifurcation theory|bifurcating]] or multifurcating, and either labeled or unlabeled. A rooted bifurcating tree has exactly two descendants arising from each [[interior node]] (that is, it forms a [[binary tree]]), and an unrooted bifurcating tree takes the form of an [[unrooted binary tree]], a [[free tree]] with exactly three neighbors at each internal node. In contrast, a rooted multifurcating tree may have more than two children at some nodes and an unrooted multifurcating tree may have more than three neighbors at some nodes. A labeled tree has specific values assigned to its leaves, while an unlabeled tree, sometimes called a tree shape, defines a topology only. The number of possible trees for a given number of leaf nodes depends on the specific type of tree, but there are always more multifurcating than bifurcating trees, more labeled than unlabeled trees, and more rooted than unrooted trees. The last distinction is the most biologically relevant; it arises because there are many places on an unrooted tree to put the root. For labeled bifurcating trees, there are:
:&lt;math&gt;
(2n-3)!! = \frac{(2n-3)!}{2^{n-2}(n-2)!} \,,\,\text{for}\,n \ge 2
&lt;/math&gt;
total rooted trees and
:&lt;math&gt;
(2n-5)!! = \frac{(2n-5)!}{2^{n-3}(n-3)!} \,,\,\text{for}\,n \ge 3
&lt;/math&gt;
total unrooted trees, where &lt;math&gt;n&lt;/math&gt; represents the number of leaf nodes. Among labeled bifurcating trees, the number of unrooted trees with &lt;math&gt;n&lt;/math&gt; leaves is equal to the number of rooted trees with &lt;math&gt;n-1&lt;/math&gt; leaves.&lt;ref name=&quot;Felsenstein&quot;&gt;Felsenstein J. (2004). ''Inferring Phylogenies'' Sinauer Associates: Sunderland, MA.&lt;/ref&gt;

===Special tree types===
{{unreferenced|section|date=October 2012}}
[[File:Spindle diagram.jpg|thumb|right|Fig. 3: A spindle diagram, showing the evolution of the [[vertebrate]]s at class level, width of spindles indicating number of families. Spindle diagrams are often used in [[evolutionary taxonomy]].]]
[[Image:Tree of life SVG.svg|thumb|340px|Fig. 4: A highly resolved, automatically generated [[tree of life (biology)|tree of life]], based on completely sequenced genomes.&lt;ref&gt;{{cite journal |last=Letunic |first=I |year=2007 |title=Interactive Tree Of Life (iTOL): an online tool for phylogenetic tree display and annotation. |journal=Bioinformatics |volume=23 |pages=127–8 |format=[[Pubmed]] |pmid=17050570 |last2=Bork |first2=P |issue=1 |doi=10.1093/bioinformatics/btl529}}&lt;/ref&gt;&lt;ref&gt;{{cite journal | last = Ciccarelli|first=FD|year=2006|title=Toward automatic reconstruction of a highly resolved tree of life|journal=Science|volume=311|pages=1283–7|format=[[Pubmed]]|doi=10.1126/science.1123061|pmid=16513982|last2=Doerks|first2=T|last3=Von Mering|first3=C|last4=Creevey|first4=CJ|last5=Snel|first5=B|last6=Bork|first6=P|issue=5765|bibcode=2006Sci...311.1283C}}&lt;/ref&gt;]]

*A [[dendrogram]] is a broad term for the diagrammatic representation of a phylogenetic tree.
*A [[cladogram]] is a phylogenetic tree formed using [[cladistics|cladistic]] methods. This type of tree only represents a branching pattern; i.e., its branch spans do not represent time or relative amount of character change.
*A phylogram is a phylogenetic tree that has branch spans proportional to the amount of character change.
*A chronogram is a phylogenetic tree that explicitly represents evolutionary time through its branch spans.
*A spindle diagram (often called a Romerogram after the American palaeontologist [[Alfred Romer]]) is the representation of the evolution and abundance of the various taxa through time.

== Construction ==
{{Main|Computational phylogenetics}}
Phylogenetic trees among a nontrivial number of input sequences are constructed using [[computational phylogenetics]] methods. Distance-matrix methods such as [[neighbor-joining]] or [[UPGMA]], which calculate [[genetic distance]] from [[multiple sequence alignment]]s, are simplest to implement, but do not invoke an evolutionary model. Many sequence alignment methods such as [[ClustalW]] also create trees by using the simpler algorithms (i.e. those based on distance) of tree construction. [[Maximum parsimony]] is another simple method of estimating phylogenetic trees, but implies an implicit model of evolution (i.e. parsimony). More advanced methods use the [[optimality criterion]] of [[maximum likelihood]], often within a [[Bayesian inference|Bayesian Framework]], and apply an explicit model of evolution to phylogenetic tree estimation.&lt;ref name=&quot;Felsenstein&quot; /&gt; Identifying the optimal tree using many of these techniques is [[NP-hard]],&lt;ref name=&quot;Felsenstein&quot; /&gt; so [[heuristic]] search and [[Optimization (mathematics)|optimization]] methods are used in combination with tree-scoring functions to identify a reasonably good tree that fits the data.

Tree-building methods can be assessed on the basis of several criteria:&lt;ref&gt;{{cite journal | last1 = Penny | first1 = D. | last2 = Hendy | first2 = M. D. | last3 = Steel | first3 = M. A. | author3-link=Mike Steel (mathematician) | year = 1992 | title = Progress with methods for constructing evolutionary trees | url = | journal = Trends in Ecology and Evolution | volume = 7 | issue = | pages = 73–79 | doi=10.1016/0169-5347(92)90244-6}}&lt;/ref&gt;
* efficiency (how long does it take to compute the answer, how much memory does it need?)
* power (does it make good use of the data, or is information being wasted?)
* consistency (will it converge on the same answer repeatedly, if each time given different data for the same model problem?)
* robustness (does it cope well with violations of the assumptions of the underlying model?)
* falsifiability (does it alert us when it is not good to use, i.e. when assumptions are violated?)

Tree-building techniques have also gained the attention of mathematicians. Trees can also be built using [[T-theory]].&lt;ref&gt;A. Dress, K. T. Huber, and V. Moulton. 2001. Metric Spaces in Pure and Applied Mathematics. ''Documenta Mathematica'' ''LSU 2001'': 121-139&lt;/ref&gt;

== Limitations ==
{{Refimprove|section|date=October 2012}}
Although phylogenetic trees produced on the basis of sequenced [[gene]]s or [[genome|genomic]] data in different species can provide evolutionary insight, they have important limitations. They do not necessarily accurately represent the evolutionary history of the included taxa. The data on which they are based is [[signal noise|noisy]]; the analysis can be confounded by [[genetic recombination]],&lt;ref name=Arenas_2010&gt;{{cite journal |author=Arenas M, Posada D |title=The effect of recombination on the reconstruction of ancestral sequences |journal=Genetics |volume=184 |issue=4 |pages=1133–1139 |year=2010 |doi=10.1534/genetics.109.113423 }}&lt;/ref&gt; [[horizontal gene transfer]],&lt;ref name=Woese_2002&gt;{{cite journal |author=Woese C |title=On the evolution of cells |journal=Proc Natl Acad Sci USA |volume=99 |issue=13 |pages=8742–7 |year=2002 |pmid=12077305 |doi=10.1073/pnas.132266999 |pmc=124369|bibcode = 2002PNAS...99.8742W }}&lt;/ref&gt; [[Hybrid (biology)|hybrid]]isation between species that were not nearest neighbors on the tree before hybridisation takes place, [[convergent evolution]], and [[conserved sequence]]s.

Also, there are problems in basing the analysis on a single type of character, such as a single [[gene]] or [[protein]] or only on morphological analysis, because such trees constructed from another unrelated data source often differ from the first, and therefore great care is needed in inferring phylogenetic relationships among species. This is most true of genetic material that is subject to lateral gene transfer and [[Genetic recombination|recombination]], where different [[haplotype]] blocks can have different histories. In general, the output tree of a phylogenetic analysis is an estimate of the ''character'''s phylogeny (i.e. a gene tree) and not the phylogeny of the [[taxa]] (i.e. species tree) from which these characters were sampled, though ideally, both should be very close. For this reason, serious phylogenetic studies generally use a combination of genes that come from different genomic sources (e.g., from mitochondrial or plastid vs. nuclear genomes), or genes that would be expected to evolve under different selective regimes, so that homoplasy (false [[homology (biology)|homology]]) would be unlikely to result from natural selection.

When extinct species are included in a tree, they are [[leaf node|terminal node]]s, as it is unlikely that they are direct ancestors of any extant species. Skepticism might be applied when extinct species are included in trees that are wholly or partly based on DNA sequence data, due to the fact that little useful &quot;[[ancient DNA]]&quot; is preserved for longer than 100,000 years, and except in the most unusual circumstances no DNA sequences long enough for use in phylogenetic analyses have yet been recovered from material over 1 million years old.

The range of useful DNA materials has expanded with advances in extraction and sequencing technologies. Development of technologies able to infer sequences from smaller fragments, or from spatial patterns of DNA degradation products, would further expand the range of DNA considered useful.

In some organisms, [[endosymbiont]]s have an independent genetic history from the host.

[[Phylogenetic network]]s are used when bifurcating trees are not suitable, due to these complications which suggest a more [[reticulate]] evolutionary history of the organisms sampled..

==See also==
{{Portal|Evolutionary biology}}

===The &quot;tree of life&quot;===
*[[Evolutionary history of life]], an overview of the major time periods of life on earth
*[[Life]], the top level for Wikipedia articles on living species, reflecting a diversity of classification systems.
*[[Three-domain system]] (cell types)
*[[Wikispecies]], an external Wikimedia Foundation project to construct a &quot;tree of life&quot; appropriate for use by scientists

===Fields of study===
* [[Cladistics]]
* [[Comparative phylogenetics]]
* [[Computational phylogenetics]]
* [[Evolutionary taxonomy]]
* [[Evolutionary biology]]
* [[Generalized tree alignment]]
* [[Phylogenetics]]

===Software===
* [[Archaeopteryx (evolutionary tree visualization and analysis)|Archaeopteryx]]
* [[Dendroscope]]
* [[SplitsTree]]
* [[Treefinder]]

==References==&lt;!-- MolPhylEvol46:375;48:23,48:313. --&gt;
{{Reflist|2}}

==Further reading==
* Schuh, R. T. and A. V. Z. Brower. 2009. ''Biological Systematics: principles and applications (2nd edn.)'' ISBN 978-0-8014-4799-0
* [[MEGA, Molecular Evolutionary Genetics Analysis|MEGA]], a free software to draw phylogenetic trees.

==External links==
{{Commons category|Phylogenetic tree of life|Phylogenetic tree}}

=== Images ===
*[http://tolweb.org/tree/eukaryotes/accessory/treeoverview.html Phylogenetic Trees Based on 16s rDNA]
*[http://www.tellapallet.com/tree_of_life.htm Poster-sized tree of life illustration]
*[http://ycc.biosci.arizona.edu/nomenclature_system/fig1.html Human Y-Chromosome 2002 Phylogenetic Tree]
* In 2003, the [[Science (journal)|''Science'']] journal dedicated a special issue to the tree of life, including an [http://www.sciencemag.org/feature/data/tol/ online version of a tree of life].
*[http://itol.embl.de/ iTOL: Interactive Tree Of Life]
*[http://picbreeder.com/tol.php Phylogenetic Tree of Artificial Organisms Evolved on Computers]
*[http://phylogram.org/ Miyamoto and Goodman's Phylogram of Eutherian Mammals]

===General===
*An overview of different methods of tree visualization is available at {{cite doi|10.1016/j.tree.2011.12.002}}
*[http://www.discoverlife.org/tree Discover Life] An interactive tree based on the U.S. National Science Foundation's Assembling the Tree of Life Project
*[http://www.ohiou.edu/phylocode/index.html PhyloCode]
*[http://www.mrc-lmb.cam.ac.uk/myosin/trees/trees.html A Multiple Alignment of 139 Myosin Sequences and a Phylogenetic Tree]
*[http://tolweb.org/tree Tree of Life Web Project]
*[http://www.trex.uqam.ca Phylogenetic inferring on the T-REX server]
*[http://www.ncbi.nlm.nih.gov/Taxonomy/ NCBI's Taxonomy Database][http://www.ncbi.nlm.nih.gov/Taxonomy/]
*[http://ete.cgenomics.org ETE: A Python Environment for Tree Exploration] This is a programming library to analyze, manipulate and visualize phylogenetic trees. [http://www.biomedcentral.com/1471-2105/11/24 Ref.]
*[http://supfam.org/SUPERFAMILY/sTOL A daily-updated tree of (sequenced) life] {{cite doi|10.1038/srep02015}}

{{Phylogenetics}}
{{Origin of life}}
{{Portal bar|Evolutionary biology}}

{{DEFAULTSORT:Phylogenetic Tree}}
[[Category:Phylogenetics]]
[[Category:Tree of life| ]]
[[Category:Trees (data structures)]]</text>
      <sha1>mfyc46mp32m1zjpaod7qj60yg5ckc52</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Polychotomous key</title>
    <ns>0</ns>
    <id>17676093</id>
    <revision>
      <id>589788724</id>
      <parentid>586874183</parentid>
      <timestamp>2014-01-08T16:38:08Z</timestamp>
      <contributor>
        <username>MVaughanK</username>
        <id>20519698</id>
      </contributor>
      <text xml:space="preserve" bytes="1738">{{Other uses|Polytomous}}
{{Confusing|date=June 2010}}
'''Polychotomous key''' refers to the number of alternatives which a decision point may have in a non-temporal hierarchy of independent variables. The number of alternatives are equivalent to the root or [[nth root]] of a mathematical or logical variable.{{citation needed|date=September 2013}} Decision points or independent variables with two states have a binary root that is referred to as a [[wiktionary:dichotomy|dichotomous]] key whereas, the term polychotomous key refers to roots which are greater than one or unitary and usually greater than two or binary. Polychotomous keys are used in [[troubleshooting]] to build troubleshooting charts and in classification/identification schemes with characteristics that have more than one attribute and the order of characteristics is not inherently based on the progression of time.   

==See also==
*[[Number prefix]]
*[[Polychotomy]]

== Examples of Use ==
*[http://www-stat.stanford.edu/~jhf/ftp/poly.pdf Another Approach to Polychotomous Classification]
*[http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/polspline/html/polyclass.html Polyclass: polychotomous regression and multiple classification]
*[http://gerontologist.oxfordjournals.org/content/30/4/497.abstract The Development of a Hierarchical Polychotomous ADL-IADL Scale for Noninstitutionalized Elders]
*[http://www.cimms.ou.edu/~doswell/probability/Probability.html Probabilistic Forecasting - A Primer]
*[http://bioinformatics.oxfordjournals.org/cgi/content/full/22/8/950 Structured polychotomous machine diagnosis of multiple cancer types using gene expression]

[[Category:Concepts in logic]]
[[Category:Decision theory]]
[[Category:Trees (data structures)]]</text>
      <sha1>9jha8fy26xmrx64w3sbgndztt2bhar6</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>PQ tree</title>
    <ns>0</ns>
    <id>597568</id>
    <revision>
      <id>573810755</id>
      <parentid>543779820</parentid>
      <timestamp>2013-09-20T18:19:03Z</timestamp>
      <contributor>
        <ip>216.66.5.53</ip>
      </contributor>
      <comment>/* Examples and notation */ each matched pair of parentheses represents an internal node</comment>
      <text xml:space="preserve" bytes="4679">A '''PQ tree''' is a tree-based [[data structure]] that represents a family of [[permutation]]s on a set of elements, discovered and named by [[Kellogg S. Booth]] and [[George S. Lueker]] in 1976. It is a rooted, labeled tree, in which each element is represented by one of the [[leaf node]]s, and each non-leaf node is labelled P or Q. A P node has at least two children, and a Q node has at least three children.

A PQ tree represents its permutations via permissible reorderings of the children of its nodes. The children of a P node may be reordered in any way. The children of a Q node may be put in reverse order, but may not otherwise be reordered. A PQ tree represents all leaf node orderings that can be achieved by any sequence of these two operations. A PQ tree with many P and Q nodes can represent complicated subsets of the set of all possible orderings. However, not every set of orderings may be representable in this way; for instance, if an ordering is represented by a PQ tree, the reverse of the ordering must also be represented by the same tree.

PQ trees are used to solve problems where the goal is to find an ordering that satisfies various constraints. In these problems, constraints on the ordering are included one at a time, by modifying the PQ tree structure in such a way that it represents only orderings satisfying the constraint.  Applications of PQ trees include creating a [[Contig|contig map]] from [[DNA]] fragments, testing a matrix for the consecutive ones property, recognizing [[interval graph]]s and determining whether a graph is [[Planar graph|planar]].

==Examples and notation==
[[Image:pq-tree-5-leaves.svg|left|thumb|The PQ tree representing 
&lt;br/&gt;[1 (2 3 4) 5] ]]

If all the leaves of a PQ tree are connected directly to a root P node then all possible orderings are allowed. If all the leaves are connected directly to a root Q node then only one order and its reverse are allowed. If nodes a,b,c connect to a P node, which connects to a root P node, with all other leaf nodes connected directly to the root, then any ordering where a,b,c are contiguous is allowed.

Where graphical presentation is unavailable PQ trees are often noted using nested parenthesized lists.  Each matched pair of square parentheses represents a Q node and each matched pair of rounded parentheses represent a P node.  Leaves are non-parentheses elements of the lists.  The image on the left is represented in this notation by [1 (2 3 4) 5]. This PQ tree represents the following twelve permutations on the set {1, 2, 3, 4, 5}:
: 12345, 12435, 13245, 13425, 14235, 14325, 52341, 52431, 53241, 53421, 54231, 54321.

==PC trees==

The '''PC tree''', developed by [[Wei-Kuan Shih]] and [[Wen-Lian Hsu]], is a more recent generalization of the PQ tree. Like the PQ tree, it represents permutations by reorderings of nodes in a tree, with elements represented at the leaves of the tree. Unlike the PQ tree, the PC tree is unrooted. The nodes adjacent to any non-leaf node labeled P may be reordered arbitrarily as in the PQ tree, while the nodes adjacent to any non-leaf node labeled C have a fixed [[cyclic order]] and may only be reordered by reversing this order.  Thus, a PC tree can only represent sets of orderings in which any circular permutation or reversal of an ordering in the set is also in the set. However, a PQ tree on ''n'' elements may be simulated by a PC tree on ''n'' + 1 elements, where the extra element serves to root the PC tree.  The data structure operations required to perform a [[planarity testing]] algorithm on PC trees are somewhat simpler than the corresponding operations on PQ trees.

==See also==
*[[Series-parallel partial order]]

==References==
* {{cite journal
  | doi = 10.1016/S0022-0000(76)80045-1
  | author = Booth, Kellogg S. and Lueker, George S.
  | title = Testing for the consecutive ones property, interval graphs, and graph planarity using PQ-tree algorithms
  | journal = [[Journal of Computer and System Sciences]]
  | volume = 13
  | issue = 3
  | pages = 335–379
  | year = 1976}}
* {{cite journal
  | author = Shih, Wei-Kuan and Hsu, Wen-Lian
  | title = A new planarity test
  | url = http://www.iis.sinica.edu.tw/IASL/webpdf/paper-1999-A_New_Planarity_test.pdf
  | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]
  | volume = 223
  | issue = 1-2
  | pages = 179–191
  | year = 1999
  | doi = 10.1016/S0304-3975(98)00120-0}}

==External links==
*[http://knol.google.com/k/greg-grothaus/pq-trees-and-the-consecutive-ones/2zbou4xzp3j9w/4 PQ Trees and the Consecutive Ones Property]* 

{{CS-Trees}}

{{DEFAULTSORT:Pq Tree}}
[[Category:Trees (data structures)]]</text>
      <sha1>8x7n0ep05ufejlm2rhds8c4ypawsfgb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Prediction Suffix Tree</title>
    <ns>0</ns>
    <id>17905449</id>
    <revision>
      <id>622884101</id>
      <parentid>622883972</parentid>
      <timestamp>2014-08-26T13:46:25Z</timestamp>
      <contributor>
        <username>Metaxal</username>
        <id>4130283</id>
      </contributor>
      <comment>/* References */ fix ref again</comment>
      <text xml:space="preserve" bytes="1193">{{cleanup|date=August 2008}}

The concept of the [[Markov chain]] of order L, which we essentially owe to the Russian
mathematician [[Andrey Markov|Andrej Andreevic Markov]] (1907), has two drawbacks. First, the number of
parameters of the model grows exponentially with the order L of the chain. This brings about
computational and storage problems during implementation, including for limited memory length
L. 

An improvement initially put forward by (Rissanen - 1983) and used particularly in compression
data (Weinberger - 1992, Willems - 1995) was the [[Variable Length Markov chain]] (Buhlmann -
1999). This model can be represented by a tree, known as '''Prediction Suffix Tree''' – PST (Ron -
1996), certain branches of which are depth L and others of an inferior depth to L, whereas the
[[Markov chain of order L]] corresponds to a complete tree of depth L. By reducing the storage cost,
pruning the branches of the tree will enable us to increase the order of the model and, thereby
improve performance.

== References ==
*Prediction suffix trees for supervised classification of sequences[http://perso.univ-st-etienne.fr/largeron/PATREC.pdf]

[[Category:Trees (data structures)]]</text>
      <sha1>6rxygc4w0pdbaxg8strkgqtgfd9g9r8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Quadtree</title>
    <ns>0</ns>
    <id>577097</id>
    <revision>
      <id>621678671</id>
      <parentid>620298181</parentid>
      <timestamp>2014-08-17T20:53:11Z</timestamp>
      <contributor>
        <ip>105.237.138.20</ip>
      </contributor>
      <comment>/* Some common uses of quadtrees */ More direct link.</comment>
      <text xml:space="preserve" bytes="11554">[[Image:Point quadtree.svg|thumb|300px|A region quadtree with point data. Bucket capacity 1.]]

A '''quadtree''' is a [[tree data structure]] in which each internal node has exactly four children. Quadtrees are most often used to partition a two-dimensional space by recursively subdividing it into four quadrants or regions. The regions may be square or rectangular, or may have arbitrary shapes. This data structure was named a quadtree by [[Raphael Finkel]] and [[J.L. Bentley]] in 1974. A similar partitioning is also known as a ''Q-tree''. All forms of quadtrees share some common features:
* They decompose space into adaptable cells
* Each cell (or bucket) has a maximum capacity. When maximum capacity is reached, the bucket splits
* The tree directory follows the spatial decomposition of the quadtree.

==Types==
Quadtrees may be classified according to the type of data they represent, including areas, points, lines and curves. Quadtrees may also be classified by whether the shape of the tree is independent of the order data is processed. Some common types of quadtrees are:

===The region quadtree===
The region quadtree represents a partition of space in two dimensions by decomposing the region into four equal quadrants, subquadrants, and so on with each leaf node containing data corresponding to a specific subregion. Each node in the tree either has exactly four children, or has no children (a leaf node). The region quadtree is a type of [[trie]].

A region quadtree with a depth of n may be used to represent an image consisting of 2&lt;sup&gt;n&lt;/sup&gt; × 2&lt;sup&gt;n&lt;/sup&gt; pixels, where each pixel value is 0 or 1. The root node represents the entire image region. If the pixels in any region are not entirely 0s or 1s, it is subdivided. In this application, each leaf node represents a block of pixels that are all 0s or all 1s.

A region quadtree may also be used as a variable resolution representation of a data field. For example, the temperatures in an area may be stored as a quadtree, with each leaf node storing the average temperature over the subregion it represents.

If a region quadtree is used to represent a set of point data (such as the latitude and longitude of a set of cities), regions are subdivided until each leaf contains at most a single point.

===Point quadtree===
The point quadtree is an adaptation of a [[binary tree]] used to represent two-dimensional point data. It shares the features of all quadtrees but is a true tree as the center of a subdivision is always on a point. The tree shape depends on the order in which data is processed. It is often very efficient in comparing two-dimensional, ordered data points, usually operating in [[Big O notation|O(log n)]] time.

====Node structure for a point quadtree====
A node of a point quadtree is similar to a node of a [[binary tree]], with the major difference being that it has four pointers (one for each quadrant) instead of two (&quot;left&quot; and &quot;right&quot;) as in an ordinary binary tree. Also a key is usually decomposed into two parts, referring to x and y coordinates. Therefore a node contains the following information:
* four pointers: quad[‘NW’], quad[‘NE’], quad[‘SW’], and quad[‘SE’]
* point; which in turn contains:
** key; usually expressed as x, y coordinates
** value; for example a name

===Edge quadtree===
Edge quadtrees are specifically used to store lines rather than points. Curves are approximated by subdividing cells to a very fine resolution. This can result in extremely unbalanced trees which may defeat the purpose of indexing.

===Polygonal map quadtree===
The polygonal map quadtree (or PM Quadtree) is a variation of quadtree which is used to store collections of polygons that may be degenerate (meaning that they have isolated vertices or edges).&lt;ref&gt; [[Hanan Samet]] and Robert Webber. &quot;Storing a Collection of Polygons Using
Quadtrees&quot;. ''ACM Transactions on Graphics'' July 1985: 182-222. ''InfoLAB''. Web. 23 March 2012 &lt;/ref&gt;
There are three main classes of PMQuadtrees, which vary depending on what information they store within each black node. PM3 quadtrees can store any amount of non-intersecting edges and at most one point. PM2 quadtrees are the same as PM3 quadtrees except that all edges must share the same end point. Finally PM1 quadtrees are similar to PM2, but black nodes can contain a point and its edges or just a set of edges that share a point, but you cannot have a point and a set of edges that do not contain the point.

==Some common uses of quadtrees==
* Image representation&lt;br /&gt;[[Image:Quad tree bitmap.svg|380px|Bitmap and its compressed quadtree representation]]
* [[Spatial index]]ing
* Efficient [[collision detection]] in two dimensions
* [[Hidden face removal#Viewing frustum culling|View frustum culling]] of terrain data
* Storing sparse data, such as a formatting information for a [[spreadsheet]] or for some matrix calculations
* Solution of multidimensional [[Field (physics)|fields]] ([[computational fluid dynamics]], [[electromagnetism]])
* [[Conway's Game of Life]] simulation program.&lt;ref&gt;{{cite web|url=http://www.ddj.com/hpc-high-performance-computing/184406478 |title=An Algorithm for Compressing Space and Time |author=Tomas G. Rokicki |date=2006-04-01 |accessdate=2009-05-20}}&lt;/ref&gt;
* [[State estimation]]&lt;ref&gt;Henning Eberhardt, Vesa Klumpp, Uwe D. Hanebeck, ''Density Trees for Efficient Nonlinear State Estimation'', Proceedings of the 13th International Conference on Information Fusion, Edinburgh, United Kingdom, July, 2010.&lt;/ref&gt;
* Quadtrees are also used in the area of fractal image analysis

Quadtrees are the two-dimensional analog of [[octree]]s.

==Pseudo code==
The following pseudo code shows one means of implementing a quadtree which handles only points. There are other approaches available.

===Prerequisites===
It is assumed these structures are used.

 ''// Simple coordinate object to represent points and vectors''
 '''struct''' XY
 {
   '''float''' x;
   '''float''' y;
 
   '''function''' __construct(''float'' _x, ''float'' _y) {...}
 }
 
 ''// Axis-aligned bounding box with half dimension and center''
 '''struct''' AABB
 {
   '''XY''' center;
   '''XY''' halfDimension;
 
   '''function''' __construct(''XY'' center, ''XY'' halfDimension) {...}
   '''function''' containsPoint(''XY'' p) {...}
   '''function''' intersectsAABB(''AABB'' other) {...}
 }

===QuadTree class===
This class represents both one quad tree and the node where it is rooted.

 '''class''' QuadTree
 {
   ''// Arbitrary constant to indicate how many elements can be stored in this quad tree node''
   '''constant int''' QT_NODE_CAPACITY = 4;
 
   ''// Axis-aligned bounding box stored as a center with half-dimensions
   // to represent the boundaries of this quad tree''
   '''AABB''' boundary;
 
   ''// Points in this quad tree node''
   ''Array of XY [size = QT_NODE_CAPACITY]'' points;
 
   ''// Children''
   '''QuadTree*''' northWest;
   '''QuadTree*''' northEast;
   '''QuadTree*''' southWest;
   '''QuadTree*''' southEast;
 
   ''// Methods''
   '''function''' __construct(''AABB'' _boundary) {...}
   '''function''' insert(''XY'' p) {...}
   '''function''' subdivide() {...} ''// create four children that fully divide this quad into four quads of equal area''
   '''function''' queryRange(''AABB'' range) {...}
 }

===Insertion===
The following method inserts a point into the appropriate quad of a quadtree, splitting if necessary.

 '''class''' QuadTree
 {
   ...
 
   ''// Insert a point into the QuadTree''
   '''function''' insert(''XY'' p)
   {
     ''// Ignore objects that do not belong in this quad tree''
     '''if''' (!boundary.containsPoint(p))
       '''return''' ''false''; ''// object cannot be added''
 
     ''// If there is space in this quad tree, add the object here''
     '''if''' (points.size &lt; QT_NODE_CAPACITY)
     {
       points.append(p);
       '''return''' ''true'';
     }
 
     ''// Otherwise, subdivide and then add the point to whichever node will accept it''
     '''if''' (northWest == ''null'')
       subdivide();
 
     '''if''' (northWest-&gt;insert(p)) '''return''' ''true'';
     '''if''' (northEast-&gt;insert(p)) '''return''' ''true'';
     '''if''' (southWest-&gt;insert(p)) '''return''' ''true'';
     '''if''' (southEast-&gt;insert(p)) '''return''' ''true'';
 
     ''// Otherwise, the point cannot be inserted for some unknown reason (this should never happen)''
     '''return''' ''false'';
   }
 }

===Query range===
The following method finds all points contained within a range.

 '''class''' QuadTree
 {
   ...
 
   ''// Find all points that appear within a range''
   '''function''' queryRange(''AABB'' range)
   {
     ''// Prepare an array of results''
     ''Array of XY'' pointsInRange;
 
     ''// Automatically abort if the range does not intersect this quad''
     '''if''' (!boundary.intersectsAABB(range))
       '''return''' pointsInRange; ''// empty list''
 
     ''// Check objects at this quad level''
     '''for''' ('''int''' p := 0; p &lt; points.size; p++)
     {
       '''if''' (range.containsPoint(points[p]))
         pointsInRange.append(points[p]);
     }
 
     ''// Terminate here, if there are no children''
     '''if''' (northWest == ''null'')
       '''return''' pointsInRange;
 
     ''// Otherwise, add the points from the children''
     pointsInRange.appendArray(northWest-&gt;queryRange(range));
     pointsInRange.appendArray(northEast-&gt;queryRange(range));
     pointsInRange.appendArray(southWest-&gt;queryRange(range));
     pointsInRange.appendArray(southEast-&gt;queryRange(range));
 
     '''return''' pointsInRange;
   }
 }

==See also==
* [[Binary space partitioning]]
* [[Kd-tree]]
* [[Octree]]
* [[R-tree]]
* [[UB-tree]]
* [[Spatial database]]
* [[Subpaving]]

==References==
===Notes===
{{Reflist}}

===General references===
# {{cite journal | author=[[Raphael Finkel]] and [[J.L. Bentley]] | title= Quad Trees: A Data Structure for Retrieval on Composite Keys | journal=Acta Informatica | year=1974 | volume=4 | issue=1 | pages= 1–9 | url= | doi= 10.1007/BF00288933}}
# {{cite book|author = [[Mark de Berg]], [[Marc van Kreveld]], [[Mark Overmars]], and [[Otfried Schwarzkopf]] | year = 2000 | title = Computational Geometry | publisher = [[Springer-Verlag]] | edition = 2nd revised | isbn = 3-540-65620-0}} Chapter 14: Quadtrees: pp.&amp;nbsp;291–306.
# {{cite web |authorlink=Hanan Samet|url= http://infolab.usc.edu/csci585/Spring2008/den_ar/p182-samet.pdf|title= Storing a Collection of Polygons Using Quadtrees|last1 = Samet|first1 = Hanan| last2 = Webber| first2 = Robert |date= July 1985|accessdate=23 March 2012}}

==External links==
* [http://www.cs.berkeley.edu/~demmel/cs267/lecture26/lecture26.html A discussion of the Quadtree and an application]
* [http://homepages.ge.ucl.ac.uk/~mhaklay/java.htm Considerable discussion and demonstrations of Spatial Indexing]{{Dead link|date=January 2014}}
* [http://www.mikechambers.com/blog/2011/03/21/javascript-quadtree-implementation/ Javascript Implementation of the QuadTree used for collision detection]
* [https://github.com/varunpant/Quadtree/ Java Implementation]
* [http://sourceforge.net/projects/quadtreesim/ C++ Implementation of a QuadTree used for spatial indexing of triangles]
* [http://robots.thoughtbot.com/how-to-handle-large-amounts-of-data-on-maps/ Objective-C implementation of QuadTree used for GPS clustering]
* [http://squarelanguage.sourceforge.net/ SquareLanguage]
{{CS-Trees}}

[[Category:Trees (data structures)]]
[[Category:Database index techniques]]
[[Category:Geometric data structures]]</text>
      <sha1>c86le9ph04c74ppe21sda7k2ywim9y2</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Radial tree</title>
    <ns>0</ns>
    <id>28502338</id>
    <revision>
      <id>607747280</id>
      <parentid>567619144</parentid>
      <timestamp>2014-05-09T08:23:18Z</timestamp>
      <contributor>
        <username>Tobias Bergemann</username>
        <id>36029</id>
      </contributor>
      <minor/>
      <comment>/* top */ Fix punctuation.</comment>
      <text xml:space="preserve" bytes="4223">[[Image:Radial tree - Graphic Statistics in Management.jpg|thumb|right|300px|Example of a radial tree, from a 1924 organization chart that emphasizes a central authority.&lt;ref&gt;W. H. Smith., Graphic Statistics in Management (McGraw-Hill Book Company, New York, ed. First, 1924) http://www.visualcomplexity.com/vc/project.cfm?id=10&lt;/ref&gt;]] 
A '''radial tree''', or '''radial map''', is a method of displaying a [[tree structure]] (e.g., a [[Tree (data structure)|tree data structure]]) in a way that expands outwards, radially. It is one of many ways to visually display a tree,&lt;ref&gt;[http://www.spicynodes.org/reference-background.html Various 2-D radial graph and network visualizations, from SpicyNodes documentation]&lt;/ref&gt;&lt;ref&gt;http://www.visualcomplexity.com/vc/project.cfm?id=26&lt;/ref&gt; with examples extending back to the early 20th century.&lt;ref&gt;http://www.visualcomplexity.com/vc/project.cfm?id=289&lt;/ref&gt;  In use, it is a type of [[Information graphics|information graphic]].

[[Image:Radial-vs-tri.gif|thumb|middle|300px|Radial vs. triangular tree layout]]

== Comparison to other layouts ==

In a simple case, the first node is at the top, and the linked nodes are beneath. As each node typically has more than one child, the resulting shape is relatively triangular. In a radial layout, instead of each successive generation being displayed a row below, each generation is displayed in a new, outer orbit. 

Since the length of each orbit increases with the radius, there tends to be more room for the nodes. A radial tree will spread the larger number of nodes over a larger area as the levels increase. We use the terms level and depth interchangeably.&lt;ref&gt;Greg Book &amp; Neeta Keshary. &quot;Radial Tree Graph Drawing Algorithm for Representing Large Hierarchies.&quot; University of Connecticut December 2001&lt;/ref&gt; Nevertheless, the number of nodes increases exponentially with the distance from the first node, whereas the circumference of each orbit increases linearly, so by the outer orbits, the nodes tend to be packed together.

== Basic layout ==

[[Image:Radial-graph-schmatic.gif|thumb|200px|Schematic radial tree.]]

The overall distance &quot;d&quot; is the distance between levels of the graph. It is chosen so that the overall layout will fit within a screen. Layouts are generated by working outward from the center, root. The first level is a special case because all the nodes have the same parent. The nodes for level 1 can be distributed evenly, or weighted depending on the number of children they have. For subsequent levels, the children are positioned within sectors of the remaining space, so that child nodes of one parent do not overlap with others. 

There are a many extensions to this algorithm to create more visually balanced layouts, to allow a user to navigate from node to node (changing the center),&lt;ref&gt;Yee, K.-P, D. Fisher, R. Dhamija, &amp; M. Hearst. “Animated Exploration of Dynamic Graphs with Radial Layout”. Proc. Information Visualization, 43-50, 2001.&lt;/ref&gt; or accommodate node labels and mix [[Force-based algorithms (graph drawing)|force-directed layouts]] with radial layouts.&lt;ref&gt;Douma, Michael, Greg Ligierko, Ovidiu Ancuta, P. Gritsai, and S. Liu. SpicyNodes: Radial Layout Authoring for the General Public. InfoVis 2009. Atlantic City, NJ. October 2009. Presentation.&lt;/ref&gt;

The layout has some similarities to a [[hyperbolic tree]], though a key difference is that hyperbolic trees are based on  [[hyperbolic geometry]], whereas in a radial tree the distance between orbits is relatively linear.

== Examples ==
* [[MindManager]] and [[MindMapper]] are [[Mind map|mindmapping]] systems, which can make radial-like layouts, though are not radial beyond the 2nd level.
* [[SpicyNodes]] is an approach to visualizing hierarchies, which allows moving from node to node.

==References==
{{reflist}}

== External links==
* [http://treevis.net Comprehensive survey and bibliography] of Tree Visualization techniques
* [http://chrisharrison.net/index.php/Visualizations/WikiViz Chris Harrison - WikiViz: Visualizing Wikipedia]
* [http://www.visualcomplexity.com/vc/search.cfm?input=radial Radial maps at &quot;Visual Complexity&quot; site]

[[Category:Trees (data structures)]]
[[Category:Graph drawing]]</text>
      <sha1>tel6leaib3nxhv6bzb0ybszuakke7c7</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Range tree</title>
    <ns>0</ns>
    <id>14514547</id>
    <revision>
      <id>545046583</id>
      <parentid>533480727</parentid>
      <timestamp>2013-03-17T20:32:32Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q818944]]</comment>
      <text xml:space="preserve" bytes="7157">In [[computer science]], a '''range tree''' is an [[ordered tree data structure|ordered tree]] [[data structure]] to hold a list of points. It allows all points within a given range to be [[range query|reported]] efficiently, and is typically used in two or higher dimensions. Range trees were introduced by [[Jon Louis Bentley]] in 1979.&lt;ref name=Bentley79&gt;{{Cite doi|10.1016/0020-0190(79)90117-0}}&lt;/ref&gt; Similar data structures were discovered independently by Lueker,&lt;ref name=Lueker78&gt;{{Cite doi|10.1109/SFCS.1978.1}}&lt;/ref&gt; Lee and Wong,&lt;ref name=LeeWong80&gt;{{Cite doi|10.1145/320613.320618}}&lt;/ref&gt; and Willard.&lt;ref name=Willard79&gt;{{cite techreport | first = Dan E. | last = Willard | title = The super-''b''-tree algorithm | number = TR-03-79 | publisher = Aiken Computer Lab, Harvard University | location = Cambridge, MA}}&lt;/ref&gt;
The range tree is an alternative to the [[k-d tree|''k''-d tree]]. Compared to ''k''-d trees, range trees offer faster query times of [[Big O notation|O]](log&lt;sup&gt;''d''&lt;/sup&gt; ''n'' + ''k'') but worse storage of O(''n'' log&lt;sup&gt;''d''−1&lt;/sup&gt; ''n''), where ''n'' is the number of points stored in the tree, ''d'' is the dimension of each point and ''k'' is the number of points reported by a given query.

== Description ==

[[File:1-dimensional-range-tree.svg|thumb|upright=2.0|alt=An example of a 1-dimensional range tree.|An example of a 1-dimensional range tree.]]

A range tree on a set of 1-dimensional points is a balanced [[binary search tree]] on those points. The points stored in the tree are stored in the leaves of the tree; each internal node stores the largest value contained in its left subtree.
A range tree on a set of points in ''d''-dimensions is a [[recursive data type|recursively defined]] multi-level [[binary search tree]]. Each level of the data structure is a binary search tree on one of the ''d''-dimensions.
The first level is a binary search tree on the first of the ''d''-coordinates. Each vertex ''v'' of this tree contains an associated structure that is a (''d''−1)-dimensional range tree on the last (''d''−1)-coordinates of the points stored in the subtree of ''v''.

== Operations ==

=== Construction ===

A 1-dimensional range tree on a set of ''n'' points is a binary search tree, which can be constructed in O(''n'' log ''n'') time. Range trees in higher dimensions are constructed recursively by constructing a balanced binary search tree on the first coordinate of the points, and then, for each vertex ''v'' in this tree, constructing a (''d''−1)-dimensional range tree on the points contained in the subtree of ''v''. Constructing a range tree this way would require O(''n'' log&lt;sup&gt;''d''&lt;/sup&gt;''n'') time.

This can be improved by noticing that a range tree on a set 2-dimensional points can be constructed in O(''n'' log ''n'') time.&lt;ref name=&quot;DutchBook3E&quot;&gt;{{Cite doi|10.1007/978-3-540-77974-2}}&lt;/ref&gt;
Let ''S'' be a set of ''n'' 2-dimensional points. If ''S'' contains only one point, return a leaf containing that point. Otherwise, construct the associated structure of ''S'', a 1-dimensional range tree on the ''y''-coordinates of the points in ''S''. Let ''x''&lt;sub&gt;m&lt;/sub&gt; be the median ''x''-coordinate of the points. Let ''S''&lt;sub&gt;L&lt;/sub&gt; be the set of points with ''x''-coordinate less than or equal to ''x''&lt;sub&gt;m&lt;/sub&gt; and let ''S''&lt;sub&gt;R&lt;/sub&gt; be the set of points with ''x''-coordinate greater than ''x''&lt;sub&gt;m&lt;/sub&gt;. Recursively construct ''v''&lt;sub&gt;L&lt;/sub&gt;, a 2-dimensional range tree on ''S''&lt;sub&gt;L&lt;/sub&gt;, and ''v''&lt;sub&gt;R&lt;/sub&gt;, a 2-dimensional range tree on ''S''&lt;sub&gt;R&lt;/sub&gt;. Create a vertex ''v'' with left-child ''v''&lt;sub&gt;L&lt;/sub&gt; and right-child ''v''&lt;sub&gt;R&lt;/sub&gt;.
If we sort the points by their ''y''-coordinates at the start of the algorithm, and maintain this ordering when splitting the points by their ''x''-coordinate, we can construct the associated structures of each subtree in linear time.
This reduces the time to construct a 2-dimensional range tree to O(''n'' log ''n''), which also reduces the time to construct a ''d''-dimensional range tree to O(''n'' log&lt;sup&gt;''d''−1&lt;/sup&gt;''n'').

=== Range queries ===

[[File:1-dimensional-range-query.svg|thumb|upright=1.0|alt=A 1-dimensional range query.|A 1-dimensional range query [''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;]. Points stored in the subtrees shaded in gray will be reported. find(''x''&lt;sub&gt;1&lt;/sub&gt;) and find(''x''&lt;sub&gt;2&lt;/sub&gt;) will be reported if they are inside the query interval.]]

Range trees can be used to find the set of points that lie inside a given interval. To report the points that lie in the interval [''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;], we start by searching for ''x''&lt;sub&gt;1&lt;/sub&gt; and ''x''&lt;sub&gt;2&lt;/sub&gt;. At some vertex in the tree, the search paths to ''x''&lt;sub&gt;1&lt;/sub&gt; and ''x''&lt;sub&gt;2&lt;/sub&gt; will diverge. Let ''v''&lt;sub&gt;split&lt;/sub&gt; be the last vertex that these two search paths have in common. Continue searching for ''x''&lt;sub&gt;1&lt;/sub&gt; in the range tree. For every vertex ''v'' in the search path from ''v''&lt;sub&gt;split&lt;/sub&gt; to ''x''&lt;sub&gt;1&lt;/sub&gt;, if the value stored at ''v'' is greater than ''x''&lt;sub&gt;1&lt;/sub&gt;, report every point in the right-subtree of ''v''. If ''v'' is a leaf, report the value stored at ''v'' if it is inside the query interval.  Similarly, reporting all of the points stored in the left-subtrees of the vertices with values less than ''x''&lt;sub&gt;2&lt;/sub&gt; along the search path from ''v''&lt;sub&gt;split&lt;/sub&gt; to ''x''&lt;sub&gt;2&lt;/sub&gt;, and report the leaf of this path if it lies within the query interval.

Since the range tree is a balanced binary tree, the search paths to ''x''&lt;sub&gt;1&lt;/sub&gt; and ''x''&lt;sub&gt;2&lt;/sub&gt; have length O(log ''n''). Reporting all of the points stored in the subtree of a vertex can be done in linear time using any [[tree traversal]] algorithm. It follows that the time to perform a range query is O(log ''n'' + ''k''), where ''k'' is the number of points in the query interval.

Range queries in ''d''-dimensions are similar. Instead of reporting all of the points stored in the subtrees of the search paths, perform a (''d''−1)-dimensional range query on the associated structure of each subtree. Eventually, a 1-dimensional range query will be performed and the correct points will be reported. Since a ''d''-dimensional query consists of O(log ''n'') (''d''−1)-dimensional range queries, it follows that the time required to perform a ''d''-dimensional range query is O(log&lt;sup&gt;''d''&lt;/sup&gt;''n'' + ''k''), where ''k'' is the number of points in the query interval. This can be reduced to O(log&lt;sup&gt;''d''−1&lt;/sup&gt;''n'' + ''k'') using the technique of [[fractional cascading]].&lt;ref name=Lueker78 /&gt;&lt;ref name=Willard79 /&gt;&lt;ref name=DutchBook3E /&gt;

== See also ==

* [[k-d tree|''k''-d tree]]
* [[Segment tree]]

== References ==
{{Reflist}}

== External links ==
* [http://www.cgal.org/Manual/latest/doc_html/cgal_manual/SearchStructures/Chapter_main.html Range and Segment Trees] in [[CGAL]], the  Computational Geometry Algorithms Library.
* [http://www.cs.uu.nl/docs/vakken/ga/slides5b.pdf Lecture 8: Range Trees], Marc van Kreveld.

{{CS-Trees}}

[[Category:Trees (data structures)]]
[[Category:Geometric data structures]]</text>
      <sha1>2g3c5rbev5tc9onkxpjtoso2hnofhug</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Segment tree</title>
    <ns>0</ns>
    <id>13682464</id>
    <revision>
      <id>599037076</id>
      <parentid>595298794</parentid>
      <timestamp>2014-03-10T20:45:59Z</timestamp>
      <contributor>
        <username>Jonesey95</username>
        <id>9755426</id>
      </contributor>
      <comment>Fixing &quot;Pages with DOI errors&quot; error. Also fixing other errors.</comment>
      <text xml:space="preserve" bytes="12464">{{One source|date=November 2007}}

In [[computer science]], a '''segment tree''' is a [[Tree (data structure)|tree]] [[data structure]] for storing [[Interval (mathematics)|intervals]], or segments. It allows querying which of the stored segments contain a given point. It is, in principle, a static structure; that is, its content cannot be modified once the structure is built.  A similar data structure is the [[interval tree]].

A segment tree for a set ''I'' of ''n'' intervals uses [[Big O notation|O]](''n'' log ''n'') storage and can be built in O(''n'' log ''n'') time. Segment trees support searching for all the intervals that contain a query point in O(log ''n'' + ''k''), ''k'' being the number of retrieved intervals or segments.&lt;ref name=&quot;Schwarzkopf1&quot;&gt;{{Harv |de Berg|van Kreveld|Overmars|Schwarzkopf|2000|p=227}}&lt;/ref&gt;

Applications of the segment tree are in the areas of [[computational geometry]], and [[geographic information systems]].

The segment tree can be generalized to higher [[dimension]] spaces as well.

==Structure description==
''This section describes the structure of a segment tree in a one-dimensional space.''

Let ''S'' be a set of intervals, or segments. Let ''p''&lt;sub&gt;1&lt;/sub&gt;, ''p''&lt;sub&gt;2&lt;/sub&gt;, ..., ''p&lt;sub&gt;m&lt;/sub&gt;'' be the list of distinct interval endpoints, sorted from left to right. Consider the partitioning of the real line induced by those points. The regions of this partitioning are called ''elementary intervals''. Thus, the elementary intervals are, from left to right:

:&lt;math&gt;(-\infty, p_1), [p_1,p_1], (p_1, p_2), [p_2, p_2], ..., (p_{m-1}, p_m), [p_m, p_m], (p_m, +\infty)&lt;/math&gt;

That is, the list of elementary intervals consists of open intervals between two consecutive endpoints ''p&lt;sub&gt;i&lt;/sub&gt;'' and ''p''&lt;sub&gt;''i''+1&lt;/sub&gt;, alternated with closed intervals consisting of a single endpoint. Single points are treated themselves as intervals because the answer to a query is not necessarily the same at the interior of an elementary interval and its endpoints.&lt;ref&gt;{{Harv |de Berg|van Kreveld|Overmars|Schwarzkopf|2000|p=224}}&lt;/ref&gt;

[[Image:Segment tree instance.gif|frame|Graphic example of the structure of the segment tree. This instance is built for the segments shown at the bottom.]]

Given a set ''I'' of intervals, or segments, a segment tree ''T'' for ''I'' is structured as follows:
* ''T'' is a [[binary tree]].
* Its [[Leaf node|leaves]] correspond to the elementary intervals induced by the endpoints in ''I'', in an ordered way: the leftmost leaf corresponds to the leftmost interval, and so on. The elementary interval corresponding to a leaf ''v'' is denoted Int(''v'').
* The [[internal node]]s of ''T''  correspond to intervals that are the [[Union (set theory)|union]]  of elementary intervals: the interval Int(''N'') corresponding to node ''N'' is the union of the intervals corresponding to the leaves of the tree rooted at ''N''. That implies that Int(''N'') is the union of the intervals of its two children.
* Each node or leaf ''v'' in ''T'' stores the interval Int(''v'') and a set   of intervals, in some data structure. This canonical subset of node ''v'' contains the intervals [''x'', ''x&amp;prime;''] from ''I'' such that [''x'', ''x&amp;prime;''] contains Int(''v'') and does not contain Int(parent(''v'')). That is, each segment in ''I'' stores the segments that span through its interval, but do not span through the interval of its parent.&lt;ref&gt;{{Harv |de Berg|van Kreveld|Overmars|Schwarzkopf|2000|pp=225&amp;ndash;226}}&lt;/ref&gt;

==Storage requirements==
''This section analyzes the storage cost of a segment tree in a one-dimensional space.''

A segment tree ''T'' on a set ''I'' of ''n'' intervals uses O(''n''log''n'') storage.

:''Proof:''

:''Lemma'': Any interval [''x'', ''x&amp;prime;''] of ''I'' is stored in the canonical set for at most two nodes at the same depth.

::''Proof'': Let ''v''&lt;sub&gt;1&lt;/sub&gt;, ''v''&lt;sub&gt;2&lt;/sub&gt;, ''v''&lt;sub&gt;3&lt;/sub&gt; be the three nodes at the same depth, numbered from left to right; and let p(''v'') be the parent node of any given node ''v''. Suppose [''x'', ''x&amp;prime;''] is stored at ''v''&lt;sub&gt;1&lt;/sub&gt; and ''v''&lt;sub&gt;3&lt;/sub&gt;. This means that [''x'', ''x&amp;prime;''] spans the whole interval from the left endpoint of Int(''v''&lt;sub&gt;1&lt;/sub&gt;) to the right endpoint of Int(''v''&lt;sub&gt;3&lt;/sub&gt;). Note that all segments at a particular level are non-overlapping and ordered from left to right: this is true by construction for the level containing the leaves, and the property is not lost when moving from any level to the one above it by combining pairs of adjacent segments. Now either p(''v''&lt;sub&gt;2&lt;/sub&gt;) = p(''v''&lt;sub&gt;1&lt;/sub&gt;), or the former is to the right of the latter (edges in the tree do not cross). In the first case, Int(p(''v''&lt;sub&gt;2&lt;/sub&gt;))'s leftmost point is the same as Int(''v''&lt;sub&gt;1&lt;/sub&gt;)'s leftmost point; in the second case, Int(p(''v''&lt;sub&gt;2&lt;/sub&gt;))'s leftmost point is to the right of Int(p(''v''&lt;sub&gt;1&lt;/sub&gt;))'s rightmost point, and therefore also to the right of Int(''v''&lt;sub&gt;1&lt;/sub&gt;)'s rightmost point. In both cases, Int(p(''v''&lt;sub&gt;2&lt;/sub&gt;)) begins at or to the right of Int(''v''&lt;sub&gt;1&lt;/sub&gt;)'s leftmost point. Similar reasoning shows that Int(p(''v''&lt;sub&gt;2&lt;/sub&gt;)) ends at or to the left of Int(''v''&lt;sub&gt;3&lt;/sub&gt;)'s rightmost point. Int(p(''v''&lt;sub&gt;2&lt;/sub&gt;)) must therefore be contained in [''x'', ''x&amp;prime;'']; hence, [''x'', ''x&amp;prime;''] will not be stored at ''v''&lt;sub&gt;2&lt;/sub&gt;.

:The set ''I'' has at most 4''n'' + 1 elementary intervals. Because ''T'' is a binary balanced tree with at most 4''n'' + 1 leaves, its height is O(log''n''). Since any interval is stored at most twice at a given depth of the tree, that the total amount of storage is O(''n''log''n'').&lt;ref name=&quot;Schwarzkopf2&quot;&gt;{{Harv |de Berg|van Kreveld|Overmars|Schwarzkopf|2000|p=226}}&lt;/ref&gt;

==Construction==
''This section describes the construction of a segment tree in a one-dimensional space.''

A segment tree from the set of segments ''I'', can be built as follows. First, the endpoints of the intervals in ''I'' are sorted. The elementary intervals are obtained from that. Then, a balanced binary tree is built on the elementary intervals, and for each node ''v'' it is determined the interval Int(''v'') it represents. It remains to compute the canonical subsets for the nodes. To achieve this, the intervals in ''I'' are inserted one by one into the segment tree. An interval ''X'' = [''x'', ''x&amp;prime;''] can be inserted in a subtree rooted at ''T'', using the following procedure:&lt;ref&gt;{{Harv |de Berg|van Kreveld|Overmars|Schwarzkopf|2000|pp=226&amp;ndash;227}}&lt;/ref&gt;
* If Int(''T'') is contained in ''X'' then store ''X'' at ''T'', and finish.
* Else:
* If ''X'' intersects the canonical subset of the left child of ''T'', then insert ''X'' in that child, recursively.
* If ''X'' intersects the canonical subset of the right child of ''T'', then insert ''X'' in that child, recursively.
The complete construction operation takes O(''n''log''n'') time, ''n'' being the number of segments in ''I''.
:''Proof''

:Sorting the endpoints takes O(''n''log''n''). Building a balanced binary tree from the sorted endpoints, takes linear time on ''n''.
:The insertion of an interval ''X'' = [''x'', ''x&amp;prime;''] into the tree, costs O(log''n'').
::''Proof:'' Visiting every node takes constant time (assuming that canonical subsets are stored in a simple data structure like a [[linked list]]). When we visit node ''v'', we either store ''X'' at ''v'', or Int(''v'') contains an endpoint of ''X''. As proved above, an interval is stored at most twice at each level of the tree. There is also at most one node at every level whose corresponding interval contains ''x'', and one node whose interval contains ''x&amp;prime;''. So, at most four nodes per level are visited. Since there are O(log''n'') levels, the total cost of the insertion is ''O(''log''n'').&lt;ref name=&quot;Schwarzkopf1&quot;/&gt;

==Query==
''This section describes the query operation of a segment tree in a one-dimensional space.''

A query for a segment tree, receives a point ''q&lt;sub&gt;x&lt;/sub&gt;'', and retrieves a list of all the segments stored which contain the point ''q&lt;sub&gt;x&lt;/sub&gt;''.

Formally stated; given a node (subtree) ''v'' and a query point ''q&lt;sub&gt;x&lt;/sub&gt;'', the query can be done using the following algorithm:
* Report all the intervals in I(''v'').
* If ''v'' is not a leaf:
** If ''q&lt;sub&gt;x&lt;/sub&gt;'' is in Int(left child of ''v'') then
*** Perform a query in the left child of ''v''.
** If ''q&lt;sub&gt;x&lt;/sub&gt;'' is in Int(right child of ''v'') then
*** Perform a query in the right child of ''v''.

In a segment tree that contains ''n'' intervals, those containing a given query point can be reported in O(log''n'' + ''k'') time, where ''k'' is the number of reported intervals.
:''Proof:'' The query algorithm visits one node per level of the tree, so O(log''n'') nodes in total. In the other hand, at a node ''v'', the segments in ''I'' are reported in O(1 + ''k&lt;sub&gt;v&lt;/sub&gt;'') time, where ''k&lt;sub&gt;v&lt;/sub&gt;'' is the number of intervals at node ''v'', reported. The sum of all the ''k&lt;sub&gt;v&lt;/sub&gt;'' for all nodes ''v'' visited, is ''k'', the number of reported segments.&lt;ref name=&quot;Schwarzkopf2&quot;/&gt;

==Generalization for higher dimensions==
The segment tree can be generalized to higher dimension spaces, in the form of multi-level segment trees. In higher dimension versions, the segment tree stores a collection of axis-parallel (hyper-)rectangles, and can retrieve the rectangles that contain a given query point. The structure uses O(''n''log&lt;sup&gt;''d''&lt;/sup&gt;''n'') storage, and answers queries in O(log''&lt;sup&gt;d&lt;/sup&gt;''n'').

The use of [[fractional cascading]] lowers the query time bound by a logarithmic factor. The use of the [[interval tree]] on the deepest level of associated structures lowers the storage bound with a logarithmic factor.&lt;ref name=&quot;Schwarzkopf3&quot;&gt;{{Harv |de Berg|van Kreveld|Overmars|Schwarzkopf|2000|p=230}}&lt;/ref&gt;

==Notes==
The query that asks for all the intervals containing a given point, is often referred as ''stabbing query''.&lt;ref name=&quot;Schwarzkopf4&quot; /&gt;

The segment tree is less efficient than the interval tree for range queries in one dimension, due to its higher storage requirement: O(''n''log''n'') against the O(''n'') of the interval tree. The importance of the segment tree is that the segments within each node’s canonical subset can be stored in any arbitrary manner.&lt;ref name=&quot;Schwarzkopf4&quot;&gt;{{Harv |de Berg|van Kreveld|Overmars|Schwarzkopf|2000|p=229}}&lt;/ref&gt;

For ''n'' intervals whose endpoints are in a small integer range (e.g., in the range [1,...,O(''n'')]), optimal data structures{{which|date=February 2014}} exist with a linear preprocessing time and query time O(1+''k'') for reporting all ''k'' intervals containing a given query point.

Another advantage of the segment tree is that it can easily be adapted to counting queries; that is, to report the number of segments containing a given point, instead of reporting the segments themselves. Instead of storing the intervals in the canonical subsets, it can simply store the number of them. Such a segment tree uses linear storage, and requires an O(log ''n'') query time, so it is optimal.&lt;ref&gt;{{Harv |de Berg|van Kreveld|Overmars|Schwarzkopf|2000|pp=229&amp;ndash;230}}&lt;/ref&gt;

A version for higher dimensions of the interval tree and the [[priority search tree]] does not exist, that is, there is no clear extension of these structures that solves the analogous problem in higher dimensions. But the structures can be used as associated structure of segment trees.&lt;ref name=&quot;Schwarzkopf3&quot;/&gt;

==History==
{{Expand section|date=November 2007}}
The segment tree was discovered by J. L. Bentley in 1977; in &quot;Solutions to Klee’s rectangle problems&quot;.&lt;ref name=&quot;Schwarzkopf4&quot;/&gt;

==References==
{{reflist}}

==Sources cited==
* {{cite book
 | last1=de Berg
 | first1=Mark
 | last2=van Kreveld
 | first2=Marc
 | last3=Overmars
 | first3=Mark
 | last4=Schwarzkopf
 | first4=Otfried
 | publication-date=2000
 | year=2000
 | title=Computational Geometry: algorithms and applications
 | edition=2nd
 | publisher=Springer-Verlag Berlin Heidelberg New York
 | isbn=3-540-65620-0
 | chapter = More Geometric Data Structures
 | doi = 10.1007/978-3-540-77974-2
 | ref=harv
}}
* http://www.cs.nthu.edu.tw/~wkhon/ds/ds10/tutorial/tutorial6.pdf

{{CS-Trees}}

{{DEFAULTSORT:Segment Tree}}
[[Category:Trees (data structures)]]
[[Category:Binary trees]]
[[Category:Computer graphics data structures]]</text>
      <sha1>rmets6uuplraoe028fm66slykigf20v</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Self-balancing binary search tree</title>
    <ns>0</ns>
    <id>378310</id>
    <revision>
      <id>599064713</id>
      <parentid>597313555</parentid>
      <timestamp>2014-03-11T00:34:01Z</timestamp>
      <contributor>
        <ip>5.65.63.85</ip>
      </contributor>
      <text xml:space="preserve" bytes="7146">{{refimprove|date=November 2010}}
[[Image:Unbalanced binary tree.svg|thumb|right|251px|An example of an '''unbalanced''' tree; following the path from the root to a node takes an average of 3.27 node accesses]]
[[Image:AVLtreef.svg|thumb|right|251px|The same tree after being height-balanced; the average path effort decreased to 3.00 node accesses]]
In [[computer science]], a '''self-balancing''' (or '''height-balanced''') '''binary search tree''' is any [[node (computer science)|node]]-based [[binary search tree]] that automatically keeps its height (maximal number of levels below the root) small in the face of arbitrary item insertions and deletions.&lt;ref name=&quot;knuth&quot;&gt;
  [[Donald Knuth]]. ''The Art of Computer Programming'', Volume 3: ''Sorting and Searching'', Second Edition. Addison-Wesley, 1998. ISBN 0-201-89685-0. Section 6.2.3: Balanced Trees, pp.458&amp;ndash;481.
&lt;/ref&gt;

These structures provide efficient implementations for mutable ordered [[list (computing)|lists]], and can be used for other [[abstract data structure]]s such as [[associative arrays]], [[priority queue]]s and [[set (computer science)|sets]].

==Overview==
[[File:BinaryTreeRotations.svg|thumb|300px|Tree rotations are very common internal operations on self-balancing binary trees to keep perfect or near-to-perfect balance.]]
Most operations on a binary search tree (BST) take time directly proportional to the height of the tree, so it is desirable to keep the height small. A binary tree with height ''h'' can contain at most [[Geometric_series#Formula|2&lt;sup&gt;0&lt;/sup&gt;+2&lt;sup&gt;1&lt;/sup&gt;+···+2&lt;sup&gt;''h''&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;2&lt;sup&gt;''h''+1&lt;/sup&gt;&amp;minus;1]] nodes. It follows that for a tree with ''n'' nodes and height ''h'': 

&lt;math&gt;n\le2^{h+1}-1&lt;/math&gt; 

And that implies:

&lt;math&gt;h\ge\lceil\log_2(n+1)-1\rceil\ge \lfloor\log_2 n\rfloor&lt;/math&gt;.

In other words, the minimum height of a tree with ''n'' nodes is [[logarithm|log]]&lt;sub&gt;2&lt;/sub&gt;(''n''), [[floor and ceiling functions|rounded down]]; that is, &lt;math&gt; \lfloor \log_2 n\rfloor&lt;/math&gt;:.&lt;ref name=&quot;knuth&quot;/&gt;

However, the simplest algorithms for BST item insertion may yield a tree with height ''n'' in rather common situations.  For example, when the items are inserted in sorted [[key (database)|key]] order, the tree degenerates into a [[linked list]] with ''n'' nodes.  The difference in performance between the two situations may be enormous: for ''n''&amp;nbsp;=&amp;nbsp;1,000,000, for example, the minimum height is &lt;math&gt; \lfloor \log_2(1,000,000) \rfloor = 19 &lt;/math&gt;.

If the data items are known ahead of time, the height can be kept small, in the average sense, by adding values in a random order, resulting in a [[random binary search tree]].  However, there are many situations (such as [[online algorithm]]s) where this [[randomized algorithm|randomization]] is not viable.

Self-balancing binary trees solve this problem by performing transformations on the tree (such as [[tree rotation]]s) at key times, in order to keep the height proportional to log&lt;sub&gt;2&lt;/sub&gt;(''n''). Although a certain [[Computational overhead|overhead]] is involved, it may be justified in the long run by ensuring fast execution of later operations.

Maintaining the height always at its minimum value &lt;math&gt;\lfloor \log_2(n) \rfloor&lt;/math&gt; is not always viable; it can be proven that any insertion algorithm which did so would have an excessive overhead.{{citation needed|date=August 2009}}  Therefore, most self-balanced BST algorithms keep the height within a constant factor of this lower bound.

In the asymptotic (&quot;[[Big O notation|Big-O]]&quot;) sense, a self-balancing BST structure containing ''n'' items allows the lookup, insertion, and removal of an item in O(log ''n'') worst-case time, and [[in-order iteration|ordered enumeration]] of all items in O(''n'') time.  For some implementations these are per-operation time bounds, while for others they are [[amortized analysis|amortized]] bounds over a sequence of operations.  These times are asymptotically optimal among all data structures that manipulate the key only through comparisons.

== Implementations ==
Popular data structures implementing this type of tree include:

* [[2-3 tree]]
* [[AA tree]]
* [[AVL tree]]
* [[Red-black tree]]
* [[Scapegoat tree]]
* [[Splay tree]]
* [[Treap]]

== Applications ==
Self-balancing binary search trees can be used in a natural way to construct and maintain ordered lists, such as [[priority queue]]s.  They can also be used for [[associative array]]s; key-value pairs are simply inserted with an ordering based on the key alone. In this capacity, self-balancing BSTs have [[associative array#Efficient representations|a number of advantages and disadvantages]] over their main competitor, [[hash table]]s. One advantage of self-balancing BSTs is that they allow fast (indeed, asymptotically optimal) enumeration of the items ''in key order'', which hash tables do not provide.  One disadvantage is that their lookup algorithms get more complicated when there may be multiple items with the same key. Self-balancing BSTs have better worst-case lookup performance than hash tables (O(log n) compared to O(n)), but have worse average-case performance (O(log n) compared to O(1)).

Self-balancing BSTs can be used to implement any algorithm that requires mutable ordered lists, to achieve optimal worst-case asymptotic performance. For example, if [[binary tree sort]] is implemented with a self-balanced BST, we have a very simple-to-describe yet [[asymptotically optimal]] O(''n'' log ''n'') sorting algorithm. Similarly, many algorithms in [[computational geometry]] exploit variations on self-balancing BSTs to solve problems such as the [[line segment intersection]] problem and the [[point location]] problem efficiently.  (For average-case performance, however, self-balanced BSTs may be less efficient than other solutions.  Binary tree sort, in particular, is likely to be slower than [[merge sort]], [[quicksort]], or [[heapsort]], because of the tree-balancing overhead as well as [[cache (computing)|cache]] access patterns.)

Self-balancing BSTs are flexible data structures, in that it's easy to extend them to efficiently record additional information or perform new operations. For example, one can record the number of nodes in each subtree having a certain property, allowing one to count the number of nodes in a certain key range with that property in O(log ''n'') time. These extensions can be used, for example, to optimize database queries or other list-processing algorithms.

== See also ==
* [[Day–Stout–Warren algorithm]]
* [[Fusion tree]]
* [[Skip list]]
* [[Sorting]]

== References ==
{{reflist}}

== External links ==
{{Commons category|Balanced Trees}}
* [http://www.nist.gov/dads/HTML/heightBalancedTree.html Dictionary of Algorithms and Data Structures: Height-balanced binary search tree]
* [http://adtinfo.org/ GNU libavl], a LGPL-licensed library of binary tree implementations in C, with documentation

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Self-Balancing Binary Search Tree}}
[[Category:Binary trees]]
[[Category:Soviet inventions]]
[[Category:Trees (data structures)]]</text>
      <sha1>qmqzqynoi5su6yxsqqpheix6fegn2jm</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Semantic resolution tree</title>
    <ns>0</ns>
    <id>3124498</id>
    <revision>
      <id>546538381</id>
      <parentid>522927392</parentid>
      <timestamp>2013-03-23T13:53:38Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q7449084]]</comment>
      <text xml:space="preserve" bytes="290">{{Unreferenced|auto=yes|date=December 2009}}
A '''semantic resolution tree''' is a [[tree (data structure)|tree]] used for the definition of the [[semantics]] of a [[programming language]].


[[Category:Trees (data structures)]]

{{DEFAULTSORT:Semantic Resolution Tree}}


{{Comp-sci-stub}}</text>
      <sha1>48uh1w7fco4g0iuwj8j14brkyehfmhu</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Split (phylogenetics)</title>
    <ns>0</ns>
    <id>11512614</id>
    <revision>
      <id>486774048</id>
      <parentid>471784980</parentid>
      <timestamp>2012-04-11T07:43:42Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>Format plain DOIs using [[Project:AWB|AWB]] (8060)</comment>
      <text xml:space="preserve" bytes="1338">[[File:Heterobranchia tree.png|thumb|Graph of [[neighbor-joining]] [[phylogenetic tree]] shows a clear split support (visualised by long parallel edges) for [[Acochlidiacea]] (in red color). The graph is based on datasets by Jörger et al. (2010)&lt;ref name=&quot;Jörger 2010&quot;&gt;Jörger K. M., Stöger I., Kano Y., Fukuda H., Knebelsberger T. &amp; Schrödl M. (2010). &quot;On the origin of Acochlidia and other enigmatic euthyneuran gastropods, with implications for the systematics of Heterobranchia&quot;. ''[[BMC Evolutionary Biology]]'' '''10''': 323. {{doi|10.1186/1471-2148-10-323}}.&lt;/ref&gt; and generated by [[SplitsTree]].]]

A '''split''' in [[phylogenetics]] is a bipartition of a set of [[taxon|taxa]], and the smallest unit of information in unrooted [[phylogenetic tree]]s: each edge of an unrooted phylogenetic tree represents one split, and the tree can be efficiently reconstructed from its set of splits. Moreover, when given several trees, the splits occurring in more than half of these trees give rise to a consensus tree, and the splits occurring in a smaller fraction of the trees generally give rise to a consensus [[Phylogenetic network|Split Network]]. 

== See also ==
[[SplitsTree]], a program for inferring phylogenetic (split) networks.

== References ==
{{reflist}}

[[Category:Phylogenetics]]
[[Category:Trees (data structures)]]</text>
      <sha1>kkzzp5sumd1q38g15ifcytljruj1no5</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>T-theory</title>
    <ns>0</ns>
    <id>7767296</id>
    <revision>
      <id>612190911</id>
      <parentid>544583122</parentid>
      <timestamp>2014-06-09T09:51:14Z</timestamp>
      <contributor>
        <username>Maproom</username>
        <id>2524922</id>
      </contributor>
      <comment>/* General history */ Various copy-edit etc.</comment>
      <text xml:space="preserve" bytes="2387">'''T-theory''' is a branch of [[discrete mathematics]] dealing with analysis of [[tree (graph theory)|tree]]s and discrete [[metric spaces]]. 

==General history==
T-theory originated from a question raised by [[Manfred Eigen]] in the late 1970s. He was trying to fit twenty distinct [[transfer RNA|t-RNA]] [[molecule]]s of the ''[[Escherichia coli]]'' bacterium into a tree.

An important concept of T-theory is the [[tight span]] of a metric space.  If ''X'' is a metric space, the tight span ''T(X)'' of ''X'' is, up to isomorphism, the unique minimal [[injective metric space]] that contains ''X''.  [[John Isbell]] was the first to discover the tight span in 1964, which he called the '''injective envelope'''. Dress independently constructed the same construct, which he called the tight span.

==Application areas==
* Phylogenetic analysis, which is used to create [[phylogenetic tree]]s.
* [[Online algorithm]]s - [[k-server problem | ''k''-server problem]]

==Recent developments==
* [[Bernd Sturmfels]], Professor of Mathematics and Computer Science at [[University of California, Berkeley | Berkeley]], and Josephine Yu classified six-point metrics using T-theory.

==References==
* {{cite journal 
        | author= Hans-Jurgen Bandelt and Andreas Dress
        | title= A canonical decomposition theory for metrics on a finite set
        | journal= Advances in Mathematics
        | year= 1992
	| volume= 92
	| pages= 47–105
        | doi= 10.1016/0001-8708(92)90061-O
        }}
* {{cite journal 
        | author=A. Dress, V. Moulton and W. Terhalle 
        | title=T-theory: An Overview 
        | journal=European Journal of Combinatorics 
        | year=1996 
        | volume=17 
        | issue=2–3 
        | pages=161–175 
        | doi=10.1006/eujc.1996.0015
        }}
* {{cite journal 
        | author=John Isbell | authorlink = John R. Isbell
        | title=Six theorems about metric spaces
        | journal=Comment. Math. Helv.
        | year=1964
        | volume=39
        | pages=65–74
        | doi=10.1007/BF02566944
        }}
* {{cite journal 
        | author=Bernd Sturmfels and Josephine Yu
        | title=Classification of Six-Point Metrics
        | journal=The Electronic Journal of Combinatorics
        | year=2004 
        | volume=11
        }}


{{combin-stub}}
[[Category:Metric geometry]]
[[Category:Trees (data structures)]]</text>
      <sha1>9zzxjkqcm5wnwbp9dxd0t2ugvy5ee5c</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Ternary search tree</title>
    <ns>0</ns>
    <id>3025819</id>
    <revision>
      <id>593490288</id>
      <parentid>593489842</parentid>
      <timestamp>2014-02-01T21:46:33Z</timestamp>
      <contributor>
        <username>Maghnus</username>
        <id>3103609</id>
      </contributor>
      <comment>Clarify that &quot;prefix tree&quot; is an alternative term for &quot;trie&quot;</comment>
      <text xml:space="preserve" bytes="8492">In [[computer science]], a '''ternary search tree''' is a type of [[trie]] (sometimes called a ''prefix tree'') where nodes are arranged as a [[binary search tree]]. Like other prefix trees, a ternary search tree can be used as an [[associative map]] structure with the ability for incremental [[string search]]. However, ternary search trees are more space efficient compared to standard prefix trees, at the cost of speed. Common applications for ternary search trees include [[spell-check]]ing and [[auto-completion]].

==Description==
Each node of a ternary search tree stores a single [[character (arts)|character]], an [[Abstract object|object]] (or a [[pointer (computer programming)|pointer]] to an object depending on implementation), and pointers to its three children conventionally named &quot;equal kid&quot; &quot;lo kid&quot; and &quot;hi kid.&quot;&lt;ref name=&quot;ostrov&quot;&gt;{{cite web|last=Ostrovsky|first=Igor|title=Efficient auto-complete with a ternary search tree|url=http://igoro.com/archive/efficient-auto-complete-with-a-ternary-search-tree/}}&lt;/ref&gt;&lt;ref name=&quot;dobbs&quot;&gt;{{cite web|last=Dobbs|title=Ternary Search Trees|url=http://www.drdobbs.com/database/184410528}}&lt;/ref&gt; A node may also have a pointer to its parent node as well as an indicator as to whether or not the node marks the end of a word.&lt;ref name=&quot;ostrov&quot; /&gt; The lo kid pointer must point to a node whose character value is less than the current node. Conversely, the hi kid pointer must point to a node whose character is greater than the current node.&lt;ref name=&quot;dobbs&quot; /&gt;
The figure below shows a ternary search tree with the strings &quot;as&quot;, &quot;at&quot;, &quot;cup&quot;, &quot;cute&quot;, &quot;he&quot;, &quot;i&quot; and &quot;us&quot;:

           c
         / | \
        a  u  h
        |  |  | \
        t  t  e  u
      /  / |   / |
     s  p  e  i  s

As with other trie data structures, each node in a ternary search tree represents a prefix of the stored strings. All strings in the middle subtree of a node start with that prefix.

==Ternary search tree operations==
===Look up===
To look up a particular node or the data associated with a node, a string key is needed. A lookup procedure begins by checking the root node of the tree and determining which of the following conditions has occurred. If the first character of the string is less than the character in the root node, a recursive lookup can be called on the tree whose root is the lo kid of the current root. Similarly, if the first character is greater than the current node in the tree, then a recursive call can be made to the tree whose root is the hi kid of the current node.&lt;ref name=&quot;dobbs&quot; /&gt;
As a final case, if the first character of the string is equal to the character of the current node then the function returns the node if there are no more characters in the key. If there are more characters in the key then the first character of the key must be removed and a recursive call is made given the equal kid node and the modified key.&lt;ref name=&quot;dobbs&quot; /&gt;
This can also be written in a non-recursive way by using a pointer to the current node and a pointer to the current character of the key.&lt;ref name=&quot;dobbs&quot; /&gt;

===Insertion===
Inserting a value into a ternary search can be defined recursively much as lookups are defined. This recursive method is continually called on nodes of the tree given a key which gets progressively shorter by pruning characters off the front of the key.
If this method reaches a node that has not been created, it creates the node and assigns it the character value of the first character in the key. Whether a new node is created or not, the method checks to see if the first character in the string is greater than or less than the character value in the node and makes a recursive call on the appropriate node as in the lookup operation. If, however, the key's first character is equal to the node's value then the insertion procedure is called on the equal kid and the key's first character is pruned away.&lt;ref name=&quot;dobbs&quot; /&gt;
Like [[binary search tree]]s and other [[data structures]], ternary search trees can become degenerate depending on the order of the keys.&lt;ref name=wrobel&gt;{{cite web|last=Wrobel|first=Lukasz|title=Ternary Search Tree|url=http://lukaszwrobel.pl/blog/ternary-search-tree}}&lt;/ref&gt; Inserting keys in order is one way to attain the worst possible degenerate tree.&lt;ref name=&quot;dobbs&quot; /&gt; Inserting the keys in random order often produces a well-balanced tree.&lt;ref name=&quot;dobbs&quot; /&gt;

===Running Time===

The running time of ternary search trees varies significantly with the input. Ternary search trees run best when given several similar strings, especially when those strings share a common prefix. Alternatively, ternary search trees are effective when storing a large number of relatively short strings (such as words in a [[dictionary]]).&lt;ref name =&quot;dobbs&quot; /&gt;
Running times for ternary search trees are similar to [[binary search trees]] in that they typically run in logarithmic time but can run in linear time in the degenerate case.

Time complexities for ternary search tree operations:&lt;ref name=&quot;dobbs&quot; /&gt;
{| class=&quot;wikitable&quot;
|-
!  !! Average-Case Running Time !! Worst-Case Running Time
|-
| Lookup || O(log n) || O(n)
|-
| Insertion || O(log n) || O(n)
|-
| Delete || O(log n) || O(n)
|}

==Comparison to other data structures==
===Tries===
While being slower than other [[trie|prefix tree]]s, ternary search trees can be better suited for larger data sets due to their space-efficiency.&lt;ref name=&quot;dobbs&quot; /&gt;

===Hash maps===
[[Hashtable]]s can also be used in place of ternary search trees for mapping strings to values. However, hash maps also frequently use more memory than ternary search trees (but not as much as tries). Additionally, hash maps are typically slower at reporting a string that is not in the same data structure because it must compare the entire string rather than just the first few characters. There is some evidence that shows ternary search trees running faster than hash maps.&lt;ref name=&quot;dobbs&quot; /&gt; Additionally, hash maps do not allow for many of the uses of ternary search trees such as near-neighbor lookups.

==Uses==
Ternary search trees can be used to solve many problems in which a large number of strings must be stored and retrieved in an arbitrary order. Some of the most common or most useful of these are below:
* Anytime a [[trie]] could be used but a less memory-consuming structure is preferred.&lt;ref name =&quot;dobbs&quot; /&gt;
* A quick and space-saving data structure for [[Data mapping|mapping]] strings to other data.&lt;ref name=&quot;wrobel&quot; /&gt;
* To implement an [[auto-complete]] feature.&lt;ref name=&quot;ostrov&quot; /&gt;
* As a [[spell check]] &lt;ref name=wally&gt;{{cite web|last=Flint|first=Wally|title=Plant your data in a ternary search tree|url=http://www.javaworld.com/javaworld/jw-02-2001/jw-0216-ternary.html}}&lt;/ref&gt;
* [[Nearest neighbor search|Near-neighbor searching]] (Of which a spell-check is a special case) &lt;ref name =&quot;dobbs&quot; /&gt;
* As a [[database]] especially when indexing by several non-key fields is desirable &lt;ref name=&quot;wally&quot; /&gt;
* In place of a [[hash table]].&lt;ref name=&quot;wally&quot; /&gt;

==See also==
[[Hashtable]]

==References==
{{reflist}}

==External links==
* [http://www.cs.princeton.edu/~rs/strings/ Ternary Search Trees]
* [https://github.com/varunpant/TernaryTree Ternary Search Tree(C#)]
* [http://search.cpan.org/~mrogaski/Tree-Ternary-0.03/Ternary.pm Tree::Ternary (Perl module)]
* [http://dasnar.sdf-eu.org/res/ctst-README.html Ternary Search Tree code(Broken link?)]
* [http://code.google.com/p/tstdb/ A key-value store implementation based on Ternary Search Tree]
* [http://abc.se/~re/code/tst/ STL-compliant Ternary Search Tree in C++]
* [http://ternary.sourceforge.net Ternary Search Tree in C++]
* [https://gist.github.com/4557872 Simple Ternary Search Tree Implementation for C]
* [https://github.com/kanwei/algorithms/tree/ Ternary Search Tree in Ruby]
* [https://github.com/nlehuen/pytst/ pytst - C++ Ternary Search Tree implementation with Python bindings]
* [http://stackoverflow.com/questions/8143527/is-it-possible-to-generate-all-possible-terms-findable-in-a-ternary-search-tree Algorithm for generating search strings given a Ternary Search Tree]
* [http://vishnuks.com/blog/wordpress/?p=34 Python Implementation Ternary Search Tree]
* [http://trasahin.blogspot.co.uk/2012/06/concurrent-ternary-search-tree.html Java Concurrent Ternary Search Tree]
* [http://monmohan.github.io/dsjslib/ Trie implementation in Javascript]
{{CS-Trees}}
[[Category:Trees (data structures)]]
[[Category:Search algorithms]]</text>
      <sha1>ppx4j2lv0f0ceqcf9t44qgrbrfd4eoq</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Ternary tree</title>
    <ns>0</ns>
    <id>21661207</id>
    <revision>
      <id>609416096</id>
      <parentid>609394086</parentid>
      <timestamp>2014-05-20T18:33:18Z</timestamp>
      <contributor>
        <username>SMG94</username>
        <id>20717814</id>
      </contributor>
      <text xml:space="preserve" bytes="6645">{{About|rooted trees with three children per node|unrooted trees with three neighbors per node|unrooted binary tree}}
:[[Image:Ternary tree.png|right|192|thumb|A simple ternary tree of size 10 and height 2.]]

In [[computer science]], a '''ternary tree''' is a [[tree data structure]] in which each node has '''at most three''' child [[Node (computer science)|nodes]], usually distinguished as &quot;left&quot;, “mid” and &quot;right&quot;. Nodes with children are parent nodes, and child nodes may contain references to their parents. Outside the tree, there is often a reference to the &quot;root&quot; node (the ancestor of all nodes), if it exists. Any node in the data structure can be reached by starting at root node and repeatedly following references to either the left, mid or right child.

Ternary trees are used to implement [[Ternary search tree]]s and [[Ternary heap]]s.

==Definition==

* '''Directed Edge''' - The link from the parent to the child.
* '''Root''' - The node with no parents. There is at most one root node in a rooted tree.
* '''Leaf Node''' - The node which has no children.
* '''Child Node''' - The nodes that are located underneath of a node.
* '''Depth''' - Length of the path from the root to the node. The set of all nodes at a given depth is sometimes called a level of the tree. The root node is at depth zero.
* '''Height''' - Length of the path from the root to the deepest node in the tree. A (rooted) tree with only one node (the root) has a height of zero. In the example diagram, the tree has height of 2.
* '''Sibling''' - Nodes that share the same parent node.

- A node p is an ancestor of a node q if it exists on the path from q to the root. The node q is then termed a descendant of p.

- A size of a node is the number of descendants it has including itself.

==Properties of ternary trees==

* '''Maximum number of nodes'''

– Let &lt;math&gt;h&lt;/math&gt; be height of a ternary tree. 

– Let &lt;math&gt;M(h)&lt;/math&gt; be the maximum number of nodes in a ternary tree of height h

{| class=&quot;wikitable&quot;
|-
! h !! ''M''(''h'')
|-
| 0 || 1
|-
| 1 || 4
|-
| 2 || 13
|-
| 3 || 40
|}

– &lt;math&gt;M(h) =1 + 3 + 9 + \cdots + 3^h = \sum_{i=0}^h 3^i&lt;/math&gt;

– Every tree of height h has at most &lt;math&gt;\sum_{i=0}^h 3^i&lt;/math&gt; nodes.

* If a node &lt;math&gt;N&lt;/math&gt; occupies TREE &lt;math&gt;[k]&lt;/math&gt;, then its '''Left Child''' is stored in TREE &lt;math&gt;[3k-1]&lt;/math&gt;.
* '''Mid Child''' is stored in TREE &lt;math&gt;[3k]&lt;/math&gt;.
* '''Right Child''' is stored in TREE &lt;math&gt;[3k+1]&lt;/math&gt;.

==Common operations==
=== Insertion ===
Nodes can be inserted into ternary trees in between three other nodes or added after an [[external node]]. In Ternary trees, a node that is inserted is specified as to which child it is.

==== External nodes ====
Say that the external node being added onto is node A. To add a new node after node A, A assigns the new node as one of its children and the new node assigns node A as its parent.

==== Internal nodes ====
Insertion on [[internal node]]s is more complex than on external nodes. Say that the internal node is node A and that node B is the child of A. (If the insertion is to insert a right child, then B is the right child of A, and similarly with a left child insertion or mid child.) A assigns its child to the new node and the new node assigns its parent to A. Then the new node assigns its child to B and B assigns its parent as the new node.

=== Deletion ===
Deletion is the process whereby a node is removed from the tree. Only certain nodes in a ternary tree can be removed unambiguously.

==== Node with zero or one child ====
Say that the node to delete is node A. If a node has no children ([[external node]]), deletion is accomplished by setting the child of A's parent to [[null pointer|null]] and A's parent to null. If it has one child, set the parent of A's child to A's parent and set the child of A's parent to A's child.

==Comparison with other trees==

The picture below is a binary search tree that represents 12 two-letter words. All nodes on the left child have smaller values, while all nodes on the right child have greater values for all nodes. A search starts from the root. To find the word &quot;ON&quot;, we compare it to &quot;IN&quot; and take the right branch. Every comparison could access each character of both words.

         in
       /    \
      be    of
     /  \  /  \
    as  by is  or
     \   \  \  / \
     at  he it on to 

Digital search tries to store strings character by character. The next picture is a tree that represents the same set of 12 words;

          _ _ _ _ _ _ _ _ _ _ _ _ _ 
         /     /    / \       \     \
        /     /    /   \       \     \
       a     b    h     i       o     t
      / \   / \   |   / | \    /|\    |
     s  t  e   y  e  n  s  t  f n r   o
    as at be  by he in is it of on or to

each input word is shown beneath the node that represents it. In a tree representing words of lower case letters, each node has 26-way branching. Searches are very fast: A search for &quot;IS&quot; starts at the root, takes the &quot;I&quot; branch, then the &quot;S&quot; branch, and ends at the desired node. At every node, one accesses an array element, tests for null, and takes a branch.

                     i
               /     |    \
              /      |     \
             b       s      o
          / |  \    / \    |  \
         a  e   h  n   t   n   t
         |   \  |         / \  |
         s    y e        f  r  o
          \
           t

The above picture is a balanced ternary search tree for the same set of 12 words. The low and high pointers are shown as solid lines, while equal pointers are shown as dashed lines. A search for the word &quot;IS&quot; starts at the root, proceeds down the equal child to the node with value &quot;S&quot;, and stops there after two comparisons. A search for &quot;AX&quot; makes three comparisons to the first letter &quot;A&quot; and two comparisons to the second letter &quot;X&quot; before reporting that the word is not in the tree.&lt;ref&gt;Jon Bentley and Bob Sedgewick (1998), Dr. Dobb's Journal&lt;/ref&gt;

==Examples of ternary trees==
* [[Ternary search tree]]
* [[Ternary heap]]
* An infinite ternary tree containing all primitive Pythagorean triples is described in [[Tree of primitive Pythagorean triples]] and in [[Formulas_for_generating_Pythagorean_triples#Pythagorean_triples_by_use_of_matrices_and_linear_transformations|Formulas for generating Pythagorean triples]].  The root node contains triple [3,4,5].&lt;ref name=Price&gt;{{cite arxiv|first=H. Lee|last=Price|title=The Pythagorean Tree: A New Species |year=2008|pages=14|eprint=0809.4324 }}&lt;/ref&gt;

==See also==
* [[Binary tree]]
* [[Tree structure]]

==References==
{{Reflist}}

[[Category:Trees (data structures)]]</text>
      <sha1>elmded5p43rqekf3f17lu2ssqb83qih</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Trace tree</title>
    <ns>0</ns>
    <id>24924589</id>
    <revision>
      <id>603402114</id>
      <parentid>545769293</parentid>
      <timestamp>2014-04-09T04:30:49Z</timestamp>
      <contributor>
        <username>Jonesey95</username>
        <id>9755426</id>
      </contributor>
      <minor/>
      <comment>Fixing &quot;Pages with citations using unsupported parameters&quot; error</comment>
      <text xml:space="preserve" bytes="2621">A '''trace tree''' is a [[data structure]] that is used in the runtime compilation of [[programming code]]. Trace trees are used in a type of 'just in time compiler' that traces code executing during hotspots and compiles it. When those hotspots are entered again the compiled code is run instead. It traces each statement executed, including within other [[function call]]s, and this entire execution path is compiled. This is different from compiling individual functions. The compiler can gain more information for the [[Compiler optimization|optimizer]] to operate on, and remove some of the overhead of the calls. Anytime the compiled code makes a call to code that has not been jitted, the interpreter is called to continue.

==References==
*{{Citation
  | last1 = Bala  | first1 = Vasanth
  | last2 = Duesterwald | first2 = Evelyn
  | last3 = Banerjia | first3 = Sanjeev
  | title = Transparent Dynamic Optimization: The Design and Implementation of Dynamo
  | date = June 1999
  | url = http://www.hpl.hp.com/techreports/1999/HPL-1999-78.html
  | year = 1999 }}
*{{Citation
  | last1 = Gal  | first1 = Andreas
  | last2 = Franz | first2 = Michael
  | title = Incremental Dynamic Code Generation with Trace Trees
  | date = November 2006
  | url = http://www.ics.uci.edu/~franz/Site/pubs-pdf/ICS-TR-06-16.pdf
 &lt;!-- | url2 = http://base.google.com/base/a/2277898/D5803457146106589808 --&gt;
  | year = 2006 }}
*{{Citation
  | last1 = Gal  | first1 = Andreas
  | last2 = Bebenita | first2 = Michael
  | last3 = Chang | first3 = Mason
  | last4 = Franz | first4 = Michael
  | title = Making the Compilation “Pipeline” Explicit: Dynamic Compilation Using Trace Tree Serialization
  | date = October 2007
  | url = http://www.ics.uci.edu/~franz/Site/pubs-pdf/ICS-TR-07-12.pdf
  | year = 2007 }}
*{{Citation
  | title = Quick Introduction to Tamarin Tracing
  | last = Double | first = Chris
  | date = February 2008
  | url = http://www.bluishcoder.co.nz/2008/02/quick-introduction-to-tamarin-tracing.html
  | year = 2008 }}
*{{Citation
  | title = The Difference Between Extended Basic Blocks and Traces
  | last = Chang | first = Mason
  | date = January 12, 2009
  | url = http://www.masonchang.com/blog/2009/1/13/the-difference-between-extended-basic-blocks-and-traces.html
  | year = 2009 }}
*{{Citation
  | title = PyPy Blog: Applying a Tracing JIT to an Interpreter
  | last = Bolz | first = Carl Friedrich
  | date = March 2, 2009
  | url = http://morepypy.blogspot.com/2009/03/applying-tracing-jit-to-interpreter.html
  | year = 2009 }}

[[Category:Compiler construction]]
[[Category:Trees (data structures)]]</text>
      <sha1>srdkn97vdr6wrk33qzf9nltz5x5d8vx</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tree (automata theory)</title>
    <ns>0</ns>
    <id>28537741</id>
    <redirect title="Tree (set theory)" />
    <revision>
      <id>608253621</id>
      <parentid>471785052</parentid>
      <timestamp>2014-05-12T17:40:39Z</timestamp>
      <contributor>
        <username>Christian75</username>
        <id>1306352</id>
      </contributor>
      <comment>Added {{[[Template:R to section|R to section]]}} tag to redirect ([[WP:TW|TW]])</comment>
      <text xml:space="preserve" bytes="137">#REDIRECT [[Tree (set theory)#Tree (automata theory)]]
[[Category:Trees (data structures)]]
[[Category:Automata theory]]
{{R to section}}</text>
      <sha1>3roldmyjphpnslsqbl504qis0axftv1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tree automaton</title>
    <ns>0</ns>
    <id>98748</id>
    <revision>
      <id>625685926</id>
      <parentid>602990844</parentid>
      <timestamp>2014-09-15T16:54:55Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>Tagging using [[Project:AWB|AWB]] (10458)</comment>
      <text xml:space="preserve" bytes="15881">{{Refimprove|date=September 2014}}
A '''tree automaton''' is a type of [[state machine]]. Tree automata deal with [[tree structure]]s, rather than the [[string (computer science)|strings]] of more conventional state machines.

The following article deals with branching tree automata, which correspond to [[regular tree language|regular languages of trees]]. For a different notion of tree automaton, see [[tree walking automaton]].

As with classical automata, finite tree automata (FTA) can be either a [[deterministic automaton]] or not. According to how the automaton processes the input tree, finite tree automata can be of two types: (a) bottom up, (b) top down. This is an important issue, as although non-deterministic (ND) top-down and ND bottom-up tree automata are equivalent in expressive power, deterministic top-down automata are strictly less powerful than their deterministic bottom-up counterparts, because tree properties specified by deterministic top-down tree automata can only depend on path properties. (Deterministic bottom-up tree automata are as powerful as ND tree automata.)

==Definitions==
A '''ranked alphabet''' is a pair of an [[Alphabet (computer science)|ordinary alphabet]] ''F'' and a function ''Arity'': ''F''→ℕ. Each letter in ''F'' has its [[arity]] so it can be used to build [[Term (mathematics)|terms]]. Nullary elements (of zero arity) are also called '''constants'''. Terms built with unary symbols and constants can be considered as [[string (computer science)|strings]]. Higher arities leads to proper [[Tree (graph theory)|trees]].

A '''bottom-up finite tree automaton''' over ''F'' is defined as a tuple
(''Q'', ''F'', ''Q''&lt;sub&gt;''f''&lt;/sub&gt;, Δ),
where ''Q'' is a set of unary letters used as states, ''F'' is a ranked alphabet, ''Q''&lt;sub&gt;''f''&lt;/sub&gt; ⊆ ''Q'' is a set of final states, and Δ is a set of [[Production (computer science)|transition rules]] of the form ''f''(''q''&lt;sub&gt;1&lt;/sub&gt;(''x''&lt;sub&gt;1&lt;/sub&gt;),...,''q''&lt;sub&gt;''n''&lt;/sub&gt;(''x''&lt;sub&gt;''n''&lt;/sub&gt;)) → ''q''(''f''(''x''&lt;sub&gt;1&lt;/sub&gt;,...,''x''&lt;sub&gt;''n''&lt;/sub&gt;)), for an ''n''-ary ''f'' ∈ ''F'', ''q'', ''q''&lt;sub&gt;''i''&lt;/sub&gt; ∈ ''Q'', and ''x''&lt;sub&gt;''i''&lt;/sub&gt; variables denoting subtrees. That is, members of Δ are rewrite rules from nodes whose childs' roots are states, to nodes whose roots are states. Thus the state of a node is deduced from the states of its children.

For ''n''=0, that is, for a constant symbol ''f'', the above transition rule definition reads ''f''() → ''q''(''f''()); often the empty parentheses are omitted for convenience: ''f'' → ''q''(''f'').
Since these transition rules for constant symbols (leaves) do not require a state, no explicitly definied initial states are needed. 
A tree automaton is run on a [[ground term]] over ''F'', starting at the leaves and moving upwards, associating a run state from ''Q'' with each subterm.
The tree is accepted if its root is associated to an accepting state from ''Q''&lt;sub&gt;''f''&lt;/sub&gt;.{{#tag:ref|Comon et al.&lt;ref name=&quot;Comon.Dauchet.Gilleron&quot;&gt;{{cite book| author=H. Comon, M. Dauchet, R. Gilleron, F. Jacquemard and D. Lugiez, S. Tison, M. Tommasi| title=Tree Automata Techniques and Applications|date=Nov 2008| url=https://gforge.inria.fr/frs/download.php/10994/tata.pdf| accessdate=11 February 2014}}&lt;/ref&gt; Sect.1.1, p.20}}

A '''top-down finite tree automaton''' over ''F'' is defined as a tuple
(''Q'', ''F'', ''Q''&lt;sub&gt;''i''&lt;/sub&gt;, Δ),
with two differences with bottom-up tree automata. First, ''Q''&lt;sub&gt;''i''&lt;/sub&gt; ⊆ ''Q'', the set of its initial states, replaces ''Q''&lt;sub&gt;''f''&lt;/sub&gt;; second, its transition rules are oriented conversely:
''q''(''f''(''x''&lt;sub&gt;1&lt;/sub&gt;,...,''x''&lt;sub&gt;''n''&lt;/sub&gt;))  → ''f''(''q''&lt;sub&gt;1&lt;/sub&gt;(''x''&lt;sub&gt;1&lt;/sub&gt;),...,''q''&lt;sub&gt;''n''&lt;/sub&gt;(''x''&lt;sub&gt;''n''&lt;/sub&gt;)), for an ''n''-ary ''f'' ∈ ''F'', ''q'', ''q''&lt;sub&gt;''i''&lt;/sub&gt; ∈ ''Q'', and ''x''&lt;sub&gt;''i''&lt;/sub&gt; variables denoting subtrees.
That is, members of Δ are here rewrite rules from nodes whose roots are states to nodes whose childs' roots are states. 
A top-down automaton starts at the root and moves downward along branches of the tree, associating along a run a state with each subterm inductively.
A tree is accepted if every branch can be gone through this way.{{#tag:ref|Comon et al.&lt;ref name=&quot;Comon.Dauchet.Gilleron&quot;/&gt; Sect.1.6, p.38}}

A bottom-up tree automaton is called '''deterministic''' (abbreviated '''DFTA''') if no two rules from Δ have the same left hand side; otherwise it is called '''nondeterministic''' ('''NFTA''').{{#tag:ref|Comon et al.&lt;ref name=&quot;Comon.Dauchet.Gilleron&quot;/&gt; Sect.1.1, p.23.}}
Non-deterministic top-down tree automata have the same expressive power as non-deterministic bottom-up ones;{{#tag:ref|Comon et al.&lt;ref name=&quot;Comon.Dauchet.Gilleron&quot;/&gt; Sect.1.6, Theorem 1.6.1, p.38}}
the transition rules are simply reversed, and the final states become the initial states.

In contrast, '''deterministic''' top-down tree automata{{#tag:ref|In a strict sense, deterministic top-down automata are not defined by Comon et al.,&lt;ref name=&quot;Comon.Dauchet.Gilleron&quot;/&gt; but they are used there (Sect.1.6, Proposition 1.6.2, p.38). They accept the class of path-closed tree languages (Sect.1.8, Exercise 1.6, p.43-44).|group=note}} are less powerful than their bottom-up counterparts, because in a deterministic tree automaton no two transition rules have the same left-hand side. For tree automata, transition rules are rewrite rules; and for top-down ones, the left-hand side will be parent nodes. Consequently a deterministic top-down tree automaton will only be able to test for tree properties that are true in all branches, because the choice of the state to write into each child branch is determined at the parent node, without knowing the child branches contents.

==Examples==

Employing coloring to distinguish members of ''F'' and ''Q'', and using the ranked alphabet ''F''={ {{color|#800000|''false''}},{{color|#800000|''true''}},{{color|#800000|''nil''}},{{color|#800000|''cons''}}(.,.) }, with {{color|#800000|''cons''}} having arity 2 and all other symbols having arity 0, a bottom-up tree automaton recognizing the set of all finite lists of boolean values can be defined as (''Q'', ''F'', ''Q''&lt;sub&gt;''f''&lt;/sub&gt;, Δ) with {{nowrap|1=''Q''={ {{color|#008000|''Bool''}},{{color|#008000|''BList''}} } }}, ''Q''&lt;sub&gt;''f''&lt;/sub&gt;={ {{color|#008000|''BList''}} }, and Δ consisting of the rules

&lt;!---tabular alignment chosen intentionally - please don't change without understanding the example or without proper reason---&gt;
{|
|-
| {{color|#800000|''false''}} || → || {{color|#008000|''Bool''}}({{color|#800000|''false''}}) || (1),
|-
| {{color|#800000|''true''}} || → || {{color|#008000|''Bool''}}({{color|#800000|''true''}}) || (2),
|-
| {{color|#800000|''nil''}} || → || {{color|#008000|''BList''}}({{color|#800000|''nil''}}) || (3), and
|-
| {{color|#800000|''cons''}}({{color|#008000|''Bool''}}(x&lt;sub&gt;1&lt;/sub&gt;),{{color|#008000|''BList''}}(x&lt;sub&gt;2&lt;/sub&gt;)) || → || {{color|#008000|''BList''}}({{color|#800000|''cons''}}(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;)) &amp;nbsp; &amp;nbsp; &amp;nbsp; || (4).
|}

An accepting example run is
&lt;!---tabular alignment chosen intentionally - please don't change without understanding the example or without proper reason---&gt;
{|
|-
|   
| ALIGN=RIGHT | {{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#800000|''false''}},
| ALIGN=RIGHT | {{color|#800000|''cons''}}( 
| ALIGN=RIGHT | {{color|#800000|''true''}}, 
| ALIGN=RIGHT | {{color|#800000|''nil''}}
| )) 
|-
| ⇒ 
| ALIGN=RIGHT | {{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#800000|''false''}},
| ALIGN=RIGHT | {{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#800000|''true''}},
| ALIGN=RIGHT | {{color|#008000|''BList''}}({{color|#800000|''nil''}})
| )) 
| by (3)
|-
| ⇒ 
| ALIGN=RIGHT | {{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#800000|''false''}},
| ALIGN=RIGHT | {{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#008000|''Bool''}}({{color|#800000|''true''}}),
| ALIGN=RIGHT | {{color|#008000|''BList''}}({{color|#800000|''nil''}})
| )) 
| by (2)
|-
| ⇒ 
| ALIGN=RIGHT | {{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#800000|''false''}},
| ALIGN=RIGHT | {{color|#008000|''BList''}}({{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#800000|''true''}},
| ALIGN=RIGHT | {{color|#800000|''nil''}}
| )))
| by (4)
|-
| ⇒ 
| ALIGN=RIGHT | {{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#008000|''Bool''}}({{color|#800000|''false''}}),
| ALIGN=RIGHT | {{color|#008000|''BList''}}({{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#800000|''true''}},
| ALIGN=RIGHT | {{color|#800000|''nil''}}
| )))
| by (1)
|-
| ⇒ 
| ALIGN=RIGHT | {{color|#008000|''BList''}}({{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#800000|''false''}},
| ALIGN=RIGHT | {{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#800000|''true''}},
| ALIGN=RIGHT | {{color|#800000|''nil''}}
| ))) &amp;nbsp; &amp;nbsp; &amp;nbsp;
| by (4), accepted.
|}
Cf. the derivation of the same term from a regular tree grammar corresponding to the automaton, shown at [[Regular tree grammar#Examples]].

An rejecting example run is
&lt;!---tabular alignment chosen intentionally - please don't change without understanding the example or without proper reason---&gt;
{|
|-
| 
| ALIGN=RIGHT | {{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#800000|''false''}},
| ALIGN=RIGHT | {{color|#800000|''true''}}
| ) 
|-
| ⇒ 
| ALIGN=RIGHT | {{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#800000|''false''}},
| ALIGN=RIGHT | {{color|#008000|''Bool''}}({{color|#800000|''true''}})
| ) 
| by (1)
|-
| ⇒ 
| ALIGN=RIGHT | {{color|#800000|''cons''}}(
| ALIGN=RIGHT | {{color|#008000|''Bool''}}({{color|#800000|''false''}}),
| ALIGN=RIGHT | {{color|#008000|''Bool''}}({{color|#800000|''true''}})
| ) &amp;nbsp; &amp;nbsp; &amp;nbsp;
| by (2), no further rule applicable.
|}

==Properties==

===Recognizability===
For a bottom-up automaton, a ground term ''t'' (that is, a tree) is accepted if there exists a reduction that starts from ''t'' and ends with ''q''(''t''), where ''q'' is a final state. For a top-down automaton, a ground term ''t'' is accepted if there exists a reduction that starts from ''q''(''t'') and ends with ''t'', where ''q''(''t'') is an initial state.

The tree language ''L''(''A'') '''recognized''' by a tree automaton ''A'' is the set of all ground terms accepted by ''A''. A set of ground terms is '''recognizable''' if there exists a tree automaton that recognizes it.

A linear (that is, arity-preserving) homomorphism preserves recognizability.{{#tag:ref|Comon et al.&lt;ref name=&quot;Comon.Dauchet.Gilleron&quot;/&gt; Sect.1.4, p.31-32, Theorem 1.4.3. The book's notion of tree homomorphism is more general than that of the article &quot;[[tree homomorphism]]&quot;.}}

===Completeness and Reduction===
A non-deterministic finite tree automaton is '''complete''' if there is at least one transition rule available for every possible symbol-states combination.
A state ''q'' is '''accessible''' if there exists a ground term ''t'' such that there exists a reduction from ''t'' to ''q''(''t''). 
An NFTA is '''reduced''' if all its states are accessible.
{{#tag:ref|Comon et al.&lt;ref name=&quot;Comon.Dauchet.Gilleron&quot;/&gt; Sect.1.1, p.23-24}}

===Pumping Lemma===

Every sufficiently large&lt;ref group=note&gt;Formally: ''[[Term (logic)#Operations with terms|height]]''(''t'') &gt; ''k'', with ''k'' &gt; 0 depending only on ''L'', not on ''t''&lt;/ref&gt; ground term ''t'' in a recognizable tree language ''L'' can be vertically tripartited&lt;ref group=note name=&quot;Context&quot;&gt;Formally: there is a context ''C''[.], a nontrivial context ''C''’[.], and a ground term ''u'' such that ''t'' = ''C''[''C''’[''u'']]. 
A &quot;context&quot; ''C''[.] is a tree with one hole (or, correspondingly, a term with one occurrence of one variable). 
A context is called &quot;trivial&quot; if the tree consists only of the hole node (or, correspondingly, if the term is just the variable). 
The notation ''C''[''t''] means the result of inserting the tree ''t'' into the hole of ''C''[.] (or, correspondingly, [[substitution (logic)#First-order logic|instantiating]] the variable to ''t''). Comon et.al. p.17 gives a formal definition.&lt;/ref&gt; such that arbitrary repetition (&quot;pumping&quot;) of the middle part keeps the resulting term in ''L''.&lt;ref group=note&gt;Formally: ''C''[''C''’&lt;sup&gt;''n''&lt;/sup&gt;[''u'']] ∈ ''L'' for all ''n'' ≥ 0. The notation ''C''&lt;sup&gt;''n''&lt;/sup&gt;[.] means the result of stacking ''n'' copies of ''C''[.] one in another, cf. Comon et.al. p.17.&lt;/ref&gt;
{{#tag:ref|Comon et al.&lt;ref name=&quot;Comon.Dauchet.Gilleron&quot;/&gt; Sect.1.2, p.29}}

For the language of all finite lists of boolean values from the above example, all terms beyond the height limit ''k''=2 can be pumped, since they need to contain an occurrence of {{color|#800000|''cons''}}. For example, 
{|
|-
| {{color|#800000|''cons''}}({{color|#800000|''false''}},
| {{color|#800000|''cons''}}({{color|#800000|''true''}},{{color|#800000|''nil''}})
| )
| ,
|-
| {{color|#800000|''cons''}}({{color|#800000|''false''}},{{color|#800000|''cons''}}({{color|#800000|''false''}},
| {{color|#800000|''cons''}}({{color|#800000|''true''}},{{color|#800000|''nil''}})
| ))
| ,
|-
| {{color|#800000|''cons''}}({{color|#800000|''false''}},{{color|#800000|''cons''}}({{color|#800000|''false''}},{{color|#800000|''cons''}}({{color|#800000|''false''}},
| {{color|#800000|''cons''}}({{color|#800000|''true''}},{{color|#800000|''nil''}})
| )))
| , ...
|}
all belong to that language.

===Closure===
The class of recognizable tree languages is closed under union, under complementation, and under intersection.

===Myhill-Nerode theorem===
A congruence on tree languages is an equivalence relation such that ''u''&lt;sub&gt;1&lt;/sub&gt; ≡ ''v''&lt;sub&gt;1&lt;/sub&gt; and ... and ''u''&lt;sub&gt;''n''&lt;/sub&gt; ≡ ''v''&lt;sub&gt;''n''&lt;/sub&gt; implies ''f''(''u''&lt;sub&gt;1&lt;/sub&gt;,...,''u''&lt;sub&gt;''n''&lt;/sub&gt;) ≡ ''f''(''v''&lt;sub&gt;1&lt;/sub&gt;,...,''v''&lt;sub&gt;''n''&lt;/sub&gt;).
It is of finite index if its number of equivalence-classes is finite.

For a given tree-language ''L'', a congruence can be defined by ''u'' ≡&lt;sub&gt;''L''&lt;/sub&gt; ''v'' if  ''C''[''u''] ∈ ''L'' ⇔ ''C''[''v''] ∈ ''L'' for each context ''C''.{{#tag:ref|See note &lt;ref group=note name=&quot;Context&quot;/&gt; for the notion of a context.|group=note}}

The [[Myhill-Nerode theorem]] for tree automaton states that the following three statements are equivalent:{{#tag:ref|Comon et al.&lt;ref name=&quot;Comon.Dauchet.Gilleron&quot;/&gt; Sect.1.5, p.36}}
# ''L'' is a recognizable tree language
# ''L'' is the union of some equivalence classes of a congruence of finite index
# the relation ≡&lt;sub&gt;''L''&lt;/sub&gt; is a congruence of finite index

==See also==
* [[Courcelle's theorem]], an application of tree automata to prove an algorithmic meta-theorem about graphs
* See [[Regular tree grammar#See also]] for more references on tree automata, including history, applications, algorithms, and computable and uncomputable extensions.

==Notes==
{{reflist|group=note}}

==References==
{{reflist}}

==External links==
All the information in this page was taken from Chapter 1 of http://tata.gforge.inria.fr

===Implementations===
* (OCaml) Grappa - Ranked and Unranked Tree Automata Libraries (http://www.grappa.univ-lille3.fr/~filiot/tata/)
* (OCaml) Timbuk - Tools for Reachability Analysis and Tree Automata Calculations (http://www.irisa.fr/celtique/genet/timbuk/)
* (Java) LETHAL - Library for working with finite tree and hedge automata (http://lethal.sf.net/)
* (Isabelle [OCaml, SML, Haskell]) - Machine-Checked Tree Automata Library (http://afp.sourceforge.net/entries/Tree-Automata.shtml)
* (C++) VATA: A Library for Efficient Manipulation of Non-Deterministic Tree Automata - (http://www.fit.vutbr.cz/research/groups/verifit/tools/libvata/)

{{Formal languages and grammars |state=collapsed}}

{{DEFAULTSORT:Tree Automaton}}
[[Category:Trees (data structures)]]
[[Category:Automata theory]]</text>
      <sha1>558a9wsss1txg00dga3ed3lggd8wjm8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tree homomorphism</title>
    <ns>0</ns>
    <id>9001642</id>
    <revision>
      <id>601729275</id>
      <parentid>471785066</parentid>
      <timestamp>2014-03-28T23:25:15Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes + other fixes using [[Project:AWB|AWB]] (10065)</comment>
      <text xml:space="preserve" bytes="1007">{{multiple issues|
{{Unreferenced|date=January 2007}}
{{orphan|date=November 2009}}
}}

In [[computer science]], a '''tree homomorphism''' is a type of [[homomorphism]] defined on [[tree (data structure)|trees]].

==Definition==
Given a pair of node-labeled trees &lt;math&gt;T_1&lt;/math&gt; and &lt;math&gt;T_2&lt;/math&gt;, a [[map (mathematics)|mapping]] &lt;math&gt;\phi&lt;/math&gt; from the nodes of &lt;math&gt;T_1&lt;/math&gt; to the nodes of &lt;math&gt;T_2&lt;/math&gt; is a ''tree homomorphism'' if the following conditions hold:

* &lt;math&gt;\phi&lt;/math&gt; maps the root of &lt;math&gt;T_1&lt;/math&gt; to the root of &lt;math&gt;T_2&lt;/math&gt;,
* if node &lt;math&gt;n_2&lt;/math&gt; is a child of node &lt;math&gt;n_1&lt;/math&gt; in &lt;math&gt;T_1&lt;/math&gt;, then &lt;math&gt;\phi(n_2)&lt;/math&gt; is a child of &lt;math&gt;\phi(n_1)&lt;/math&gt; in &lt;math&gt;T_2&lt;/math&gt;, and
* for every node &lt;math&gt;n \in T_1&lt;/math&gt;, the label of &lt;math&gt;n&lt;/math&gt; in &lt;math&gt;T_1&lt;/math&gt; is the same as the label of &lt;math&gt;\phi(n)&lt;/math&gt; in &lt;math&gt;T_2&lt;/math&gt;.

==See also==
* [[Homomorphism]]

{{DEFAULTSORT:Tree Homomorphism}}
[[Category:Trees (data structures)]]</text>
      <sha1>42v7m7hkj6ci4oq79y8ztu85iey89e1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tree rearrangement</title>
    <ns>0</ns>
    <id>7625755</id>
    <revision>
      <id>593906233</id>
      <parentid>593906167</parentid>
      <timestamp>2014-02-04T16:34:53Z</timestamp>
      <contributor>
        <ip>207.10.176.35</ip>
      </contributor>
      <comment>Undid revision 593906167 by [[Special:Contributions/207.10.176.35|207.10.176.35]] ([[User talk:207.10.176.35|talk]])</comment>
      <text xml:space="preserve" bytes="4155">'''Tree rearrangements''' are used in [[heuristic]] [[algorithm]]s devoted to searching for an [[Optimization (mathematics)|optimal]] [[tree structure]]. They can be applied to any set of data that are naturally arranged into a tree, but have most applications in [[computational phylogenetics]], especially in [[maximum parsimony]] and [[maximum likelihood]] searches of [[phylogenetic tree]]s, which seek to identify one among many possible trees that best explains the [[evolution]]ary history of a particular [[gene]] or [[species]].

==Basic tree rearrangements==
&lt;gallery&gt;
Image:NNI.svg|Nearest neighbor interchange (NNI)
Image:SPR.svg|Subtree pruning and regrafting (SPR)
Image:TBR.svg|Tree bisection and reconnection (TBR)
&lt;/gallery&gt;

The simplest tree-rearrangement, known as '''nearest-neighbor interchange''', exchanges the connectivity of four subtrees within the main tree. Because there are three possible ways of connecting four subtrees,&lt;ref name=&quot;felsenstein&quot;&gt;Felsenstein J. (2004). ''Inferring Phylogenies'' Sinauer Associates: Sunderland, MA.&lt;/ref&gt; and one is the original connectivity, each interchange creates two new trees. Exhaustively searching the possible nearest-neighbors for each possible set of subtrees is the slowest but most optimizing way of performing this search. An alternative, more wide-ranging search, '''subtree pruning and regrafting''' (SPR), selects and removes a subtree from the main tree and reinserts it elsewhere on the main tree to create a new node. Finally, '''tree bisection and reconnection''' (TBR) detaches a subtree from the main tree at an interior node and then attempts all possible connections between branches of the two trees thus created. The increasing complexity of the tree rearrangement technique correlates with increasing computational time required for the search, although not necessarily with their performance.&lt;ref name=&quot;takahashi&quot;&gt;Takahashi K, Nei M. (2000). Efficiencies of fast algorithms of phylogenetic inference under the criteria of maximum parsimony, minimum evolution, and maximum likelihood when a large number of sequences are used. ''Mol Biol Evol'' 17(8):1251-8.&lt;/ref&gt;

==Tree fusion==
The simplest type of tree fusion begins with two trees already identified as near-optimal; thus, they most likely have the majority of their nodes correct but may fail to resolve individual tree &quot;leaves&quot; properly; for example, the separation ((A,B),(C,D)) at a branch tip versus ((A,C),(B,D)) may be unresolved.&lt;ref name=&quot;felsenstein&quot; /&gt; Tree fusion swaps these two solutions between two otherwise near-optimal trees. Variants of the method use standard [[genetic algorithm]]s with a defined [[objective function]] to swap high-scoring subtrees into main trees that are high-scoring overall.&lt;ref name=&quot;matsuda&quot;&gt;Matsuda H. (1996). Protein phylogenetic inference using maximum likelihood with a genetic algorithm. ''Pacific Symposium on Biocomputing 1996'', pp512-23.&lt;/ref&gt;

== Sectorial search ==
An alternative strategy is to detach part of the tree (which can be selected at random, or using a more strategic approach) and to perform TBR/SPR/NNI on this sub-tree.  This optimized sub-tree can then be replaced on the main tree, hopefully improving the p-score.&lt;ref name=Goloboff1999&gt;Goloboff, P. (1999). Analyzing Large Data Sets in Reasonable Times: Solutions for Composite Optima. Cladistics, 15(4), 415–428. doi:10.1006/clad.1999.0122&lt;/ref&gt;

== Tree drifting ==
To avoid entrapment in local optima, a 'simulated annealing' approach can be used, whereby the algorithm is occasionally permitted to entertain sub-optimal candidate trees, with a probability related to how far they are from the optimum.&lt;ref name=Goloboff1999/&gt;

== Tree fusing ==
Once a range of equally-optimal trees have been gathered, it is often possible to find a better tree by combining the &quot;good bits&quot; of separate trees.  Sub-groups with an identical composition but different topology can be switched and the resultant trees evaluated.&lt;ref name=Goloboff1999/&gt;

==References==
&lt;references /&gt;

[[Category:Phylogenetics]]
[[Category:Optimization algorithms and methods]]
[[Category:Trees (data structures)]]</text>
      <sha1>qw2ic8m8x8w7lpw8wjzcen9o9y4o91k</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tree structure</title>
    <ns>0</ns>
    <id>41821</id>
    <revision>
      <id>619240335</id>
      <parentid>618751212</parentid>
      <timestamp>2014-07-31T04:55:40Z</timestamp>
      <contributor>
        <ip>117.239.73.66</ip>
      </contributor>
      <text xml:space="preserve" bytes="9731">[[File:Binary tree structure.svg|300px|thumb|A tree structure showing the possible hierarchical organization of an encyclopedia.]]
[[File:ENC SYSTEME FIGURE.jpeg|300px|thumb|The original [[Encyclopédie]] used a tree diagram to show the way in which its subjects were ordered.]]
{{moreinline|date=July 2014}}

==Terminology and properties==
The tree elements are called &quot;[[Node (computer science)|node]]s&quot;.
The lines connecting elements are called &quot;branches&quot;. 
Nodes without children are called [[leaf node]]s, &quot;end-nodes&quot;, or &quot;leaves&quot;.

Every [[Finite set|finite]] tree structure has a member that has no [[superior (hierarchy)|superior]]. This member is called the &quot;root&quot; or [[root node]]. The root is the starting node. But the converse is not true: infinite tree structures may or may not have a root node.

The names of relationships between nodes are modeled after family relations. The gender-neutral names &quot;parent&quot; and &quot;child&quot; have largely displaced the older &quot;father&quot; and &quot;son&quot; terminology, although the term &quot;uncle&quot; is still used for other nodes at the same level as the parent.
* A node's &quot;parent&quot; is a node one step higher in the hierarchy (i.e. closer to the root node) and lying on the same branch.
* &quot;Sibling&quot; (&quot;brother&quot; or &quot;sister&quot;) nodes share the same parent node.
* A node's &quot;uncles&quot; are siblings of that node's parent.
* A node that is connected to all lower-level nodes is called an &quot;ancestor&quot;.  The connected lower-level nodes are &quot;descendants&quot; of the ancestor node.

In the example, &quot;encyclopedia&quot; is the parent of &quot;science&quot; and &quot;culture&quot;, its children.  &quot;Art&quot; and &quot;craft&quot; are siblings, and children of &quot;culture&quot;, which is their parent and thus one of their ancestors.  Also, &quot;encyclopedia&quot;, being the root of the tree, is the ancestor of &quot;science&quot;, &quot;culture&quot;, &quot;art&quot; and &quot;craft&quot;.  Finally, &quot;science&quot;, &quot;art&quot; and &quot;craft&quot;, being leaves, are ancestors of no other node.

Tree structures are used to depict all kinds of [[Taxonomy (general)|taxonomic]] knowledge, such as [[family tree]]s, the biological [[evolutionary tree]], the [[Indo-European languages#Classification|evolutionary tree of a language family]], the [[Generative grammar#Context-free grammars|grammatical structure]] of a language (a key example being S → NP VP, meaning a sentence is a noun phrase and a verb phrase, with each in turn having other components which have other components), the way web pages are logically ordered in a web site, [[Tree of primitive Pythagorean triples|mathematical trees of integer sets]], et cetera.

In a tree structure there is one and only one [[path (graph theory)|path]] from any point to any other point.

Tree structures are used extensively in [[computer science]] (''see'' [[Tree (data structure)]] and [[telecommunications]].)

For a formal definition see [[Tree (set theory)|set theory]], and for a generalization in which children are not necessarily successors, see [[prefix order]].

== Examples of tree structures ==
[[File:Tree Map.png|thumb|A tree map used to represent a [[directory structure]] as a nested set.]]
* Internet:
** [[usenet hierarchy]]
** [[Document Object Model]]'s logical structure,&lt;ref&gt;{{cite web |url=http://www.w3.org/TR/DOM-Level-2-Core/introduction.html |title=What is the Document Object Model? |accessdate=2006-12-05 |work=W3C Architecture domain }}&lt;/ref&gt; [[Yahoo!]] subject index, [[DMOZ]]
* [[Operating system]]: [[directory structure]]
* Information management: [[Dewey Decimal System]], [[Polythematic Structured Subject Heading System|PSH]], this hierarchical bulleted list
* Management: hierarchical [[organization]]al structures
* Computer Science:
** [[binary search tree]]
** [[Red-Black Tree]]
** [[AVL tree]]
** [[R-tree]]
* Biology: [[evolutionary tree]]
* Business: [[pyramid selling scheme]]
* Project management: [[work breakdown structure]]
* Linguistics (syntax): [[Phrase structure rules|Phrase structure trees]]
* Sports: [[business chess]], [[Bracket (tournament)|playoffs brackets]]
* Mathematics: [[Von Neumann universe]]
* Group theory: [[Descendant tree (group theory)|descendant trees]]

== Representing trees ==
There are many ways of visually representing tree structures.
Almost always, these boil down to variations, or combinations,
of a few basic styles:

=== Classical node-link diagrams ===
&lt;div style=&quot;float:right;clear:right;margin:0.2em 0 0.2em 1em;border:1px solid silver;{{border-radius|1em}}&quot;&gt;
{| border=&quot;0&quot; style=&quot;border-collapse:collapse;text-align:center;&quot;
|-
! colspan=&quot;3&quot; style=&quot;padding:0 0.5em;&quot; | encyclopedia
|-
! colspan=&quot;2&quot; style=&quot;padding:0 0.5em;&quot; | /&lt;br/&gt;culture
! style=&quot;padding:0 0.5em;&quot; | \&lt;br/&gt;science
|-
! style=&quot;padding:0 0.5em;&quot; | /&lt;br/&gt;art
! style=&quot;padding:0 0.5em;&quot; | \&lt;br/&gt;craft
|}&lt;/div&gt;
Classical node-link diagrams, that connect nodes together with line segments.
{{clear right}}

=== Nested sets ===
&lt;div style=&quot;float:right;clear:right;text-align:center;margin:0.2em 0 0.2em 1em;padding:0.5em 1em 1em 1em;border:1px solid silver;{{border-radius|1em}}&quot;&gt;
{| border=&quot;0&quot; style=&quot;border-collapse:collapse;text-align:center;&quot;
|-
! height=&quot;8px&quot; | [[File:Blank.png]]
! rowspan=&quot;2&quot; | encyclopedia
|-
! width=&quot;20px&quot; style=&quot;border-width:1px 0 0 1px;border-style:solid;&quot; | [[File:Blank.png]]
! width=&quot;20px&quot; style=&quot;border-width:1px 1px 0 0;border-style:solid;&quot; | [[File:Blank.png]]
|-
! colspan=&quot;3&quot; style=&quot;border-width:0 1px 1px 1px;border-style:solid;&quot; |
{| border=&quot;0&quot; style=&quot;border-collapse:collapse;&quot;
|-
!
{| border=&quot;0&quot; style=&quot;border-collapse:collapse;margin:0 0.5em 0.5em 0.5em;text-align:center;&quot;
|-
! height=&quot;8px&quot; | [[File:Blank.png]]
! rowspan=&quot;2&quot; | culture
|-
! style=&quot;border-width:1px 0 0 1px;border-style:solid;&quot; | [[File:Blank.png]]
! style=&quot;border-width:1px 1px 0 0;border-style:solid;&quot; | [[File:Blank.png]]
|-
! colspan=&quot;3&quot; style=&quot;border-width:0 1px 1px 1px;border-style:solid;&quot; | art &amp;nbsp; craft
|}
! science&amp;nbsp;
|}
|}&lt;/div&gt;
[[nested set model|Nested sets]] that use enclosure/containment to show parenthood, examples include [[treemapping|TreeMaps]] and [[fractal space map|fractal maps]].
{{clear right}}

=== Layered &quot;icicle&quot; diagrams ===
&lt;div style=&quot;float:right;clear:right;text-align:center;margin:0.2em 0 0.2em 1em;padding:1em;border:1px solid silver;{{border-radius|1em}}&quot;&gt;
{| border=&quot;1&quot; style=&quot;border:none;border-collapse:collapse;rules:all;text-align:center;&quot;
|-
! colspan=&quot;3&quot; | encyclopedia
|-
! colspan=&quot;2&quot; | culture
! science
|-
! art
! craft
|}&lt;/div&gt;
Layered &quot;icicle&quot; diagrams that use alignment/adjacency.
{{clear right}}

=== Outlines and tree views ===
{{anchor|Outlines|Tree views}}
&lt;div style=&quot;float:right;clear:right;margin:0.2em 0 0.2em 1em;padding:0 1em;border:1px solid silver;{{border-radius|1em}}&quot;&gt;
:'''encyclopedia'''
::'''culture'''
:::'''art'''
:::'''craft'''
::'''science'''
&lt;/div&gt;
&lt;div style=&quot;float:right;margin:0.2em 0 0.2em 1em;padding:0 1em;border:1px solid silver;{{border-radius|1em}}&quot;&gt;
{{Tree list}}
*'''encyclopedia'''
**'''culture'''
***'''art'''
***{{Tree list/final branch}}'''craft'''
**{{Tree list/final branch}}'''science'''
{{Tree list/end}}
&lt;/div&gt;
Lists or diagrams that use indentation, sometimes called &quot;[[Outline (hierarchical)|outline]]s&quot; or &quot;[[tree view]]s&quot;.
{{clear right}}

=== Nested parentheses ===
&lt;div style=&quot;float:right;clear:right;text-align:center;margin:0.2em 0 0.2em 1em;padding:0 1em;border:1px solid silver;{{border-radius|1em}}&quot;&gt;
'''((art,craft)culture,science)encyclopedia'''&lt;br /&gt;
or&lt;br /&gt;
'''encyclopedia(culture(art,craft),science)'''
&lt;/div&gt;
{{See also|Newick format|Dyck language}}
A correspondence to nested parentheses was first noticed by Sir [[Arthur Cayley]].
{{clear right}}

=== Radial trees ===
&lt;div style=&quot;float:right;clear:right;margin:0.2em 0 0.2em 1em;border:1px solid silver;{{border-radius|1em}}&quot;&gt;
{| border=&quot;0&quot; style=&quot;border-collapse:collapse;margin:1em;text-align:center;&quot;
|-
! art&lt;br/&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; \
! craft&lt;br/&gt;/ &amp;nbsp; &amp;nbsp;
|-
! colspan=&quot;2&quot; | culture&lt;br/&gt;|
|-
! colspan=&quot;2&quot; style=&quot;font-size:125%;&quot; | encyclopedia
|-
! colspan=&quot;2&quot; | |&lt;br/&gt;science
|}&lt;/div&gt;
{{See also|Radial tree}}
Trees can also be [[Radial tree|represented radially]].
{{clear right}}

== See also ==
;Kinds of trees:
* [[B-tree]]
* [[Dancing trees]]
* [[Decision tree]]
* [[Left child-right sibling binary tree]]
* [[Tree (data structure)]]
* [[Tree (graph theory)]]
* [[Tree (set theory)]]

;Related articles:
* [[Data drilling]]
* [[Hierarchical model]]: [[Hierarchical clustering|clustering]] and [[Hierarchical query|query]]
* [[Tree testing (information architecture)]]

==References==

&lt;references /&gt;

== Further reading ==
Identification of some of the basic styles of tree structures can be found in:
*[[Jacques Bertin]], ''[[Sémiologie graphique]]'', 1967, Éditions Gauthier-Villars, Paris (2nd edition 1973, English translation 1983);
*[[Donald E. Knuth]], ''[[The Art of Computer Programming]]'', Volume I: Fundamental Algorithms, 1968, Addison-Wesley, pp.&amp;nbsp;309–310;
*Brian Johnson and [[Ben Shneiderman]], ''Tree-maps: A space-filling approach to the visualization of hierarchical information structures'', in Proceedings of [[IEEE]] Visualization (VIS), 1991, pp.&amp;nbsp;284–291;
*[[Peter Eades]], Tao Lin, and Xuemin Lin, ''Two Tree Drawing Conventions'', International Journal of Computational Geometry and Applications, 1993, volume 3, number 2, pp.&amp;nbsp;133–153.

== External links ==
{{Commons category}}
* [http://www.trex.uqam.ca Visualization of phylogenetic trees on the T-REX server]
* [http://www.stcwdc.org/PDF/newsletter_may05.pdf Using a tree structure to design a business process] - from the [[Society for Technical Communication]]

{{DEFAULTSORT:Tree Structure}}
[[Category:Trees (data structures)]]

[[de:Baum (Datenstruktur)]]
[[nl:Boomstructuur]]
[[pl:Struktura drzewiasta]]
[[zh:树结构]]</text>
      <sha1>ovh0hqf71j4nrt4i5baxenk079o0npf</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tree traversal</title>
    <ns>0</ns>
    <id>597584</id>
    <revision>
      <id>626782208</id>
      <parentid>626779418</parentid>
      <timestamp>2014-09-23T16:32:39Z</timestamp>
      <contributor>
        <username>Kri</username>
        <id>253188</id>
      </contributor>
      <comment>/* Types */ + Other types (neither Depths-first not Breadth-first)</comment>
      <text xml:space="preserve" bytes="16712">{{refimprove|date=May 2009}}

{{graph search algorithm}}

In [[computer science]], '''tree traversal''' (also known as '''tree search''') is a form of [[graph traversal]] and refers to the process of visiting (examining and/or updating) each node in a [[tree (data structure)|tree data structure]], exactly once, in a systematic way. Such traversals are classified by the order in which the nodes are visited. The following algorithms are described for a [[binary tree]], but they may be generalized to other trees as well.

== Types==
[[File:Sorted binary tree preorder.svg|thumb|Pre-order: F, B, A, D, C, E, G, I, H]]
[[File:Sorted binary tree inorder.svg|thumb|In-order: A, B, C, D, E, F, G, H, I]]
[[File:Sorted binary tree postorder.svg|thumb|Post-order: A, C, E, D, B, H, I, G, F]]
[[File:Sorted binary tree breadth-first traversal.svg|thumb|Level-order: F, B, G, A, D, I, C, E, H]]
Compared to [[List of data structures#Linear data structures|linear data structures]] like [[linked list]]s and one-dimensional [[Array data structure|arrays]], which have a canonical method of traversal (namely in linear order), tree structures can be traversed in many different ways. Starting at the root of a binary tree, there are three main steps that can be performed and the order in which they are performed defines the traversal type. These steps (in no particular order) are: performing an action on the current node (referred to as &quot;visiting&quot; the node), traversing to the left child node, and traversing to the right child node.

Traversing a tree involves iterating (looping) over all nodes in some manner. Because from a given node there is more than one possible next node (it is not a linear data structure), then, assuming sequential computation (not parallel), some nodes must be deferred – stored in some way for later visiting. This is often done via a [[Stack (abstract data type)|stack]] (LIFO) or [[Queue (abstract data type)|queue]] (FIFO). As a tree is a self-referential (recursively defined) data structure, traversal can naturally be described by [[recursion]] or, more subtly, [[corecursion]], in which case the deferred nodes are stored implicitly – in the case of recursion, in the [[call stack]].

The name given to a particular style of traversal comes from the order in which nodes are visited. Most simply, does one go down first (depth-first: first child, then grandchild before second child) or across first (breadth-first: first child, then second child before grandchildren)? Depth-first traversal is further classified by position of the root element with regard to the left and right nodes. Imagine that the left and right nodes are constant in space, then the root node could be placed to the left of the left node (pre-order), between the left and right node (in-order), or to the right of the right node (post-order). There is no equivalent variation in breadth-first traversal – given an ordering of children, &quot;breadth-first&quot; is unambiguous.

For the purpose of illustration, it is assumed that left nodes always have priority over right nodes. This ordering can be reversed as long as the same ordering is assumed for all traversal methods.

Depth-first traversal is easily implemented via a stack, including recursively (via the call stack), while breadth-first traversal is easily implemented via a queue, including corecursively.

Beyond these basic traversals, various more complex or hybrid schemes are possible, such as [[depth-limited search]]es such as [[iterative deepening depth-first search]].

===Depth-first===
{{see also|Depth-first search}}
There are three types of depth-first traversal: pre-order,&lt;ref name=&quot;holtenotes&quot;&gt;http://webdocs.cs.ualberta.ca/~holte/T26/tree-traversal.html&lt;/ref&gt; in-order,&lt;ref name=&quot;holtenotes&quot;/&gt; and post-order.&lt;ref name=&quot;holtenotes&quot;/&gt; For a binary tree, they are defined as operations recursively at each node, starting with the root node as follows:

==== Pre-order ====
# Visit the root. 
# Traverse the left subtree.
# Traverse the right subtree.

==== {{anchor|In-order}} In-order (symmetric) ====
# Traverse the left subtree.
# Visit the root.
# Traverse the right subtree.

==== Post-order ====
# Traverse the left subtree.
# Traverse the right subtree
# Visit the root

The trace of a traversal is called a sequentialisation of the tree. No one sequentialisation according to pre-, in- or post-order describes the underlying tree uniquely. Given a tree with distinct elements, either pre-order or post-order paired with in-order is sufficient to describe the tree uniquely. However, pre-order with post-order leaves some ambiguity in the tree structure.&lt;ref&gt;[http://cs.stackexchange.com/questions/439/which-combinations-of-pre-post-and-in-order-sequentialisation-are-unique Which combinations of pre-, post- and in-order sequentialisation are unique?]&lt;/ref&gt;

====Generic tree====
To traverse any tree in '''depth-first order''', perform the following operations recursively at each node:
# Perform pre-order operation
# For each ''i'' (with ''i'' = 1 to ''n'' − 1) do:
## Visit ''i''-th, if present
## Perform in-order operation
# Visit ''n''-th (last) child, if present
# Perform post-order operation

where ''n'' is the number of child nodes. Depending on the problem at hand, the pre-order, in-order or post-order operations may be void, or you may only want to visit a specific child node, so these operations are optional. Also, in practice more than one of pre-order, in-order and post-order operations may be required. For example, when inserting into a ternary tree, a pre-order operation is performed by comparing items. A post-order operation may be needed afterwards to re-balance the tree.

===Breadth-first===
{{see also|Breadth-first search}}
Trees can also be traversed in '''level-order''', where we visit every node on a level before going to a lower level. This search is referred to as ''[[breadth-first search]]'', as the search tree is broadened as much as possible on each depth before making going to the next depth.

===Other types===
There are also tree traversal algorithms that classify as neither depth-first search nor breadth-first search. One such algorithm is [[Monte Carlo tree search]], which concentrates on analyzing the most promising moves, basing the expansion of the [[search tree]] on [[Monte Carlo method|random sampling]] of the search space.

== Applications ==
Pre-order traversal while duplicating nodes and edges can make a complete duplicate of a [[binary tree]]. It can also be used to make a prefix expression ([[Polish notation]]) from [[Parse tree|expression trees]]: traverse the expression tree pre-orderly.

In-order traversal is very commonly used on [[binary search tree]]s because it returns values from the underlying set in order, according to the comparator that set up the binary search tree (hence the name).

Post-order traversal while deleting or freeing nodes and values can delete or free an entire binary tree. It can also generate a [[Reverse Polish notation|postfix]] representation of a binary tree.

==Implementations==
{{unreferenced section|date=June 2013}}

===Depth-first===

====Pre-order====
{|
|-valign=&quot;top&quot;
|
 '''preorder'''(node)
   '''if''' node == '''null then return'''
   visit(node)
   preorder(node.left) 
   preorder(node.right)
|
 '''iterativePreorder'''(node)
   parentStack = '''empty stack'''
   parentStack.push(null)
   top =  node 
   '''while ( top != null )
       visit( top )
       if ( top.right ≠ null ) 
           parentStack.push(top.right)
       if ( top.left ≠ null ) 
           parentStack.push(top.left)
       top = parentStack.pop()
|}

====In-order====
{|
|-valign=&quot;top&quot;
|
 '''inorder'''(node)
   '''if''' node == '''null then return'''
   inorder(node.left)
   visit(node)
   inorder(node.right)
|
 '''iterativeInorder'''(node)
   parentStack = '''empty stack'''
   '''while''' ('''not''' parentStack.isEmpty() '''or''' node ≠ '''null''')
     '''if''' (node ≠ '''null''')
       parentStack.push(node)
       node = node.left
     '''else'''
       node = parentStack.pop()
       visit(node)
       node = node.right
|}

====Post-order====
{|
|-valign=&quot;top&quot;
|
 '''postorder'''(node)
   '''if''' node == '''null then return'''
   postorder(node.left)
   postorder(node.right)
   visit(node)
|
 '''iterativePostorder'''(node)
   parentStack = '''empty stack'''  
   lastnodevisited = '''null''' 
   '''while''' ('''not''' parentStack.isEmpty() '''or''' node ≠ '''null''')
     '''if''' (node ≠ '''null''')
       parentStack.push(node)
       node = node.left
     '''else'''
       peeknode = parentStack.peek()
       '''if''' (peeknode.right ≠ '''null''' '''and''' lastnodevisited ≠ peeknode.right) 
         /* if right child exists AND traversing node from left child, move right */
         node = peeknode.right
       '''else'''
         parentStack.pop() 
         visit(peeknode)
         lastnodevisited = peeknode

|}
All the above implementations require [[call stack]] space proportional to the height of the tree. In a poorly balanced tree, this can be considerable. We can remove the stack requirement by maintaining parent pointers in each node, or by [[#Morris in-order traversal using threading|threading the tree]] (next section).

====Morris in-order traversal using threading====

A binary tree is [[threaded binary tree|threaded]] by making every left child pointer (that would otherwise be null) point to the in-order predecessor of the node (if it exists) and every right child pointer (that would otherwise be null) point to the in-order successor of the node (if it exists).

Advantages:
# Avoids recursion, which uses a call stack and consumes memory and time.
# The node keeps a record of its parent.

Disadvantages:
# The tree is more complex.
# It is more prone to errors when both the children are not present and both values of nodes point to their ancestors.

Morris traversal is an implementation of in-order traversal that uses threading:
# Create links to the in-order successor 
# Print the data using these links
# Revert the changes to restore original tree.

===Breadth-first===

Also, listed below is pseudocode for a simple [[queue (data structure)|queue]] based level order traversal, and will require space proportional to the maximum number of nodes at a given depth. This can be as much as the total number of nodes / 2. A more space-efficient approach for this type of traversal can be implemented using an [[iterative deepening depth-first search]].

 '''levelorder'''(root)
   q = empty queue
   q.enqueue(root)
   '''while''' not q.empty '''do'''
     node := q.dequeue()
     visit(node)
     '''if''' node.left ≠ '''null then'''
       q.enqueue(node.left)
     '''if''' node.right ≠ '''null then'''
       q.enqueue(node.right)

==Infinite trees==

While traversal is usually done for trees with a finite number of nodes (and hence finite depth and finite [[branching factor]]) it can also be done for infinite trees. This is of particular interest in [[functional programming]] (particularly with [[lazy evaluation]]), as infinite data structures can often be easily defined and worked with, though they are not (strictly) evaluated, as this would take infinite time. Some finite trees are too large to represent explicitly, such as the [[game tree]] for [[chess]] or [[Go (game)|go]], and so it is useful to analyze them as if they were infinite.

A basic requirement for traversal is to visit every node. For infinite trees, simple algorithms often fail this. For example, given a binary tree of infinite depth, a depth-first traversal will go down one side (by convention the left side) of the tree, never visiting the rest, and indeed if in-order or post-order will never visit ''any'' nodes, as it has not reached a leaf (and in fact never will). By contrast, a breadth-first (level-order) traversal will traverse a binary tree of infinite depth without problem, and indeed will traverse any tree with bounded branching factor.

On the other hand, given a tree of depth 2, where the root node has infinitely many children, and each of these children has two children, a depth-first traversal will visit all nodes, as once it exhausts the grandchildren (children of children of one node), it will move on to the next (assuming it is not post-order, in which case it never reaches the root). By contrast, a breadth-first traversal will never reach the grandchildren, as it seeks to exhaust the children first.

A more sophisticated analysis of running time can be given via infinite [[ordinal number]]s; for example, the breadth-first traversal of the depth 2 tree above will take ?·2 steps: ? for the first level, and then another ? for the second level.

Thus, simple depth-first or breadth-first searches do not traverse every infinite tree, and are not efficient on very large trees. However, hybrid methods can traverse any (countably) infinite tree, essentially via a [[Diagonal argument (disambiguation)|diagonal argument]] (&quot;diagonal&quot; – a combination of vertical and horizontal – corresponds to a combination of depth and breadth).

Concretely, given the infinitely branching tree of infinite depth, label the root node &lt;math&gt;(),&lt;/math&gt; the children of the root node &lt;math&gt;(1), (2), \dots,&lt;/math&gt; the grandchildren &lt;math&gt;(1,1), (1,2), \ldots, (2,1), (2,2), \ldots,&lt;/math&gt; and so on. The nodes are thus in a [[bijection|one-to-one]] correspondence with finite (possibly empty) sequences of positive numbers, which are countable and can be placed in order first by sum of entries, and then by [[lexicographic order]] within a given sum (only finitely many sequences sum to a given value, so all entries are reached – formally there are a finite number of [[Composition (number theory)|compositions]] of a given natural number, specifically 2&lt;sup&gt;''n''-1&lt;/sup&gt;compositions of ''n''&amp;nbsp;=&amp;nbsp;1;), which gives a traversal. Explicitly:

 0: ()
 1: (1)
 2: (1,1) (2)
 3: (1,1,1) (1,2) (2,1) (3)
 4: (1,1,1,1) (1,1,2) (1,2,1) (1,3) (2,1,1) (2,2) (3,1) (4)

etc.

This can be interpreted as mapping the infinite depth binary tree onto this tree and then applying breadth-first traversal: replace the &quot;down&quot; edges connecting a parent node to its second and later children with &quot;right&quot; edges from the 1st child to the 2nd child, 2nd child to third child, etc. Thus at each step one can either go down (append a (,1) to the end) or go right (add 1 to the last number) (except the root, which is extra and can only go down), which shows the correspondence between the infinite binary tree and the above numbering; the sum of the entries (minus 1) corresponds to the distance from the root, which agrees with the 2&lt;sup&gt;''n''-1&lt;/sup&gt; nodes at depth ''n''-1 in the infinite binary tree (2 corresponds to binary).

== References ==
{{Reflist}}

; General
* Dale, Nell. Lilly, Susan D. &quot;Pascal Plus Data Structures&quot;. D. C. Heath and Company. Lexington, MA. 1995. Fourth Edition.
* Drozdek, Adam. &quot;Data Structures and Algorithms in C++&quot;. Brook/Cole. Pacific Grove, CA. 2001. Second edition.
* http://www.math.northwestern.edu/~mlerma/courses/cs310-05s/notes/dm-treetran

== External links ==
* [http://www.cosc.canterbury.ac.nz/people/mukundan/dsal/BTree.html Animation Applet of Binary Tree Traversal]
* [http://www.SQLSummit.com/AdjacencyList.htm The Adjacency List Model for Processing Trees with SQL]
* [http://www.sitepoint.com/hierarchical-data-database/ Storing Hierarchical Data in a Database] with traversal examples in PHP
* [http://dev.mysql.com/tech-resources/articles/hierarchical-data.html Managing Hierarchical Data in MySQL]
* [http://www.artfulsoftware.com/mysqlbook/sampler/mysqled1ch20.html Working with Graphs in MySQL]
* [http://www.jslab.dk/articles/non.recursive.preorder.traversal Non-recursive traversal of DOM trees in JavaScript]
* [http://code.google.com/p/treetraversal/ Sample code for recursive and iterative tree traversal implemented in C.]
* [http://arachnode.net/blogs/programming_challenges/archive/2009/09/25/recursive-tree-traversal-orders.aspx Sample code for recursive tree traversal in C#.]
* [http://rosettacode.org/wiki/Tree_traversal See tree traversal implemented in various programming language] on [[Rosetta Code]]
* [http://www.perlmonks.org/?node_id=600456 Tree traversal without recursion]

{{DEFAULTSORT:Tree Traversal}}
[[Category:Trees (data structures)]]
[[Category:Articles with example Haskell code]]
[[Category:Articles with example Java code]]
[[Category:Articles with example pseudocode]]
[[Category:Graph algorithms]]
[[Category:Recursion]]
[[Category:Iteration in programming]]

[[de:Binärbaum#Traversierung]]
[[ja:木構造 (データ構造)]]
[[zh:树的遍历]]</text>
      <sha1>k6qztfnkamu7ssljhvyjs9m601dwjta</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tree walking automaton</title>
    <ns>0</ns>
    <id>16024502</id>
    <revision>
      <id>594958023</id>
      <parentid>594956885</parentid>
      <timestamp>2014-02-11T10:21:32Z</timestamp>
      <contributor>
        <username>Jochen Burghardt</username>
        <id>17350134</id>
      </contributor>
      <comment>/* Definition */ unified ((math)) to unicode; rephrased</comment>
      <text xml:space="preserve" bytes="3794">A '''tree walking automaton''' (TWA) is a type of [[finite automaton]] that deals with [[tree structure]]s rather than strings. The concept was originally proposed in {{harvtxt|Aho|Ullman|1971}}.

The following article deals with tree walking automata. For a different notion of tree automaton, closely related to [[regular tree language]]s, see [[tree automaton|branching automaton]].

==Definition==

All [[tree (data structure)|trees]] are assumed to be [[Binary tree|binary]], with labels from a fixed alphabet Σ.

Informally, a tree walking automaton ''A'' (TWA) is a [[Finite-state machine|finite state device]] which walks over the tree in a sequential manner. At each moment ''A'' visits a node ''v'' in state ''q''. Depending on the state ''q'', the label of the node ''v'', and whether the node is the root, a left child, a right child or a leaf, ''A'' changes its state from ''q'' to ''q''‘ and moves to the parent of ''v'' or its left or right child. A TWA accepts a tree if it enters an accepting state, and rejects if its enters a rejecting state or makes an infinite loop. As with string automata, a TWA may be deterministic or nondeterministic.

More formally, a (nondeterministic) tree walking automaton over an alphabet Σ is a tuple
{{nowrap|1=''A'' = (''Q'', Σ, ''I'', ''F'', ''R'', δ)}}
where ''Q'' is a finite set of states, its subset ''I'', ''F'', and ''R'' is the set of initial, accepting and rejecting states, respectively, and {{nowrap|δ ⊆ (''Q'' × { ''root'', ''left'', ''right'', ''leaf'' } × Σ × { ''up'', ''left'', ''right'' } × Q)}} is the transition relation.

==Example==

A simple example of a tree walking automaton is a TWA that performs [[depth-first search]] (DFS) on the input tree. The automaton &lt;math&gt;A&lt;/math&gt; has 3 states, &lt;math&gt;Q = \{ q_{0}, q_{\mathit{left}}, q_{\mathit{right}} \}&lt;/math&gt;. &lt;math&gt;A&lt;/math&gt; begins in the root in state &lt;math&gt;q_{0}&lt;/math&gt; and descends to the left subtree. Then it processes the tree recursively. Whenever &lt;math&gt;A&lt;/math&gt; enters a node &lt;math&gt;v&lt;/math&gt; in state &lt;math&gt;q_{\mathit{left}}&lt;/math&gt;, it means that the left subtree of &lt;math&gt;v&lt;/math&gt; has just been processed, so it proceeds to the right subtree of &lt;math&gt;v&lt;/math&gt;. If &lt;math&gt;A&lt;/math&gt; enters a node &lt;math&gt;v&lt;/math&gt; in state &lt;math&gt;q_{\mathit{right}}&lt;/math&gt;, it means that the whole subtree with root &lt;math&gt;v&lt;/math&gt; has been processed and &lt;math&gt;A&lt;/math&gt; walks to the parent of &lt;math&gt;v&lt;/math&gt; and changes its state to &lt;math&gt;q_{\mathit{left}}&lt;/math&gt; or &lt;math&gt;q_{\mathit{right}}&lt;/math&gt;, depending on whether &lt;math&gt;v&lt;/math&gt; is a left or right child.

==Properties==

Unlike [[tree automaton|branching automata]], tree walking automata are difficult to analyze and even simple properties are nontrivial to prove. The following list summarizes some known facts related to TWA:
*As shown by {{harvtxt|Bojanczyk|Colcombet|2006}}, deterministic TWA are strictly weaker than nondeterministic ones (&lt;math&gt;\mathit{DTWA} \subsetneq \mathit{TWA}&lt;/math&gt;)
*deterministic TWA are closed under complementation (but it is not known whether the same holds for nondeterministic ones)
*the set of languages recognized by TWA is strictly contained in regular tree languages (&lt;math&gt;\mathit{TWA} \subsetneq \mathit{REG}&lt;/math&gt;), i.e. there exist regular languages which are not recognized by any tree walking automaton {{harv|Bojanczyk|Colcombet|2008}}.

==See also==

*[[Pebble automaton|Pebble automata]], an extension of tree walking automata

==References==
* {{cite doi|10.1016/S0019-9958(71)90706-6}}
* {{cite doi|10.1016/j.tcs.2005.10.031}}
* {{cite doi|10.1137/050645427}}

==External links==
*Mikołaj Bojanczyk: [http://www.mimuw.edu.pl/~bojan/papers/twasurvey.pdf Tree-walking automata]. A brief survey.

[[Category:Trees (data structures)]]
[[Category:Automata theory]]</text>
      <sha1>rqmuwtwenhwgczza7hah5i7sjbhbpsj</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Trie</title>
    <ns>0</ns>
    <id>31274</id>
    <revision>
      <id>616659272</id>
      <parentid>616659178</parentid>
      <timestamp>2014-07-12T14:48:20Z</timestamp>
      <contributor>
        <username>Mikeblas</username>
        <id>327592</id>
      </contributor>
      <minor/>
      <comment>/* External links */ +1</comment>
      <text xml:space="preserve" bytes="17832">{{about|a tree data structure|the French commune|Trie-sur-Baïse}}
[[Image:trie example.svg|thumb|right|250px|A trie for keys &quot;A&quot;, &quot;to&quot;, &quot;tea&quot;, &quot;ted&quot;, &quot;ten&quot;, &quot;i&quot;, &quot;in&quot;, and &quot;inn&quot;.]]

In [[computer science]], a '''trie''', also called '''digital tree''' and sometimes '''[[radix tree]]''' or '''prefix tree''' (as they can be searched by prefixes), is an [[ordered tree data structure|ordered tree]] [[data structure]] that is used to store a [[Set (abstract data type)|dynamic set]] or [[associative array]] where the keys are usually [[string (computer science)|string]]s. Unlike a [[binary search tree]], no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common [[prefix]] of the string associated with that node, and the root is associated with the [[string (computer science)|empty string]]. Values are normally not associated with every node, only with leaves and some inner nodes that correspond to keys of interest. For the space-optimized presentation of prefix tree, see [[compact prefix tree]].

The term trie comes from re'''trie'''val. This term was coined by  [[Edward Fredkin]], who pronounces it {{IPAc-en|ˈ|t|r|iː}} &quot;tree&quot; as in the word retrieval.&lt;ref name = DADS&gt;{{cite web|url=http://www.nist.gov/dads/HTML/trie.html|title=trie|first=Paul E.|last=Black|date=2009-11-16|work=Dictionary of Algorithms and Data Structures|publisher=[[National Institute of Standards and Technology]]|archiveurl=http://www.webcitation.org/5pqUULy24|archivedate=2010-05-19}}&lt;/ref&gt;&lt;ref name=&quot;Liang1983&quot;/&gt; However, other authors pronounce it {{IPAc-en|ˈ|t|r|aɪ}} &quot;try&quot;, in an attempt to distinguish it verbally from &quot;tree&quot;.&lt;ref name=DADS/&gt;&lt;ref name=&quot;Liang1983&quot;/&gt;&lt;ref name = KnuthVol3&gt;{{cite book|last=Knuth|first=Donald|authorlink=Donald Knuth|title=The Art of Computer Programming Volume 3: Sorting and Searching|edition=2nd|year=1997|publisher=Addison-Wesley|isbn=0-201-89685-0|page=492|chapter=6.3: Digital Searching}}&lt;/ref&gt;

In the example shown, keys are listed in the nodes and values below them. Each complete English word has an arbitrary integer value associated with it. A trie can be seen as a [[deterministic finite automaton]] without loops. Each [[finite language]] is generated by a trie automaton, and each trie can be compressed into a [[Deterministic acyclic finite state automaton|DAFSA]].

It is not necessary for keys to be explicitly stored in nodes. (In the figure, words are shown only to illustrate how the trie works.)

Though tries are most commonly keyed by character strings, they don't need to be. The same algorithms can easily be adapted to serve similar functions of ordered lists of any construct, e.g., permutations on a list of digits or shapes. In particular, a '''bitwise trie''' is keyed on the individual bits making up a short, fixed size of bits such as an integer number or memory address.

== Applications ==

=== As a replacement for other data structures ===
As discussed below, a trie has a number of advantages over binary search trees.&lt;ref name=&quot;trieoverbinary&quot;&gt;{{cite journal |last1=Bentley|first1=Jon|last2=Sedgewick|first2=Robert|authorlink2=Robert Sedgewick (computer scientist)| title=Ternary Search Trees| journal=[[Dr. Dobb's Journal]]|date=1998-04-01|publisher=Dr Dobb's|url=http://www.ddj.com/windows/184410528|archiveurl=http://web.archive.org/web/20080623071352/http://www.ddj.com/windows/184410528|archivedate=2008-06-23}}&lt;/ref&gt; A trie can also be used to replace a [[hash table]], over which it has the following advantages:

* Looking up data in a trie is faster in the worst case, O(m) time (where m is the length of a search string), compared to an imperfect hash table. An imperfect hash table can have key collisions. A key collision is the hash function mapping of different keys to the same position in a hash table. The worst-case lookup speed in an imperfect hash table is [[Hash table#Chaining|O(N)]] time, but far more typically is O(1), with O(m) time spent evaluating the hash.
* There are no collisions of different keys in a trie.
* Buckets in a trie, which are analogous to hash table buckets that store key collisions, are necessary only if a single key is associated with more than one value.
* There is no need to provide a hash function or to change hash functions as more keys are added to a trie.
* A trie can provide an alphabetical ordering of the entries by key.

Tries do have some drawbacks as well:

* Tries can be slower in some cases than hash tables for looking up data, especially if the data is directly accessed on a hard disk drive or some other secondary storage device where the random-access time is high compared to main memory.&lt;ref name=&quot;triememory&quot;&gt;{{cite journal | author=Edward Fredkin| authorlink=Edward Fredkin| title=Trie Memory| journal=Communications of the ACM| year=1960| volume=3| issue=9| pages=490–499| doi=10.1145/367390.367400 }}&lt;/ref&gt;
* Some keys, such as floating point numbers, can lead to long chains and prefixes that are not particularly meaningful. Nevertheless a bitwise trie can handle standard IEEE single and double format floating point numbers.
* Some tries can require more space than a hash table, as memory may be allocated for each character in the search string, rather than a single chunk of memory for the whole entry, as in most hash tables.

=== Dictionary representation ===
A common application of a trie is storing a [[predictive text]] or [[autocomplete]] dictionary, such as found on a [[mobile telephone]]. Such applications take advantage of a trie's ability to quickly search for, insert, and delete entries; however, if storing dictionary words is all that is required (i.e. storage of information auxiliary to each word is not required), a minimal [[deterministic acyclic finite state automaton]] would use less space than a trie. This is because an acyclic deterministic finite automaton can compress identical branches from the trie which correspond to the same suffixes (or parts) of different words being stored.

Tries are also well suited for implementing approximate matching algorithms,&lt;ref&gt;{{cite doi| 10.1145/360825.360855}}&lt;/ref&gt; including those used in [[spell checking]] and [[Hyphenation algorithm|hyphenation]]&lt;ref name=&quot;Liang1983&quot;/&gt; software.

=== Algorithms ===
We can describe lookup (and membership) easily. Given a recursive trie type, storing an optional value at each node, and a list of children tries, indexed by the next character (here, represented as a [[Haskell (programming language)|Haskell]] data type):

&lt;source lang=&quot;haskell&quot;&gt;
 import Prelude hiding (lookup)
 import Data.Map (Map, lookup)
 
 data Trie a = Trie { value    :: Maybe a,
                      children :: Map Char (Trie a) }
&lt;/source&gt;

We can look up a value in the trie as follows:

&lt;source lang=&quot;haskell&quot;&gt;
 find :: String -&gt; Trie a -&gt; Maybe a
 find []     t = value t
 find (k:ks) t = do
   ct &lt;- lookup k (children t)
   find ks ct
&lt;/source&gt;

In an imperative style, and assuming an appropriate data type in place, we can describe the same algorithm in [[Python (programming language)|Python]] (here, specifically for testing membership). Note that &lt;code&gt;children&lt;/code&gt; is a map of a node's children; and we say that a &quot;terminal&quot; node is one which contains a valid word.
&lt;source lang=python&gt;
def find(node, key):
    for char in key:
        if char not in node.children:
            return None
        else:
            node = node.children[char]
    return node.value
&lt;/source&gt;

Insertion proceeds by walking the trie according to the string to be inserted, then appending new nodes for the suffix of the string that is not contained in the trie. In imperative pseudocode,

&lt;source lang=&quot;pascal&quot;&gt;
algorithm insert(root : node, s : string, value : any):
    node = root
    i    = 0
    n    = length(s)

    while i &lt; n:
        if node.child(s[i]) != nil:
            node = node.child(s[i])
            i = i + 1
        else:
            break

    (* append new nodes, if necessary *)
    while i &lt; n:
        node.child(s[i]) = new node
        node = node.child(s[i])
        i = i + 1

    node.value = value
&lt;/source&gt;

=== Sorting ===
Lexicographic sorting of a set of keys can be accomplished with a simple trie-based algorithm as follows:

* Insert all keys in a trie.
* Output all keys in the trie by means of [[pre-order traversal]], which results in output that is in [[lexicographic order|lexicographically]] increasing order.  [[Pre-order traversal]] is a kind of [[depth-first search|depth-first traversal]].

This algorithm is a form of [[radix sort]].

A trie forms the fundamental data structure of [[Burstsort]], which (in 2007) was the fastest known string sorting algorithm.&lt;ref name=&quot;cachestringsort&quot;&gt;{{Cite web|url=http://www.cs.mu.oz.au/~rsinha/papers/SinhaRingZobel-2006.pdf|format=PDF|title=Cache-Efficient String Sorting Using Copying|accessdate=2008-11-15}}&lt;/ref&gt; However, now there are faster string sorting algorithms.&lt;ref name=&quot;stringradix&quot;&gt;{{Cite web|url=http://dx.doi.org/10.1007/978-3-540-89097-3_3|format=PDF|title=Engineering Radix Sort for Strings.|accessdate=2013-03-11}}&lt;/ref&gt;

=== Full text search ===
A special kind of trie, called a [[suffix tree]], can be used to index all suffixes in a text in order to carry out fast full text searches.

=== Bitwise tries ===
Bitwise tries are much the same as a normal character based trie except that individual bits are used to traverse what effectively becomes a form of binary tree. Generally, implementations use a special CPU instruction to very quickly find the first set bit in a fixed length key (e.g. GCC's &lt;code&gt;__builtin_clz()&lt;/code&gt; intrinsic). This value is then used to index a 32- or 64-entry table which points to the first item in the bitwise trie with that number of leading zero bits. The search then proceeds by testing each subsequent bit in the key and choosing &lt;code&gt;child[0]&lt;/code&gt; or &lt;code&gt;child[1]&lt;/code&gt; appropriately until the item is found.

Although this process might sound slow, it is very cache-local and highly parallelizable due to the lack of register dependencies and therefore in fact has excellent performance on modern out-of-order execution CPUs. A [[red-black tree]] for example performs much better on paper, but is highly cache-unfriendly and causes multiple pipeline and [[Translation lookaside buffer|TLB]] stalls on modern CPUs which makes that algorithm bound by memory latency rather than CPU speed. In comparison, a bitwise trie rarely accesses memory and when it does it does so only to read, thus avoiding SMP cache coherency overhead, and hence is becoming increasingly the algorithm of choice for code which does a lot of insertions and deletions such as memory allocators (e.g. recent versions of the famous [[Malloc#dlmalloc|Doug Lea's allocator (dlmalloc) and its descendents]]).

=== Compressing tries ===
When the trie is mostly static, i.e. all insertions or deletions of keys from a prefilled trie are disabled and only lookups are needed, and when the trie nodes are not keyed by node specific data (or if the node's data is common) it is possible to compress the trie representation by merging the common branches.&lt;ref&gt;{{cite journal
|url        = http://www.pg.gda.pl/~jandac/daciuk98.ps.gz
|title      = Incremental Construction of Minimal Acyclic Finite-State Automata
|year       = 2000
|author     = Jan Daciuk, Stoyan Mihov, Bruce W. Watson, Richard E. Watson
|journal       = Computational Linguistics 
|publisher  = Association for Computational Linguistics
|pages      = 3
|doi        = 10.1162/089120100561601
|archiveurl = http://www.mitpressjournals.org/doi/abs/10.1162/089120100561601
|archivedate= 2006-03-13
|quote      = This paper presents a method for direct building of minimal acyclic finite states automaton which recognizes a given finite list of words in lexicographical order. Our approach is to construct a minimal automaton in a single phase by adding new strings one by one and minimizing the resulting automaton on-the-fly
|accessdate = 2009-05-28
|volume        = 26
}}&lt;/ref&gt;
This application is typically used for compressing lookup tables when the total set of stored keys is very sparse within their representation space.

For example it may be used to represent sparse [[bitset]]s (i.e. subsets of a much larger fixed enumerable set) using a trie keyed by the bit element position within the full set, with the key created from the string of bits needed to encode the integral position of each element. The trie will then have a very degenerate form with many missing branches, and compression becomes possible by storing the leaf nodes (set segments with fixed length) and combining them after detecting the repetition of common patterns or by filling the unused gaps.

Such compression is also typically used in the implementation of the various fast lookup tables needed to retrieve [[Unicode]] character properties (for example to represent case mapping tables, or lookup tables containing the combination of base and combining characters needed to support Unicode normalization). For such application, the representation is similar to transforming a very large unidimensional sparse table into a multidimensional matrix, and then using the coordinates in the hyper-matrix as the string key of an uncompressed trie. The compression will then consist of detecting and merging the common columns within the hyper-matrix to compress the last dimension in the key; each dimension of the hypermatrix stores the start position within a storage vector of the next dimension for each coordinate value, and the resulting vector is itself compressible when it is also sparse, so each dimension (associated to a layer level in the trie) is compressed separately.

Some implementations do support such data compression within dynamic sparse tries and allow insertions and deletions in compressed tries, but generally this has a significant cost when compressed segments need to be split or merged, and some tradeoff has to be made between the smallest size of the compressed trie and the speed of updates, by limiting the range of global lookups for comparing the common branches in the sparse trie.

The result of such compression may look similar to trying to transform the trie into a [[directed acyclic graph]] (DAG), because the reverse transform from a DAG to a trie is obvious and always possible, however it is constrained by the form of the key chosen to index the nodes.

Another compression approach is to &quot;unravel&quot; the data structure into a single byte array.&lt;ref&gt;{{Cite web
|url=http://www.aclweb.org/anthology/W/W09/W09-1505.pdf
|format=PDF
|title = Tightly packed tries: how to fit large models into memory, and make them load fast, too
|year = 2009
|author = Ulrich Germann, Eric Joanis, Samuel Larkin
|work = ACL Workshops: Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing
|publisher = Association for Computational Linguistics
|pages = 31–39
|quote = We present Tightly Packed Tries (TPTs), a compact implementation of read-only, compressed trie structures with fast on-demand paging and short load times. We demonstrate the benefits of TPTs for storing n-gram back-off language models and phrase tables for [[statistical machine translation]]. Encoded as TPTs, these databases require less space than flat text file representations of the same data compressed with the gzip utility. At the same time, they can be mapped into memory quickly and be searched directly in time linear in the length of the key, without the need to decompress the entire file. The overhead for local decompression during search is marginal.
}}&lt;/ref&gt;
This approach eliminates the need for node pointers which reduces the memory requirements substantially and makes memory mapping possible which allows the virtual memory manager to load the data into memory very efficiently.

Another compression approach is to &quot;pack&quot; the trie.&lt;ref name=Liang1983&gt;{{cite thesis|degree=Doctor of Philosophy|title=Word Hy-phen-a-tion By Com-put-er|url=http://www.tug.org/docs/liang/liang-thesis.pdf|author=Franklin Mark Liang|year=1983|publisher=Stanford University|accessdate=2010-03-28|archiveurl=http://www.webcitation.org/5pqOfzlIA|archivedate=2010-05-19}}&lt;!-- At time of writing, this template doesn't support archive URLs. I've included one anyway, in case of future link rot (see [[WP:ROT]])--&gt;&lt;/ref&gt; Liang describes a space-efficient implementation of a sparse packed trie applied to [[hyphenation algorithm|hyphenation]], in which the descendants of each node may be interleaved in memory.

== See also ==
{{div col|3}}
* [[Suffix tree]]
* [[Radix tree]]
* [[Directed acyclic word graph]] (aka DAWG)
* [[Ternary search tries]]
* [[Acyclic deterministic finite automata]]
* [[Hash trie]]
* [[Deterministic finite automata]]
* [[Judy array]]
* [[Search algorithm]]
* [[Extendible hashing]]
* [[Hash array mapped trie]]
* [[Prefix Hash Tree]]
* [[Burstsort]]
* [[Luleå algorithm]]
* [[Huffman coding]]
* [[Ctrie]]
* [[HAT-trie]]
{{div col end}}

== Notes ==
{{reflist}}

==References==
* {{Cite journal | first1=R. | last1=de la Briandais |
title=File Searching Using Variable Length Keys |
journal=Proceedings of the Western Joint Computer Conference |
year=1959 | pages=295–298 | url=http://dl.acm.org/citation.cfm?id=1457895 }}

== External links ==
{{Commons category}}
{{wiktionary}}
*[http://www.nist.gov/dads/HTML/trie.html NIST's Dictionary of Algorithms and Data Structures: Trie]
*[http://www.csse.monash.edu.au/~lloyd/tildeAlgDS/Tree/Trie/ Tries] by Lloyd Allison
*[http://www.nedprod.com/programs/portable/nedtries/ Comparison and Analysis]

{{CS-Trees}}
{{Data structures}}

[[Category:Trees (data structures)]]</text>
      <sha1>1k5dot5mvjdegmkr6vk5zhzxcyaktrl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Trinomial tree</title>
    <ns>0</ns>
    <id>20262149</id>
    <revision>
      <id>623848168</id>
      <parentid>580864251</parentid>
      <timestamp>2014-09-02T11:47:38Z</timestamp>
      <contributor>
        <username>Fintor</username>
        <id>69488</id>
      </contributor>
      <minor/>
      <comment>/* See also */</comment>
      <text xml:space="preserve" bytes="6303">The '''Trinomial tree''' is a [[Lattice model (finance)|lattice based]] [[computational model]] used in [[financial mathematics]] to price [[option (finance)|options]]. It was developed by [[Phelim Boyle]] in 1986. It is an extension of the [[Binomial options pricing model]], and is conceptually similar.&lt;ref&gt;[http://www.global-derivatives.com/index.php/options-database-topmenu/13-options-database/15-european-options#Trinomial Trinomial Method (Boyle) 1986]&lt;/ref&gt; It can also be shown that the approach is equivalent to the [[Finite_difference_method#Explicit_method|explicit]] [[finite difference methods for option pricing|finite difference method for option pricing]].&lt;ref&gt;[http://web.archive.org/web/20070622150346/www.in-the-money.com/pages/author.htm Mark Rubinstein]&lt;/ref&gt;

==Formula==
Under the trinomial method, the [[underlying]] stock price is modeled as a recombining tree, where, at each node the price has three possible paths: an up, down and stable or middle path.&lt;ref&gt;[http://www.sitmo.com/article/binomial-and-trinomial-trees/ Trinomial Tree, geometric Brownian motion]&lt;/ref&gt; These values are found by multiplying the value at the current node by the appropriate factor &lt;math&gt; u\,&lt;/math&gt;, &lt;math&gt; d\,&lt;/math&gt; or &lt;math&gt; m\,&lt;/math&gt; where
:&lt;math&gt; u = e^{\sigma\sqrt {2\Delta t}}&lt;/math&gt;
:&lt;math&gt; d = e^{-\sigma\sqrt {2\Delta t}} = \frac{1}{u} \,&lt;/math&gt; (the structure is recombining)
:&lt;math&gt; m = 1 \,&lt;/math&gt;

and the corresponding probabilities are: 
:&lt;math&gt; p_u = \left(\frac{e^{(r - q)  \Delta t / 2}- e^{-\sigma\sqrt {\Delta t/2}}}{e^{\sigma\sqrt {\Delta t/2}}- e^{-\sigma\sqrt {\Delta t/2}}}\right)^2 \,&lt;/math&gt; 
:&lt;math&gt; p_d = \left(\frac{e^{\sigma\sqrt {\Delta t/2}}-e^{(r - q)  \Delta t / 2}}{e^{\sigma\sqrt {\Delta t/2}}- e^{-\sigma\sqrt {\Delta t/2}}}\right)^2 \,&lt;/math&gt; 
:&lt;math&gt; p_m = 1 - (p_u + p_d) \,&lt;/math&gt;.

In the above formulae: &lt;math&gt; \Delta t \,&lt;/math&gt; is the length of time per step in the tree and is simply time to maturity divided by the number of time steps; &lt;math&gt; r\,&lt;/math&gt; is the [[risk-free interest rate]] over this maturity; &lt;math&gt; \sigma\,&lt;/math&gt; is the corresponding [[volatility (finance)|volatility of the underlying]]; &lt;math&gt; q\,&lt;/math&gt; is its corresponding [[dividend yield]].&lt;ref name=&quot;JHull&quot;&gt;[[John C. Hull|John Hull]] presents alternative formulae; see: {{cite book | last = Hull
| first = John C. | edition = 5th | title = Options, Futures and Other Derivatives | year = 2002 | publisher = [[Prentice Hall]] | isbn = 0-13-009056-5 }}.&lt;/ref&gt;

As with the binomial model, these factors and probabilities are specified so as to ensure that the price of the [[underlying]] evolves as a [[Martingale (probability theory)|martingale]], while the [[Moment (mathematics)|moments]] are matched approximately&lt;ref&gt;[http://www2.warwick.ac.uk/fac/sci/maths/people/staff/oleg_zaboronski/fm/trinomial_tree_2008.pdf Pricing Options Using Trinomial Trees]&lt;/ref&gt; (and with increasing accuracy for smaller time-steps). Note that for &lt;math&gt; p_u &lt;/math&gt;, &lt;math&gt; p_d &lt;/math&gt;, and &lt;math&gt; p_m &lt;/math&gt; to be in the interval &lt;math&gt; (0,1) &lt;/math&gt; the following condition on &lt;math&gt; \Delta t &lt;/math&gt; has to be satisfied &lt;math&gt; \Delta t &lt; 2\frac{\sigma^2}{(r-q)^2} &lt;/math&gt;.

Once the tree of prices has been calculated, the option price is found at each node largely [[Binomial_options_pricing_model#Methodology|as for the binomial model]], by working backwards from the final nodes to today. The difference being that the option value at each non-final node is determined based on the three - as opposed to ''two'' - later nodes and their corresponding probabilities. The model is best understood visually - see, for example [http://www.hoadley.net/options/binomialtree.aspx?tree=T Trinomial Tree Option Calculator] (Peter Hoadley).

If the length of time-steps &lt;math&gt; \Delta t &lt;/math&gt; is taken as an exponentially distributed random variable and interpreted as the waiting time between two movements of the stock price then the resulting stochastic process is a [[birth-death process]]. The resulting [[Korn-Kreer-Lenssen Model|model]] is soluble and there exist analytic pricing and hedging formulae for various options.

==Application==
The trinomial model is considered&lt;ref&gt;[http://www.hoadley.net/options/calculators.htm On-Line Options Pricing &amp; Probability Calculators]&lt;/ref&gt; to produce more accurate results than the binomial model when fewer time steps are modelled, and is therefore used when computational speed or resources may be an issue. For [[vanilla option]]s, as the number of steps increases, the results rapidly converge, and the binomial model is then preferred due to its simpler implementation. For [[exotic option]]s the trinomial model (or adaptations) is sometimes more stable and accurate, regardless of step-size.

==See also==
*[[Binomial options pricing model]]
*[[Valuation of options]]
*[[Option_(finance)#Model_implementation|Option: Model implementation]]
*[[Korn-Kreer-Lenssen Model]]
*[[Implied trinomial tree]]

== References ==
&lt;references/&gt;

==External links==
*[[Phelim Boyle]], 1986. &quot;Option Valuation Using a Three-Jump Process&quot;, ''International Options Journal'' 3, 7-12.
*{{cite journal |last=Rubinstein |first= M.|authorlink= Mark Rubinstein|coauthors= |year= 2000 |month= |title= On the Relation Between Binomial and Trinomial Option Pricing Models|journal= [[Journal of Derivatives]]|volume= 8|issue= 2|pages= 47&amp;ndash;50 |url= http://web.archive.org/web/20070622150346/www.in-the-money.com/pages/author.htm |accessdate= |doi=10.3905/jod.2000.319149 }}
*Tero Haahtela, 2010. [http://www.realoptions.org/papers2010/241.pdf &quot;Recombining Trinomial Tree for Real Option Valuation with Changing Volatility&quot;], [[Aalto University]], Working Paper Series.
* Ralf Korn, Markus Kreer and Mark Lenssen, 1998. &quot;Pricing of european options when the underlying stock price follows a linear birth-death process&quot;, Stochastic Models Vol. 14(3), pp 647 – 662
*Tariq Scherer, 2010. [http://www.24-something.com/2011/03/07/how-to-create-trinomial-option-pricing-trees-with-excel-applescripts/ &quot;Create Trinomial Option Pricing Trees Using Excel Applescripts&quot;]

{{Derivatives market}}

[[Category:Mathematical finance]]
[[Category:Options (finance)]]
[[Category:Finance theories]]
[[Category:Models of computation]]
[[Category:Trees (data structures)]]</text>
      <sha1>mlmx64rb204xe7pcjyte1mifxg512no</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>UB-tree</title>
    <ns>0</ns>
    <id>5786138</id>
    <revision>
      <id>560293970</id>
      <parentid>544439316</parentid>
      <timestamp>2013-06-17T13:23:18Z</timestamp>
      <contributor>
        <username>Svick</username>
        <id>4576738</id>
      </contributor>
      <minor/>
      <comment>mv stub tag</comment>
      <text xml:space="preserve" bytes="2392">The '''UB-tree''' as proposed by [[Rudolf Bayer]] and [[Volker Markl]] is a [[balanced tree]] for storing and efficiently retrieving multidimensional data. It is basically a [[B+ tree]] (information only in the leaves) with records stored according to [[Z-order (curve)|Z-order]], also called Morton order. Z-order is simply calculated by bitwise interlacing the keys. 

Insertion, deletion, and point query are done as with ordinary B+ trees. To perform range searches in multidimensional point data, however, an algorithm must be provided for calculating, from a point encountered in the data base, the next Z-value which is in the multidimensional search range. 

The original algorithm to solve this key problem was exponential with the dimensionality and thus not feasible&lt;ref&gt;{{cite paper | first = V. | last = Markl | title = MISTRAL: Processing Relational Queries using a Multidimensional Access Technique | year = 1999 | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.6487}}&lt;/ref&gt; (&quot;GetNextZ-address&quot;). A solution to this &quot;crucial part of the UB-tree range query&quot; linear with the z-address bit length has been described later.&lt;ref&gt;{{cite conference | first1 = Frank | last1 = Ramsak | first2 = Volker | last2 = Markl | first3 = Robert | last3 = Fenk | first4 = Martin | last4 = Zirkel | first5 = Klaus | last5 = Elhardt | first6 = Rudolf | last6 = Bayer | title = Integrating the UB-tree into a Database System Kernel | conference = 26th International Conference on Very Large Data Bases | conferenceurl = http://www.vldb.org/dblp/db/conf/vldb/vldb2000.html | date = September 10–14, 2000 | pages = 263–272 | url = http://www.vldb.org/dblp/db/conf/vldb/RamsakMFZEB00.html}}&lt;/ref&gt; This method has already been described in an older paper&lt;ref&gt;{{cite journal | url = http://www.vision-tools.com/h-tropf/multidimensionalrangequery.pdf | format = [[PDF]] | first1 = H. | last1 = Tropf | first2 = H. | last2 = Herzog | title = Multidimensional Range Search in Dynamically Balanced Trees | journal = Angewandte Informatik (Applied Informatics) | issue = 2/1981 | pages = 71–77 | issn = 0013-5704}}&lt;/ref&gt; where using Z-order with search trees has first been proposed.

==References==
&lt;references/&gt; 

==External links==
* [http://mistral.in.tum.de/ Mistral]

{{CS-Trees}}

{{algorithm-stub}}

[[Category:Trees (data structures)]]
[[Category:Database index techniques]]</text>
      <sha1>22ianrwyk2k2ucrc30monfzuw4t9lxc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Vantage-point tree</title>
    <ns>0</ns>
    <id>2879067</id>
    <revision>
      <id>618374623</id>
      <parentid>612690210</parentid>
      <timestamp>2014-07-25T05:17:52Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>Task 5: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <text xml:space="preserve" bytes="5853">A '''vantage-point tree''', or '''VP tree''' is a [[Binary space partitioning|BSP tree]] that segregates data in a [[metric space]] by choosing a position in the space (the &quot;vantage point&quot;) and dividing the data points into two partitions: those that are nearer to the vantage point than a threshold, and those that are not. By repeatedly applying this procedure to partition the data into smaller and smaller sets, a [[Tree (data structure)|tree data structure]] is created where neighbors in the tree are likely to be neighbors in the space.&lt;ref name=&quot;pny93&quot;&gt;{{cite conference
 | last = Yianilos
 | authorlink = 
 | title = Data structures and algorithms for nearest neighbor search in general metric spaces
 | booktitle = Proceedings of the fourth annual ACM-SIAM Symposium on Discrete algorithms
 | pages = 311–321
 | publisher = Society for Industrial and Applied Mathematics Philadelphia, PA, USA
 | year = 1993
 | location = 
 | url = http://pnylab.com/pny/papers/vptree/vptree/
 | accessdate = 2008-08-22
 | id = pny93
}}&lt;/ref&gt;
Peter Yianilos claimed that 
the VP-tree was discovered independently by him (Peter Yianilos)
and by [[Jeffrey Uhlmann]].&lt;ref name=&quot;pny93&quot; /&gt;
Yet, [[Jeffrey Uhlmann|Uhlmann]] published this method before Yianilos in 1991.&lt;ref&gt;{{cite journal |last=Uhlmann |first=Jeffrey |title=Satisfying General Proximity/Similarity Queries with Metric Trees |journal=Information Processing Letters |volume=40 |number=4 |year=1991}}&lt;/ref&gt;
Uhlmann called the data structure a [[metric tree]], the name VP-tree was
proposed by Yianiolos.

This iterative partitioning process is similar to that of a [[k-d tree|''k''-d tree]], but uses circular (or spherical, hyperspherical, etc.) rather than rectilinear partitions. In 2D Euclidean space, this can be visualized as a series of circles segregating the data.

The VP tree is particularly useful in dividing data in a non-standard metric space into a [[Binary space partitioning|BSP tree]].

==Understanding a VP tree==

The way a VP tree stores data can be represented by a circle.&lt;ref name=&quot;vp&quot; /&gt; First, understand that each [[Node (computer science)|node]] of this [[Tree (data structure)|tree]] contains an input point and a radius. All the left children of a given [[Node (computer science)|node]] are the points inside the circle and all the right children of a given [[Node (computer science)|node]] are outside of the circle. The [[Tree (data structure)|tree]] itself does not need to know any other information about what is being stored. All it needs is the distance function that satisfies the properties of the [[metric space]].&lt;ref name=&quot;vp&quot;&gt;{{cite conference
 | first = Ada Wai-chee
 | last = Fu
 | authorlink = 
 | coauthors = Polly Mei-shuen Chan, Yin-Ling Cheung, Yiu Sang Moon
 | title = Dynamic vp-tree indexing for ''n''-nearest neighbor search given pair-wise distances
 | booktitle = The VLDB Journal — The International Journal on Very Large Data Bases
 | pages = 154–173
 | publisher = Springer-Verlag New York, Inc. Secaucus, NJ, USA
 | year = 2000
 | location = 
 | url = http://dl.acm.org/citation.cfm?id=765232&amp;bnc=1
 | accessdate = 2012-10-02
 | id = vp
}}&lt;/ref&gt;
Just imagine a circle with a radius. The left children are all located inside the circle and the right children are located outside the circle.

==Searching through a VP tree==

Suppose there is a need to find the two nearest targets from a given point (The point will be placed relatively close to distance). Since there are no points yet, it is assumed that the middle point (center) is the closest target. Now a [[Variable (computer science)|variable]] is needed to keep track of the distance X (This will change if another distance is greater). To determine whether we go to left or right child will depend on the given point.&lt;ref name=&quot;vp&quot; /&gt; If the point is closer to the radius than the outer shell, search the left child. Otherwise, search the right child. Once the point (the neighbor) is found, the [[Variable (computer science)|variable]] will be updated because the distance has increased.

From here, all the points within the radius have been considered.  To complete the search, we will now find the closest point outside the radius (right child) and determine the second neighbor. The search method will be the same, but it will be for the right child.&lt;ref name=&quot;vp&quot; /&gt;

==Advantages of a VP tree==

# Instead of inferring multidimensional points for domain before the index being built, we build the index directly based on the distance.&lt;ref name=&quot;vp&quot; /&gt; Doing this, avoids pre-processing steps.
# Updating a VP tree is relatively easy compared to the fast-map approach. For fast maps, after inserting or deleting data, there will come a time when fast-map will have to rescan itself. That takes up too much time and it is unclear to know when the rescanning will start.
# Distance based [[Method (computer programming)|methods]] are flexible.  It is “able to index objects that are represented as feature vectors of a fixed number of dimensions.&quot;&lt;ref name=&quot;vp&quot; /&gt;

==Implementation examples==

# [http://www.logarithmic.net/pfh-files/blog/01164790008/VP_tree.py In Python]
# [http://read.pudn.com/downloads150/sourcecode/math/650936/vptree.c__.htm In C]
# [http://code.google.com/p/google-refine/source/browse/trunk/src/main/java/edu/mit/simile/vicino/vptree/?r=181 In Java]
# [https://code.google.com/p/vptree/source/browse/#git%2Fsrc%2Fvptree In Java (alternative implementation)]
# [http://fpirsch.github.io/vptree.js In JavaScript]

==References==

&lt;references/&gt;

{{CS-Trees}}

== External links ==
* [http://stevehanov.ca/blog/index.php?id=130 Understanding VP Trees]

== Further reading ==
* [http://aidblab.cse.iitm.ac.in/cs625/vptree.pdf Data Structures and Algorithms for Nearest Neighbor Search in General Metric Spaces]

[[Category:Trees (data structures)]]</text>
      <sha1>lmfkkvr2zz04mwjxz2drl80f4g0il5z</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>X-fast trie</title>
    <ns>0</ns>
    <id>31450846</id>
    <revision>
      <id>607164606</id>
      <parentid>602581689</parentid>
      <timestamp>2014-05-05T12:32:31Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (10093)</comment>
      <text xml:space="preserve" bytes="9921">{| class=&quot;infobox&quot; style=&quot;width: 20em&quot;
! colspan=&quot;2&quot; style=&quot;font-size: 125%; text-align: center&quot; | X-fast trie
|-
! [[List of data structures|Type]]
| [[Trie]]
|-
! Invented
| 1982
|-
! Invented by
| [[Dan Willard]]
|-
! colspan=&quot;2&quot; style=&quot;text-align: center; background-color: #CCCCFF; color: #000000;&quot; | Asymptotic complexity&lt;br /&gt;in [[big O notation]]
|-
! Space
| ''O''(''n'' log ''M'')
|-
! Search
| ''O''(log log ''M'')
|-
! Insert
| ''O''(log ''M'') [[Amortized analysis|amortized]]
|-
! Delete
| ''O''(log ''M'') amortized
|}

In [[computer science]], an '''x-fast trie''' is a [[data structure]] for storing [[integer]]s from a bounded domain. It supports exact and predecessor or successor queries in time [[Big-O notation|''O'']](log&amp;nbsp;log&amp;nbsp;''M''), using ''O''(''n''&amp;nbsp;log&amp;nbsp;''M'') space, where ''n'' is the number of stored values and ''M'' is the maximum value in the domain. The structure was proposed by [[Dan Willard]] in 1982,&lt;ref name=&quot;PaperWillard&quot; /&gt; along with the more complicated [[y-fast trie]], as a way to improve the space usage of [[van Emde Boas tree]]s, while retaining the ''O''(log&amp;nbsp;log&amp;nbsp;''M'') query time.

==Structure==

[[File:Xfast trie example.svg|thumb|upright=1.7|alt=A binary tree with 4 levels. The nodes on each level are: 3: (), 2: (0) and (1), 1: (00) and (10), 0: (001), (100) and (101). The unlabeled node is the root. There are directed edges between the folllowing nodes: ()-&gt;(0), ()-&gt;(1), (0)-&gt;(00), (0)-&gt;(001) in blue, (1)-&gt;(10), (1)-&gt;(101) in blue, (00)-&gt;(001) twice, once in blue, (10)-&gt;(100), (10)-&gt;(101), (001)&lt;-&gt;(100), (100)&lt;-&gt;(101). The nodes on each level are contained in a box, labeled with LSS(&lt;level&gt;).|An x-fast trie containing the integers 1 (001&lt;sub&gt;2&lt;/sub&gt;), 4 (100&lt;sub&gt;2&lt;/sub&gt;) and 5 (101&lt;sub&gt;2&lt;/sub&gt;). Blue edges indicate descendant pointers.]]

An x-fast trie is a [[Trie#Bitwise tries|bitwise trie]]: a [[binary tree]] where each subtree stores values whose [[Binary numeral system|binary representations]] start with a common prefix. Each internal node is labeled with the common prefix of the values in its subtree and typically, the left child adds a 0 to the end of the prefix, while the right child adds a 1. The binary representation of an integer between 0 and ''M''&amp;nbsp;&amp;minus;&amp;nbsp;1 uses ⌈log&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;''M''⌉ bits, so the height of the trie is ''O''(log&amp;nbsp;''M'').

All values in the x-fast trie are stored at the leaves. Internal nodes are stored only if they have leaves in their subtree. If an internal node would have no left child, it stores a pointer to the smallest leaf in its right subtree instead, called a ''descendant'' pointer. Likewise, if it would have no right child, it stores a pointer to the largest leaf in its left subtree. Each leaf stores a pointer to its predecessor and successor, thereby forming a [[doubly linked list]]. Finally, there is a [[hash table]] for each level that contains all the nodes on that level. Together, these hash tables form the level-search structure (LSS). To guarantee the worst-case query times, these hash tables should use [[dynamic perfect hashing]] or [[cuckoo hashing]].

The total space usage is ''O''(''n''&amp;nbsp;log&amp;nbsp;''M''), since each element has a root-to-leaf path of length ''O''(log&amp;nbsp;''M'').

==Operations==

Like [[van Emde Boas tree]]s, x-fast tries support the operations of an ''ordered [[associative array]]''. This includes the usual associative array operations, along with two more ''order'' operations, ''Successor'' and ''Predecessor'':
*''Find''(''k''): find the value associated with the given key
*''Successor''(''k''): find the key/value pair with the smallest key larger than or equal to the given key
*''Predecessor''(''k''): find the key/value pair with the largest key less than or equal to the given key
*''Insert''(''k'', ''v''): insert the given key/value pair
*''Delete''(''k''): remove the key/value pair with the given key

===Find===

Finding the value associated with a key ''k'' that is in the data structure can be done in constant time by looking up ''k'' in ''LSS''[0], which is a hash table on all the leaves.&lt;ref name=&quot;PaperBose&quot; /&gt;

===Successor and Predecessor===

To find the successor or predecessor of a key ''k'', we first find ''A''&lt;sub&gt;''k''&lt;/sub&gt;, the lowest ancestor of ''k''. This is the node in the trie that has the longest common prefix with ''k''. To find ''A''&lt;sub&gt;''k''&lt;/sub&gt;, we perform a [[binary search]] on the levels. We start at level ''h''/2, where ''h'' is the height of the trie. On each level, we query the corresponding hash table in the level-search structure with the prefix of ''k'' of the right length. If a node with that prefix does not exist, we know that ''A''&lt;sub&gt;''k''&lt;/sub&gt; must be at a higher level and we restrict our search to those. If a node with that prefix does exist, ''A''&lt;sub&gt;''k''&lt;/sub&gt; can not be at a higher level, so we restrict our search to the current and lower levels.

Once we find the lowest ancestor of ''k'', we know that it has leaves in one of its subtrees (otherwise it wouldn't be in the trie) and ''k'' should be in the other subtree. Therefore the descendant pointer points to the successor or the predecessor of ''k''. Depending on which one we are looking for, we might have to take one step in the linked list to the next or previous leaf.

Since the trie has height ''O''(log&amp;nbsp;''M''), the binary search for the lowest ancestor takes ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time. After that, the successor or predecessor can be found in constant time, so the total query time is ''O''(log&amp;nbsp;log&amp;nbsp;''M'').&lt;ref name=&quot;PaperWillard&quot; /&gt;

===Insert===

To insert a key-value pair (''k'', ''v''), we first find the predecessor and successor of ''k''. Then we create a new leaf for ''k'', insert it in the linked list of leaves between the successor and predecessor, and give it a pointer to ''v''. Next, we walk from the root to the new leaf, creating the necessary nodes on the way down, inserting them into the respective hash tables and updating descendant pointers where necessary.

Since we have to walk down the entire height of the trie, this process takes ''O''(log&amp;nbsp;''M'') time.&lt;ref name=&quot;NotesSchulz&quot; /&gt;

===Delete===

To delete a key ''k'', we find its leaf using the hash table on the leaves. We remove it from the linked list, but remember which were the successor and predecessor. Then we walk from the leaf to the root of the trie, removing all nodes whose subtree only contained ''k'' and updating the descendant pointers where necessary. Descendant pointers that used to point to ''k'' will now point to either the successor or predecessor of ''k'', depending on which subtree is missing.

Like insertion, this takes ''O''(log&amp;nbsp;''M'') time, as we have to walk through every level of the trie.&lt;ref name=&quot;NotesSchulz&quot; /&gt;

==Discussion==

Willard introduced x-fast tries largely as an introduction to [[y-fast trie]]s, which provide the same query time, while using only ''O''(''n'') space and allowing insertions and deletions in ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time.&lt;ref name=&quot;PaperWillard&quot; /&gt;

A compression technique similar to [[patricia trie]]s can be used to significantly reduce the space usage of x-fast tries in practice.&lt;ref name=&quot;PaperKementsietsidis&quot; /&gt;

By using an [[Binary search#one-sided search|exponential search]] before the binary search over the levels and by querying not only the current prefix ''x'', but also its successor ''x''&amp;nbsp;+&amp;nbsp;1, x-fast tries can answer predecessor and successor queries in time ''O''(log&amp;nbsp;log&amp;nbsp;''&amp;Delta;''), where ''&amp;Delta;'' is the difference between the query value and its predecessor or successor.&lt;ref name=&quot;PaperBose&quot; /&gt;

== References ==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist|refs=
&lt;ref name=&quot;PaperWillard&quot;&gt;
{{cite journal
| last      = Willard 
| first     = Dan E.
| title     = Log-logarithmic worst-case range queries are possible in space &amp;Theta;(''N'')
| year      = 1983 
| journal   = Information Processing Letters 
| volume    = 17 
| issue     = 2 
| pages     = 81–84 
| issn      = 0020-0190 
| doi       = 10.1016/0020-0190(83)90075-3
| publisher = Elsevier
}}
&lt;/ref&gt;
&lt;ref name=&quot;PaperKementsietsidis&quot;&gt;
{{Citation 
| last1        = Kementsietsidis 
| first1       = Anastasios 
| last2        = Wang 
| first2       = Min 
| contribution = Provenance Query Evaluation: What’s so special about it? 
| series       = Proceedings of the 18th ACM conference on Information and knowledge management
| year         = 2009 
| pages        = 681–690
}}
&lt;/ref&gt;
&lt;ref name=&quot;PaperBose&quot;&gt;
{{Citation 
| last1        = Bose 
| first1       = Prosenjit 
| last2        = Douïeb 
| first2       = Karim 
| last3        = Dujmović
| first3       = Vida 
| last4        = Howat 
| first4       = John 
| last5        = Morin 
| first5       = Pat 
| contribution = Fast Local Searches and Updates in Bounded Universes
| series       = Proceedings of the 22nd Canadian Conference on Computational Geometry (CCCG2010)
| year         = 2010 
| pages        = 261–264
| url          = http://cccg.ca/proceedings/2010/paper69.pdf
}}
&lt;/ref&gt;
&lt;ref name=&quot;NotesSchulz&quot;&gt;
{{cite web
| url        = http://courses.csail.mit.edu/6.851/spring10/scribe/lec09.pdf
| accessdate = 2011-04-13
| title      = Lecture Notes from Lecture 9 of Advanced Data Structures (Spring '10, 6.851)
| first1     = André
| last1      = Schulz
| first2     = Paul
| last2      = Christiano
| date       = 2010-03-04
}}
&lt;/ref&gt;
}}

==External links==
*[http://opendatastructures.org/versions/edition-0.1c/ods-java/node64.html Open Data Structure - Chapter 13 - Data Structures for Integers]

&lt;!--- Categories ---&gt;
{{CS-Trees}}

[[Category:Trees (data structures)]]
[[Category:Associative arrays]]
[[Category:Articles created via the Article Wizard]]</text>
      <sha1>jhmgkzj562mrowisnpgzt0n4l2j2f0s</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Y-fast trie</title>
    <ns>0</ns>
    <id>31499648</id>
    <revision>
      <id>618534908</id>
      <parentid>618534509</parentid>
      <timestamp>2014-07-26T12:07:48Z</timestamp>
      <contributor>
        <ip>85.64.66.129</ip>
      </contributor>
      <comment>splitting/merging a BST containing O(logM) elements takes O(loglogM) time and not O(logM)</comment>
      <text xml:space="preserve" bytes="10407">{| class=&quot;infobox&quot; style=&quot;width: 20em&quot;
! colspan=&quot;2&quot; style=&quot;font-size: 125%; text-align: center&quot; | Y-fast trie
|-
! [[List of data structures|Type]]
| [[Trie]]
|-
! Invented
| 1982
|-
! Invented by
| [[Dan Willard]]
|-
! colspan=&quot;2&quot; style=&quot;text-align: center; background-color: #CCCCFF; color: #000000;&quot; | Asymptotic complexity&lt;br /&gt;in [[big O notation]]
|-
! Space
| ''O''(''n'')
|-
! Search
| ''O''(log log ''M'')
|-
! Insert
| ''O''(log log ''M'') [[Amortized analysis|amortized]]
|-
! Delete
| ''O''(log log ''M'') amortized
|}

In [[computer science]], a '''y-fast trie''' is a [[data structure]] for storing [[integer]]s from a bounded domain. It supports exact and predecessor or successor queries in time [[Big-O notation|''O'']](log&amp;nbsp;log&amp;nbsp;''M''), using ''O''(''n'') space, where ''n'' is the number of stored values and ''M'' is the maximum value in the domain. The structure was proposed by [[Dan Willard]] in 1982&lt;ref name=&quot;PaperWillard&quot; /&gt; to decrease the ''O''(''n''&amp;nbsp;log&amp;nbsp;''M'') space used by an [[x-fast trie]].

==Structure==

[[File:y-fast trie.svg|thumb|upright=1.4|alt=An example of a y-fast trie.|An example of a y-fast trie. The nodes shown in the x-fast trie are the representatives of the ''O''(''n''&amp;nbsp;/&amp;nbsp;log&amp;nbsp;''M'') balanced binary search trees.]]

A y-fast trie consists of two data structures: the top half is an x-fast trie and the lower half consists of a number of [[balanced binary tree]]s. The keys are divided into groups of ''O''(log&amp;nbsp;''M'') consecutive elements and for each group a balanced binary search tree is created. To facilitate efficient insertion and deletion, each group contains at least (log&amp;nbsp;''M'')/4 and at most 2&amp;nbsp;log&amp;nbsp;''M'' elements.&lt;ref name=&quot;PaperBose&quot; /&gt; For each balanced binary search tree a representative ''r'' is chosen. These representatives are stored in the x-fast trie. A representative ''r'' need not to be an element of the tree associated with it, but it does need be an integer smaller than the successor of ''r'' and the minimum element of the tree associated with that successor and greater than the predecessor of ''r'' and the maximum element of the tree associated with that predecessor. Initially, the representative of a tree will be an integer between the minimum and maximum element in its tree.

Since the x-fast trie stores ''O''(''n''&amp;nbsp;/&amp;nbsp;log&amp;nbsp;''M'') representatives and each representative occurs in ''O''(log&amp;nbsp;''M'') hash tables, this part of the y-fast trie uses ''O''(''n'') space. The balanced binary search trees store ''n'' elements in total which uses ''O''(''n'') space. Hence, in total a y-fast trie uses ''O''(''n'') space.

==Operations==

Like [[van Emde Boas tree]]s and x-fast tries, y-fast tries support the operations of an ''ordered [[associative array]]''. This includes the usual associative array operations, along with two more ''order'' operations, ''Successor'' and ''Predecessor'':
*''Find''(''k''): find the value associated with the given key
*''Successor''(''k''): find the key/value pair with the smallest key larger than or equal to the given key
*''Predecessor''(''k''): find the key/value pair with the largest key less than or equal to the given key
*''Insert''(''k'', ''v''): insert the given key/value pair
*''Delete''(''k''): remove the key/value pair with the given key

===Find===

A key ''k'' can be stored in either the tree of the smallest representative ''r'' greater than ''k'' or in the tree of the predecessor of ''r'' since the representative of a binary search tree need not be an element stored in its tree. Hence, we first find the smallest representative ''r'' greater than ''k'' in the x-fast trie. Using this representative, we retrieve the predecessor of ''r''. These two representatives point to two balanced binary search trees, which we both search for ''k''.

Finding the smallest representative ''r'' greater than ''k'' in the x-fast trie takes ''O''(log&amp;nbsp;log&amp;nbsp;''M''). Using ''r'', finding its predecessor takes constant time. Searching the two balanced binary search trees containing ''O''(log&amp;nbsp;''M'') elements each takes ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time. Hence, a key ''k'' can be found, and its value retrieved, in ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time.&lt;ref name=&quot;PaperWillard&quot; /&gt;

===Successor and Predecessor===

Similarly to the key ''k'' itself, its successor can be stored in either the tree of the smallest representative ''r'' greater than ''k'' or in the tree of the predecessor of ''r''. Hence, to find the successor of a key ''k'', we first search the x-fast trie for the smallest representative greater than ''k''. Next, we use this representative to retrieve its predecessor in the x-fast trie. These two representatives point to two balanced binary search trees, which we search for the successor of ''k''.&lt;ref name=&quot;NotesSchulz&quot; /&gt;

Finding the smallest representative ''r'' greater than ''k'' in the x-fast trie takes ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time and using ''r'' to find its predecessor takes constant time. Searching the two balanced binary search trees containing ''O''(log&amp;nbsp;''M'') elements each takes ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time. Hence, the successor of a key ''k'' can be found, and its value retrieved, in ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time.&lt;ref name=&quot;PaperWillard&quot; /&gt;

Searching for the predecessor of a key ''k'' is highly similar to finding its successor. We search the x-fast trie for the largest representative ''r'' smaller than ''k'' and we use ''r'' to retrieve its predecessor in the x-fast trie. Finally, we search the two balanced binary search trees of these two representatives for the predecessor of ''k''. This takes ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time.

===Insert===

To insert a new key/value pair (''k'', ''v''), we first need to determine in which balanced binary search tree we need to insert ''k''. To this end, we find the tree ''T'' containing the successor of ''k''. Next, we insert ''k'' into ''T''. To ensure that all balanced binary search trees contain ''O''(log&amp;nbsp;''M'') elements, we split ''T'' into two balanced binary trees and remove its representative from the x-fast trie if it contains more than 2&amp;nbsp;log&amp;nbsp;''M'' elements. Each of the two new balanced binary search trees contains at most log&amp;nbsp;''M''&amp;nbsp;+&amp;nbsp;1 elements. We pick a representative for each tree and insert these into the x-fast trie.

Finding the successor of ''k'' takes ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time. Inserting ''k'' into a balanced binary search tree that contains ''O''(log&amp;nbsp;''M'') elements also takes ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time. Splitting a binary search tree that contains ''O''(log&amp;nbsp;''M'') elements can be done in ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time. Finally, inserting and deleting the three representatives takes ''O''(log&amp;nbsp;''M'') time. However, since we split the tree at most once every ''O''(log&amp;nbsp;''M'') insertions and deletions, this takes constant amortized time. Therefore, inserting a new key/value pair takes ''O''(log&amp;nbsp;log&amp;nbsp;''M'') amortized time.&lt;ref name=&quot;NotesSchulz&quot; /&gt;

===Delete===

Deletions are very similar to insertions. We first find the key ''k'' in one of the balanced binary search trees and delete it from this tree ''T''. To ensure that all balanced binary search trees contain ''O''(log&amp;nbsp;''M'') elements, we merge ''T'' with the balanced binary search tree of its successor or predecessor if it contains less than (log&amp;nbsp;''M'')/4 elements. The representatives of the merged trees are removed from the x-fast trie. It is possible for the merged tree to contain more than 2&amp;nbsp;log&amp;nbsp;''M'' elements. If this is the case, the newly formed tree is split into two trees of about equal size. Next, we pick a new representative for each of the new trees and we insert these into the x-fast trie.

Finding the key ''k'' takes ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time. Deleting ''k'' from a balanced binary search tree that contains ''O''(log&amp;nbsp;''M'') elements also takes ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time. Merging and possibly splitting the balanced binary search trees takes ''O''(log&amp;nbsp;log&amp;nbsp;''M'') time. Finally, deleting the old representatives and inserting the new representatives into the x-fast trie takes ''O''(log&amp;nbsp;''M'') time. Merging and possibly splitting the balanced binary search tree, however, is done at most once for every ''O''(log&amp;nbsp;''M'') insertions and deletions. Hence, it takes constant amortized time. Therefore, deleting a key/value pair takes ''O''(log&amp;nbsp;log&amp;nbsp;''M'') amortized time.&lt;ref name=&quot;NotesSchulz&quot; /&gt;

== References ==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist|refs=
&lt;ref name=&quot;PaperWillard&quot;&gt;
{{cite journal
| last      = Willard 
| first     = Dan E.
| title     = Log-logarithmic worst-case range queries are possible in space &amp;Theta;(''N'')
| year      = 1983 
| journal   = Information Processing Letters 
| volume    = 17 
| issue     = 2 
| pages     = 81–84 
| issn      = 0020-0190 
| doi       = 10.1016/0020-0190(83)90075-3
| publisher = Elsevier
}}
&lt;/ref&gt;
&lt;ref name=&quot;PaperBose&quot;&gt;
{{Citation 
| last1        = Bose 
| first1       = Prosenjit 
| last2        = Douïeb 
| first2       = Karim 
| last3        = Dujmović
| first3       = Vida 
| last4        = Howat 
| first4       = John 
| last5        = Morin 
| first5       = Pat 
| contribution = Fast Local Searches and Updates in Bounded Universes
| series       = Proceedings of the 22nd Canadian Conference on Computational Geometry (CCCG2010)
| year         = 2010 
| pages        = 261–264
| url          = http://cccg.ca/proceedings/2010/paper69.pdf
}}
&lt;/ref&gt;
&lt;ref name=&quot;NotesSchulz&quot;&gt;
{{cite web
| url        = http://courses.csail.mit.edu/6.851/spring10/scribe/lec09.pdf
| accessdate = 2011-04-14
| title      = Lecture Notes from Lecture 9 of Advanced Data Structures (Spring '10, 6.851)
| first1     = André
| last1      = Schulz
| first2     = Paul
| last2      = Christiano
| date       = 2010-03-04
}}
&lt;/ref&gt;
}}

==External links==
*[http://opendatastructures.org/versions/edition-0.1c/ods-java/node64.html Open Data Structure - Chapter 13 - Data Structures for Integers]

&lt;!--- Categories ---&gt;
{{CS-Trees}}

[[Category:Trees (data structures)]]
[[Category:Articles created via the Article Wizard]]</text>
      <sha1>p8duxkt1rhx1x5v4gnilvnui6u4rhyi</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Rose tree</title>
    <ns>0</ns>
    <id>8163824</id>
    <revision>
      <id>570409042</id>
      <parentid>566229597</parentid>
      <timestamp>2013-08-27T15:41:47Z</timestamp>
      <contributor>
        <username>Ruud Koot</username>
        <id>170083</id>
      </contributor>
      <minor/>
      <comment>Ruud Koot moved page [[Rose Tree]] to [[Rose tree]]</comment>
      <text xml:space="preserve" bytes="1211">{{for|the species of [[Rhododendron]]|Rhododendron maximum}}

A '''Rose Tree''', also called a '''Multi-way Tree''',&lt;ref&gt;[http://stackoverflow.com/questions/734110/persistent-data-structures-in-java#answer-735330 Answer on Stack Overflow], accessed 26 January 2012&lt;/ref&gt; is a [[Tree (data structure)|Tree]] data structure.&lt;ref&gt;[http://www.haskell.org/haskellwiki/Algebraic_data_type#Rose_tree Haskell Wiki], accessed 26 January 2012&lt;/ref&gt;  It represents a tree in which each node can have an arbitrary number of sub-trees (e.g., an [[XML]] tree).

==Definition==

The following is a definition in [[Haskell (programming language)|Haskell]]:

&lt;syntaxhighlight lang=&quot;haskell&quot;&gt;
data RoseTree a = RoseTree a [RoseTree a]
&lt;/syntaxhighlight&gt;

The following is a definition in [[Scala (programming language)|Scala]]:

&lt;syntaxhighlight lang=&quot;scala&quot;&gt;
case class RoseTree[A](root: A, children: List[RoseTree[A]])
&lt;/syntaxhighlight&gt;

==Sources==
&lt;references/&gt;

==External links==
* [http://www.haskell.org/haskellwiki/Algebraic_data_type#Rose_tree Rose tree] on the Haskell wiki
* [http://www.gatsby.ucl.ac.uk/~heller/brt.pdf Bayesian Rose Trees]

{{CS-Trees}}






{{Comp-sci-stub}}

[[Category:Trees (data structures)]]</text>
      <sha1>i37m8el2jwlwgitymr73b6sa47mc9ur</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Left-leaning red–black tree</title>
    <ns>0</ns>
    <id>35239409</id>
    <revision>
      <id>618871809</id>
      <parentid>613968411</parentid>
      <timestamp>2014-07-28T21:02:14Z</timestamp>
      <contributor>
        <ip>204.128.192.33</ip>
      </contributor>
      <comment>JAVA code is bad</comment>
      <text xml:space="preserve" bytes="4357">{{Infobox data structure
|name=Left-leaning red–black tree
|type=tree
|invented_by=[[Robert Sedgewick (computer scientist)|Robert Sedgewick]]
|invented_year=2008
|space_avg=O(n)
|space_worst=O(n)
|search_avg=O(log n)
|search_worst=O(log n)
|insert_avg=O(log n)
|insert_worst=O(log n)
|delete_avg=O(log n)
|delete_worst=O(log n)
}}

A '''left-leaning red–black''' ('''LLRB''') tree is a type of [[self-balancing binary search tree]]. It is a variant of the [[red–black tree]] and guarantees the same asymptotic complexity for operations, but is designed to be easier to implement.

==External links==

===Papers===

* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.282 Robert Sedgewick. Left-leaning Red–Black Trees]. [http://www.cs.princeton.edu/~rs/talks/LLRB/LLRB.pdf Direct link to PDF].
* Robert Sedgewick. Left-Leaning Red–Black Trees (slides).  Two versions:
** [http://www.cs.princeton.edu/~rs/talks/LLRB/08Dagstuhl/RedBlack.pdf Robert Sedgewick. Left-Leaning Red–Black Trees (slides), from seminar at Dagstuhl in February 2008. Outdated.]
** [http://www.cs.princeton.edu/~rs/talks/LLRB/RedBlack.pdf Robert Sedgewick. Left-Leaning Red–Black Trees (slides), from April 2008; updated]
* [http://web.student.chalmers.se/groups/datx02-dtp/ Linus Ek, Ola Holmström and Stevan Andjelkovic. May 19, 2009. Formalizing Arne Andersson trees and Left-leaning Red–Black trees in Agda]
* [http://www.reinference.net/llrb-delete-julien-oster.pdf Julien Oster. March 22, 2011. An Agda implementation of deletion in Left-leaning Red–Black trees]
* [http://www.mew.org/~kazu/proj/red-black-tree/ Kazu Yamamoto. 2011.10.19. Purely Functional Left-Leaning Red–Black Trees]

===Implementations===

{| class=&quot;wikitable sortable&quot;

|-
! Author
! Date
! Language
! Variant
! Notes
! Link

|-
| Robert Sedgewick, rkapsi
| 2008
| [[Java (programming language)|Java]]
| 
| From [http://www.cs.princeton.edu/~rs/talks/LLRB/LLRB.pdf this Sedgewick paper]
| [https://gist.github.com/741080 Left-leaning Red–Black Tree (LLRB)] -- this code has errors, see github comments

|-
| David Anson
| 2 Jun 2009
| [[C Sharp (programming language)|C#]]
| 
| 
| [http://blogs.msdn.com/b/delay/archive/2009/06/02/maintaining-balance-a-versatile-red-black-tree-implementation-for-net-via-silverlight-wpf-charting.aspx Maintaining balance: A versatile red-black tree implementation for .NET]

|-
| kazu-yamamoto
| 2011
| [[Haskell (programming language)|Haskell]]
| 
| 
| [https://github.com/kazu-yamamoto/llrbtree llrbtree]

|-
| 
| 
| [[Haskell (programming language)|Haskell]]
| 
| 
| [http://hackage.haskell.org/packages/archive/llrbtree/latest/doc/html/Data-Set-LLRBTree.html Data.Set.LLRBTree]

|-
| gradbot
| 2010
| [[F Sharp (programming language)|F#]]
| 
| 
| [https://github.com/gradbot/f-sharp-llrbt f-sharp-llrbt]

|-
| Lee Stanza
| 2010
| [[C (programming language)|C]]
|
| Includes discussion
| [http://www.teachsolaisgames.com/articles/balanced_left_leaning.html Balanced Trees, Part 4: Left Leaning Red–Black Trees]

|-
| Jason Evans
| 2010
| [[C (programming language)|C]]
| 2-3
| 
| [http://www.canonware.com/rb/ rb.h]

|-
| Nicola Bortignon
| December 8, 2010
| [[ActionScript_3#ActionScript_3.0|ActionScript 3]]
| 
| 
| [http://www.nicolabortignon.com/an-as3-left-leaning-red-black-tree/ AS3 implementation and discussion]

|-
| william at 25thandClement.com
| 2011
| [[C (programming language)|C]]
| 2-3 variant using iteration with parent pointers
|
| [http://25thandclement.com/~william/projects/llrb.h.html llrb.h: Left-leaning Red–Black Tree]

|-
| Maciej Piechotka
| 2009
| [[Vala (programming language)|Vala]]
| 
| 
| [http://www.valadoc.org/#!api=gee-1.0/Gee.TreeMap Gee.TreeMap]

|-
| Petar Maymounkov
| 2010
| [[Go (programming language)|Go]]
| 
| 
| [https://github.com/petar/GoLLRB GoLLRB]
|}

===Other===

* [http://www.cs.princeton.edu/~rs/talks/LLRB/movies/ Robert Segdewick. 20 Apr 2008. Animations of LLRB operations]
* [http://opendatastructures.org/versions/edition-0.1e/ods-java/9_2_RedBlackTree_Simulated_.html#SECTION001222000000000000000 Open Data Structures - Section 9.2.2 - Left-Leaning Red–Black Trees]
* [http://read.seas.harvard.edu/~kohler/notes/llrb.html Left-Leaning Red-Black Trees Considered Harmful]

{{algorithm-stub}}

{{CS-Trees}}

{{DEFAULTSORT:Llrb Tree}}
[[Category:Trees (data structures)]]</text>
      <sha1>h09cc46chxtq8gde0p8yt8em9tg1ljy</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Binary space partitioning</title>
    <ns>0</ns>
    <id>73613</id>
    <revision>
      <id>606616883</id>
      <parentid>601151605</parentid>
      <timestamp>2014-05-01T12:25:27Z</timestamp>
      <contributor>
        <username>M-le-mot-dit</username>
        <id>2665896</id>
      </contributor>
      <minor/>
      <comment>[[:en:WP:CLEANER|WPCleaner]] v1.32 - Repaired 1 link to disambiguation page - [[WP:DPL|(You can help)]] - [[Ray tracing]]</comment>
      <text xml:space="preserve" bytes="22322">{{For|the .BSP file extension|BSP (file format)}}

In [[computer science]], '''binary space partitioning''' ('''BSP''') is a method for recursively subdividing a [[Euclidean space|space]] into [[convex set]]s by [[hyperplane]]s. This subdivision gives rise to a representation of objects within the space by means of a [[Tree (data structure)|tree data structure]] known as a '''BSP tree'''.

Binary space partitioning was developed in the context of [[3D computer graphics]],&lt;ref name=&quot;schumacker69&quot;/&gt;&lt;ref name=&quot;fuchs80&quot;/&gt; where the structure of a BSP tree allows spatial information about the objects in a scene that is useful in  [[rendering (computer graphics)|rendering]], such as their ordering from front-to-back with respect to a viewer at a given location, to be accessed rapidly. Other applications include performing geometrical operations with shapes ([[constructive solid geometry]]) in [[CAD]],&lt;ref name=&quot;thibault87&quot;&gt;
{{cite conference 
 | last      = Thibault | first1 = William C.
 | last2     = Naylor     | first2 = Bruce F.
 |title=Set operations on polyhedra using binary space partitioning trees
|booktitle=SIGGRAPH '87 Proceedings of the 14th annual conference on Computer graphics and interactive techniques
| year=1987
|pages=153–162 
| publisher=ACM, New York
| doi=10.1145/37402.37421}}&lt;/ref&gt; [[collision detection]] in [[robotics]] and 3-D [[video game]]s, [[ray tracing (graphics)|ray tracing]] and other computer applications that involve handling of complex spatial scenes.

==Overview==
Binary space partitioning is a generic process of [[recursion|recursively]] dividing a scene into two until the partitioning satisfies one or more requirements. It can be seen as a generalisation of other spatial tree structures such as [[K-d tree|''k''-d trees]] and [[quadtree]]s, one where hyperplanes that partition the space may have any orientation, rather than being aligned with the coordinate axes as they are in ''k''-d trees or quadtrees. When used in computer graphics to render scenes composed of planar [[Polygon mesh|polygons]], the partitioning planes are frequently (but not always) chosen to coincide with the planes defined by polygons in the scene.

The specific choice of partitioning plane and criterion for terminating the partitioning process varies depending on the purpose of the BSP tree. For example, in computer graphics rendering, the scene is divided until each node of the BSP tree contains only polygons that can render in arbitrary order. When [[back-face culling]] is used, each node therefore contains a convex set of polygons, whereas when rendering double-sided polygons, each node of the BSP tree contains only polygons in a single plane. In collision detection or ray tracing, a scene may be divided up into [[Geometric primitive|primitives]] on which collision or ray intersection tests are straightforward.

Binary space partitioning arose from the computer graphics need to rapidly draw three-dimensional scenes composed of polygons. A simple way to draw such scenes is the [[painter's algorithm]], which produces polygons in order of distance from the viewer, back to front, painting over the background and previous polygons with each closer object. This approach has two disadvantages: time required to sort polygons in back to front order, and the possibility of errors in overlapping polygons. Fuchs and co-authors&lt;ref name=&quot;fuchs80&quot;/&gt; showed that constructing a BSP tree solved both of these problems by providing a rapid method of sorting polygons with respect to a given viewpoint (linear in the number of polygons in the scene) and by subdividing overlapping polygons to avoid errors that can occur with the painter's algorithm. A disadvantage of binary space partitioning is that generating a BSP tree can be time-consuming. Typically, it is therefore performed once on static geometry, as a pre-calculation step, prior to rendering or other realtime operations on a scene. The expense of constructing a BSP tree makes it difficult and inefficient to directly implement moving objects into a tree. 

BSP trees are often used by 3D [[video game]]s, particularly [[first-person shooter]]s and those with indoor environments. [[Game engine]]s utilising BSP trees include the [[Doom engine]] (probably the earliest game to use a BSP data structure was ''[[Doom (video game)|Doom]]''), the [[Quake engine]] and its descendants. In video games, BSP trees containing the static geometry of a scene are often used together with a [[Z-buffer]], to correctly merge movable objects such as doors and characters onto the background scene. While binary space partitioning provides a convenient way to store and retrieve spatial information about polygons in a scene, it does not solve the problem of [[Hidden surface determination|visible surface determination]].

==Generation==
The canonical use of a BSP tree is for rendering polygons (that are double-sided, that is, without [[back-face culling]]) with the painter's algorithm. Each polygon is designated with a front side and a back side which could be chosen arbitrarily and only affects the structure of the tree but not the required result. &lt;ref name=&quot;fuchs80&quot;/&gt; Such a tree is constructed from an unsorted list of all the polygons in a scene. The recursive algorithm for construction of a BSP tree from that list of polygons is:&lt;ref name=&quot;fuchs80&quot;&gt;
{{cite conference 
 | last      = Fuchs | first1 = Henry
 | last2     = Kedem     | first2 = Zvi. M
 | last3     = Naylor     | first3 = Bruce F.
 |title=On Visible Surface Generation by A Priori Tree Structures
|booktitle=SIGGRAPH '80 Proceedings of the 7th annual conference on Computer graphics and interactive techniques
| year=1980 
|pages=124–133
| publisher=ACM, New York
| doi=10.1145/965105.807481}}&lt;/ref&gt;
# Choose a polygon ''P'' from the list.
# Make a node ''N'' in the BSP tree, and add ''P'' to the list of polygons at that node.
# For each other polygon in the list:
## If that polygon is wholly in front of the plane containing ''P'', move that polygon to the list of nodes in front of ''P''.
## If that polygon is wholly behind the plane containing ''P'', move that polygon to the list of nodes behind ''P''.
## If that polygon is intersected by the plane containing ''P'', split it into two polygons and move them to the respective lists of polygons behind and in front of ''P''.
## If that polygon lies in the plane containing ''P'', add it to the list of polygons at node ''N''.
# Apply this algorithm to the list of polygons in front of ''P''.
# Apply this algorithm to the list of polygons behind ''P''.

The following diagram illustrates the use of this algorithm in converting a list of lines or polygons into a BSP tree. At each of the eight steps (i.-viii.), the algorithm above is applied to a list of lines, and one new node is added to the tree.

{| class=&quot;wikitable&quot;
|- valign=&quot;top&quot;
| 
| width=&quot;450pt&quot; | Start with a list of lines, (or in 3-D, polygons) making up the scene. In the tree diagrams, lists are denoted by rounded rectangles and nodes in the BSP tree by circles. In the spatial diagram of the lines, direction chosen to be the 'front' of a line is denoted by an arrow.
| [[File:Example of BSP tree construction - step 1.svg]]
|- valign=&quot;top&quot;
| '''i.'''
|Following the steps of the algorithm above, 
# We choose a line, A, from the list and,...
# ...add it to a node.
# We split the remaining lines in the list into those in front of A (i.e. B2, C2, D2), and those behind (B1, C1, D1).
# We first process the lines in front of A (in steps ii–v),...
# ...followed by those behind (in steps vi–vii).
| [[File:Example of BSP tree construction - step 2.svg]]
|- valign=&quot;top&quot;
| '''ii.''' || We now apply the algorithm to the list of lines in front of A (containing B2, C2, D2). We choose a line, B2, add it to a node and split the rest of the list into those lines that are in front of B2 (D2), and those that are behind it (C2, D3). || [[File:Example of BSP tree construction - step 3.svg]]
|- valign=&quot;top&quot;
| '''iii.''' 
| Choose a line, D2, from the list of lines in front of B2. It is the only line in the list, so after adding it to a node, nothing further needs to be done. 
| [[File:Example of BSP tree construction - step 4.svg]]
|- valign=&quot;top&quot;
| '''iv.''' 
| We are done with the lines in front of B2, so consider the lines behind B2 (C2 and D3). Choose one of these (C2), add it to a node, and put the other line in the list (D3) into the list of lines in front of C2.  
| [[File:Example of BSP tree construction - step 5.svg]]
|- valign=&quot;top&quot;
| '''v.''' 
| Now look at the list of lines in front of C2. There is only one line (D3), so add this to a node and continue. 
| [[File:Example of BSP tree construction - step 6.svg]]
|- valign=&quot;top&quot;
| '''vi.''' || We have now added all of the lines in front of A to the BSP tree, so we now start on the list of lines behind A. Choosing a line (B1) from this list, we add B1 to a node and split the remainder of the list into lines in front of B1 (i.e. D1), and lines behind B1 (i.e. C1).  || [[File:Example of BSP tree construction - step 7.svg]]
|- valign=&quot;top&quot;
| '''vii.''' || Processing first the list of lines in front of B1, D1 is the only line in this list, so add this to a node and continue.  || [[File:Example of BSP tree construction - step 8.svg]]
|- valign=&quot;top&quot;
| '''viii.''' || Looking next at the list of lines behind B1, the only line in this list is C1, so add this to a node, and the BSP tree is complete. || [[File:Example of BSP tree construction - step 9.svg]]
|}

The final number of polygons or lines in a tree is often larger (sometimes much larger&lt;ref name=&quot;fuchs80&quot;/&gt;) than the original list, since lines or polygons that cross the partitioning plane must be split into two. It is desirable to minimize this increase, but also to maintain reasonable [[Binary_tree#Types_of_binary_trees|balance]] in the final tree. The choice of which polygon or line is used as a partitioning plane (in step 1 of the algorithm) is therefore important in creating an efficient BSP tree.

==Traversal==
A BSP tree is [[Tree traversal|traversed]] in a linear time, in an order determined by the particular function of the tree. Again using the example of rendering double-sided polygons using the painter's algorithm, to draw a polygon ''P'' correctly requires that all polygons behind the plane ''P'' lies in must be drawn first, then polygon ''P'', then finally the polygons in front of ''P''. If this drawing order is satisfied for all polygons in a scene, then the entire scene renders in the correct order. This procedure can be implemented by recursively traversing a BSP tree using the following algorithm.&lt;ref name=&quot;fuchs80&quot;/&gt; From a given viewing location ''V'', to render a BSP tree,
# If the current node is a leaf node, render the polygons at the current node.
# Otherwise, if the viewing location ''V'' is in front of the current node:
## Render the child BSP tree containing polygons behind the current node
## Render the polygons at the current node
## Render the child BSP tree containing polygons in front of the current node
# Otherwise, if the viewing location ''V'' is behind the current node:
## Render the child BSP tree containing polygons in front of the current node
## Render the polygons at the current node
## Render the child BSP tree containing polygons behind the current node
# Otherwise, the viewing location ''V'' must be exactly on the plane associated with the current node. Then:
## Render the child BSP tree containing polygons in front of the current node
## Render the child BSP tree containing polygons behind the current node

 [[File:Example of BSP tree traversal.svg]]
Applying this algorithm recursively to the BSP tree generated above results in the following steps:
* The algorithm is first applied to the root node of the tree, node ''A''. ''V'' is in front of node ''A'', so we apply the algorithm first to the child BSP tree containing polygons behind ''A''
** This tree has root node ''B1''. ''V'' is behind ''B1'' so first we apply the algorithm to the child BSP tree containing polygons in front of ''B1'':
*** This tree is just the leaf node ''D1'', so the polygon ''D1'' is rendered.
** We then render the polygon ''B1''.
** We then apply the algorithm to the child BSP tree containing polygons behind ''B1'':
*** This tree is just the leaf node ''C1'', so the polygon ''C1'' is rendered.
* We then draw the polygons of ''A''
* We then apply the algorithm to the child BSP tree containing polygons in front of ''A''
** This tree has root node ''B2''. ''V'' is behind ''B2'' so first we apply the algorithm to the child BSP tree containing polygons in front of ''B2'':
*** This tree is just the leaf node ''D2'', so the polygon ''D2'' is rendered.
** We then render the polygon ''B2''.
** We then apply the algorithm to the child BSP tree containing polygons behind ''B2'':
***  This tree has root node ''C2''. ''V'' is in front of ''C2'' so first we would apply the algorithm to the child BSP tree containing polygons behind ''C2''. There is no such tree, however, so we continue.
*** We render the polygon ''C2''.
*** We apply the algorithm to the child BSP tree containing polygons in front of ''C2''
**** This tree is just the leaf node ''D3'', so the polygon ''D3'' is rendered.

The tree is traversed in linear time and renders the polygons in a far-to-near ordering (''D1'', ''B1'', ''C1'', ''A'', ''D2'', ''B2'', ''C2'', ''D3'') suitable for the painter's algorithm.

==Brushes==
&quot;Brushes&quot; are templates, used in some [[3D computer graphics|3D]] video games such as games based on the [[Source (game engine)|Source game engine]], its predecessor the [[Goldsrc]] engine, [[Unreal Engine]]'s tool [[UnrealEd|Unreal Editor]], etc. to construct [[Level (video gaming)|levels]].&lt;ref&gt;{{cite web |accessdate=2011-03-24|url=http://developer.valvesoftware.com/wiki/Brush |title=Definition of Brush in the Valve Developer Community|publisher=Valve}}&lt;/ref&gt; Brushes can be primitive shapes (such as cubes, spheres &amp; cones), pre-defined shapes (such as staircases), or custom shapes (such as prisms and other [[polyhedra]]). Using [[Constructive solid geometry|CSG operations]], complex rooms and objects can be created by adding, subtracting and  intersecting brushes to and from one another.&lt;ref&gt;{{cite web |accessdate=2012-04-21|url=http://udn.epicgames.com/Two/BspBrushesTutorial.html |title=UDN – Two – BspBrushesTutorial|publisher=Epic Games, Inc.}}&lt;/ref&gt;

==Timeline==

*1969 Schumacker et al.&lt;ref name=&quot;schumacker69&quot;&gt;{{Cite report
 | author     = Schumacker, Robert A. ; 
 | authorlink =
 | coauthors  = Brand, Brigitta;  Gilliland, Maurice G.; Sharp, Werner H
 | date       = 1969
 | title      = Study for Applying Computer-Generated Images to Visual Simulation
 | publisher  = U.S. Air Force Human Resources Laboratory
 | pages      = 142
 | id = AFHRL-TR-69-14
}}&lt;/ref&gt; published a report that described how carefully positioned planes in a virtual environment could be used to accelerate polygon ordering. The technique made use of depth coherence, which states that a polygon on the far side of the plane cannot, in any way, obstruct a closer polygon. This was used in flight simulators made by GE as well as Evans and Sutherland. However, creation of the polygonal data organization was performed manually by scene designer.

*1980 [[Henry Fuchs|Fuchs]] et al.&lt;ref name=&quot;fuchs80&quot;/&gt; extended Schumacker’s idea to the representation of 3D objects in a virtual environment by using planes that lie coincident with polygons to recursively partition the 3D space. This provided a fully automated and algorithmic generation of a hierarchical polygonal data structure known as a Binary Space Partitioning Tree (BSP Tree). The process took place as an off-line preprocessing step that was performed once per environment/object. At run-time, the view-dependent visibility ordering was generated by traversing the tree.

*1981 Naylor's Ph.D thesis containing a full development of both BSP trees and a graph-theoretic approach using strongly connected components for pre-computing visibility, as well as the connection between the two methods. BSP trees as a dimension independent spatial search structure was emphasized, with applications to visible surface determination. The thesis also included the first empirical data demonstrating that the size of the tree and the number of new polygons was reasonable (using a model of the Space Shuttle).

*1983 [[Henry Fuchs|Fuchs]] et al. describe a micro-code implementation of the BSP tree algorithm on an Ikonas frame buffer system. This was the first demonstration of real-time visible surface determination using BSP trees.

*1987 Thibault and Naylor&lt;ref name=&quot;thibault87&quot;/&gt; described how arbitrary polyhedra may be represented using a BSP tree as opposed to the traditional b-rep (boundary representation). This provided a solid representation vs. a surface based-representation. Set operations on polyhedra were described using a tool, enabling [[Constructive Solid Geometry]] (CSG) in real-time. This was the fore runner of BSP level design using brushes, introduced in the Quake editor and picked up in the Unreal Editor.

*1990 Naylor, Amanatides, and Thibault provide an algorithm for merging two BSP trees to form a new BSP tree from the two original trees. This provides many benefits including: combining moving objects represented by BSP trees with a static environment (also represented by a BSP tree), very efficient CSG operations on polyhedra, exact collisions detection in O(log n * log n), and proper ordering of transparent surfaces contained in two interpenetrating objects (has been used for an x-ray vision effect).

*1990 [[Seth J. Teller|Teller]] and Séquin proposed the offline generation of potentially visible sets to accelerate visible surface determination in orthogonal 2D environments.

*1991 Gordon and Chen [CHEN91] described an efficient method of performing front-to-back rendering from a BSP tree, rather than the traditional back-to-front approach. They utilised a special data structure to record, efficiently, parts of the screen that have been drawn, and those yet to be rendered. This algorithm, together with the description of BSP Trees in the standard computer graphics textbook of the day ([[Computer Graphics: Principles and Practice]]) was used by [[John D. Carmack|John Carmack]] in the making of ''[[Doom (video game)|Doom]]''.

*1992 [[Seth J. Teller|Teller]]’s PhD thesis described the efficient generation of potentially visible sets as a pre-processing step to acceleration real-time visible surface determination in arbitrary 3D polygonal environments. This was used in ''[[Quake (video game)|Quake]]'' and contributed significantly to that game's performance.

*1993 Naylor answers the question of what characterizes a good BSP tree. He used expected case models (rather than worst case analysis) to mathematically measure the expected cost of searching a tree and used this measure to build good BSP trees. Intuitively, the tree represents an object in a multi-resolution fashion (more exactly, as a tree of approximations). Parallels with Huffman codes and probabilistic binary search trees are drawn.

*1993 Hayder Radha's PhD thesis described (natural) image representation methods using BSP trees. This includes the development of an optimal BSP-tree construction framework for any arbitrary input image. This framework is based on a new image transform, known as the Least-Square-Error (LSE) Partitioning Line (LPE) transform. H. Radha' thesis also developed an optimal rate-distortion (RD) image compression framework and image manipulation approaches using BSP trees.

==References==

{{reflist}}

==Additional references==
* [NAYLOR90] B. Naylor, J. Amanatides, and W. Thibualt, &quot;Merging BSP Trees Yields Polyhedral Set Operations&quot;, Computer Graphics (Siggraph '90), 24(3), 1990.
* [NAYLOR93] B. Naylor, &quot;Constructing Good Partitioning Trees&quot;, Graphics Interface (annual Canadian CG conference) May, 1993.
* [CHEN91] S. Chen and D. Gordon. [http://cs.haifa.ac.il/~gordon/ftb-bsp.pdf “Front-to-Back Display of BSP Trees.”] IEEE Computer Graphics &amp; Algorithms, pp 79–85. September 1991.
* [RADHA91] H. Radha, R. Leoonardi, M. Vetterli, and B. Naylor “Binary Space Partitioning Tree Representation of Images,” Journal of Visual Communications and Image Processing 1991, vol. 2(3).
* [RADHA93] H. Radha, &quot;Efficient Image Representation using Binary Space Partitioning Trees.&quot;, Ph.D. Thesis, Columbia University, 1993.
* [RADHA96] H. Radha, M. Vetterli, and R. Leoonardi, “Image Compression Using Binary Space Partitioning Trees,” IEEE Transactions on Image Processing, vol. 5, No.12, December 1996, pp.&amp;nbsp;1610–1624.
* [WINTER99] AN INVESTIGATION INTO REAL-TIME 3D POLYGON RENDERING USING BSP TREES. Andrew Steven Winter. April 1999. available online
* {{cite book|author = [[Mark de Berg]], [[Marc van Kreveld]], [[Mark Overmars]], and [[Otfried Schwarzkopf]] | year = 2000 | title = Computational Geometry | publisher = [[Springer-Verlag]] | edition = 2nd revised | isbn = 3-540-65620-0}} Section 12: Binary Space Partitions: pp.&amp;nbsp;251&amp;ndash;265. Describes a randomized Painter's Algorithm.
* Christer Ericson: ''Real-Time [[collision detection|Collision Detection]] (The Morgan Kaufmann Series in Interactive 3-D Technology)''. Verlag ''Morgan Kaufmann'', S. 349-382, Jahr 2005, ISBN 1-55860-732-3

==External links==
*[http://www.cs.wpi.edu/~matt/courses/cs563/talks/bsp/bsp.html BSP trees presentation]
*[http://web.archive.org/web/20110719195212/http://www.cc.gatech.edu/classes/AY2004/cs4451a_fall/bsp.pdf Another BSP trees presentation]
*[http://symbolcraft.com/graphics/bsp/ A Java applet that demonstrates the process of tree generation]
*[http://archive.gamedev.net/archive/reference/programming/features/bsptree/bsp.pdf A Master Thesis about BSP generating]
*[http://www.devmaster.net/articles/bsp-trees/ BSP Trees: Theory and Implementation]
*[http://www.euclideanspace.com/threed/solidmodel/spatialdecomposition/bsp/index.htm BSP in 3D space]

{{CS-Trees}}

[[Category:Trees (data structures)]]
[[Category:Geometric data structures]]
[[Category:3D computer graphics]]
[[Category:Articles with example C code]]
[[Category:Video game development]]</text>
      <sha1>el0rvo623e18t48268jqgzo6dmp753a</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>C-trie</title>
    <ns>0</ns>
    <id>36305755</id>
    <revision>
      <id>516102754</id>
      <parentid>512576403</parentid>
      <timestamp>2012-10-05T06:27:51Z</timestamp>
      <contributor>
        <username>Paul A</username>
        <id>7104</id>
      </contributor>
      <comment>category</comment>
      <text xml:space="preserve" bytes="349">{{distinguish|Ctrie}}

A '''C-trie''' is a compressed [[trie]] data structure. It achieves lower memory and query time requirements at the expense of reduced flexibility.

== References ==

* Maly, K. Compressed tries. Commun. ACM 19, 7, 409-415. [https://dl.acm.org/citation.cfm?id=360258]


[[Category:Trees (data structures)]]

{{algorithm-stub}}</text>
      <sha1>181wqu5bgajurs8lfdteluc6q0ierj7</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tree accumulation</title>
    <ns>0</ns>
    <id>37525985</id>
    <revision>
      <id>537306769</id>
      <parentid>537305923</parentid>
      <timestamp>2013-02-09T00:02:16Z</timestamp>
      <contributor>
        <username>Donner60</username>
        <id>12744454</id>
      </contributor>
      <minor/>
      <comment>Reverted 1 edit by [[Special:Contributions/67.251.120.21|67.251.120.21]] ([[User talk:67.251.120.21|talk]]) identified as [[WP:VAND|vandalism]] to last revision by BG19bot. ([[WP:TW|TW]])</comment>
      <text xml:space="preserve" bytes="1538">In [[computer science]], '''tree accumulation''' is the process of accumulating data placed in [[Tree (data structure)|tree]]
nodes according to their [[Tree (data structure)|tree]] structure.&lt;ref&gt;{{cite thesis |type=Ph.D. |first=Jeremy |last=Gibbons |title=Algebras for Tree Algorithms |publisher=Oxford University |year=1991 |url=http://www.cs.ox.ac.uk/files/3422/PRG94.pdf }}&lt;/ref&gt; Formally, this operation is a [[catamorphism]].

Upward accumulation refers to accumulating on each node information about all decedents. Downward accumulation refers to accumulating on each node information of every ancestor.

One application would be calculating national election results. Construct a tree with the root node as the entire nation and each level representing refined geographical areas such as states/provinces, counties/parishes, cities/townships, and polling districts as the leaves. By accumulating the vote totals from the polling districts, one can compute the vote totals for each of the larger geographic areas.

==Formal analysis==
Gibbons et al.&lt;ref&gt;{{cite journal|last1=Gibbons|first1=Jeremy| last2=Cai |first2=Wentong |last3=Skillcorn |first3=David B. |title=Efficient parallel algorithms for tree accumulations|journal=Science of Computer Programming|year=1994|series=Elsiver}}&lt;/ref&gt; formally define binary tree accumulation as iterative application of a ternary operator &lt;math&gt;\otimes(A,B,A)&lt;/math&gt;; where A are descendant labels and B is a junction label.

==References==
{{Reflist}}

[[Category:Trees (data structures)]]</text>
      <sha1>c8abu4mnq64r7zl8c03ocatfg04irq8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Min/max kd-tree</title>
    <ns>0</ns>
    <id>12481023</id>
    <revision>
      <id>602703067</id>
      <parentid>526583576</parentid>
      <timestamp>2014-04-04T09:38:24Z</timestamp>
      <contributor>
        <username>Daniel Strobusch</username>
        <id>8669880</id>
      </contributor>
      <minor/>
      <comment>grammar</comment>
      <text xml:space="preserve" bytes="3754">{{lowercase|title=min/max kd-tree}}
A '''min/max ''k''d-tree''' is a [[K-d tree|''k''-d tree]] with two scalar values - a minimum and a maximum - assigned to its nodes. The minimum/maximum of an inner node is equal to the minimum/maximum of its children's minima/maxima.

==Construction==
Min/max ''k''d-trees may be constructed recursively. Starting with the root node, the splitting plane orientation and position is evaluated. Then the children's splitting planes and min/max values are evaluated recursively. The min/max value of the current node is simply the minimum/maximum of its children's minima/maxima.

==Properties==
The min/max ''k''dtree has - besides the properties of an ''k''d-tree - the special property that an inner node's min/max values coincide each with a min/max value of either one child. This allows to discard the storage of min/max values at the leaf nodes by storing two bits at inner nodes, assigning min/max values to the children: Each inner node's min/max values will be known in advance, where the root node's min/max values are stored separately. Each inner node has besides two min/max values also two bits given, defining to which child those min/max values are assigned (0: to the left child 1: to the right child). The non-assigned min/max values of the children are &lt;!-- ? --&gt; the from the current node already known min/max values. The two bits may also be stored in the least significant bits of the min/max values which have therefore to be approximated by fractioning them down/up.

The resulting memory reduction is not minor, as the leaf nodes of full binary ''k''d-trees are one half of the tree's nodes.

==Applications==
Min/max ''k''d-trees are used for [[ray casting]] [[isosurface]]s/MIP ([[maximum intensity projection]]). Isosurface ray casting only traverses nodes for which the chosen isovalue lies between the min/max values of the current node. Nodes that do not fulfill this requirement do not contain an isosurface to the given isovalue and are therefore skipped (empty space skipping). For MIP, nodes are not traversed if their maximum is smaller than the current maximum intensity along the ray. The favorible visualization complexity of ray casting allows to ray cast (and even change the isosurface for) very large scalar fields at interactive framerates on commodity PCs. Especially [[implicit kd tree|implicit max ''k''d-trees]] are an optimal choice for visualizing scalar fields defined on rectilinear grids (see &lt;ref&gt;Matthias Groß, Carsten Lojewski, Martin Bertram and Hans Hagen &quot;Fast Implicit KD-Trees: Accelerated Isosurface Ray Tracing and Maximum Intensity Projection for Large Scalar Fields&quot; CGIM07: Proceedings of Computer Graphics and Imaging (2007) 67-74&lt;/ref&gt;&lt;ref&gt;Ingo Wald, Heiko Friedrich, Gerd Marmitt, Philipp Slusallek and Hans-Peter Seidel &quot;Faster Isosurface Ray Tracing using Implicit KD-Trees&quot; IEEE Transactions on Visualization and Computer Graphics (2005)&lt;/ref&gt;&lt;ref&gt;Matthias Groß (PhD, 2009) [http://kluedo.ub.uni-kl.de/volltexte/2009/2361/ Towards Scientific Applications for Interactive Ray Casting]&lt;/ref&gt;). Similarly an [[implicit kd tree|implicit]] min/max kd-tree may be used to efficiently evaluate queries such as terrain [[Line of sight (gaming)|line of sight]].&lt;ref&gt;Bernardt Duvenhage &quot;Using An Implicit Min/Max KD-Tree for Doing Efficient Terrain Line of Sight Calculations&quot; in &quot;Proceedings of the 6th International Conference on Computer Graphics, Virtual Reality, Visualisation and Interaction in Africa&quot;, 2009.&lt;/ref&gt;

==See also==
* [[K-d tree|''k''-d tree]]
* [[implicit kd tree|implicit ''k''d-tree]]

==References==
&lt;references/&gt;

{{DEFAULTSORT:Min max kd-tree}}
[[Category:Computer graphics data structures]]
[[Category:Trees (data structures)]]</text>
      <sha1>nf1t43v01try939wjcw6raruik6tcd3</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tree (data structure)</title>
    <ns>0</ns>
    <id>30806</id>
    <revision>
      <id>626337084</id>
      <parentid>626336588</parentid>
      <timestamp>2014-09-20T12:50:10Z</timestamp>
      <contributor>
        <username>Pratyya Ghosh</username>
        <id>16166519</id>
      </contributor>
      <minor/>
      <comment>Reverted 2 edits by [[Special:Contributions/115.249.10.252|115.249.10.252]] identified as test/vandalism using [[WP:STiki|STiki]]</comment>
      <text xml:space="preserve" bytes="20096">{{hatnote|Not to be confused with [[trie]], a specific type of tree data structure.}}
{{Refimprove|date=August 2010}}
[[File:binary tree.svg|right|192|thumb|A simple unordered tree; in this diagram, the node labeled 7 has two children, labeled 2 and 6, and one parent, labeled 2. The root node, at the top, has no parent.]]
In [[computer science]], a '''tree''' is a widely used [[abstract data type]] (ADT) or [[data structure]] implementing this ADT that simulates a hierarchical [[tree structure]], with a root value and subtrees of children, represented as a set of linked [[Vertex (graph theory)|nodes]].

A tree data structure can be defined recursively (locally) as a collection of [[node (computer science)|node]]s (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the &quot;children&quot;), with the constraints that no reference is duplicated, and none points to the root.

Alternatively, a tree can be defined abstractly as a whole (globally) as an [[ordered tree]], with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node (rather than as a list of nodes and an [[adjacency list]] of edges between nodes, as one may represent a [[#Digraphs|digraph]], for instance). For example, looking at a tree as a whole, one can talk about &quot;the parent node&quot; of a given node, but in general as a data structure a given node only contains the list of its children, but does not contain a reference to its parent (if any).

==Definition==
A tree is a non-linear data structure that consists of a root node and potentially many levels of additional nodes that form a hierarchy. A tree can be empty with no nodes called the '''null''' or empty tree or a tree is a structure consisting of one node called the '''root''' and one or more subtrees.

==Terminologies used in Trees==
* '''Root''' - The top node in a tree.
* '''Parent''' - The converse notion of ''child''.
* '''Siblings''' - Nodes with the same parent.
* '''Descendant''' - a node reachable by repeated proceeding from parent to child.
* '''Ancestor''' - a node reachable by repeated proceeding from child to parent.
* '''Leaf''' - a node with no children.
* '''Internal node''' - a node with at least one child.
* '''External node''' - a node with no children.
* '''Degree''' - number of sub trees of a node.
* '''Edge''' - connection between one node to another.
* '''Path''' - a sequence of nodes and edges connecting a node with a descendant.
* '''Level''' - The level of a node is defined by 1 + the number of connections between the node and the root.
* '''Height''' - The height of a node is the number of edges on the longest downward path between the node and a leaf.
* '''Forest''' - A forest is a set of n ≥ 0 disjoint trees.
===Data type vs. data structure===
There is a distinction between a tree as an abstract data type and as a data structure, analogous to the distinction between a [[List (abstract data type)|list]] and a [[linked list]].

As a data type, a tree has a value and children, and the children are themselves trees; the value and children of the tree are interpreted as the value of the root node and the subtrees of the children of the root node. To allow finite trees, one must either allow the list of children to be empty (in which case trees can be required to be non-empty, an &quot;empty tree&quot; instead being represented by a forest of zero trees), or allow trees to be empty, in which case the list of children can be of fixed size ([[branching factor]], especially 2 or &quot;binary&quot;), if desired.

As a data structure, a linked tree is a group of [[Node (computer science)|nodes]], where each node has a value and a list of [[Reference (computer science)|references]] to other nodes (its children). This data structure actually defines a directed graph,{{efn|Properly, a rooted, ordered directed graph.}} because it may have loops or several references to the same node, just as a linked list may have a loop. Thus there is also the requirement that no two references point to the same node (that each node has at most a single parent, and in fact exactly one parent, except for the root), and a tree that violates this is &quot;corrupt&quot;.

Due to the use of ''references'' to trees in the linked tree data structure, trees are often discussed implicitly assuming that they are being represented by references to the root node, as this is often how they are actually implemented. For example, rather than an empty tree, one may have a null reference: a tree is always non-empty, but a reference to a tree may be null.

===Recursive===
Recursively, as a data type a tree is defined as a value (of some data type, possibly empty), together with a list of trees (possibly an empty list), the subtrees of its children; symbolically:
 t: v &lt;nowiki&gt;[t[1], ..., t[k]]&lt;/nowiki&gt;
(A tree ''t'' consists of a value ''v'' and a list of other trees.)

More elegantly, via [[mutual recursion]], of which a tree is one of the most basic examples, a tree can be defined in terms of a forest (a list of trees), where a tree consists of a value and a forest (the subtrees of its children):
 f: &lt;nowiki&gt;[t[1], ..., t[k]]&lt;/nowiki&gt;
 t: v f

Note that this definition is in terms of values, and is appropriate in [[functional language]]s (it assumes [[Referential transparency (computer science)|referential transparency]]); different trees have no connections, as they are simply lists of values.

As a data structure, a tree is defined as a node (the root), which itself consists of a value (of some data type, possibly empty), together with a list of references to other nodes (list possibly empty, references possibly null); symbolically:
 n: v &lt;nowiki&gt;[&amp;amp;n[1], ..., &amp;amp;n[k]]&lt;/nowiki&gt;
(A node ''n'' consists of a value ''v'' and a list of other references to other nodes.)

This data structure defines a directed graph,{{efn|Properly, a rooted, ordered directed graph.}} and for it to be a tree one must add a condition on its global structure (its topology), namely that at most one reference can point to any given node (a node has at most a single parent), and no node in the tree point to the root. In fact, every node (other than the root) must have exactly one parent, and the root must have no parents.

Indeed, given a list of nodes, and for each node a list of references to its children, one cannot tell if this structure is a tree or not without analyzing its global structure and checking that it is in fact topologically a tree, as defined below.

===Type theory===
As an ADT, the abstract tree type ''T'' with values of some type ''E'' is defined, using the  abstract forest type ''F'' (list of trees), by the functions:
:value: ''T'' → ''E''
:children: ''T'' → ''F''
:nil: () → ''F''
:node: ''E'' × ''F'' → ''T''
with the axioms:
:value(node(''e'', ''f'')) = ''e''
:children(node(''e'', ''f'')) = ''f''
In terms of [[type theory]], a tree is an [[Recursive data type|inductive type]] defined by the constructors ''nil'' (empty forest) and ''node'' (tree with root node with given value and children).

===Mathematical===
Viewed as a whole, a tree data structure is an [[ordered tree]], generally with values attached to each node. Concretely, it is (if required to be non-empty):
* A [[rooted tree]] with the &quot;away from root&quot; direction (a more narrow term is an &quot;[[Arborescence (graph theory)|arborescence]]&quot;), meaning:
** A [[directed graph]],
** whose underlying [[undirected graph]] is a [[tree (graph theory)|tree]] (any two vertices are connected by exactly one simple path),
** with a distinguished root (one vertex is designated as the root),
** which determines the direction on the edges (arrows point away from the root; given an edge, the node that the edge points from is called the ''parent'' and the node that the edge points to is called the ''child''),
together with:
* an ordering on the child nodes of a given node, and
* a value (of some data type) at each node.
Often trees have a fixed (more properly, bounded) [[branching factor]] ([[outdegree]]), particularly always having two child nodes (possibly empty, hence ''at most'' two ''non-empty'' child nodes), hence a &quot;binary tree&quot;.

Allowing empty trees makes some definitions simpler, some more complicated: a rooted tree must be non-empty, hence if empty trees are allowed the above definition instead becomes &quot;an empty tree, or a rooted tree such that ...&quot;. On the other hand, empty trees simplify defining fixed branching factor: with empty trees allowed, a binary tree is a tree such that every node has exactly two children, each of which is a tree (possibly empty).The complete sets of operations on tree must include fork operation.

==Terminology==
A '''[[node (computer science)|node]]''' is a structure which may contain a value or condition, or represent a separate data structure (which could be a tree of its own). Each node in a tree has zero or more '''child nodes''', which are below it in the tree (by convention, trees are drawn growing downwards). A node that has a child is called the child's '''parent node''' (or ''ancestor node'', or [[Superior (hierarchy)|superior]]). A node has at most one parent.

An '''internal node''' (also known as an '''inner node''', '''inode''' for short, or '''branch node''') is any node of a tree that has child nodes. Similarly, an '''external node''' (also known as an '''outer node''', '''leaf node''', or '''terminal node''') is any node that does not have child nodes.

The topmost node in a tree is called the '''root node'''. Depending on definition, a tree may be required to have a root node (in which case all trees are non-empty), or may be allowed to be empty, in which case it does not necessarily have a root node. Being the topmost node, the root node will not have a parent. It is the node at which algorithms on the tree begin, since as a data structure, one can only pass from parents to children. Note that some algorithms (such as post-order depth-first search) begin at the root, but first visit leaf nodes (access the value of leaf nodes), only visit the root last (i.e., they first access the children of the root, but only access the ''value'' of the root last). All other nodes can be reached from it by following '''edges''' or '''links'''. (In the formal definition, each such path is also unique.) In diagrams, the root node is conventionally drawn at the top. In some trees, such as [[heap (data structure)|heaps]], the root node has special properties. Every node in a tree can be seen as the root node of the subtree rooted at that node.

The '''height''' of a node is the length of the longest downward path to a leaf from that node. The height of the root is the height of the tree. The '''depth''' of a node is the length of the path to its root (i.e., its ''root path''). This is commonly needed in the manipulation of the various self-balancing trees, [[AVL Trees]] in particular. The root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. Conventionally, an empty tree (tree with no nodes, if such are allowed) has depth and height &amp;minus;1.

A '''subtree''' of a tree ''T'' is a tree consisting of a node in ''T'' and all of its descendants in ''T''.{{efn|This is different from the formal definition of subtree used in graph theory, which is a subgraph that forms a tree – it need not include all descendants. For example, the root node by itself is a subtree in the graph theory sense, but not in the data structure sense (unless there are no descendants).}}&lt;ref&gt;{{MathWorld|id=Subtree|title=Subtree}}&lt;/ref&gt; Nodes thus correspond to subtrees (each node corresponds to the subtree of itself and all its descendants) – the subtree corresponding to the root node is the entire tree, and each node is the root node of the subtree it determines; the subtree corresponding to any other node is called a '''proper subtree''' (by analogy to a [[proper subset]]).

==Drawing graphs==
Trees are often drawn in the plane. Ordered trees can be represented essentially uniquely in the plane, and are hence called ''plane trees,'' as follows: if one fixes a conventional order (say, counterclockwise), and arranges the child nodes in that order (first incoming parent edge, then first child edge, etc.), this yields an embedding of the tree in the plane, unique up to [[ambient isotopy]]. Conversely, such an embedding determines an ordering of the child nodes.

If one places the root at the top (parents above children, as in a [[family tree]]) and places all nodes that are a given distance from the root (in terms of number of edges: the &quot;level&quot; of a tree) on a given horizontal line, one obtains a standard drawing of the tree. Given a binary tree, the first child is on the left (the &quot;left node&quot;), and the second child is on the right (the &quot;right node&quot;).

==Representations==
There are many different ways to represent trees; common representations represent the nodes as [[Dynamic memory allocation|dynamically allocated]] records with pointers to their children, their parents, or both, or as items in an [[Array data structure|array]], with relationships between them determined by their positions in the array (e.g., [[binary heap]]).

Indeed, a binary tree can be implemented as a list of lists (a list where the values are lists): the head of a list (the value of the first term) is the left child (subtree), while the tail (the list of second and future terms) is the right child (subtree). This can be modified to allow values as well, as in Lisp [[S-expression]]s, where the head (value of first term) is the value of the node, the head of the tail (value of second term) is the left child, and the tail of the tail (list of third and future terms) is the right child.

In general a node in a tree will not have pointers to its parents, but this information can be included (expanding the data structure to also include a pointer to the parent) or stored separately. Alternatively, upward links can be included in the child node data, as in a [[threaded binary tree]].

==Generalizations==

===Digraphs===
If edges (to child nodes) are thought of as references, then a tree is a special case of a digraph, and the tree data structure can be generalized to represent [[directed graph]]s by removing the constraints that a node may have at most one parent, and that no cycles are allowed. Edges are still abstractly considered as pairs of nodes, however, the terms ''parent'' and ''child'' are usually replaced by different terminology (for example, ''source'' and ''target''). Different [[graph (data structure)#Representations|implementation strategies]] exist: a digraph can be represented by the same local data structure as a tree (node with value and list of children), assuming that &quot;list of children&quot; is a list of references, or globally by such structures as [[adjacency list]]s.

In [[graph theory]], a [[tree (graph theory)|tree]] is a connected acyclic [[Graph (data structure)|graph]]; unless stated otherwise, in graph theory trees and graphs are assumed undirected. There is no one-to-one correspondence between such trees and trees as data structure. We can take an arbitrary undirected tree, arbitrarily pick one of its [[vertex (graph theory)|vertices]] as the ''root'', make all its edges directed by making them point away from the root node – producing an [[Arborescence (graph theory)|arborescence]] – and assign an order to all the nodes. The result corresponds to a tree data structure. Picking a different root or different ordering produces a different one.

Given a node in a tree, its children define an ordered forest (the union of subtrees given by all the children, or equivalently taking the subtree given by the node itself and erasing the root). Just as subtrees are natural for recursion (as in a depth-first search), forests are natural for [[corecursion]] (as in a breadth-first search).

Via [[mutual recursion]], a forest can be defined as a list of trees (represented by root nodes), where a node (of a tree) consists of a value and a forest (its children):
 f: &lt;nowiki&gt;[n[1], ..., n[[k]]&lt;/nowiki&gt;
 n: v f

==Traversal methods==
{{Main|Tree traversal}}
Stepping through the items of a tree, by means of the connections between parents and children, is called '''walking the tree''', and the action is a '''walk''' of the tree. Often, an operation might be performed when a pointer arrives at a particular node. A walk in which each parent node is traversed before its children is called a '''pre-order''' walk; a walk in which the children are traversed before their respective parents are traversed is called a '''post-order''' walk; a walk in which a node's left subtree, then the node itself, and finally its right subtree are traversed is called an '''in-order''' traversal. (This last scenario, referring to exactly two subtrees, a left subtree and a right subtree, assumes specifically a [[binary tree]].)
A '''level-order''' walk effectively performs a [[breadth-first search]] search over the entirety of a tree; nodes are traversed level by level, where the root node is visited first, followed by its direct child nodes and their siblings, followed by its grandchild nodes and their siblings, etc., until all nodes in the tree have been traversed.

==Common operations==
* Enumerating all the items
* Enumerating a section of a tree
* Searching for an item
* Adding a new item at a certain position on the tree
* Deleting an item
* '''[[Pruning (algorithm)|Pruning]]''': Removing a whole section of a tree
* '''[[Grafting (algorithm)|Grafting]]''': Adding a whole section to a tree
* Finding the root for any node

==Common uses==
* Representing [[hierarchical]] data
* Storing data in a way that makes it easily [[search algorithm|searchable]] (see [[binary search tree]] and [[tree traversal]])
* Representing [[sorting algorithm|sorted lists]] of data
* As a workflow for [[Digital compositing|compositing]] digital images for [[visual effects]]
* [[Routing]] algorithms

==See also==
* [[Tree structure]]
* [[Tree (graph theory)]]
* [[Tree (set theory)]]
* [[Hierarchy (mathematics)]]
* [[Dialog tree]]
* [[Single inheritance]]

===Other trees===
* [[DSW algorithm]]
* [[Enfilade (Xanadu)|Enfilade]]
* [[Left child-right sibling binary tree]]
* [[Hierarchical temporal memory]]

==Notes==
{{notelist}}

==References==
{{Reflist}}
{{refbegin}}
* [[Donald Knuth]]. ''The Art of Computer Programming: Fundamental Algorithms'', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4 . Section 2.3: Trees, pp.&amp;nbsp;308–423.
* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7 . Section 10.4: Representing rooted trees, pp.&amp;nbsp;214–217. Chapters 12–14 (Binary Search Trees, Red-Black Trees, Augmenting Data Structures), pp.&amp;nbsp;253–320.
{{refend}}

==External links==
{{Commons category|Tree data structure}}
* [http://www.community-of-knowledge.de/beitrag/data-trees-as-a-means-of-presenting-complex-data-analysis/ Data Trees as a Means of Presenting Complex Data Analysis] by Sally Knipe
* [http://www.nist.gov/dads/HTML/tree.html Description] from the [[Dictionary of Algorithms and Data Structures]]
* [http://tree.phi-sci.com STL-like C++ tree class]
* [http://ideainfo.8m.com Description of tree data structures from ideainfo.8m.com]
* [http://wormweb.org/celllineage WormWeb.org: Interactive Visualization of the ''C. elegans'' Cell Tree] - Visualize the entire cell lineage tree of the nematode ''C. elegans'' (javascript)
* [ref : http://www.allisons.org/ll/AlgDS/Tree/]

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Tree (Data Structure)}}
[[Category:Data types]]
[[Category:Trees (data structures)| ]]
[[Category:Knowledge representation]]

[[de:Baum (Graphentheorie)]]</text>
      <sha1>f9inu6d150ppzo29ij6cwrzppzyr4k0</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Finger search tree</title>
    <ns>0</ns>
    <id>38136820</id>
    <revision>
      <id>612278231</id>
      <parentid>609121548</parentid>
      <timestamp>2014-06-09T23:09:16Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <comment>[[WP:AWB/GF|General fixes]], removed orphan tag using [[Project:AWB|AWB]] (10242)</comment>
      <text xml:space="preserve" bytes="4369">{{about|search trees that allow searching from interior nodes|the purely functional data structure|finger tree}}

In computer science, '''finger search trees''' are a type of [[binary search tree]] that keeps pointers to interior nodes, called ''fingers''.  The fingers speed up searches, insertions, and deletions for elements close to the fingers, giving amortized [[Big O notation|O]](log n) lookups, and amortized O(1) insertions and deletions. It should not be confused with a [[finger tree]] nor a [[splay tree]], although both can be used to implement finger search trees.

Guibas et al.&lt;ref&gt;{{citation|last=Guibas|first=L.J.|year=1977|title=A new representation for linear lists|journal=Proceedings of the ninth annual ACM symposium on Theory of computing|pages=49–60
}}&lt;/ref&gt;
introduced ﬁnger search trees, by building upon [[B-tree]]s.  The original version supports ﬁnger searches in O(log d) time, where d is the number of elements between the ﬁnger and the search target. Updates take O(1) time, when only O(1) moveable ﬁngers are maintained.  Moving a ﬁnger p positions requires O(log p) time. Huddleston and Mehlhorn refined this idea as level-linked B-trees.&lt;ref&gt;{{cite journal  |last= Huddleston  |first2= Kurt |last2= Mehlhorn |year=1982 |title= A New Data Structure for Representing Sorted Lists|url= |journal= Acta Informatica |publisher= |volume= 17 |issue=2 |pages= 157–184 |doi=10.1007/BF00288968 |accessdate=25 April 2014}}&lt;/ref&gt;

[[Athanasios Tsakalidis|Tsakalidis]] proposed a version based on [[AVL tree]]s that facilitates searching from the ends of the tree; it can be used to implement a data structure with multiple fingers by using multiple of such trees.&lt;ref name=&quot;tsakalidis&quot;&gt;{{cite journal |first= Athanasios K.
|last= Tsakalidis |last2= |first2= |date= 1985 |title=AVL-Trees for Localized Search |url= |journal=Information and Control |publisher= |volume=67 |issue= |pages=173–194 |doi=10.1016/S0019-9958(85)80034-6 |accessdate=25 April 2014}}&lt;/ref&gt;

To perform a finger search on a binary tree, the ideal way is to start from the finger, and search upwards to the root, until we reach the ''turning node''&lt;ref name=tsakalidis /&gt; or the ''least common ancestor''&lt;ref name=&quot;brodal&quot; /&gt;&lt;ref name=&quot;treap&quot; /&gt; of x and y, and then go downwards to find the element we're looking for. Determining if a node is the ancestor of another is non-trivial.

[[File:Performing finger searches on treaps.svg|center|thumb|300px|An example of performing finger search on a treap.]]

[[Treap]]s, a randomized tree structure proposed by Seidel and Aragon,&lt;ref name=&quot;treap&quot;&gt;{{cite journal |last= Seidel |first=R.| authorlink=Raimund Seidel |last2=Aragon |first2=C.R. |authorlink2= Cecilia R. Aragon |date= 1996 |title= Randomized search trees |journal= Algorithmica |publisher= |volume= 16|issue= 4-5 |pages=464–497 |doi=10.1007/BF01940876 |accessdate=25 April 2014}}&lt;/ref&gt; has the property that the expected path length of two elements of distance ''d'' is O(log ''d''). For finger searching, they proposed adding pointers to determine the ''least common ancestor''(LCA) quickly, or in every node maintain the minimum and maximum values of its subtree.

A book chapter has been written that covers finger search trees in depth.&lt;ref name=&quot;brodal&quot;&gt;
{{cite book
| last          = Brodal
| first         = Gerth Stølting
| year          = 2005
| chapter       = 11. Finger Search
| chapterurl    = http://www.cs.au.dk/~gerth/papers/finger05.pdf
| editor1-last  = Mehta
| editor1-first = Dinesh P. 
| editor2-last  = Sahni
| editor2-first = Sartaj
| editor2-link  = Sartaj_Sahni
| title         = Handbook of Data Structures and Applications
| publisher     = [[Chapman &amp; Hall]] / [[CRC Press]]
| isbn          = 1584884355
| accessdate    = 1 January 2013
}}&lt;/ref&gt; In which, Brodal suggested an algorithm to perform finger search on treaps in O(log d) time, without needing any extra bookkeeping information; this algorithm accomplishes this by concurrently searching downward from the last candidate LCA.

==See also==
* [[Finger search]]

==References==
{{reflist}}
&lt;!--{{cite web|title=Randomized Splay Trees: Theoretical and Experimental Results|url=http://www.cs.au.dk/~gerth/papers/finger05.pdf|accessdate=1 January 2013}}--&gt;

{{CS-Trees}}

[[Category:Trees (data structures)]]
[[Category:Search algorithms]]


{{datastructure-stub}}</text>
      <sha1>bishotsxmoxi7k61u4efx5hq71oucyj</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Link/cut tree</title>
    <ns>0</ns>
    <id>9328337</id>
    <revision>
      <id>586671022</id>
      <parentid>586219185</parentid>
      <timestamp>2013-12-18T18:05:03Z</timestamp>
      <contributor>
        <username>ChrisGualtieri</username>
        <id>16333418</id>
      </contributor>
      <minor/>
      <comment>Remove stub template(s). Page is start class or higher. Also check for and do General Fixes + Checkwiki fixes using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="13195">{{Use dmy dates|date=July 2012}}
&lt;!--{{Infobox data structure
|name=Link-Cut Tree
|type=tree
|invented_by=[[Daniel Dominic Sleator]] and [[Robert Endre Tarjan]]
|invented_year=1982
|space_avg=O(n)
|space_worst=O(n)
|link=amortized O(log n)
|cut=amortized O(log n)
|path=amortized O(log n)
|findroot=amortized O(log n)
}}--&gt;
{| class=infobox width=245px
|colspan=3 align=center| '''Link-Cut Tree'''
|-
|'''Type''' ||[[Tree]]
|-
|'''Invented''' || 1982
|-
&lt;tr&gt; &lt;td&gt;'''Invented by''' &lt;/td&gt;&lt;td colspan = 2&gt; [[Daniel Dominic Sleator]] and [[Robert Endre Tarjan]] &lt;/td&gt;&lt;/tr&gt;
|-
|colspan=3 align=center style=&quot;background:#D6D6F5&quot;|'''[[Time complexity]] in [[big O notation]]'''
|-
| ||Average||Worst Case
|-
|'''Link''' || O(log n) || amortized O(log n)
|-
|'''Cut''' ||  O(log n) ||amortized O(log n)
|-
|'''Path''' ||  O(log n) ||amortized O(log n)
|-
|'''FindRoot''' ||  O(log n) ||amortized O(log n)
|}

A '''link/cut tree''' is a type of [[data structure]] that can merge (link) and split (cut) data sets in [[big O notation|O]]([[logarithm|log]](n)) [[amortized analysis|amortized]] time, and can find which tree an element belongs to in O(log(n)) amortized time. The overall structure is a forest of trees. Link-Cut trees divide a given represented tree into vertex-disjoint paths, where each path is represented by an auxiliary tree (often [[splay trees]], though the original paper pre-dates splay trees and thus uses biased binary search trees). The nodes in the auxiliary trees are keyed by their depth in the corresponding represented tree. In one variation, ''Naive Partitioning'', the paths are determined by the most recently accessed paths and nodes, similar to [[Tango Trees]]. In ''Partitioning by Size'' paths are determined by the heaviest child (child with the most children) of the given node. This gives a more complicated structure, but reduces the cost of the operations from amortized O(log n) to worst case O(log n). It has uses in solving a variety of network flow problems and to jive data sets. In the original publication, [[Daniel Sleator|Sleator]] and [[Robert Tarjan|Tarjan]] referred to link/cut trees as “dynamic trees”, or &quot;dynamic dyno trees&quot;.

==Structure==

We take a tree where each node has an arbitrary degree of unordered nodes and split it into paths. We call this the ''represented tree''. These paths are represented internally by auxiliary trees (here we will use splay trees), where the nodes from left to right represent the path from root to the last node on the path. Nodes that are connected in the represented tree that are not on the same preferred path (and therefore not in the same auxiliary tree) are connected via a ''path-parent pointer''. This pointer is stored in the root of the auxiliary tree representing the path.

[[File:LinkCutAccess1.png|frame|Demonstrating how nodes are stored by depth in the link-cut tree]]

===Preferred Paths===

When an access to a node ''v'' is made on the ''represented tree'', the path that is taken becomes the '''preferred path'''. The '''preferred child''' of a node is the last child that was on the access path, or null if the last access was to ''v'' or if no accesses were made to this particular branch of the tree. A '''preferred edge''' is the edge that connects the '''preferred child''' to ''v''.

In an alternate version, preferred paths are determined by the heaviest child.

[[File:Linkcuttree1.png|frame|Showing how a link cut tree transforms preferred paths into a forest of trees.]]

==Operations==

The operations we are interested in are FindRoot(Node v), Cut(Node v), Link(Node v, Node w), and Path(Node v). 
Every operation is implemented using the Access(Node v) subroutine. When we ''access'' a vertex ''v'', the preferred path of the represented tree is changed to a path from the root ''R'' of the represented tree to the node ''v''. If a node on
the access path previously had a preferred child ''u'', and the path now goes to child ''w'', the old ''preferred edge''
is deleted (changed to a ''path-parent pointer''), and the new path now goes through ''u''.

===Access===

After performing an access to node ''v'', it will no longer have any preferred children, and will be at the end of the path. Since nodes in the auxiliary tree are keyed by depth, this means that any nodes to the right of ''v'' in the auxiliary tree must be disconnected. In a splay tree this is a relatively simple procedure; we splay at ''v'', which brings ''v'' to the root of the auxiliary tree. We then disconnect the right subtree of ''v'', which is every node that came below it on the previous preferred path. The root of the disconnected tree will have a path-parent pointer, which we point to ''v''.

We now walk up the represented tree to the root ''R'', breaking and resetting the preferred path where necessary. To do this we follow the path-parent pointer from ''v'' (since ''v'' is now the root, we have direct access to the path-parent pointer). If the path that ''v'' is on already contains the root ''R'' (since the nodes are keyed by depth, it would be the left most node in the auxiliary tree), the path-parent pointer will be null, and we are done the access. Otherwise we follow the pointer to some node on another path ''w''. We want to break the old preferred path of ''w'' and reconnect it to the path ''v'' is on. To do this we splay at ''w'', and disconnect its right subtree, setting its path-parent pointer to ''w''. Since all nodes are keyed by depth, and every node in the path of ''v'' is deeper than every node in the path of ''w'' (since they are children of ''w'' in the represented tree), we simply connect the tree of ''v'' as the right child of ''w''. We splay at ''v'' again, which, since ''v'' is a child of the root ''w'', simply rotates ''v'' to root. We repeat this entire process until the path-parent pointer of ''v'' is null, at which point it is on the same preferred path as the root of the represented tree ''R''.

[[File:LinkCutAccess2.png|frame|During an access old preferred paths are broken and replaced with path-parent pointers, while the accessed node is splayed to the root of the tree]]

===FindRoot===

FindRoot refers to finding the root of the represented tree that contains the node ''v''. Since the ''access'' subroutine puts ''v'' on the preferred path, we first execute an access. Now the node ''v'' is on the same preferred path, and thus the same auxiliary tree as the root ''R''. Since the auxiliary trees are keyed by depth, the root ''R'' will be the leftmost node of the auxiliary tree. So we simply choose the left child of ''v'' recursively until we can go no further, and this node is the
root ''R''. The root may be linearly deep (which is worst case for a splay tree), we therefore splay it so that the next access will be quick.

===Cut===

Here we would like to cut the represented tree at node ''v''. First we access ''v''. This puts all the elements lower than ''v'' in the represented tree as the right child of ''v'' in the auxiliary tree. All the elements now in the left subtree of ''v'' are the nodes higher than ''v'' in the represented tree. We therefore disconnect the left child of ''v'' (which still maintains an attachment to the original represented tree through its path-parent pointer). Now ''v'' is the root of a represented tree. Accessing ''v'' breaks the preferred path below ''v'' as well, but that subtree maintains its connection to ''v'' through its path-parent pointer.

===Link===

If ''v'' is a tree root and ''w'' is a vertex in another tree, link the trees
containing ''v'' and ''w'' by adding the edge(v, w), making ''w'' the parent of ''v''.
To do this we access both ''v'' and ''w'' in their respective trees, and make ''w'' the left
child of ''v''. Since ''v'' is the root, and nodes are keyed by depth in the auxiliary tree, accessing ''v'' means
that ''v'' will have no left child in the auxiliary tree (since as root it is the minimum depth). Adding ''w'' as a left
child effectively makes it the parent of ''v'' in the represented tree.

===Path===

For this operation we wish to do some aggregate function over all the nodes (or edges) on the path to ''v'' (such as &quot;sum&quot; or &quot;min&quot; or &quot;max&quot; or &quot;increase&quot;, etc.). To do this we access ''v'', which gives us an auxiliary tree with all the nodes on the path from root ''R'' to node ''v''. The data structure can be augmented with data we wish to retrieve, such as min or max values, or the sum of the costs in the subtree, which can then be returned from a given path in constant time.

[[File:Pseudo-code.png|frame|Pseudo-code for link-cut tree operations]]

==Analysis==

Cut and link have O(1) cost, plus that of the access. FindRoot has an O(log n) amortized upper bound, plus the cost of the access. The data structure can be augmented with additional information (such as the min or max valued node in its subtrees, or the sum), depending on the implementation. Thus Path can return this information in constant time plus the access bound.

So the it remains to bound the ''access'' to find our running time.

Access makes use of splaying, which we know has an O(log n) amortized upper bound. So the remaining analysis deals with the number of times we need to splay.  This is equal to the number of preferred child changes (the number of edges changed in the preferred path) as we traverse up the tree.

We bound ''access'' by using a technique called '''Heavy-Light Decomposition'''.

===Heavy-Light Decomposition===
{{main|Heavy path decomposition}}
This technique calls an edge heavy or light depending on the number of nodes in the subtree. ''Size(v)'' represents the number of nodes in the subtree of ''v'' in the represented tree. An edge is called ''heavy'' if  size(v) &gt; ½ size(parent(v)).  Thus we can see that each node can have at most 1 ''heavy'' edge. An edge that is not a ''heavy'' edge is referred to as a ''light'' edge.

The ''light-depth'' refers to the number of light edges on a given path from root to vertex ''v''. ''Light-depth'' &lt;math&gt;\leq &lt;/math&gt; lg ''n'' because each time we traverse a light-edge we decrease the number of nodes by at least a factor of 2 (since it can have at most half the nodes of the parent).

So a given edge in the represented tree can be any of four possibilities: ''heavy-preferred'', ''heavy-unpreferred'', ''light-preferred'' or ''light-unpreferred''.

First we prove an &lt;math&gt;O(\log^{2} n)&lt;/math&gt; upper bound.

====O(log &lt;sup&gt;2&lt;/sup&gt; n) upper bound====

The splay operation of the access gives us log ''n'', so we need to bound the number of accesses to log ''n'' to prove the O(log &lt;sup&gt;2&lt;/sup&gt; ''n'') upper bound.
 
Every change of preferred edge results in a new preferred edge being formed. So we count the number of preferred edges formed. Since there are at most log ''n'' edges that are light on any given path, there are at most log ''n'' light edges changing to preferred.

The number of heavy edges becoming preferred can be O(n) for any given operation, but it is O(log n) amortized. Over a series of executions we can have ''n''-1 heavy edges become preferred (as there are at most ''n''-1 heavy edges total in the represented tree), but from then on the number of heavy edges that become preferred is equal to the number of heavy edges that became unpreferred on a previous step. For every heavy edge that becomes unpreferred a light edge must become preferred. We have seen already that the number of light edges that can become preferred is at most log ''n''. So the number of heavy edges that become preferred for ''m'' operations is ''m(log n) + (n - 1)''. Over enough operations (''m &gt; n-1'') this averages to O(log n).

====Improving to O(log n) upper bound====

We have bound the number of preferred child changes at O(log n), so if we can show that each preferred child change has cost O(1) amortized we can bound the ''access'' operation at O(log n). This is done using the [[potential method]].

Let s(v) be the number of nodes under ''v'' in the tree of auxiliary trees. Then the potential function &lt;math&gt;\Phi = \sum_{v} \log{s(v)}&lt;/math&gt;. We know that the amortized cost of splaying is bounded by:

&lt;math&gt;cost(splay(v)) \leq 3 \left( \log{s(root(v))} - \log{s(v)} \right) + 1&lt;/math&gt;

We know that after splaying, ''v'' is the child of its path-parent node ''w''. So we know that:

&lt;math&gt; s(v) \leq s(w) &lt;/math&gt;

We use this inequality and the amortized cost of access to achieve a telescoping sum that is bounded by:

&lt;math&gt; 3\left(\log{s(R)} - \log{s(v)}\right) &lt;/math&gt; ''+ O(# of preferred child changes)''

where ''R'' is the root of the represented tree, and we know the number of preferred child changes is O(log n). s(''R'') = ''n'', so we have O(log n) amortized.

==See also==
* [[Splay tree]]
* [[Potential method]]

== Further reading ==
* {{cite doi|10.1145/800076.802464}}
* {{cite doi|10.1145/3828.3835}}
* {{cite doi|10.1145/76359.76368}} — Application to min-cost circulation
* http://compgeom.cs.uiuc.edu/~jeffe/teaching/datastructures/2006/notes/07-linkcut.pdf
* http://courses.csail.mit.edu/6.851/spring12/scribe/L19.pdf

== External links ==
{{CS-Trees}}

&lt;!----&gt;

{{DEFAULTSORT:Link cut tree}}
[[Category:Trees (data structures)]]</text>
      <sha1>mr6pe7varulbq8hs5edznkge4bvotdl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Log-structured merge-tree</title>
    <ns>0</ns>
    <id>38562148</id>
    <revision>
      <id>622493555</id>
      <parentid>610226152</parentid>
      <timestamp>2014-08-23T17:28:01Z</timestamp>
      <contributor>
        <ip>99.235.207.14</ip>
      </contributor>
      <text xml:space="preserve" bytes="3091">{{more footnotes|date=April 2013}}

{{Infobox data structure
|name=Log-structured merge-tree
|type=Hybrid (two tree-like components)
|invented_by=Patrick O'Neil, Edward Cheng, Dieter Gawlick, Elizabeth O'Neil
|invented_year=1996
}}
In [[computer science]], the Log-Structured Merge-Tree (or LSM tree) is a [[data structure]] with performance characteristics that make it attractive for providing [[Database index|indexed]] access to files with high insert volume, such as [[transaction log|transactional log data]]. LSM trees, like other search trees, maintain key-value pairs.  LSM trees maintain data in two or more separate structures, each of which is optimized for its respective underlying storage medium; data is synchronized between the two structures efficiently, in batches.

One simple version of the LSM tree is a two-level LSM tree.&lt;ref&gt; O'Neil 1996, p. 4&lt;/ref&gt;
As described by O'Neil, a two-level  LSM tree comprises two [[Tree (data structure)|tree-like]] structures, called C0 and C1. C0 is smaller and entirely resident in memory, whereas C1 is resident on disk. New records are inserted into the memory-resident C0 component. If the insertion causes the C0 component to exceed a certain size threshold, a contiguous segment of entries is removed from C0 and merged into C1 on disk. The performance characteristics of LSM trees stem from the fact that each component is tuned to the characteristics of its underlying storage medium, and that data is efficiently migrated across media in rolling batches, using an algorithm reminiscent of [[merge sort]].

Most LSM trees used in practice employ multiple levels.  Level 0 is kept in main memory, and might be represented using a tree.   The on-disk data is organized into sorted runs of data.  Each run contains data sorted by the index key.  A run can be represented on disk as a single file, or alternatively as a collection of files with nonoverlapping key ranges.  To perform a query on a particular key to get its associated value, one must search in the Level 0 tree, as well as each run.

A particular key may appear in several runs, and what happens depends on the application.  Some applications simply want the newest key-value pair with a given key.  Some applications must combine the values in some way to get the proper aggregate value to return.  For example, in [[Apache Cassandra]], each value represents a row in a database, and different versions of the row may have different sets of columns.&lt;ref&gt;[https://web.archive.org/web/20140213134601/http://www.datastax.com/dev/blog/leveled-compaction-in-apache-cassandra Leveled Compaction in Apache Cassandra]&lt;/ref&gt;

In order to keep down the cost of queries, the system must avoid a situation where there are too many runs.

LSM trees are used in database management systems such as [[HBase]], [[LevelDB]], [[SQLite4]], [[RocksDB]] and [[Apache Cassandra]].

== References ==
{{reflist}}

;General
* {{Cite doi|10.1007/s002360050048}}
* {{Cite doi|10.1109/ICDE.2009.226}}

{{CS trees}}

[[Category:Trees (data structures)]]
[[Category:Database index techniques]]</text>
      <sha1>niw0wnjjhpuwy1falgizvffzibhs0n4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Day–Stout–Warren algorithm</title>
    <ns>0</ns>
    <id>1699060</id>
    <revision>
      <id>551263535</id>
      <parentid>549148874</parentid>
      <timestamp>2013-04-20T10:16:12Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes - Replaced endash with hyphen  in sortkey per [[WP:MCSTJR]] using [[Project:AWB|AWB]] (9100)</comment>
      <text xml:space="preserve" bytes="2241">The '''Day–Stout–Warren (DSW) algorithm''' is a method for efficiently balancing [[binary search tree]]s &amp;mdash; that is, decreasing their height to [[Big-O notation|O]](log ''n'') nodes, where ''n'' is the total number of nodes. Unlike a [[self-balancing binary search tree]], it does not do this incrementally during each operation, but periodically, so that its cost can be [[amortized analysis|amortized]] over many operations. The algorithm was designed by Quentin F. Stout and Bette Warren in their 1986 paper ''Tree Rebalancing in Optimal Time and Space'', based on work done by Colin Day in 1976.

The algorithm requires linear (O(''n'')) time and is [[in-place algorithm|in-place]]. The original algorithm by Day generates as compact a tree as possible:  all levels of the tree are completely full except possibly the bottom-most.  The Stout/Warren modification generates a complete binary tree, namely one in which the bottom-most level is filled strictly from left to right.  This is a useful transformation to perform if it is known that no more inserts will be done.

A 2002 article by Timothy J. Rolfe has recently brought attention back to the DSW algorithm after a long hiatus; the naming is from the section title &quot;6.7.1:  The DSW Algorithm&quot; in Adam Drozdek's ''Data Structures and Algorithms in C++'' (PWS Publishing Co., 1996) pp.&amp;nbsp;173–175. Rolfe cites two main advantages: &quot;in circumstances in which one generates an entire binary search tree at the beginning of processing, followed by item look-up access for the rest of processing&quot; and &quot;pedagogically within a course on data structures where one progresses from the binary search tree into self-adjusting trees, since it gives a first exposure to doing [[Tree rotation|rotations within a binary search tree]].&quot;

== External links ==
* [http://penguin.ewu.edu/~trolfe/DSWpaper/ An explanation and some experiments] by Timothy J. Rolfe
* [http://www.eecs.umich.edu/~qstout/pap/CACM86.pdf The original paper] by Stout and Warren
* [http://www.eecs.umich.edu/~qstout/ Prof. Quentin Stout's homepage] at University of Michigan

{{DEFAULTSORT:Day-Stout-Warren algorithm}}
[[Category:Trees (data structures)]]
[[Category:Search algorithms]]


{{datastructure-stub}}</text>
      <sha1>sg2uz35ubh58lws3f8qgmd11gby9cgq</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Stern–Brocot tree</title>
    <ns>0</ns>
    <id>2546511</id>
    <revision>
      <id>612550894</id>
      <parentid>612541642</parentid>
      <timestamp>2014-06-11T21:24:52Z</timestamp>
      <contributor>
        <username>Trovatore</username>
        <id>310173</id>
      </contributor>
      <comment>dummy edit:  Why do you want to link [[bijection]] anyway?  [[Order isomorphism]] is the more appropriate link, and surely we can afford another sentence rather than turning this one solid blue</comment>
      <text xml:space="preserve" bytes="14559">[[Image:SternBrocotTree.svg|thumb|400px|The Stern–Brocot tree, and the Stern–Brocot sequences of order ''i'' for ''i'' = 1, 2, 3, 4.]]

In [[number theory]], the '''Stern–Brocot tree''' is an [[Binary tree#Types of binary trees|infinite complete binary]] [[tree (graph theory)|tree]] in which the [[vertex (graph theory)|vertices]] correspond [[bijection|one-for-one]] to the [[positive number|positive]] [[rational number]]s, whose values are ordered from left to right as in a [[search tree]].
 
The Stern–Brocot tree was discovered independently by {{harvs|first=Moritz|last=Stern|authorlink=Moritz Stern|year=1858|txt}} and {{harvs|first=Achille|last=Brocot|authorlink=Achille Brocot|year=1861|txt}}. Stern was a German number theorist; Brocot was a French [[clockmaker]] who used the Stern–Brocot tree to design systems of gears with a [[gear ratio]] close to some desired value by finding a ratio of [[smooth number]]s near that value.

The root of the Stern–Brocot tree corresponds to the number 1. The parent-child relation between numbers in the Stern–Brocot tree may be defined in terms of [[continued fraction]]s or [[mediant (mathematics)|mediants]], and a path in the tree from the root to any other number ''q'' provides a sequence of [[Diophantine approximation|approximation]]s to ''q'' with smaller [[denominator]]s than ''q''. Because the tree contains each positive rational number exactly once, a [[breadth first search]] of the tree provides a method of listing all positive rationals that is closely related to [[Farey sequence]]s.

==A tree of continued fractions==

Every positive rational number ''q'' may be expressed as a continued fraction of the form
:&lt;math&gt;q = a_0 + \cfrac{1}{a_1 + \cfrac{1}{a_2 + \cfrac{1}{a_3 + \cfrac{1}{\ddots+\tfrac1{a_k}}}}}= [a_0; a_1, a_2, \ldots, a_k] &lt;/math&gt;
where ''k'' and ''a''&lt;sub&gt;0&lt;/sub&gt; are non-negative integers, and each subsequent coefficient ''a&lt;sub&gt;i&lt;/sub&gt;'' is a positive integer. This representation is not unique because one has

:&lt;math&gt;\displaystyle[a_0; a_1, a_2, \ldots, a_{k-1}, 1] = [a_0; a_1, a_2, \ldots, a_{k-1}+1]&lt;/math&gt;

for every sequence of coefficients ''a''&lt;sub&gt;0&lt;/sub&gt;, ..., ''a''&lt;sub&gt;''k''−1&lt;/sub&gt;.
Using this identity to rewrite any representation of the former form into the latter form, one may obtain that the final coefficient satisfies {{nowrap|''a''&lt;sub&gt;''k''&lt;/sub&gt; ≥ 2}} (unless {{nowrap|''k'' {{=}} 0}}, in which case one obtains ''a''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;≥&amp;nbsp;1); with this additional requirement the representation becomes unique. Then, unless {{nowrap|''q'' {{=}} 1}}, then number ''q'' has a parent in the Stern–Brocot tree given by the continued fraction expression

:&lt;math&gt;\displaystyle [a_0; a_1, a_2, \ldots, a_k-1],&lt;/math&gt;

which in case {{nowrap|''a''&lt;sub&gt;''k''&lt;/sub&gt; {{=}} 2}} one can rewrite as &lt;math&gt;[a_0; a_1, a_2, \ldots, a_{k-1}+1]&lt;/math&gt;.
For instance, the rational number {{fraction|23|16}} has the continued fraction representation

:&lt;math&gt;\frac{23}{16}=1+\cfrac1{2+\cfrac1{3+\frac12}} = [1;2,3,2],&lt;/math&gt;

so its parent in the Stern–Brocot tree is the number

:&lt;math&gt;[1;2,3,1]=[1;2,4]=1+\cfrac1{2+\frac14}=\frac{13}{9}.&lt;/math&gt;

This parent is formed by decreasing the denominator in the innermost term of the continued fraction by&amp;nbsp;1, and contracting with the previous term if the fraction becomes &lt;math&gt;\tfrac11&lt;/math&gt;.

Conversely each number ''q'' in the Stern–Brocot tree has exactly two children: if

:&lt;math&gt;\displaystyle q = [a_0; a_1, a_2, \ldots, a_k] = [a_0; a_1, a_2, \ldots, a_k-1,1]&lt;/math&gt;

then one child is the number represented by the continued fraction

:&lt;math&gt;\displaystyle [a_0; a_1, a_2, \ldots, a_k+1]&lt;/math&gt;

while the other child is represented by the continued fraction

:&lt;math&gt;\displaystyle [a_0; a_1, a_2, \ldots, a_k-1,2].&lt;/math&gt;

One of these children is less than ''q'' and this is the left child; the other is greater than ''q'' and it is the right child (in fact the former expression gives the left child if ''k'' is odd, and the right child if ''k'' is even).
For instance, the continued fraction representation of {{fraction|13|9}} is &lt;nowiki&gt;[&lt;/nowiki&gt;1;2,4&lt;nowiki&gt;]&lt;/nowiki&gt; and its two children are &lt;nowiki&gt;[&lt;/nowiki&gt;1;2,5&lt;nowiki&gt;]&lt;/nowiki&gt;&amp;nbsp;=&amp;nbsp;{{fraction|16|11}} (the right child) and &lt;nowiki&gt;[&lt;/nowiki&gt;1;2,3,2&lt;nowiki&gt;]&lt;/nowiki&gt;&amp;nbsp;=&amp;nbsp;{{fraction|23|16}} (the left child).

It is clear that for each finite continued fraction expression one can repeatedly move to its parent, and reach the root &lt;nowiki&gt;[&lt;/nowiki&gt;1;&lt;nowiki&gt;]&lt;/nowiki&gt;={{fraction|1|1}} of the tree in finitely many steps (in {{nowrap|''a''&lt;sub&gt;0&lt;/sub&gt; + ... + ''a''&lt;sub&gt;''k''&lt;/sub&gt; − 1}} steps to be precise). Therefore every positive rational number appears exactly once in this tree. Moreover all descendants of the left child of any number ''q'' are less than ''q'', and all descendants of the right child of ''q'' are greater than ''q''. The numbers at depth ''d'' in the tree are the numbers for which the sum of the continued fraction coefficients is {{nowrap|''d'' + 1}}.

==Mediants and binary search==

The Stern–Brocot tree forms an infinite [[binary search tree]] with respect to the usual ordering of the rational numbers.&lt;ref&gt;{{citation
 | last1 = Gibbons | first1 = Jeremy
 | last2 = Lester | first2 = David
 | last3 = Bird | first3 = Richard | author3-link = Richard Bird (computer scientist)
 | doi = 10.1017/S0956796806005880
 | issue = 3
 | journal = Journal of Functional Programming
 | pages = 281–291
 | title = Functional pearl: Enumerating the rationals
 | volume = 16
 | year = 2006}}.&lt;/ref&gt; The set of rational numbers descending from a node ''q'' is defined by the open interval (''L&lt;sub&gt;q&lt;/sub&gt;'',''H&lt;sub&gt;q&lt;/sub&gt;'') where ''L&lt;sub&gt;q&lt;/sub&gt;'' is the ancestor of ''q'' that is smaller than ''q'' and closest to it in the tree (or ''L&lt;sub&gt;q&lt;/sub&gt;''&amp;nbsp;=&amp;nbsp;0 if ''q'' has no smaller ancestor) while ''H&lt;sub&gt;q&lt;/sub&gt;'' is the ancestor of ''q'' that is larger than ''q'' and closest to it in the tree (or ''H&lt;sub&gt;q&lt;/sub&gt;''&amp;nbsp;=&amp;nbsp;+∞ if ''q'' has no larger ancestor).

The path from the root 1 to a number ''q'' in the Stern–Brocot tree may be found by a [[binary search]] algorithm, which may be expressed in a simple way using [[Mediant (mathematics)|mediants]]. Augment the non-negative rational numbers to including a value 1/0 (representing +∞) that is by definition greater than all other rationals. The [[binary search algorithm]] proceeds as follows:
*Initialize two values ''L'' and ''H'' to 0/1 and 1/0, respectively.
*Until ''q'' is found, repeat the following steps:
**Let ''L'' = ''a''/''b'' and ''H'' = ''c''/''d''; compute the mediant ''M''&amp;nbsp;=&amp;nbsp;(''a''&amp;nbsp;+&amp;nbsp;''c'')/(''b''&amp;nbsp;+&amp;nbsp;''d'').
**If ''M'' is less than ''q'', then ''q'' is in the open interval (''M'',''H''); replace ''L'' by ''M'' and continue.
**If ''M'' is greater than ''q'', then ''q'' is in the open interval (''L'',''M''); replace ''H'' by ''M'' and continue.
**In the remaining case, ''q'' = ''M''; terminate the search algorithm.
The sequence of values ''M'' computed by this search is exactly the sequence of values on the path from the root to ''q'' in the Stern–Brocot tree. Each open interval (''L'',''H'') occurring at some step in the search is the interval (''L&lt;sub&gt;M&lt;/sub&gt;'',''H&lt;sub&gt;M&lt;/sub&gt;'') representing the descendants of the mediant ''M''. The parent of ''q'' in the Stern–Brocot tree is the last mediant found that is not equal to ''q''.

This binary search procedure can be used to convert [[floating-point]] numbers into rational numbers. By stopping once the desired precision is reached, floating-point numbers can be approximated to arbitrary precision.&lt;ref&gt;Sedgewick and Wayne, ''Introduction to Programming in Java''. A Java implementation of this algorithm can be found [http://www.cs.princeton.edu/introcs/92symbolic/RationalApprox.java.html here].&lt;/ref&gt; If a real number ''x'' is approximated by any rational number ''a''/''b'' that is not in the sequence of mediants found by the algorithm above, then the sequence of mediants contains a closer approximation to ''x'' that has a denominator at most equal to ''b''; in that sense, these mediants form the [[best rational approximation]]s to ''x''.

The Stern–Brocot tree may itself be defined directly in terms of mediants: the left child of any number ''q'' is the mediant of ''q'' with its closest smaller ancestor, and the right child of ''q'' is the mediant of ''q'' with its closest larger ancestor. In this formula, ''q'' and its ancestor must both be taken in lowest terms, and if there is no smaller or larger ancestor then 0/1 or 1/0 should be used respectively. Again, using 7/5 as an example, its closest smaller ancestor is 4/3, so its left child is (4&amp;nbsp;+&amp;nbsp;7)/(3&amp;nbsp;+&amp;nbsp;5)&amp;nbsp;=&amp;nbsp;11/8, and its closest larger ancestor is 3/2, so its right child is (7&amp;nbsp;+&amp;nbsp;3)/(5&amp;nbsp;+&amp;nbsp;2)&amp;nbsp;=&amp;nbsp;10/7.

==Relation to Farey sequences==
The [[Farey sequence]] of order ''n'' is the sorted sequence of fractions in the closed interval [0,1] that have denominator less than or equal to ''n''. As in the binary search technique for generating the Stern–Brocot tree, the Farey sequences can be constructed using mediants: the Farey sequence of order ''n''&amp;nbsp;+&amp;nbsp;1 is formed from the Farey sequence of order ''n'' by computing the mediant of each two consecutive values in the Farey sequence of order ''n'', keeping the subset of mediants that have denominator exactly equal to ''n''&amp;nbsp;+&amp;nbsp;1, and placing these mediants between the two values from which they were computed.

A similar process of mediant insertion, starting with a different pair of interval endpoints [0/1,1/0], may also be seen to describe the construction of the vertices at each level of the Stern–Brocot tree. The '''Stern–Brocot sequence''' of order 0 is the sequence [0/1,1/0], and the Stern–Brocot sequence of order ''i'' is the sequence formed by inserting a mediant between each consecutive pair of values in the Stern–Brocot sequence of order ''i''&amp;nbsp;&amp;minus;&amp;nbsp;1. The Stern–Brocot sequence of order ''i'' consists of all values at the first ''i'' levels of the Stern–Brocot tree, together with the boundary values 0/1 and 1/0, in numerical order.

Thus the Stern–Brocot sequences differ from the Farey sequences in two ways: they eventually include all positive rationals, not just the rationals within the interval [0,1], and at the ''n''th step all mediants are included, not only the ones with denominator equal to ''n''. The Farey sequence of order ''n'' may be found by an inorder traversal of the left subtree of the Stern–Brocot tree, backtracking whenever a number with denominator greater than ''n'' is reached.

==Additional properties==

If &lt;math&gt;{ \frac{p_1}{q_1}, \frac{p_2}{q_2}, \dots, \frac{p_n}{q_n}  }&lt;/math&gt; are all the rationals at the same depth in the Stern–Brocot tree, then

: &lt;math&gt; \sum_{k=1}^n \frac{1}{p_kq_k} = 1&lt;/math&gt;.&lt;ref&gt;Bogomolny credits this property to Pierre Lamothe, a Canadian music theorist.&lt;/ref&gt;

Along with the definitions in terms of continued fractions and mediants described above, the Stern–Brocot tree may also be defined as a [[Cartesian tree]] for the rational numbers, prioritized by their denominators. That is, it is the unique binary search tree of the rational numbers in which the parent of any vertex ''q'' has a smaller denominator than ''q'' (or, if ''q'' and its parent are both integers, in which the parent is smaller than ''q''). It follows from the theory of Cartesian trees that the [[lowest common ancestor]] of any two numbers ''q'' and ''r'' in the Stern–Brocot tree is the rational number in the closed interval [''q'',&amp;nbsp;''r''] that has the smallest denominator among all numbers in this interval.

Permuting the vertices on each level of the Stern–Brocot tree by a [[bit-reversal permutation]] produces a different tree, the [[Calkin–Wilf tree]], in which the children of each number ''a''/''b'' are the two numbers ''a''/(''a''&amp;nbsp;+&amp;nbsp;''b'') and (''a''&amp;nbsp;+&amp;nbsp;''b'')/''b''. Like the Stern–Brocot tree, the Calkin–Wilf tree contains each positive rational number exactly once, but it is not a binary search tree.

==See also==
* [[Calkin–Wilf tree]]
* [[Minkowski's question mark function]], whose definition for rational arguments is closely related to the Stern–Brocot tree

==Notes==
{{reflist}}

==References==
*{{citation
 | last = Brocot | first = Achille
 | journal = Revue Chronométrique
 | pages = 186–194
 | title = Calcul des rouages par approximation, nouvelle méthode
 | volume = 3
 | year = 1861}}.
*{{citation
 | last = Stern | first = Moritz A. | author-link = Moritz Abraham Stern
 | journal = [[Crelle's Journal|Journal für die reine und angewandte Mathematik]]
 | pages = 193–220
 | title = Ueber eine zahlentheoretische Funktion
 | url = http://www.digizeitschriften.de/resolveppn/GDZPPN002150301
 | volume = 55
 | year = 1858}}.
* {{citation | last1=Berstel | first1=Jean | last2=Lauve | first2=Aaron | last3=Reutenauer | first3=Christophe | last4=Saliola | first4=Franco V. | title=Combinatorics on words. Christoffel words and repetitions in words | series=CRM Monograph Series | volume=27 | location=Providence, RI | publisher=[[American Mathematical Society]] | year=2009 | isbn=978-0-8218-4480-9 | zbl=1161.68043 }}

==External links==
*{{citation|first=Dhroova|last=Aiylam|url=http://arxiv.org/abs/1301.6807|title=Modified Stern-Brocot Sequences}}
*{{citation|first=David|last=Austin|url=http://www.ams.org/featurecolumn/archive/stern-brocot.html|title=Trees, Teeth, and Time: The mathematics of clock making|publisher=Feature Column from the AMS}}
*{{citation | first=Alexander |last=Bogomolny | authorlink = Alexander Bogomolny | title=Stern Brocot-Tree | url=http://www.cut-the-knot.org/blue/Stern.shtml | accessdate=2008-09-03 | publisher=[[cut-the-knot]]}}.
*{{citation | first = N. J. A. | last = Sloane | authorlink = Neil Sloane | url = http://oeis.org/stern_brocot.html | title = The Stern–Brocot or Farey Tree | publisher = [[On-line Encyclopedia of Integer Sequences]]}}.
*{{citation | first = Norman | last = Wildberger | url = https://www.youtube.com/watch?v=CiO8iAYC6xI | title = MF96: Fractions and the Stern-Brocot tree}}.
*{{MathWorld|urlname=Stern-BrocotTree|title=Stern–Brocot Tree}}
*{{PlanetMath|urlname=SternBrocotTree|title=Stern–Brocot tree}}

{{DEFAULTSORT:Stern-Brocot tree}}
[[Category:Continued fractions]]
[[Category:Number theory]]
[[Category:Trees (data structures)]]</text>
      <sha1>mctx4pr4mrhsx0w75g6q937qj3dld64</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>And–or tree</title>
    <ns>0</ns>
    <id>12379384</id>
    <revision>
      <id>607124953</id>
      <parentid>607123725</parentid>
      <timestamp>2014-05-05T05:54:12Z</timestamp>
      <contributor>
        <username>Widr</username>
        <id>13975403</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/59.145.160.238|59.145.160.238]] ([[User talk:59.145.160.238|talk]]) to last version by ClueBot NG</comment>
      <text xml:space="preserve" bytes="3841">An '''and–or tree''' is a [[graphical representation]] of the reduction of [[problem]]s (or goals) to [[Logical conjunction|conjunctions]] and [[disjunction]]s of subproblems (or subgoals).

==Example==
The and-or tree:

[[Image:Andortree.png|frameless|319px]]

represents the [[Candidate solution|search space]] for solving the problem P, using the goal-reduction methods:

:P if Q and R

:P if S

:Q if T

:Q if U

==Definitions==
Given an initial problem P0 and set of problem solving methods of the form:

:P if P1 and … and Pn

the associated and-or tree is a set of labelled nodes such that: 

# The root of the tree is a node labelled by P0.
# For every node N labelled by a problem or sub-problem P and for every method of the form P if P1 and … and Pn, there exists a set of children nodes N1, …, Nn of the node N, such that each node Ni is labelled by Pi. The nodes are conjoined by an arc, to distinguish them from children of N that might be associated with other methods.

A node N, labelled by a problem P, is a success node if there is a method of the form P if nothing (i.e., P is a &quot;fact&quot;). The node is a failure node if there is no method for solving P.

If all of the children of a node N, conjoined by the same arc, are success nodes, then the node N is also a success node. Otherwise the node is a failure node.

==Search strategies==

An and-or tree specifies only the search space for solving a problem. Different [[Tree traversal|search strategies]] for searching the space are possible. These include searching the tree depth-first, breadth-first, or best-first using some measure of desirability of solutions. The search strategy can be sequential, searching or generating one node at a time, or parallel, searching or generating several nodes in parallel.

==Relationship with logic programming==
The methods used for generating and-or trees are propositional [[Logic programming|logic programs]] (without variables). In the case of logic programs containing variables, the solutions of conjoint sub-problems must be compatible. Subject to this complication, sequential and parallel search strategies for and-or trees provide a computational model for executing logic programs.

==Relationship with two-player games==
And–or trees can also be used to represent the search spaces for two-person games. The root node of such a tree represents the problem of one of the players winning the game, starting from the initial state of the game. Given a node N, labelled by the problem P of the player winning the game from a particular state of play, there exists a single set of conjoint children nodes, corresponding to all of the opponents responding moves. 
For each of these children nodes, there exists a set of non-conjoint children nodes, corresponding to all of the player's defending moves.

For solving game trees with [[proof-number search]] family of algorithms, game trees are to be mapped to And/Or trees. MAX-nodes (i.e. maximizing player to move) are represented as OR nodes, MIN-nodes map to AND nodes. The mapping is possible, when the search is done with only a binary goal, which usually is &quot;player to move wins the game&quot;.

==Bibliography==
* {{cite book|last1=Luger|first1=George F.|last2=Stubblefield|first2=William A.|title=Artificial intelligence: structures and strategies for complex problem solving|url=http://books.google.com/books?id=eBg_7zpNqWkC|accessdate=28 February 2013|edition=2|year=1993|publisher=The Benjamin/Cummings|isbn=978-0-8053-4785-2}}
* {{cite book|last=Nilsson|first=Nils J.|title=Artificial Intelligence: A New Synthesis|url=http://books.google.com/books?id=LIXBRwkibdEC|accessdate=28 February 2013|year=1998|publisher=Morgan Kaufmann|isbn=978-1-55860-467-4}}

{{DEFAULTSORT:And-or tree}}
[[Category:Trees (data structures)]]
[[Category:Artificial intelligence]]</text>
      <sha1>h37345xt8dg7196s5z8jbaxyvyjckf9</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tree of virtues and tree of vices</title>
    <ns>0</ns>
    <id>7911759</id>
    <revision>
      <id>609504964</id>
      <parentid>599626128</parentid>
      <timestamp>2014-05-21T10:25:47Z</timestamp>
      <contributor>
        <username>Pjoef</username>
        <id>3804503</id>
      </contributor>
      <comment>Disambiguated: [[tree-diagram]] → [[tree structure]], [[fides]] → [[Faith]], [[superbia]] → [[Pride]]</comment>
      <text xml:space="preserve" bytes="5524">{{multiple image
| footer    =  A matching pair of a tree of vices and a tree of virtues, from a manuscript of ''[[Speculum Virginum]]''  (Walters Art Museum Ms. W.72, foll. 25v &amp; 26r, dated ca. 1200)
| align     = 
| image1    = Vices Speculum Virginum W72 25v.jpg
| width1    = 180
| alt1      = 
| caption1  =  
| link1     = File:Vices Speculum Virginum W72 25v.jpg
| image2    = Virtues Speculum Virginum W72 26r.jpg
| width2    = 180
| alt2      =  
| caption2  =  
| link2     = File:Virtues Speculum Virginum W72 26r.jpg
}}
A '''tree of virtues''' (''arbor virtutum'') is a diagram used in  [[medieval Christianity|medieval Christian]] tradition to display  the relationships between [[virtue]]s, usually juxtaposed with a '''tree of vices''' (''arbor vitiorum'') where the [[vice]]s are treated in a parallel fashion.
Together with  [[genealogical tree]]s,&lt;!--even earlier? genealogical trees certainly from the 14th century, e.g. File:Cod Crem 243 img01.jpg -- but these trees possibly from the 12th?--&gt; 
these diagrams qualify as among the earliest explicit [[tree structure|tree-diagrams]] in history, emerging in the [[High Middle Ages]].&lt;ref&gt;genealogical stemmata since at least the 11th century, the ''arbor virtutum'' since at least the 12th; see Nora Gädeke, ''Zeugnisse Bildlicher Darstellung Der Nachkommenschaft Heinrichs I'',  Arbeiten zur Frühmittelalterforschung 22 (1992), p. 2. For the possibility of a genealogiacal stemma of the early 10th century, see Nora Gädeke, ''Eine Karolingergenealogie des frühen 10. Jahrhunderts?'', Francia 15 (1987), [http://francia.digitale-sammlungen.de/Blatt_bsb00016291,00793.html 778-792]&lt;/ref&gt;

At first appearing as illustrations in certain theological tracts, the concept becomes more popular in the [[Late Middle Ages]] and is also seen in courtly manuscripts such as the psalter of Robert de Lisle (c. 1310-1340).

The nodes of the tree-diagrams are the [[Cardinal Virtues]] and the [[Cardinal Vices]], respectively, each with a number of secondary virtues or secondary vices shown as leaves of the respective nodes.
While on a tree of virtues, the leaves point upward toward heaven, on a tree of vices the leaves point downward toward hell.
At the root of the trees, the virtues of ''humilitas'' &quot;humility&quot; and the vice of ''superbia'' &quot;pride&quot; is shown as the origin of all other virtues and vices, respectively. 
By this time, the concept of showing hierarchical concepts of medieval philosophy in diagrams also becomes more widespread. E.g. ms. [[Bibliothèque de l'Arsenal|Arsenal]] 1037 (14th century)  has a tree of virtue on [http://gallica.bnf.fr/ark:/12148/btv1b52500993t/f18.image fol. 4v] and a tree of vices on [http://gallica.bnf.fr/ark:/12148/btv1b52500993t/f19.image fol. 5r] as part of a collection of diagrams on a variety of topics.&lt;ref&gt;''Septem hore canonice; Septem actus passionis Christi; Septem dona gratuita; Arbor virtutum; Arbor vicioru; Arbor sapientie; Duodecim prophete; Duodecim articuli fidei; Duodecim apostoli.''&lt;/ref&gt; In this example, the trees are also further subdivided into a ternary structure, as follows:
*''[[humilitas]] radix virtutum''
**I. ''[[prudentia]]'' (seven sub-virtues)
**II. ''[[fortitudo]]'' (seven sub-virtues)
**''semita vitalis''
***III. ''[[Justice (virtue)|iustitia]]'' (seven sub-virtues)
***IIII. ''[[temperantia]]'' (seven sub-virtues)
***''fructus spiritus''
****V. ''[[Faith|fides]]'' (seven sub-virtues)
****VI. ''[[spes]]'' (seven sub-virtues)
****VII. ''[[Charity (virtue)|caritas]]'' (seven sub-virtues)
*''[[Pride|superbia]] radix vitiorum''
**I. ''[[avaritia]]'' (seven sub-vices)
**II. ''[[invidia]]'' (seven sub-vices)
**''semita mortis''
***III. ''[[vanagloria|inanis gloria]]'' (seven sub-vices)
***IIII. ''[[wrath|ira]]'' (seven sub-vices)
***''fructus carnis''
****V. ''[[gluttony|gula]]'' (seven sub-vices)
****VI. ''[[acedia]]'' (seven sub-vices)
****VII. ''[[luxuria (vice)|luxuria]]'' (seven sub-vices)

In the [[Italian Renaissance]], [[Pietro Bembo]] developed a similar flow-chart-like &quot;moral schema&quot; of sins punished in [[Divine Comedy|Dante]]'s ''Inferno'' and ''Purgatory''.&lt;ref&gt;
in an early edition of Dante printed by Andrea Torresani (1451-1529), Venice 1515[http://www.italnet.nd.edu/Dante/text/1515.venice.html][http://www.italnet.nd.edu/Dante/images/tp1515/1515.wc2.150dpi.jpeg][http://www.italnet.nd.edu/Dante/images/tp1515/1515.wc3.150dpi.jpeg]&lt;/ref&gt;

==See also==
*[[Seven Virtues]]
*[[Seven Cardinal Sins]]
*[[Works of Mercy]]
*[[Tree of Jesse]]

==References==
{{Reflist}}
*[[Serenus de Cressy]], ''Arbor virtutum or, An exact modell in the which are represented all manner of vertues and graces''
*Michael W. Evans, &quot;§3.5 Arbor virtutum&quot; in ''The Geometry of the Mind'' (1980)[http://www.she-philosopher.com/gallery/gallery_cat07.html]
*A. Watson, &quot;The ''Speculum Virginum'' with Special Reference to the Tree of Jesse&quot;, ''Speculum'' 3.4, October 1928, 445-469.

== External links ==
* {{cite web |url=http://beinecke.library.yale.edu/speculum/3v-4r-virtues-and-vices.html |title=The Tree of Virtues &amp; The Tree of Vices |author= Matt Aleksinas |publisher=Yale University |year=2006 |accessdate=2008-06-03}}
* [http://www.paintedchurch.org/hessds.htm 15th-century painted mural 'Tree of vices'] as the seven-headed dragon of the ''Apocalypse of St. John'' (Rev. 12) at Anglican parish church of St Ethelbert’s, Hessett, Suffolk, England  
[[Category:Medieval culture]]
[[Category:Christian ethics]]
[[Category:Trees (data structures)]]
[[Category:Virtue]]</text>
      <sha1>n781loyny4d0x27338u7tn8tk1hypiz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Iacono's working set structure</title>
    <ns>0</ns>
    <id>38948929</id>
    <revision>
      <id>604784819</id>
      <parentid>557064657</parentid>
      <timestamp>2014-04-18T19:53:37Z</timestamp>
      <contributor>
        <username>Tredda</username>
        <id>21142753</id>
      </contributor>
      <comment>/* Structure */</comment>
      <text xml:space="preserve" bytes="8531">{| class=&quot;infobox&quot; style=&quot;width: 20em&quot;
! colspan=&quot;2&quot; style=&quot;font-size: 125%; text-align: center&quot; | Iacono's working set data structure
|-
! Invented
| 2001
|-
! Invented by
| [[John Iacono]]
|-
! colspan=&quot;2&quot; style=&quot;text-align: center; background-color: #CCCCFF; color: #000000;&quot; | Asymptotic complexity&lt;br /&gt;in [[big O notation]]
|-
! Space
| ''O''(''n'')
|-
! Search
| ''O''(log ''w(x)'')
|-
! Insert
| ''O''(log ''n'')
|-
! Delete
| ''O''(log ''n'')
|}

In computer science, Iacono's working set structure&lt;ref name=WorkingSetStructure /&gt;  is a comparison based [[associative array|dictionary]]. It supports insertion, deletion and access operation to maintain a dynamic set of &lt;math&gt;n&lt;/math&gt; elements. 
The working set of an item &lt;math&gt;x&lt;/math&gt; is the set of elements that have been accessed in the structure since the last time that &lt;math&gt;x&lt;/math&gt; was accessed (or inserted if it was never accessed).
Inserting and deleting in the working set structure takes &lt;math&gt;O(\log n)&lt;/math&gt; time while accessing an element &lt;math&gt;x&lt;/math&gt; takes &lt;math&gt;O(\log w(x))&lt;/math&gt;. Here, &lt;math&gt;w(x)&lt;/math&gt; represents the size of  the working set of &lt;math&gt;x&lt;/math&gt;.

==Structure==

[[File:Working Set DS.svg|thumbnail|An example of a search for &lt;math&gt;x&lt;/math&gt; in the working set structure. After finding &lt;math&gt;x&lt;/math&gt;, it is removed from &lt;math&gt;T_4&lt;/math&gt; and inserted into &lt;math&gt;T_1&lt;/math&gt;. Finally, a shift from 1 to 4 is performed in which an element is removed from &lt;math&gt;T_i&lt;/math&gt; and inserted into &lt;math&gt;T_{i+1}&lt;/math&gt; for &lt;math&gt;1\leq i &lt;4&lt;/math&gt;.]]
To store a dynamic set of &lt;math&gt;n&lt;/math&gt; elements, this structure consists of a series of ''[[Red–black tree]]s'', or other ''[[Self-balancing binary search tree]]s'' &lt;math&gt;T_1, T_2, \ldots, T_k&lt;/math&gt;, and a series of ''[[Double-ended queue|deques]]'' (Double-ended queues) &lt;math&gt;Q_1, Q_2, \ldots Q_k&lt;/math&gt;, where &lt;math&gt;k = \lceil \log\log n\rceil&lt;/math&gt;. For every &lt;math&gt;1\leq i\leq k&lt;/math&gt;, tree &lt;math&gt;T_i&lt;/math&gt; and deque &lt;math&gt;Q_i&lt;/math&gt; share the same contents and pointers are maintained between their corresponding elements. For every &lt;math&gt; i &lt; k &lt;/math&gt;, the size of &lt;math&gt;T_i&lt;/math&gt; and &lt;math&gt;Q_i&lt;/math&gt; is &lt;math&gt;2^{2^i}&lt;/math&gt;. Tree &lt;math&gt;T_k&lt;/math&gt; and deque &lt;math&gt;Q_k&lt;/math&gt; consist of the remaining elements, i.e.,  their size is &lt;math&gt;n - \sum_{i=1}^{k-1} 2^{2^i}&lt;/math&gt;. Therefore, the number of items in all trees and the number of elements in all deques both add up to &lt;math&gt;n&lt;/math&gt;.
Every element that has been inserted in the data structure is stored in exactly one of the trees and its corresponding deque.

==Working set Invariant==
In the deques of this structure, elements are kept in sorted order according to their working set size.
Formally, element &lt;math&gt;x&lt;/math&gt; lies after &lt;math&gt;y&lt;/math&gt; in deque &lt;math&gt;Q_i&lt;/math&gt; if and only if &lt;math&gt;w(x)&lt; w(y)&lt;/math&gt;. Moreover, for every &lt;math&gt;1\leq i &lt; k&lt;/math&gt;, the elements in deque &lt;math&gt;Q_i&lt;/math&gt; have a smaller working sets than the elements in deque &lt;math&gt;Q_{i+1}&lt;/math&gt;. This property is referred to as the Working set invariant. Every operation in the data structure maintains the Working set invariant.

==Operations==
The basic operation in this structure is called shift from &lt;math&gt;h&lt;/math&gt; to &lt;math&gt;j&lt;/math&gt;, where &lt;math&gt;h&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt; are indices of some trees in the structure. 
Two cases are considered in a shift from &lt;math&gt;h&lt;/math&gt; to &lt;math&gt;j&lt;/math&gt;: If &lt;math&gt;h&lt; j&lt;/math&gt;, then for every &lt;math&gt;h\leq i &lt; j&lt;/math&gt;, taken in increasing order, an item is dequeued from &lt;math&gt;Q_i&lt;/math&gt; and enqueued into &lt;math&gt;Q_{i+1}&lt;/math&gt;. The corresponding item is deleted from &lt;math&gt;T_i&lt;/math&gt; and inserted into &lt;math&gt;T_{i+1}&lt;/math&gt;. The running time of this operation is &lt;math&gt;O(\sum_{i=h}^{j} \log |T_i|) = O(\sum_{i=h}^{j} \log 2^{2^i}) = O(2^ j)&lt;/math&gt;. 
Analogously, if &lt;math&gt; j&lt; h&lt;/math&gt;, then for every &lt;math&gt;j &lt; i \leq h&lt;/math&gt;, taken in decreasing order, an item is dequeued from &lt;math&gt;Q_i&lt;/math&gt; and enqueued into &lt;math&gt;Q_{i-1}&lt;/math&gt;. The corresponding item is deleted from &lt;math&gt;T_i&lt;/math&gt; and inserted into &lt;math&gt;T_{i-1}&lt;/math&gt;. The running time of this operation is &lt;math&gt;O(\sum_{i=j}^{h} \log |T_i|) = O(\sum_{i=j}^{h} \log 2^{2^i}) = O(2^ h)&lt;/math&gt;. 
Regardless of the case, after a shift operation, the size of &lt;math&gt;T_h&lt;/math&gt; decreases by one whereas the size of &lt;math&gt;T_j&lt;/math&gt; increases by one.
Since that elements in the deques are sorted with respect to their working sets sizes, a shift operation maintains the Working set invariant.

===Search===
To search for an element &lt;math&gt;x&lt;/math&gt;, search for &lt;math&gt;x&lt;/math&gt; in &lt;math&gt;T_1, T_2, \ldots T_k&lt;/math&gt;, in increasing order, until finding a tree &lt;math&gt;T_j&lt;/math&gt; containing &lt;math&gt;x&lt;/math&gt;. If no tree is found, the search is unsuccessful. If &lt;math&gt;x&lt;/math&gt; is found, it is deleted from &lt;math&gt;T_j&lt;/math&gt; and then inserted into &lt;math&gt;T_1&lt;/math&gt;, i.e., it is moved to the front of the structure. The search finishes by performing a shift from &lt;math&gt;1&lt;/math&gt; to &lt;math&gt;j&lt;/math&gt; which restores the size of every tree and every deque to their size prior to the search operation.
The running time of this search is &lt;math&gt;O(\sum_{i=1}^{j} \log 2^{2^i}) = O(2^ j)&lt;/math&gt; if the search was successful, or &lt;math&gt;O(\sum_{i=j}^{k} \log 2^{2^i}) = O(2^k) = O(\log n)&lt;/math&gt; otherwise.
By the Working set property, every element in trees &lt;math&gt;T_1, T_2, \ldots, T_{j-1}&lt;/math&gt; belongs to the working set of &lt;math&gt;x&lt;/math&gt;. In particular, every element in &lt;math&gt;T_{j-1}&lt;/math&gt; belongs to the working set of &lt;math&gt;x&lt;/math&gt; and hence, &lt;math&gt;w(x) &gt; |T_{j-1}| = 2^{2^{j-1}}&lt;/math&gt;. Thus, the running time of a successful search is &lt;math&gt;O(2^j) = O(\log 2^{2^{j-1}}) = O(\log w(x))&lt;/math&gt;.

===Insert===
Inserting an element &lt;math&gt;x&lt;/math&gt; into the structure is performed by inserting &lt;math&gt;x&lt;/math&gt; into &lt;math&gt;T_1&lt;/math&gt; and enqueuing it into &lt;math&gt;Q_1&lt;/math&gt;. Insertion is completed by performing a shift from &lt;math&gt;1&lt;/math&gt; to &lt;math&gt;k&lt;/math&gt;. 
To avoid overflow, if &lt;math&gt;|T_k| = 2^{2^k}&lt;/math&gt; before the shift, i.e., if the last tree is full, then &lt;math&gt;k&lt;/math&gt; is incremented and a new empty &lt;math&gt;T_k&lt;/math&gt; and &lt;math&gt;Q_k&lt;/math&gt; is created. The running time of this operation is dominated by the shift from &lt;math&gt;1&lt;/math&gt; to &lt;math&gt;k&lt;/math&gt; whose running time is &lt;math&gt;O(2^k) = O(2^{\log\log n}) = O(\log n)&lt;/math&gt;.
Since element &lt;math&gt;x&lt;/math&gt;, whose working set is the smallest, is enqueued in &lt;math&gt;Q_1&lt;/math&gt;, the Working set invariant is preserved after the shift.

===Delete===
Deleting an element &lt;math&gt;x&lt;/math&gt; is done by searching for &lt;math&gt;x&lt;/math&gt; on each tree in the structure, in increasing order, until finding a tree &lt;math&gt;T_j&lt;/math&gt; that contains it (if non is found the deletion is unsuccessful). Item &lt;math&gt;x&lt;/math&gt; is deleted from &lt;math&gt;T_j&lt;/math&gt; and &lt;math&gt;Q_j&lt;/math&gt;. Finally, a shift from &lt;math&gt;k&lt;/math&gt; to &lt;math&gt;j&lt;/math&gt; maintains the size of &lt;math&gt;T_j&lt;/math&gt; equal to &lt;math&gt;2^{2^j}&lt;/math&gt;. The running time of this operation is &lt;math&gt;O(2^k) = O(\log n)&lt;/math&gt;. The working set invariant is preserved as deleting an element does not change the order of  the working set of the elements.

==Discussion==
''[[Splay tree]]''s are self adjusting search trees introduced by Sleator and Tarjan&lt;ref name=&quot;SleatorTarjan&quot; /&gt; in 1985. Using restructuring heuristic, splay trees are able to achieve insert and delete operations in &lt;math&gt;O(\log n)&lt;/math&gt; [[amortized analysis|amortized]] time, without storing any balance information at the nodes. Moreover, the Working Set Theorem for splay trees states that the cost to access an element in a splay tree is &lt;math&gt;O(\log w(x))&lt;/math&gt; [[amortized analysis|amortized]]. 
Iacono's workings set structure obtains the same running time for search, insert and delete in the worst-case. Therefore, offering an alternative to splay trees.

==References==
{{Reflist|refs=
&lt;ref name=WorkingSetStructure&gt;{{cite journal|last=Iacono|first=John|title=Alternatives to splay trees with O(log n) worst-case access times|journal=Proceedings of the twelfth annual ACM-SIAM symposium on Discrete algorithms|year=2001|pages=516–522| url= http://divespot.ca/~morin/teaching/5408/refs/i2001.pdf}}&lt;/ref&gt;

&lt;ref name=&quot;SleatorTarjan&quot;&gt;{{Citation |first1=Daniel D. |last1=Sleator |first2=Robert E. |last2=Tarjan |title=Self-Adjusting Binary Search Trees |url=http://www.cs.cmu.edu/~sleator/papers/self-adjusting.pdf |journal=Journal of the ACM ([[Association for Computing Machinery]]) |volume=32 |issue=3 |pages=652–686 |year= 1985 |doi=10.1145/3828.3835 }}&lt;/ref&gt;
}}



[[Category:Trees (data structures)]]</text>
      <sha1>5lgfmfpnel3bmcfkespr1dw5nko9oou</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tree kernel</title>
    <ns>0</ns>
    <id>40184815</id>
    <revision>
      <id>623009021</id>
      <parentid>567658117</parentid>
      <timestamp>2014-08-27T11:07:40Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>expand</comment>
      <text xml:space="preserve" bytes="1883">In [[machine learning]], a '''tree kernel''' is a [[Positive-definite kernel|kernel function]] that computes a similarity score between two [[Tree (graph theory)|trees]]. Such kernels find applications in [[natural language processing]], where they can be used for machine-learned [[parsing]]&lt;ref name=&quot;collins&quot;&gt;{{cite conference |authorlink1=Michael Collins (computational linguist) |last1=Collins |first1=Michael |first2=Nigel |last2=Duffy |title=Convolution kernels for natural language |conference=[[Conference on Neural Information Processing Systems|Advances in Neural Information Processing Systems]] |year=2001}}&lt;/ref&gt; or for the [[Statistical classification|classification]] of parse results (e.g. to distinguish different types of questions).&lt;ref name=&quot;zhang&quot;&gt;{{cite conference |last1=Zhang |first1=Dell |first2=Wee Sun |last2=Lee |title=Question classification using support vector machines |conference=SIGIR |year=2003}}&lt;/ref&gt;

An tree kernel can be defined by considering a [[feature vector|feature representation]] &lt;math&gt;\varphi(T)&lt;/math&gt; of parse trees {{mvar|T}} that includes one feature (dimension) for each possible fragment of a parse tree; a fragment is a fixed-size part of the tree, corresponding to a finite number of grammar rule applications.&lt;!-- This is very hard to explain in words; needs a picture of some parse trees. --&gt; The tree kernel is then the dot product &lt;math&gt;\varphi(T)^\mathsf{T} \varphi(U)&lt;/math&gt; for two trees {{mvar|T}} and {{mvar|U}}. It can be computed by means of a [[dynamic programming]] algorithm that does not need to construct the actual feature vectors.&lt;ref name=&quot;collins&quot;/&gt;&lt;ref name=&quot;zhang&quot;/&gt;

==References==
{{reflist}}

==See also==
* [[Data-oriented parsing]]
* [[Graph kernel]]

{{compu-sci-stub}}

[[Category:Kernel methods for machine learning]]
[[Category:Natural language processing]]
[[Category:Trees (data structures)]]</text>
      <sha1>lb03yoxr5j82kufmtwwa814xohhksib</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Prefix order</title>
    <ns>0</ns>
    <id>7006917</id>
    <revision>
      <id>570455830</id>
      <parentid>570454275</parentid>
      <timestamp>2013-08-27T22:14:10Z</timestamp>
      <contributor>
        <ip>190.111.246.15</ip>
      </contributor>
      <comment>Added categories</comment>
      <text xml:space="preserve" bytes="4181">In [[mathematics]], especially [[order theory]], a '''prefix ordered set''' generalizes the intuitive concept of a [[Tree (set theory)|tree]] by introducing the possibility of continuous progress and continuous branching. Natural prefix orders often occur when considering [[dynamical systems]] as a set of functions from ''time'' (a [[totally ordered set]]) to some [[phase space]]. In this case, the elements of the set are usually referred to as ''executions'' of the system.

The name ''prefix order'' stems from the prefix order on words, which is a special kind of [[substring]] relation and, because of its discrete character, a tree. 

== Formal definition ==

A '''prefix order''' is a [[binary relation]] &quot;≤&quot; over  a [[Set (mathematics)|set]] ''P'' which is [[antisymmetric relation|antisymmetric]], [[transitive relation|transitive]], [[reflexive relation|reflexive]], and '''downward total''', i.e., for all ''a'', ''b'', and ''c'' in ''P'', we have that:

*''a ≤ a'' (reflexivity);
*if ''a ≤ b'' and ''b ≤ a'' then ''a'' = ''b'' (antisymmetry);
*if ''a ≤ b'' and ''b ≤ c'' then ''a ≤ c'' (transitivity);
*if ''a ≤ c'' and ''b ≤ c'' then ''a ≤ b'' or ''b ≤ a'' (downward totality).

== Functions between prefix orders ==

While between partial orders it is usual to consider [[monotonic function|order preserving]] functions, the most important type of functions between prefix orders are so-called '''history preserving''' functions. Given a prefix ordered set ''P'', a '''history''' of a point ''p∈P'' is the (by definition totally ordered) set ''p- ≜ {q | q ≤ p}''. A function ''f : P → Q'' between prefix orders P and Q is then '''history preserving''' if and only if for every ''p∈P'' we find ''f(p-) = f(p)-''. Similarly, a '''future''' of a point ''p∈P'' is the (prefix ordered) set ''p+ ≜ {q | p ≤ q}'' and ''f'' is future preserving if for all ''p∈P'' we find ''f(p+) = f(p)+''.

Every history preserving function and every future preserving function is also order preserving, but not vice versa.
In the theory of dynamical systems, history preserving maps capture the intuition that the behavior in one system is a ''refinement'' of the behavior in another. Furthermore, functions that are history and future preserving [[surjective function|surjections]] capture the notion of [[bisimulation]] between systems, and thus the intuition that a given refinement is ''correct'' with respect to a specification.

The [[range]] of a history preserving function is always a [[prefix closed]] subset, where a subset ''S ⊆ P'' is '''prefix closed''' if for all ''s,t ∈ P'' with ''t∈S'' and ''s≤t'' we find ''s∈S''.

== Product and union ==

Taking history preserving maps as ''morphisms'' in the [[category theory|category]] of prefix orders, leads to a notion of product that is ''not'' the Cartesian product of the two orders (since the Cartesian product is not always a prefix orders). Instead, it leads to an ''arbitrary interleaving'' of the original prefix orders. The union of two prefix orders is the [[disjoint union]], as it is with partial orders.

== Isomorphism ==

Any bijective history preserving function is an [[order isomorphism]]. Furthermore, if for a given prefix ordered set ''P'' we construct the set ''P- ≜ { p- | p∈ P}'' we find that this set is prefix ordered by the subset relation ⊆, and furthermore, that the function ''max : P- → P'' is an isomorphism, where ''max(S)'' returns for each set ''S∈P-'' the maximum element in terms of the order on P (i.e. ''max(p-) ≜ p'').

==References==

* {{cite article|first=Pieter|last=Cuijpers|title=Prefix Orders as a General Model of Dynamics|journal=Proceedings of the 9th International Workshop on Developments in Computational Models (DCM)|pages=25-29|year=2013|url=http://www.dcm-workshop.org.uk/2013/dcm-2013-pre-proc.pdf#page=30}}

* {{cite article|first=Pieter|last=Cuijpers|title=The Categorical Limit of a Sequence of Dynamical Systems|journal=EPTCS 120 : Proceedings EXPRESS/SOS 2013|pages=78-92|year=2013|doi=10.4204/EPTCS.120.7}}

[[Category:Order theory]]
[[Category:Trees (data structures)]]
[[Category:Dynamical systems]]</text>
      <sha1>qwxo1l5b0um4cyrfj4h3pk43mebne3q</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>HAT-trie</title>
    <ns>0</ns>
    <id>41730843</id>
    <revision>
      <id>619864000</id>
      <parentid>619863553</parentid>
      <timestamp>2014-08-04T20:10:15Z</timestamp>
      <contributor>
        <username>Mattbierner</username>
        <id>11285742</id>
      </contributor>
      <text xml:space="preserve" bytes="4556">{{cleanup|reason=formatting problems|date=August 2014}}

The '''HAT-Trie''' is a type of [[radix trie]] that uses array nodes to collect individual [[Attribute-value pair|key–value pairs]] under radix nodes and hash buckets into an [[associative array]].  Unlike a simple [[hash table]], HAT-tries store key–value in an ordered collection. The original inventors are Nikolas Askitis and Ranjan Sinha who describe their HAT-Trie in an article published in Proc. Thirtieth Australasian Computer Science Conference (ACSC2007), Ballarat Australia. CRPIT, 62. Dobbie, G., Ed. ACS. 97-105.&lt;ref&gt;http://crpit.com/confpapers/CRPITV62Askitis.pdf HAT-trie: A Cache-conscious Trie-based Data Structure for Strings&lt;/ref&gt; Dr. Askitis shows that building and accessing the HAT-trie key/value collection is considerably faster than other sorted access methods and is comparable to the Array Hash which is an unsorted collection.&lt;ref&gt;Askitis, N. &amp; Zobel, J. (2005), Cache-conscious collision resolution for string hash tables, in ‘Proc. SPIRE String Processing and Information Retrieval Symp.’, Springer-Verlag, pp. 92–104&lt;/ref&gt;  This is due to the cache-friendly nature of the data structure which attempts to group access to data in time and space into the 64 byte cache line size of the modern CPU (see [[CPU Cache]]).

A new HAT-Trie starts out as a NULL pointer representing an empty node.  The first added key allocates the smallest array node and copies into it the key/value pair, which becomes the first root of the trie.  Each subsequent key/value pair is added to the initial array node until a maximum size is reached after which the node is burst by re-distributing its keys into a hash bucket with new underlying array nodes, one for each occupied hash slot in the bucket.  The hash bucket becomes the new root of the trie. The key strings are stored in the array nodes with a length encoding byte prefixed to the key value bytes.  The value associated with each key can be stored either in-line alternating with the key strings, or placed in a second array, e.g., memory immediately after and joined to the array node.&lt;ref&gt;Askitis, N. and Zobel, J. 2011. Redesigning the string hash table, burst trie, and BST to exploit cache. ACM J. Exp. Algor. 15, 1, Article 1.7 (January 2011)&lt;/ref&gt;

Once the trie has grown into its first hash bucket node, the hash bucket distributes new keys according to a [[hash function]] of the key value into array nodes contained underneath the bucket node.  Keys continue to be added until a maximum number of keys for a particular hash bucket node is reached.  The bucket contents are then re-distributed into a new radix node according to the stored key value's first character, which replaces the hash bucket node as the trie root&lt;ref&gt;Burst tries: a fast, efficient data structure for string keys ACM Trans. Inf. Syst., Vol. 20, No. 2. (April 2002), pp. 192-223, doi:10.1145/506309.506312 by Steffen Heinz, Justin Zobel, Hugh E. Williams&lt;/ref&gt; (e.g. see [[Burstsort]]&lt;ref&gt;Sinha, R. and Wirth, A. 2010. Engineering burstsort: Toward fast in-place string sorting. ACM J. Exp. Algor. 15, Article 2.5 (March 2010)&lt;/ref&gt;).  The existing keys and values contained in the hash bucket are each shortened by one character and placed under the new radix node in a set of new array nodes.

Sorted access to the collection is provided by enumerating keys into a cursor by branching down the radix trie to assemble the leading characters, ending at either a hash bucket or an array node.  Pointers to the keys contained in the hash bucket or array node are assembled into an array that is part of the cursor for sorting.  Since there is a maximum number of keys in a hash bucket or array node, there is a pre-set fixed limit to the size of the cursor at all points in time. After the keys for the hash bucket or array node are exhausted by get-next (or get-previous) (see [[Iterator]]) the cursor is moved into the next radix node entry and the process repeats.&lt;ref&gt;http://www.siam.org/meetings/alenex03/Abstracts/rsinha.pdf Cache-Conscious Sorting of Large Sets of Strings with Dynamic Tries&lt;/ref&gt;

== References ==
{{Reflist}}

== External links ==
* [http://code.google.com/p/hat-trie Reference HAT-Trie implementation in C]
* [http://ww2.cs.mu.oz.au/~jz/fulltext/acmtois02.pdf Burst Trie Paper]
* [http://goanna.cs.rmit.edu.au/~jz/fulltext/spire05.pdf Cache-Conscious Collision Resolution in String Hash Tables]
* [http://www.naskitis.com Benchmark comparisons of string access methods]

[[Category:Trees (data structures)]]</text>
      <sha1>mjzctfvfzngqzm9gg0wcn4ax3sktvud</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>K-D-B-tree</title>
    <ns>0</ns>
    <id>42442225</id>
    <revision>
      <id>608556864</id>
      <parentid>608380497</parentid>
      <timestamp>2014-05-14T15:26:46Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* Related Work */Task 5: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <text xml:space="preserve" bytes="10276">{{Orphan|date=April 2014}}

In [[computer science]], a '''K-D-B-tree''' (''k''-dimensional [[B-tree]]) is a tree data structure for subdividing a ''k''-dimensional search space. The aim of the '''K-D-B-tree''' is to provide the search efficiency of a balanced [[k-d tree]], while providing the block-oriented storage of a B-tree for optimizing external memory accesses.&lt;ref&gt;{{cite journal|last=Robinson|first=John|title=The K-D-B-Tree: A Search Structure for Large Multidimensional Dynamic Indexes|journal=Proceedings of the 1981 ACM SIGMOD international conference on Management of data|year=1981|pages=10–18|doi=10.1145/582318.582321|url=http://dl.acm.org/citation.cfm?id=582321|accessdate=Apr 8, 2014}}&lt;/ref&gt;

== Informal description ==

Much like the ''k''-d tree, a K-D-B-tree organizes points in ''k''-dimensional space, useful for tasks such as range-searching and multi-dimensional database queries. K-D-B-trees subdivide space into two subspaces by comparing elements in a single domain. Using a 2-D-B-tree (2-dimensional K-D-B-tree) as an example, space is subdivided in the same manner as a ''k''-d tree: using a point in just one of the domains, or axes in this case, all other values are either less than or greater than the current value, and fall to the left and right of the splitting plane respectively.

Unlike a ''k''-d tree, each half-space is not its own node. Instead, as in a B-tree, nodes in the K-D-B-tree are stored as pages and the tree stores a pointer to the root page.

== Structure ==

[[File:KBDtreeStructure.png|thumb|right|The basic structure of a K-D-B-tree.]]

The K-D-B-tree contains two types of pages:
* Region pages: A collection of ''(region, child)'' pairs containing a description of the bounding region along with a pointer to the child page corresponding to that region.
* Point pages: A collection of ''(point, location)'' pairs. In the case of databases, ''location'' may point to the index of the database record, while for points in ''k''-dimensional space, it can be seen as the point's coordinates in that space.

Page overflows occur when inserting an element into a K-D-B-tree results in the size of a node exceeding its optimal size. Since the purpose of the K-D-B-tree is to optimize external memory accesses like those from a hard-disk, a page is considered to have overflowed or be overfilled if the size of the node exceeds the external memory page size.

Throughout insertion/deletion operations, the K-D-B-tree maintains a certain set of properties:
* The graph is a multi-way tree. Region pages always point to child pages, and can not be empty. Point pages are the leaf nodes of the tree.
* Like a B-tree, the path length to the leaves of the tree is the same for all queries.
* The regions that make up a region page are disjoint.
* If the root is a region page the union of its regions is the entire search space.
* When the ''child'' of a ''(region, child)'' pair in a region page is also a region page, the union of all the regions in the child is ''region''.
* Conversely in the case above, if ''child'' is a point page, all points in ''child'' must be contained in ''region''.

== Operations on a K-D-B-tree ==

=== Queries ===

Queries on a K-D-B-tree are a range search over intervals in all domains or axes in the tree. This collection of intervals is called the ''query region''. In ''k''-space, the ''query region'' can be visualized as a bounding volume around some subspace in the entire ''k''-dimensional search space. A query can fall into one of three categories:
* Some intervals span an entire domain or axis, making the query a ''partial range'' query.
* Some intervals are points, the others full domains, and so the query is a ''partial match'' query.
* The intervals are all points, and so the bounding volume is also just a point. This is an ''exact match'' query.

==== Algorithm ====

# If the ''root'' of the tree is null, terminate, otherwise let ''page'' be ''root''.
# If ''page'' is a point page, return every ''point'' in a ''(point, location)'' pair that lies within the ''query region''.
# Otherwise, ''page'' is a region page, so for all ''(region, child)'' pairs where ''region'' and ''query region'' intersect, set ''page'' to be ''child'' and recurse from step 2.

=== Insertions ===

Since an insertion into a K-D-B-tree may require the splitting of a page in the case of a page overflow, it is important to first define the splitting operation.

==== Splitting algorithm ====

First, a region page is split along some plane to create two new region pages, the left and right pages. These pages are filled with the regions from the old region page, and the old region page is deleted. Then, for every (''region'', ''child'') in the original region page, remembering ''child'' is a page and ''region'' specifies an actual bounding region:
# If ''region'' lies entirely to the left of the splitting plane, add ''(region, child)'' to the left page.
# If ''region'' lies entirely to the right of the splitting plane, add ''(region, child)'' to the right page.
# Otherwise:
## Recursively split ''child'' by the splitting plane, resulting in the pages ''new_left_page'' and ''new_right_page''
## Split ''region'' by the splitting plane, resulting in ''left_region'' and ''right_region''
## Add ''(left_region, new_left_page)'' to the left page, and ''(right_region, new_right_page)'' to the right page.

==== Insertion algorithm ====

[[File:KDBTreeSplits.png|thumb|right|The importance of choosing the correct splitting domain.]]

Using the splitting algorithm, insertions of a new ''(point, location)'' pair can be implemented as follows:
# If the root page is null, simply make the root page a new point page containing ''(point, location)''
# If an exact match query on ''point'' to find the page that ''point' should be added to. If it already exists in the page, terminate.
# Add ''(point, location)'' to the page. If the page overflows, let ''page'' denote that page.
# Let ''old_page'' be equal to ''page''. Choose some element and a domain/axis to define a plane to split ''page'' by that results in two pages that will not also result in one of the pages being overfilled with the addition of a new point. Split ''page'' by the plane to make two new pages, ''new_left_page'' and ''new_right_page'', and two new regions ''left_region'' and ''right_region''.
# If ''page'' was the root page, go to step 6. Otherwise, ''page'' becomes the parent of ''page''. Replace ''(region, old_page)'' in ''page'' with ''(left_region, new_left_page)'' and ''(right_region, new_right_page)''. If ''page'' overflows, repeat step 4, otherwise terminate.
# Let ''left_region'' be the entire search space to the left of the splitting plane, and ''right_region'' be the search space to the right, resulting from the split in Step 4. Set the root page to be a page containing to the regions ''left_region'' and ''right_region''.

It is important to take care in the domain and element chosen to split ''page'' by, since it is desirable to try to balance the number of points on either side of the splitting plane. In some cases, a poor choice of splitting domain can result in undesirable splits. It is also possible that a page cannot be split by a certain domain.

=== Deletions ===

Deletions from a K-D-B-tree are incredibly simple if no minimum requirements are placed on storage utilization. Using an exact match query to find a ''(point, location)'' pair, we simply remove the record from the tree if it exists.

==== Reorganization algorithm ====

Since deletions can result in pages that contain very little data, it may be necessary to reorganize the K-D-B-tree to meet some minimum storage utilization criteria. The reorganization algorithm to be used when a page contains too little data is as follows:

# Let ''page'' be the parent of ''P'', containing ''(region, P)''.
# Find regions in ''page'' such that the regions are adjacent and the union of which forms a rectangular region. These regions are considered &quot;joinable&quot;. Let ''R'' denote the set of these regions.
# Merge the set ''R'' into one page ''S'', and if the ''S'' is overfull, repeatedly split until none of the resulting pages are overfull.
# Replace the set ''R'' of regions in ''page'' with the resulting pages from splitting ''S''.

=== Related Work ===

Like in a ''k''-d tree, updates in a K-D-B-tree may result in the requirement for the splitting of several nodes recursively. This is incredibly inefficient and can result in sub-optimal memory utilization as it may result in many near-empty leaves. Lomet and Salzberg proposed a structure called the [[hB-tree]] (holey brick tree) to improve performance of K-D-B-trees by limiting the splits that occur after an insertion to only one root-to-leaf path. This was achieved by storing regions not only as rectangles, but as rectangles with a rectangle removed from the center.&lt;ref&gt;{{cite journal|last=Lomet|first=David|author2=Betty Salzberg|title=The hB-tree: a multiattribute indexing method with good guaranteed performance|journal=ACM Transactions on Database Systems (TODS)|date=Dec 1990|volume=15|issue=4|pages=625–658|doi=10.1145/99935.99949|url=http://dl.acm.org/citation.cfm?id=99949|accessdate=Apr 8, 2014}}&lt;/ref&gt;

More recently, the Bkd-tree was proposed as a means to provide the fast queries and near 100% space utilization of a static K-D-B-tree. Instead of maintaining a single tree and re-balancing, a set of &lt;math&gt;\log_{2} (N/M)&lt;/math&gt; K-D-B-trees are maintained and rebuilt at regular intervals.&lt;ref&gt;{{cite journal|last=Procopiuc|first=Octavian|author2=Pankaj Agarwal |author3=Lars Arge |author4=Jeffrey Scott Vitter |title=Bkd-Tree: A Dynamic Scalable kd-Tree|journal=Advances in Spatial and Temporal Databases|year=2003|volume=2750|series=Lecture Notes in Computer Science|pages=46–65|doi=10.1007/978-3-540-45072-6_4|url=http://dx.doi.org/10.1007/978-3-540-45072-6_4|accessdate=Apr 8, 2014}}&lt;/ref&gt; In this case, &lt;math&gt;M&lt;/math&gt; is the size of the memory buffer in number of points.

=== References ===

{{Reflist}}

{{Data structures}}
{{CS-Trees}}

[[Category:Computer graphics data structures]]
[[Category:Trees (data structures)]]
[[Category:Geometric data structures]]
[[Category:Database index techniques]]
[[Category:Data types]]</text>
      <sha1>51vp2e5kow7oyeuvh9kg987r8dljtln</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Fractal tree index</title>
    <ns>0</ns>
    <id>41058483</id>
    <revision>
      <id>626836633</id>
      <parentid>615410989</parentid>
      <timestamp>2014-09-24T00:43:07Z</timestamp>
      <contributor>
        <ip>72.20.109.118</ip>
      </contributor>
      <comment>/* Upserts */</comment>
      <text xml:space="preserve" bytes="16695">{{Multiple issues|
  {{Notability|date=April 2014}}
  {{COI|date=April 2014}}
}}

{{Infobox data structure
|name=Fractal Tree index
|type=tree
|invented_by=[[Michael A. Bender]], [[Martin Farach-Colton]], [[Bradley C. Kuszmaul]]
|invented_year=2007
|
|space_avg=O(''N''/''B'')
|space_worst= O(''N''/''B'')
|search_worst= O(log&lt;sub&gt;''B''&lt;/sub&gt; ''N'')
|search_avg= O(log&lt;sub&gt;''B''&lt;/sub&gt; ''N'')
|insert_avg=  O(log&lt;sub&gt;''B''&lt;/sub&gt; ''N''/''B''&lt;sup&gt;''ε''&lt;/sup&gt;)
|insert_worst=  O(log&lt;sub&gt;''B''&lt;/sub&gt; ''N''/''B''&lt;sup&gt;''ε''&lt;/sup&gt;)
|delete_avg=  O(log&lt;sub&gt;''B''&lt;/sub&gt; ''N''/''B''&lt;sup&gt;''ε''&lt;/sup&gt;)
|delete_worst=  O(log&lt;sub&gt;''B''&lt;/sub&gt; ''N''/''B''&lt;sup&gt;''ε''&lt;/sup&gt;)
}}

In [[computer science]], a '''Fractal Tree index''' is a [[tree data structure]] that keeps data sorted and allows searches and sequential access in the same time as a [[B-tree]] but  with insertions and deletions that are asymptotically faster than a B-tree.  Like a B-tree, a Fractal Tree index is a generalization of a [[binary search tree]] in that a node can have more than two children.  Furthermore, unlike a B-tree, a Fractal Tree index has buffers at each node, which allow insertions, deletions and other changes to be stored in intermediate locations.  The goal of the buffers is to schedule disk writes so that each write performs a large amount of useful work, thereby avoiding the worst-case performance of B-trees, in which each disk write may change a small amount of data on disk.   Like a B-tree, Fractal Tree indexes are optimized for systems that read and write large blocks of data.  The Fractal Tree index has been commercialized in [[database]]s by [[Tokutek]].  Originally, it was implemented as a [[cache-oblivious lookahead array]],&lt;ref name=&quot;cola&quot;&gt;{{cite journal
|last=Bender
|first=M. A.
|first2=M.|last2=Farach-Colton |first3=J.|last3=Fineman |first4=Y.|last4=Fogel |first5=B.|last5=Kuszmaul |first6=J.|last6=Nelson
 |title=Cache-Oblivoius streaming B-trees
|journal=Proceedings of the 19th Annual ACM Symposium on Parallelism in Algorithms and Architectures
|pages=81–92
|publisher=ACM Press
|location=[[San Diego|CA]]
|date=June 2007
|url=http://supertech.csail.mit.edu/cacheObliviousBTree.html}}&lt;/ref&gt; but the current implementation is an extension of the B&lt;sup&gt;''ε''&lt;/sup&gt; tree.&lt;ref&gt;{{cite journal
|last=Brodal
|first=G.
|first2=R.|last2=Fagerberg
|title=Lower Bounds for External Memory Dictionaries
|journal=Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms
|pages=546–554
|publisher=ACM Press
|location=[[New York City|N.Y.]]
|date=Jan 2003
|url=http://www.madalgo.au.dk/~gerth/pub/soda03.html}}&lt;/ref&gt;  The B&lt;sup&gt;''ε''&lt;/sup&gt; is related to the Buffered Repository Tree.&lt;ref&gt;{{cite journal
|last=Buchsbaum
|first=A.
|first2=M.|last2=Goldwasswer |first3=S.|last3=Venkatasubramanian |first4=J.|last4=Westbrook
 |title=On External Memory Graph Traversal
|journal=Porceedings of the Eleventh Annual ACM-SIAM Symposium on Discrete Algorithms
|pages=859–860
|date=Jan 2000
|id = {{citeseerx|10.1.1.27.9904}} }}&lt;/ref&gt; The Buffered Repository Tree has degree 2, whereas the  B&lt;sup&gt;''ε''&lt;/sup&gt; tree has degree  B&lt;sup&gt;''ε''&lt;/sup&gt;.  The Fractal Tree index has also been used in a prototype [[filesystem]].&lt;ref name=&quot;tokufs&quot;&gt;{{cite journal
|last=Esmet
|first=J.
|first2=M.|last2=Bender |first3=M.|last3=Farach-Colton |first4=B.|last4=Kuszmaul
 |title=The TokuFS Streaming File System
|journal=Proceedings of the 4th USENIX Conference on Hot Topics in Storage and File Systems
|pages=14–14
|publisher=USENIX Association
|location=[[Boston|MA]]
|date=June 2007
|url=http://www.cs.sunysb.edu/~bender/newpub/2012-EsmetBeFa-HotStorage.pdf}}&lt;/ref&gt;  An [[open source]] implementation of the Fractal Tree index is available,&lt;ref name=&quot;ftisource&quot;&gt;[https://github.com/Tokutek Github Repository]&lt;/ref&gt; which demonstrates the implementation details outlined below.

== Overview ==

In Fractal Tree indexes, internal ([[Leaf node|non-leaf]]) nodes can have a variable number of child nodes within some pre-defined range. When data is inserted or removed from a node, its number of child nodes changes. In order to maintain the pre-defined range, internal nodes may be joined or split. Each internal node of a B-tree will contain a number of keys that is one less than its [[branching factor]]. The keys act as separation values which divide its [[subtree]]s. Keys in subtrees are stored in [[search tree]] order, that is, all keys in a  subtree are between the two bracketing values. In this regard, they are just like B-trees.

Fractal Tree indexes and B-trees both exploit the fact that when a node is fetched from storage, a block of memory, whose size is denoted by &lt;math&gt;B&lt;/math&gt;, is fetched.  Thus, nodes are tuned to be of size approximately  &lt;math&gt;B&lt;/math&gt;.  Since access to storage can dominate the running time of a data structure, the time-complexity of [[external memory algorithm]]s is dominated by the number of read/writes a data structure induces.  (See, e.g.,&lt;ref name=&quot;clrs&quot;&gt;
{{cite journal
|last=Cormen
|first=T.
|first2= C.E.|last2=Leiserson|first3=R.|last3=Rivest|first4=C.|last4=Stein
| edition = 2nd
 | isbn = 0-262-03293-7
 | publisher = [[MIT Press]] and [[McGraw-Hill]]
 | title = [[Introduction to Algorithms]]
 | year = 2001
}}&lt;/ref&gt; for the following analyses.)

In a B-tree, this means that the number of keys in a node is targeted to be enough to fill the node, with some variability for node splits and merges.  For the purposes of theoretical analysis, if &lt;math&gt;O(B)&lt;/math&gt; keys fit in a node, then the tree has depth &lt;math&gt;O(\log_B N)&lt;/math&gt;, and this is the I/O complexity of both searches and insertions.

Fractal Trees nodes use a smaller branching factor, say, of &lt;math&gt;\sqrt{B}&lt;/math&gt;.  The depth of the tree is then &lt;math&gt;O(\log_\sqrt{B} N) = O(\log_B N)&lt;/math&gt;, thereby matching the B-tree asymptotically.  The remaining space in each node is used to buffer insertions, deletion and updates, which we refer to in aggregate as messages.  When a buffer is full, it is flushed to the children in bulk.  There are several choices for how the buffers are flushed, all leading to similar I/O complexity.  Each message in a node buffer will be flushed to a particular child, as determined by its key.  Suppose, for concreteness, that messages are flushed that are heading to the same child, and that among the &lt;math&gt;\sqrt{B}+1&lt;/math&gt; children, we pick the one with the most messages.  Then there are at least &lt;math&gt;B/(\sqrt{B}+1) \approx \sqrt{B}&lt;/math&gt; messages that can be flushed to the child.  Each flush requires &lt;math&gt;O(1)&lt;/math&gt; flushes, and therefore the per-message cost of a flush is &lt;math&gt;O(1/\sqrt{B})&lt;/math&gt;.

Consider the cost of an insertion.  Each message gets flushed &lt;math&gt;O(\log_B N)&lt;/math&gt; times, and the cost of a flush is &lt;math&gt;O(1/\sqrt{B})&lt;/math&gt;.  Therefore, the cost of an insertion is &lt;math&gt;O((\log_B N)/\sqrt{B})&lt;/math&gt;.    Finally, note that the branching factor can vary, but for any branching factor &lt;math&gt;B^{\varepsilon}&lt;/math&gt;, the cost of a flush is &lt;math&gt;O(1/B^{1-\varepsilon})&lt;/math&gt;, thereby providing a smooth tradeoff between search cost, which depends on the depth of the search tree, and therefore the branching factor, versus the insertion time, which depends on the depth of the tree but more sensitively on the size of the buffer flushes.

== Comparisons with other External-Memory Indexes ==

This section compares Fractal Tree indexes with other external memory indexing data structures.  The theoretical literature on this topic is very large, so this discussion is limited to a comparison with popular data structures that are in use in databases and file systems.

=== B-trees ===

The search time of a B-tree is asymptotically the same as that of a Fractal Tree index.  However, a Fractal Tree index has deeper trees than a B-tree, and if each node were to require an I/O, say if the cache is cold, then a Fractal Tree index would induce more IO.  However, for many workloads most or all internal nodes of both B-trees and Fractal Tree indexes are already cached in RAM.  In this case, the cost of a search is dominated by the cost of fetching the leaf, which is the same in both cases.  Thus, for many workloads, Fractal Tree indexes can match B-trees in terms of search time.

Where they differ is on insertions, deletions and updates.  An insertion in a Fractal Tree index takes &lt;math&gt;O(\log_B N/\sqrt{B})&lt;/math&gt; whereas B-trees require &lt;math&gt;O(\log_B N)&lt;/math&gt;.  Thus, Fractal Tree indexes are faster than B-trees by a factor of &lt;math&gt;O(\sqrt{B})&lt;/math&gt;.  Since &lt;math&gt;B&lt;/math&gt; can be quite large, this yields a potential two-order-of-magnitude improvement in worst-case insertion times, which is observed in practice.    Both B-trees and Fractal Tree indexes can perform insertions faster in the best case.  For example, if keys are inserted in sequential order, both data structures achieve a &lt;math&gt;O(1/B)&lt;/math&gt; I/Os per insertion.   Thus, because the best and worst cases of B-trees differ so widely, whereas Fractal Tree indexes are always near their best case, the actual speedup that Fractal Tree indexes achieve over B-trees depends on the details of the workload.

=== Log-structured Merge-trees ===

[[Log-structured merge-tree]]s (LSMs) refer to a class of data structures which consists of two or more index structures of exponentially growing capacities.  When a tree at some level reaches its capacity, it is merged into the next bigger level.   The IO-complexity of an LSM depends on parameters such as the growth factor between levels and the data structure chosen at each levee, so in order to analyze the complexity of LSMs, we need to pick a specific version.  For comparison purposes, we select the version of LSMs that match Fractal Tree indexes on insertion performance.

Suppose an LSM is implemented via &lt;math&gt;O(\log_B N)&lt;/math&gt; B-trees, each of which has a capacity that is &lt;math&gt;\sqrt{B}&lt;/math&gt; larger than its predecessor.
The merge time depends on three facts: The sorted order of keys in an &lt;math&gt;N&lt;/math&gt;-item B-tree can be produced in &lt;math&gt;O(N/B)&lt;/math&gt; IOs; Two sorted lists of &lt;math&gt;N&lt;/math&gt; and &lt;math&gt;M&lt;/math&gt; items can be merged into a sorted list in &lt;math&gt;O((N+M)/B)&lt;/math&gt; IOs; and a B-tree of a sorted list of &lt;math&gt;N&lt;/math&gt; items can be built in &lt;math&gt;O(N/B)&lt;/math&gt; IOs.   When a tree overflows, it is merged into a tree whose size is &lt;math&gt;O(\sqrt{B})&lt;/math&gt; larger, therefore a level that holds &lt;math&gt;k&lt;/math&gt; items requires &lt;math&gt;O(k/\sqrt{B})&lt;/math&gt; IOs to merge.   An item may be merged once per level, giving a total time of &lt;math&gt;O((\log_B N)/\sqrt{B})&lt;/math&gt;, which matches the Fractal Tree index.

The query time is simply the B-tree query time at each level.  The query time into the  &lt;math&gt;i&lt;/math&gt;&lt;sup&gt;th&lt;/sup&gt; level is &lt;math&gt;O(\log_B B^{i/2}) = O(i)&lt;/math&gt;, since the &lt;math&gt;i&lt;/math&gt;&lt;sup&gt;th&lt;/sup&gt; level has capacity &lt;math&gt;B^{i/2}&lt;/math&gt;.  The total time is therefore &lt;math&gt;O(\log^2_B N)&lt;/math&gt;.  This is larger than both the B-tree and Fractal Tree indexes by a logarithmic factor.  In fact, although B-trees and Fractal Tree indexes are both on the optimal tradeoff curve between insertions and queries, LSMs are not.  They are incomparable with B-trees and are dominated by Fractal Tree indexes.

A few notes about LSMs: there are ways to make the queries faster.  For example, if only membership queries are required and no successor/predecessor/range queries are, then [[Bloom filters]] can be used to speed up queries.  Also, the growth factor between levels can be set to some other value, giving a range of insertion/query tradeoffs.   However, for every choice of insertion rate, the corresponding Fractal Tree index has faster queries.

=== B&lt;sup&gt;ε&lt;/sup&gt; Trees ===

The Fractal Tree index is a refinement of the B&lt;sup&gt;ε&lt;/sup&gt; tree.  Like a B&lt;sup&gt;ε&lt;/sup&gt; tree, it consists of nodes with keys and buffers and realizes the optimal insertion/query tradeoff.  The Fractal Tree index differs in including performance optimization and in extending the functionality.  Examples of improved functionality include [[ACID]] semantics.  B-tree implementations of ACID semantics typically involve locking rows that are involved in an active transactions.  Such a scheme works well in a B-tree because both insertions and queries involve fetching the same leaf into memory.  Thus, locking an inserted row does not incur an IO penalty.  However, in Fractal Tree indexes, insertions are messages, and a row may reside in more than one node at the same time.  Fractal Tree indexes therefore require a separate locking structure that is IO-efficient or resides in memory in order to implement the locking involved in implementing ACID semantics.

Fractal Tree indexes also have several performance optimizations.  First, buffers are themselves indexed in order to speed up searches.  Second, leaves are much larger than in B-trees, which allows for greater compression.  In fact, the leaves are chosen to be large enough that their access time is dominated by the bandwidth time, and therefore amortizes away the seek and rotational latency.  Large leaves are an advantage with large range queries but slow down point queries, which require accessing a small portion of the leaf.  The solution implemented in Fractal Tree indexes is to have large leaves that can be fetched as a whole for fast range queries but are broken into smaller pieces call basement nodes which can be fetched individually.  Accessing a basement node is faster than accessing a leaf, because of the reduced bandwidth time.  Thus the substructure of leaves in Fractal Tree indexes, as compared to B&lt;sup&gt;ε&lt;/sup&gt; trees allows both range and point queries to be fast.

== Messaging and Fractal Tree Indexes ==

Insertions, deletions and updates are inserted as message into buffers that make their way towards the leaves.  The messaging infrastructure can be exploited to implement a variety of other operations, some of which are discussed below.

=== Upserts ===

An [[upsert]] is a statement that inserts a row if it does not exist and updates it if it does.  In a B-tree, an upsert is implemented by first searching for the row and then implementing an insertion or an update, depending on the result of the search.  This requires fetching the row into memory if it is not already cached.  A Fractal Tree index can implement an upsert by inserting an special upsert message.  Such a message can, in theory, implement arbitrary pieces of code during the update.  In practice, four update operations are supported:

# x = constant
# x = x + constant (a generalized increment)
# x = x - constant (a generalized decrement)
# x = if(x=0,0,x-1) (a decrement with a floor at 0)

These correspond to the update operations used in LinkBench,&lt;ref name=&quot;link bench&quot;&gt;
{{cite journal
|last=Armstrong
|first=T.
|first2=V.|last2=Ponnekanti |first3=D.|last3=Borthakur |first4=M.|last4=Callaghan
 |title=LinkBench: A Database Benchmark Based on the Facebook Social Graph
|journal=Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data
|pages=1185–1196
|publisher=ACM Press
|location=[[New York|NY]]
|date=2013
}}&lt;/ref&gt; a benchmark proposed by Facebook.  By avoiding the initial search, upsert messages can improve the speed of upserts by orders of magnitude.

=== Schema Changes ===

So far, all message types have modified single rows.  However, broadcast messages, which are copied to all outgoing buffers, can modify all rows in a database.  For example, broadcast messages can be used to change the format of all rows in a database.  Although the total work required to change all rows is unchanged over the brute-force method of traversing the table, the latency is improved, since, once the message is injected into the root buffer, all subsequent queries will be able to apply the schema modification to any rows they encounter.  The schema change is immediate and the work is deferred to such a time when buffers overflow and leaves would have gotten updated anyway.

&lt;!--

== Block size ==

Every time a leaf is updated, a substantial fraction of it is rewritten.

=== Compression ===

=== SSDs ===

--&gt;

== Implementations ==

The Fractal Tree index has been implemented and commercialized by [[Tokutek]].  It is available as [[TokuDB]] as a storage engine for [[MySQL]] and [[MariaDB]], and as [[TokuMX]], a more complete integration with [[MongoDB]].  Fractal Tree indexes have also been used in  a prototype file system, [[TokuFS]].&lt;ref name=&quot;tokufs&quot; /&gt;

== References ==

{{reflist}}

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Fractal Tree indexes}}
[[Category:Trees (data structures)]]
[[Category:Database index techniques]]

{{Link GA|de}}</text>
      <sha1>bpunynnsrd7lf8r4f11znoi99wbcrqb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Ball tree</title>
    <ns>0</ns>
    <id>31877832</id>
    <revision>
      <id>624728361</id>
      <parentid>624158449</parentid>
      <timestamp>2014-09-08T22:20:57Z</timestamp>
      <contributor>
        <username>Mogism</username>
        <id>16902756</id>
      </contributor>
      <minor/>
      <comment>/* Construction */Cleanup/[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: acchieved → achieved using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="8647">{{About|the binary tree variant|a metric-tree involving multi-way splits|M-tree}}

In [[computer science]], a '''ball tree''', '''balltree''' or '''metric tree''',&lt;ref name=&quot;kibriya&quot;&gt;{{cite doi|10.1007/978-3-540-74976-9_16}}&lt;/ref&gt; is a [[space partitioning]] [[data structure]] for organizing points in a multi-dimensional space. The ball tree gets its name from the fact that it partitions data points into a nested set of hyperspheres known as &quot;balls&quot;. The resulting data structure has characteristics that make it useful for a number of applications, most notably [[nearest neighbor search]].

== Informal description ==
A ball tree is a [[binary tree]] in which every node defines a D-dimensional [[hypersphere]], or ball, containing a subset of the points to be searched. Each internal node of the tree partitions the data points into two disjoint sets which are associated with different balls. While the balls themselves may intersect, each point is assigned to one or the other ball in the partition according to its distance from the ball's center. Each leaf node in the tree defines a ball and all enumerates all data points inside that ball.

Each node in the tree defines the smallest ball that contains all data points in its subtree. This gives rise to the useful property that, for a given test point ''t'', the distance to any point in a ball ''B'' in the tree is greater than or equal to the distance from &lt;math&gt;t&lt;/math&gt; to the ball. &lt;!-- an explanation with a graphic might be nice here --&gt;  Formally:
&lt;ref name=&quot;liu&quot;&gt;{{cite journal|author=Liu, T.; Moore, A. and Gray, A. |year=2006|url=http://people.ee.duke.edu/~lcarin/liu06a.pdf |title=New Algorithms for Efficient High-Dimensional Nonparametric Classification|journal=Journal of Machine Learning Research|volume=7|pages=1135–1158|year= 2006}}&lt;/ref&gt;
 &lt;math&gt;
    D^{Node}(t) = 
        \begin{cases}
            max(|t - B.pivot| - B.radius, D^{B.parent}),
                &amp; \text{if }Node \neq Root \\
            max(|t - B.pivot| - B.radius, 0),
                &amp; \text{if }B = Root \\
        \end{cases}
&lt;/math&gt;
Where &lt;math&gt;D^{B}(t)&lt;/math&gt; is the minimum possible distance from any point in the ball ''B'' to some point ''t''.

Ball-trees are related to the [[M-tree]], but only support binary splits, whereas in the M-tree each level splits &lt;math&gt;m&lt;/math&gt; to &lt;math&gt;2m&lt;/math&gt; fold, thus leading to a less deep tree structure. The M-tree also keeps the distances from the parent node precomputed to speed up queries.

== Construction ==
A number of ball tree construction algorithms are available.&lt;ref name=r1&gt;Omohundro, Stephen M. (1989) [ftp://ftp.icsi.berkeley.edu/pub/techreports/1989/tr-89-063.pdf &quot;Five Balltree Construction Algorithms&quot;]&lt;/ref&gt; The goal of such an algorithm is to produce a tree that will efficiently support queries of the desired type (e.g. nearest-neighbor) efficiently in the average case. The specific criteria of an ideal tree will depend on the type of question being answered and the distribution of the underlying data. However, a generally applicable measure of an efficient tree is one that minimizes the total volume of its internal nodes. Given the varied distributions of real-world data sets, this is a difficult task, but there are several heuristics that partition the data well in practice. In general, there is a tradeoff between the cost of constructing a tree and the efficiency achieved by this metric. 
&lt;ref name=&quot;liu&quot; /&gt;

This section briefly describes the simplest of these algorithms. A more in-depth discussion of five algoriths was given by Stephen Omohundro.&lt;ref name=r1/&gt;

=== k-d Construction Algorithm ===
The simplest such procedure is termed the &quot;k-d Construction Algorithm&quot;, by analogy with the process used to construct [[k-d tree|k-d trees]]. This is an [[off-line algorithm]], that is, an algorithm that operates on the entire data set at once. The tree is built top-down by recursively splitting the data points into two sets. Splits are chosen along the single dimension with the greatest spread of points, with the sets partitioned by the median value of all points along that dimension. Finding the split for each internal node requires linear time in the number of samples contained in that node, yielding an algorithm with [[time complexity]] &lt;math&gt;O(n\, lg\, n)&lt;/math&gt;, where ''n'' is the number of data points.

==== Pseudocode ====
    '''function''' construct_balltree '''is'''
        '''input:''' 
            D, an array of data points
        '''output:''' 
            B, the root of a constructed ball tree
        '''if''' a single point remains '''then'''
            create a leaf B containing the single point in D
            return B
        '''else'''
            let c be the dimension of greatest spread
            let L,R be the sets of points lying to the left and right of the median along dimension c
            create B with two children: 
                B.pivot = c
                B.child1 = construct_balltree(L),
                B.child2 = construct_balltree(R)
            return B
        '''end if'''
    '''end function'''

== Nearest-neighbor search ==

An important application of ball trees is expediting [[nearest neighbor search]] queries, in which the objective is to find the k points in the tree that are closest to a given test point by some distance metric (e.g. [[Euclidean distance]]). A simple search algorithm, sometimes called KNS1, exploits the distance property of the ball tree. In particular, if the algorithm is searching the data structure with a test point ''t'', and has already seensome point ''p'' that is closest to ''t'' among the points have encountered so far, then any subtree whose ball is further from ''t'' than ''p'' can be ignored for the rest of the search.

=== Description ===

The ball tree nearest-neighbor algorithm examines nodes in depth-first order, starting at the root. During the search, the algorithm
maintains a max-first [[priority queue]] (often implemented with a [[Heap (data structure)|heap]]), denoted ''Q'' here, of the k nearest points encountered so far. At each node ''B'', it may perform one of three operations, before finally returning an updated version of the priority queue:

# If the distance from the test point ''t'' to the current node ''B'' is greater than the furthest point in ''Q'', ignore ''B'' and return ''Q''.
# If ''B'' is a leaf node, scan through every point enumerated in ''B'' and update the nearest-neighbor queue appropriately. Return the updated queue.
# If ''B'' is an internal node, call the algorithm recursively on ''B'''s two children, searching the child whose center is closer to ''t'' first. Return the queue after each of these calls has updated it in turn.

Performing the recursive search in the order described in point 3 above increases likelihood that the further child will be pruned 
entirely during the search. 

=== Pseudocode ===
    '''function''' knn_search '''is'''
        '''input:''' 
            t, the target point for the query
            k, the number of nearest neighbors of t to search for
            Q, max-first priority queue containing at most k points
            B, a node, or ball, in the tree
        '''output:''' 
            Q, containing the k nearest neighbors from within B
        '''if''' distance(t, B.pivot) ≥ distance(t, Q.first) '''then'''
            '''return''' Q unchanged
        '''else if''' B is a leaf node '''then'''
            '''for each''' point p in B '''do'''
                '''if''' distance(t, p) &lt; distance(t, Q.first) '''then'''
                    add p to Q
                    '''if''' size(Q) &gt; k '''then'''
                        remove the furthest neighbor from Q
                    '''end if'''
                '''end if'''
            '''repeat'''
        '''else'''
            let child1 be the child node closest to t
            let child2 be the child node furthest from t
            knn_search(t, k, Q, child1)
            knn_search(t, k, Q, child2)
        '''end if'''
    '''end function'''&lt;ref name=&quot;liu&quot; /&gt;

=== Performance ===
In comparison with several other data structures, ball trees have been shown to perform fairly well on 
the nearest-neighbor search problem, particularly as their number of dimensions grows.&lt;ref name=&quot;kibriya&quot; /&gt;&lt;ref name=&quot;kumar&quot;&gt;{{cite doi|10.1007/978-3-540-88688-4_27}}&lt;/ref&gt;
However, the best nearest-neighbor data structure for a given application will depend on the dimensionality, number of data points, and underlying structure of the data. 

==References==

{{reflist}}


[[Category:Trees (data structures)]]
[[Category:Learning in computer vision]]</text>
      <sha1>86fbcybb63qe5zctp3yip9vakl3rtjb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Descendant tree (group theory)</title>
    <ns>0</ns>
    <id>42823620</id>
    <revision>
      <id>621919680</id>
      <parentid>621023643</parentid>
      <timestamp>2014-08-19T14:26:34Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* Abelianization of type (''p'',''p'') */Added 1 doi to a journal cite using [[Project:AWB|AWB]] (10388)</comment>
      <text xml:space="preserve" bytes="44134">In mathematics, specifically [[group theory]],
a '''descendant tree''' is a [[Tree structure|hierarchical structure]]
for visualizing parent-descendant relations
between [[isomorphism class]]es of finite groups of prime power order &lt;math&gt;p^n&lt;/math&gt;,
for a fixed prime number &lt;math&gt;p&lt;/math&gt; and varying integer exponents &lt;math&gt;n\ge 0&lt;/math&gt;.
Such groups are briefly called ''finite'' [[p-group|''p-groups'']].
The ''vertices'' of a [[Tree (graph theory)|descendant tree]] are isomorphism classes of finite ''p''-groups.

Additionally to their ''order'' &lt;math&gt;p^n&lt;/math&gt;, finite ''p''-groups have two further related invariants,
the [[Nilpotent group|''nilpotency class'']] &lt;math&gt;c&lt;/math&gt; and the '''coclass''' &lt;math&gt;r=n-c&lt;/math&gt;.
It turned out that descendant trees of a particular kind,
the so-called '''pruned coclass trees''' whose infinitely many vertices share a common coclass &lt;math&gt;r&lt;/math&gt;,
reveal a repeating finite pattern.
These two crucial properties of '''finiteness''' and '''periodicity'''
admit a characterization of all members of the tree
by finitely many parametrized [[Presentation of a group|presentations]].
Consequently, descendant trees play a fundamental role
in the classification of finite ''p''-groups.
By means of kernels and targets of [[Artin transfer (group theory)#Structured descendant trees (SDTs)|Artin transfer homomorphisms]],
descendant trees can be endowed with additional structure.

==Definitions and terminology==
According to M. F. Newman
,&lt;ref name=&quot;Nm&quot;&gt;
{{cite journal|
author=Newman, M. F.|
year=1990| 
title=Groups of prime-power order|
journal=Groups – Canberra 1989, Lecture Notes in Mathematics|
publisher=Springer|
volume=1456|
pages=49–62
|doi=10.1007/bfb0100730}}
&lt;/ref&gt;
there exist several distinct definitions
of the '''parent''' &lt;math&gt;\pi(G)&lt;/math&gt;
of a finite ''p''-group &lt;math&gt;G&lt;/math&gt;.
The common principle is to form
the [[Quotient group|quotient]] &lt;math&gt;\pi(G)=G/N&lt;/math&gt; of &lt;math&gt;G&lt;/math&gt;
by a suitable [[normal subgroup]] &lt;math&gt;N\triangleleft G&lt;/math&gt;
which can be

{{EquationNote|1}}
:# either the [[Central series#Upper central series|centre]] &lt;math&gt;N=\zeta_1(G)&lt;/math&gt; of &lt;math&gt;G&lt;/math&gt;, whence &lt;math&gt;\pi(G)=G/\zeta_1(G)&lt;/math&gt; is called the ''central quotient'' of &lt;math&gt;G&lt;/math&gt;
:# or the last non-trivial term &lt;math&gt;N=\gamma_c(G)&lt;/math&gt; of the [[Central series#Lower central series|lower central series]] of &lt;math&gt;G&lt;/math&gt;, where &lt;math&gt;c&lt;/math&gt; denotes the nilpotency class of &lt;math&gt;G&lt;/math&gt;
:# or the last non-trivial term &lt;math&gt;N=P_{c-1}(G)&lt;/math&gt; of the [[Central series#Refined central series|lower exponent-p central series]] of &lt;math&gt;G&lt;/math&gt;, where &lt;math&gt;c&lt;/math&gt; denotes the exponent-''p'' class of &lt;math&gt;G&lt;/math&gt;
:# or the last non-trivial term &lt;math&gt;N=G^{(d-1)}&lt;/math&gt; of the [[Commutator subgroup#Derived series|derived series]] of &lt;math&gt;G&lt;/math&gt;, where &lt;math&gt;d&lt;/math&gt; denotes the derived length of &lt;math&gt;G&lt;/math&gt;.

In each case,
&lt;math&gt;G&lt;/math&gt; is called an '''immediate descendant''' of &lt;math&gt;\pi(G)&lt;/math&gt;
and a ''directed edge'' of the tree is defined either by &lt;math&gt;G\to\pi(G)&lt;/math&gt;
in the direction of the [[Quotient group|canonical projection]] &lt;math&gt;\pi:G\to\pi(G)&lt;/math&gt; onto the quotient &lt;math&gt;\pi(G)=G/N&lt;/math&gt;
or by &lt;math&gt;\pi(G)\to G&lt;/math&gt; in the opposite direction, which is more usual for descendant trees.
The former convention is adopted by C. R. Leedham-Green and M. F. Newman
,&lt;ref name=&quot;LgNm&quot;&gt;
{{cite journal|
author=Leedham-Green, C. R., Newman, M. F.|
year=1980|
title=Space groups and groups of prime power order I|
journal=Arch. Math.|
volume=35|
pages=193–203
|doi=10.1007/bf01235338}}
&lt;/ref&gt;
by M. du Sautoy and D. Segal
,&lt;ref name=&quot;dSSg&quot;&gt;
{{cite book|
author=du Sautoy, M., Segal, D.|
year=2000|
title=Zeta functions of groups|
publisher=pp. 249–286, in: New horizons in pro-''p'' groups, Progress in Mathematics, Vol. 184, Birkh&amp;auml;user, Basel}}
&lt;/ref&gt;
by C. R. Leedham-Green and S. McKay
,&lt;ref name=&quot;LgMk&quot;&gt;
{{cite book|
author=Leedham-Green, C. R., McKay, S.|
year=2002|
title=The structure of groups of prime power order|
publisher=London Mathematical Society Monographs, New Series, Vol. 27, Oxford University Press}}
&lt;/ref&gt;
and by B. Eick, C. R. Leedham-Green, M. F. Newman and E. A. O'Brien
.&lt;ref name=&quot;ELNO&quot;&gt;
{{cite journal|
author=Eick, B., Leedham-Green, C. R., Newman, M. F., O'Brien, E. A.|
year=2013|
title=On the classification of groups of prime-power order by coclass: the 3-groups of coclass 2|
journal=Int. J. Algebra Comput.|
volume=23|
number=5|
pages=1243–1288
|doi=10.1142/s0218196713500252}}
&lt;/ref&gt;
The latter definition is used by M. F. Newman
,&lt;ref name=&quot;Nm&quot; /&gt;
by M. F. Newman and E. A. O'Brien
,&lt;ref name=&quot;NmOb&quot;&gt;
{{cite journal|
author=Newman, M. F., O'Brien, E. A.|
year=1999|
title=Classifying 2-groups by coclass|
journal=Trans. Amer. Math. Soc.|
volume=351|
pages=131–169}}
&lt;/ref&gt;
by M. du Sautoy
,&lt;ref name=&quot;dS&quot;&gt;
{{cite journal| author=du Sautoy, M.|
year=2001| 
title=Counting p-groups and nilpotent groups|
journal=Inst. Hautes &amp;Eacute;tudes Sci. Publ. Math.|
volume=92|
pages=63–112}}
&lt;/ref&gt;
and by B. Eick and C. R. Leedham-Green
.&lt;ref name=&quot;EkLg&quot;&gt;
{{cite journal|
author=Eick, B., Leedham-Green, C. R.|
year=2008|
title=On the classification of prime-power groups by coclass|
journal=Bull. London Math. Soc.|
volume=40|
number=2|
pages=274–288
|doi=10.1112/blms/bdn007}}
&lt;/ref&gt;

In the following, the direction of the canonical projections is selected for all edges.
Then, more generally, a vertex &lt;math&gt;R&lt;/math&gt; is a '''descendant''' of a vertex &lt;math&gt;P&lt;/math&gt;,
and &lt;math&gt;P&lt;/math&gt; is an '''ancestor''' of &lt;math&gt;R&lt;/math&gt;,
if either &lt;math&gt;R&lt;/math&gt; is equal to &lt;math&gt;P&lt;/math&gt;
or there is a ''path'' &lt;math&gt;R=Q_0\to Q_1\to\cdots\to Q_{m-1}\to Q_m=P&lt;/math&gt;, with &lt;math&gt;m\ge 1&lt;/math&gt;, of directed edges from &lt;math&gt;R&lt;/math&gt; to &lt;math&gt;P&lt;/math&gt;.
The vertices forming the path necessarily coincide with the [[Tree (set theory)|''iterated parents'']] &lt;math&gt;Q_j=\pi^{j}(R)&lt;/math&gt; of &lt;math&gt;R&lt;/math&gt;, with &lt;math&gt;0\le j\le m&lt;/math&gt;.
In the most important special case (2.) of parents defined as last non-trivial lower central quotients,
they can also be viewed as the successive ''quotients'' &lt;math&gt;R/\gamma_{c+1-j}(R)&lt;/math&gt; ''of class'' &lt;math&gt;c-j&lt;/math&gt; of &lt;math&gt;R&lt;/math&gt;
when the nilpotency class of &lt;math&gt;R&lt;/math&gt; is given by &lt;math&gt;c\ge m&lt;/math&gt;.

Generally, the '''descendant tree''' &lt;math&gt;\mathcal{T}(G)&lt;/math&gt; of a vertex &lt;math&gt;G&lt;/math&gt; is the subtree of all descendants of &lt;math&gt;G&lt;/math&gt;, starting at the '''root''' &lt;math&gt;G&lt;/math&gt;.
The maximal possible descendant tree &lt;math&gt;\mathcal{T}(1)&lt;/math&gt; of the trivial group &lt;math&gt;1&lt;/math&gt; contains all finite ''p''-groups and is somewhat exceptional,
since, for any parent definition (1.–4.), the trivial group &lt;math&gt;1&lt;/math&gt; has infinitely many abelian ''p''-groups as its immediate descendants.
The parent definitions (2.–3.) have the advantage that any non-trivial finite ''p''-group (of order divisible by &lt;math&gt;p&lt;/math&gt;) possesses only finitely many immediate descendants.

==Pro-''p'' groups and coclass trees==
For a sound understanding of ''coclass trees'' as a particular instance of descendant trees,
it is necessary to summarize some facts concerning infinite [[Topological group|topological]] [[Pro-p group|pro-''p'' groups]].
The members &lt;math&gt;\gamma_j(S)&lt;/math&gt;, with &lt;math&gt;j\ge 1&lt;/math&gt;, of the lower central series of a pro-''p'' group &lt;math&gt;S&lt;/math&gt;
are closed subgroups of finite index, and therefore the corresponding quotients &lt;math&gt;S/\gamma_j(S)&lt;/math&gt; are finite ''p''-groups.
The pro-''p'' group &lt;math&gt;S&lt;/math&gt; is said to be of '''coclass''' &lt;math&gt;\mathrm{cc}(S)=r&lt;/math&gt;
when the limit &lt;math&gt;r=\lim_{j\to\infty}\,\mathrm{cc}(S/\gamma_j(S))&lt;/math&gt; of the coclass of the successive quotients exists and is finite.
An infinite pro-''p'' group &lt;math&gt;S&lt;/math&gt; of coclass &lt;math&gt;r&lt;/math&gt; is a ''p''-adic pre-[[space group]]
,&lt;ref name=&quot;ELNO&quot; /&gt;
since it has a normal subgroup &lt;math&gt;T&lt;/math&gt;, the ''translation group'',
which is a free module over the ring &lt;math&gt;\mathbb{Z}_p&lt;/math&gt; of ''p''-adic integers of uniquely determined rank &lt;math&gt;d&lt;/math&gt;, the ''dimension'',
such that the quotient &lt;math&gt;P=S/T&lt;/math&gt; is a finite ''p''-group, the ''point group'', which acts on &lt;math&gt;T&lt;/math&gt; [[Serial module|uniserially]].
The dimension is given by &lt;math&gt;d=(p-1)p^{s}&lt;/math&gt;, with some &lt;math&gt;0\le s&lt;r&lt;/math&gt;.

A central '''finiteness''' result for infinite pro-''p'' groups of coclass &lt;math&gt;r&lt;/math&gt; is provided by the so-called ''Theorem D'',
which is one of the five ''Coclass Theorems'' proved in 1994 independently by A. Shalev
&lt;ref name=&quot;Sh&quot;&gt;
{{cite journal|
author=Shalev, A.|
year=1994|
title=The structure of finite ''p''-groups: effective proof of the coclass conjectures|
journal=Invent. Math.|
volume=115|
pages=315–345
|doi=10.1007/bf01231763}}
&lt;/ref&gt;
and by C. R. Leedham-Green
,&lt;ref name=&quot;Lg&quot;&gt;
{{cite journal|
author=Leedham-Green, C. R.|
year=1994|
title=The structure of finite ''p''-groups|
journal=J. London Math. Soc.|
volume=50|
pages=49–67
|doi=10.1112/jlms/50.1.49}}
&lt;/ref&gt;
and conjectured in 1980 already by C. R. Leedham-Green and M. F. Newman.&lt;ref name=&quot;LgNm&quot; /&gt;
Theorem D asserts that there are only finitely many isomorphism classes of infinite pro-''p'' groups of coclass &lt;math&gt;r&lt;/math&gt;,
for any fixed prime &lt;math&gt;p&lt;/math&gt; and any fixed non-negative integer &lt;math&gt;r&lt;/math&gt;.
As a consequence, if &lt;math&gt;S&lt;/math&gt; is an infinite pro-''p'' group of coclass &lt;math&gt;r&lt;/math&gt;, then
there exists a minimal integer &lt;math&gt;i\ge 1&lt;/math&gt; such that the following three conditions are satisfied for any integer &lt;math&gt;j\ge i&lt;/math&gt;.
:*&lt;math&gt;\mathrm{cc}(S/\gamma_j(S))=r&lt;/math&gt;,
:*&lt;math&gt;S/\gamma_j(S)&lt;/math&gt; is not a lower central quotient of any infinite pro-''p'' group of coclass &lt;math&gt;r&lt;/math&gt; which is not isomorphic to &lt;math&gt;S&lt;/math&gt;,
:*&lt;math&gt;\gamma_j/\gamma_{j+1}(S)&lt;/math&gt; is cyclic of order &lt;math&gt;p&lt;/math&gt;.
The descendant tree &lt;math&gt;\mathcal{T}(R)&lt;/math&gt;, with respect to the parent definition (2.),
of the root &lt;math&gt;R=S/\gamma_i(S)&lt;/math&gt; with minimal &lt;math&gt;i&lt;/math&gt; is called the '''coclass tree''' &lt;math&gt;\mathcal{T}(S)&lt;/math&gt; of &lt;math&gt;S&lt;/math&gt;
and its unique maximal infinite (reverse-directed) path &lt;math&gt;R=S/\gamma_i(S)\leftarrow S/\gamma_{i+1}(S)\leftarrow\cdots&lt;/math&gt; is called the '''mainline''' (or ''trunk'') of the tree.

[[File:Terminology of trees.png|thumb|alt=treediagram|Figure 1: A descendant tree. The branches B(2),B(4) have depth 0, and B(5),B(7), resp. B(6),B(8), are isomorphic as trees.]]

==Tree diagram==
Further terminology, used in diagrams visualizing descendant trees, is explained in Figure 1 by means of an artificial abstract tree.
On the left hand side, a ''level'' indicates the basic top-down design of a descendant tree.
For concrete trees, such as those in Figure 2,3 etc., the level is usually replaced by a scale of orders increasing from the top to the bottom.
A vertex is '''capable''' (or ''extendable'') if it has at least one immediate descendant, otherwise it is '''terminal''' (or a ''leaf'').
Vertices sharing a common parent are called '''siblings'''.

If the descendant tree is a coclass tree &lt;math&gt;\mathcal{T}(R)&lt;/math&gt; with root &lt;math&gt;R=R_0&lt;/math&gt;
and with mainline vertices &lt;math&gt;(R_n)_{n\ge 0}&lt;/math&gt; labelled according to the level &lt;math&gt;n&lt;/math&gt;,
then the finite subtree defined as the difference set &lt;math&gt;\mathcal{B}(n)=\mathcal{T}(R_n)\setminus\mathcal{T}(R_{n+1})&lt;/math&gt;
is called the '''''n''th branch''' (or ''twig'') of the tree or also the ''branch'' &lt;math&gt;\mathcal{B}(R_n)&lt;/math&gt; ''with root'' &lt;math&gt;R_n&lt;/math&gt;, for any &lt;math&gt;n\ge 0&lt;/math&gt;.
The '''depth''' of a branch is the maximal length of the paths connecting its vertices with its root.
If all vertices of depth bigger than a given integer &lt;math&gt;k\ge 0&lt;/math&gt; are removed from branch &lt;math&gt;\mathcal{B}(n)&lt;/math&gt;,
then we obtain the (depth-)'''pruned branch''' &lt;math&gt;\mathcal{B}_k(n)&lt;/math&gt;.
Correspondingly, the '''pruned coclass tree''' &lt;math&gt;\mathcal{T}_k(R)&lt;/math&gt;, resp. the entire coclass tree &lt;math&gt;\mathcal{T}(R)&lt;/math&gt;,
consists of the infinite sequence of its pruned branches &lt;math&gt;(\mathcal{B}_k(n))_{n\ge 0}&lt;/math&gt;, resp. branches &lt;math&gt;(\mathcal{B}(n))_{n\ge 0}&lt;/math&gt;,
connected by the mainline, whose vertices &lt;math&gt;R_n&lt;/math&gt; are called '''infinitely capable'''.

==Virtual periodicity==
The periodicity of branches of depth-pruned coclass trees
has been proved with [[Mathematical analysis|analytic methods]] using zeta functions
&lt;ref name=&quot;dSSg&quot; /&gt;
of groups by M. du Sautoy
,&lt;ref name=&quot;dS&quot; /&gt;
and with [[Algebra|algebraic techniques]] using [[Cohomology|cohomology groups]] by B. Eick and C. R. Leedham-Green
.&lt;ref name=&quot;EkLg&quot; /&gt;
The former methods admit the qualitative insight of ''ultimate virtual periodicity'',
the latter techniques determine the quantitative structure:

For any infinite pro-''p'' group &lt;math&gt;S&lt;/math&gt; of coclass &lt;math&gt;r\ge 1&lt;/math&gt; and dimension &lt;math&gt;d&lt;/math&gt;,
and for any given depth &lt;math&gt;k\ge 1&lt;/math&gt;,
there exists an effective minimal lower bound &lt;math&gt;f(k)\ge 1&lt;/math&gt;,
where '''periodicity of length''' &lt;math&gt;d&lt;/math&gt; of pruned branches of the coclass tree &lt;math&gt;\mathcal{T}(S)&lt;/math&gt; sets in,
that is, there exist graph isomorphisms &lt;math&gt;\mathcal{B}_k(n+d)\simeq\mathcal{B}_k(n)&lt;/math&gt; for all &lt;math&gt;n\ge f(k)&lt;/math&gt;.

These central results can be expressed ostensively:
When we look at a coclass tree through a pair of blinkers
and ignore a finite number of pre-periodic branches at the top,
then we shall see a repeating finite pattern ('''ultimate''' periodicity).
However, if we take wider blinkers
the pre-periodic initial section may become longer ('''virtual''' periodicity).

The vertex &lt;math&gt;P=R_{f(k)}&lt;/math&gt; is called the '''periodic root''' of the pruned coclass tree, for a fixed value of the depth &lt;math&gt;k&lt;/math&gt;.

==Multifurcation and coclass graphs==
Assume that parents of finite ''p''-groups are defined as last non-trivial lower central quotients (2.).
For a ''p''-group &lt;math&gt;G&lt;/math&gt; of coclass &lt;math&gt;\mathrm{cc}(G)=r&lt;/math&gt;,
we can distinguish its (entire) descendant tree &lt;math&gt;\mathcal{T}(G)&lt;/math&gt;
and its '''coclass'''-&lt;math&gt;r&lt;/math&gt; '''descendent tree''' &lt;math&gt;\mathcal{T}^r(G)&lt;/math&gt;,
the subtree consisting of descendants of coclass &lt;math&gt;r&lt;/math&gt; only.
The group &lt;math&gt;G&lt;/math&gt; is '''coclass settled''' if &lt;math&gt;\mathcal{T}(G)=\mathcal{T}^r(G)&lt;/math&gt;.

The '''nuclear rank''' &lt;math&gt;\nu(G)&lt;/math&gt; of &lt;math&gt;G&lt;/math&gt;
in the theory of the [[P-group generation algorithm|''p''-group generation algorithm]] by M. F. Newman
&lt;ref name=&quot;Nm2&quot;&gt;
{{cite book|
author=Newman, M. F.|
year=1977|
title=Determination of groups of prime-power order|
publisher=pp. 73-84, in: Group Theory, Canberra, 1975, Lecture Notes in Math., Vol. 573, Springer, Berlin}}
&lt;/ref&gt;
and E. A. O'Brien
&lt;ref name=&quot;Ob&quot;&gt;
{{cite journal|
author=O'Brien, E. A.|
year=1990|
title=The ''p''-group generation algorithm|
journal=J. Symbolic Comput.|
volume=9|
pages=677–698
|doi=10.1016/s0747-7171(08)80082-x}}
&lt;/ref&gt;
provides the following criteria.

:*&lt;math&gt;G&lt;/math&gt; is terminal (and thus trivially coclass settled) if and only if &lt;math&gt;\nu(G)=0&lt;/math&gt;.
:*If &lt;math&gt;\nu(G)=1&lt;/math&gt;, then &lt;math&gt;G&lt;/math&gt; is capable. (But it remains unknown whether &lt;math&gt;G&lt;/math&gt; is coclass settled.)
:*If &lt;math&gt;\nu(G)=m\ge 2&lt;/math&gt;, then &lt;math&gt;G&lt;/math&gt; is capable but not coclass settled.

In the last case, a more precise assertion is possible:
If &lt;math&gt;G&lt;/math&gt; has coclass &lt;math&gt;r&lt;/math&gt; and nuclear rank &lt;math&gt;\nu(G)=m\ge 2&lt;/math&gt;, then it gives rise to
an '''''m''-fold multifurcation'''
into a '''regular''' coclass-''r'' descendant '''tree''' &lt;math&gt;\mathcal{T}^r(G)&lt;/math&gt;
and &lt;math&gt;m-1&lt;/math&gt; '''irregular''' descendant '''trees''' &lt;math&gt;\mathcal{T}^{r+j}(G)&lt;/math&gt; of coclass &lt;math&gt;r+j&lt;/math&gt;,
for &lt;math&gt;1\le j\le m-1&lt;/math&gt;.
Consequently, the descendant tree of &lt;math&gt;G&lt;/math&gt; is the disjoint union &lt;math&gt;\mathcal{T}(G)=\dot{\cup}_{j=0}^{m-1}\,\mathcal{T}^{r+j}(G)&lt;/math&gt;.

Multifurcation is correlated with different orders of the last non-trivial lower central of immediate descendants.
Since the nilpotency class increases exactly by a unit, &lt;math&gt;c=\mathrm{cl}(Q)=\mathrm{cl}(P)+1&lt;/math&gt;, from a parent &lt;math&gt;P=\pi(Q)&lt;/math&gt; to any immediate descendant &lt;math&gt;Q&lt;/math&gt;,
the coclass remains stable, &lt;math&gt;r=\mathrm{cc}(Q)=\mathrm{cc}(P)&lt;/math&gt;, if &lt;math&gt;\vert\gamma_c(Q)\vert=p&lt;/math&gt;.
In this case, &lt;math&gt;Q&lt;/math&gt; is a '''regular''' immediate '''descendant''' with directed edge &lt;math&gt;P\leftarrow Q&lt;/math&gt; of depth 1 (as usual).
However, the coclass increases by &lt;math&gt;m-1&lt;/math&gt;, if &lt;math&gt;\vert\gamma_c(Q)\vert=p^m&lt;/math&gt; with &lt;math&gt;m\ge 2&lt;/math&gt;.
Then &lt;math&gt;Q&lt;/math&gt; is called an '''irregular''' immediate '''descendant''' with directed '''edge of depth''' &lt;math&gt;m&lt;/math&gt;.

If the condition of depth (or ''step size'') 1 is imposed on all directed edges, then the maximal descendant tree &lt;math&gt;\mathcal{T}(1)&lt;/math&gt; of the trivial group &lt;math&gt;1&lt;/math&gt;
splits into a countably infinite disjoint union &lt;math&gt;\dot{\cup}_{r=0}^\infty\,\mathcal{G}(p,r)&lt;/math&gt; of directed '''coclass graphs''' &lt;math&gt;\mathcal{G}(p,r)&lt;/math&gt;,
which are rather ''forests'' than trees.
More precisely, the above mentioned Coclass Theorems imply that &lt;math&gt;\mathcal{G}(p,r)=\left(\dot{\cup}_i\,\mathcal{T}(S_i)\right)\dot{\cup}\mathcal{G}_0(p,r)&lt;/math&gt; is the disjoint union of
''finitely many'' coclass trees &lt;math&gt;\mathcal{T}(S_i)&lt;/math&gt; of (pairwise non-isomorphic) infinite pro-''p'' groups &lt;math&gt;S_i&lt;/math&gt; of coclass &lt;math&gt;r&lt;/math&gt; (Theorem D)
and a ''finite'' subgraph &lt;math&gt;\mathcal{G}_0(p,r)&lt;/math&gt; of '''sporadic groups''' lying outside of any coclass tree.

==Identifiers==
The '''SmallGroups''' Library '''identifiers''' of finite groups, in particular ''p''-groups, given in the form
&lt;math&gt;\langle\text{order},\text{counting number}\rangle&lt;/math&gt;
in the following concrete examples of descendant trees,
are due to H. U. Besche, B. Eick and E. A. O'Brien
.&lt;ref name=&quot;BEO&quot;&gt;
{{cite book|
author=Besche, H. U., Eick, B., O'Brien, E. A.|
year=2005|
title=The SmallGroups Library – a library of groups of small order|
publisher=An accepted and refereed GAP 4 package, available also in MAGMA}}
&lt;/ref&gt;
&lt;ref name=&quot;BEO2&quot;&gt;
{{cite journal|
author=Besche, H. U., Eick, B., O'Brien, E. A.|
year=2002|
title=A millennium project: constructing small groups|
journal=Int. J. Algebra Comput.|
volume=12|
pages=623–644
|doi=10.1142/s0218196702001115}}
&lt;/ref&gt;
When the group orders are given in a scale on the left hand side as in Figure 2 and Figure 3,
the identifiers are briefly denoted by &lt;math&gt;\langle\text{counting number}\rangle&lt;/math&gt;.

Depending on the prime &lt;math&gt;p&lt;/math&gt;, there is an upper bound on the order of groups for which a SmallGroup identifier exists,
e. g. &lt;math&gt;512=2^9&lt;/math&gt; for &lt;math&gt;p=2&lt;/math&gt;, and &lt;math&gt;2187=3^7&lt;/math&gt; for &lt;math&gt;p=3&lt;/math&gt;.
For groups of bigger orders, a notation resembling the descendant structure is employed:
A regular immediate descendant, connected by an edge of depth &lt;math&gt;1&lt;/math&gt; with its parent &lt;math&gt;P&lt;/math&gt;, is denoted by &lt;math&gt;P-\#1;\text{counting number}&lt;/math&gt;, and
an irregular immediate descendant, connected by an edge of depth &lt;math&gt;d\ge 2&lt;/math&gt; with its parent &lt;math&gt;P&lt;/math&gt;, is denoted by &lt;math&gt;P-\#d;\text{counting number}&lt;/math&gt;.

==Concrete examples==
In all examples, the underlying parent definition (2.) corresponds to the usual lower central series.
Occasional differences to the parent definition (3.) with respect to the lower exponent-''p'' central series are pointed out.

===Coclass 0===
The coclass graph &lt;math&gt;\mathcal{G}(p,0)=\mathcal{G}_0(p,0)&lt;/math&gt; of finite ''p''-groups of coclass &lt;math&gt;0&lt;/math&gt; does not contain a coclass tree and consists of the ''trivial group'' &lt;math&gt;1&lt;/math&gt; and the ''cyclic group'' &lt;math&gt;C_p&lt;/math&gt; of order &lt;math&gt;p&lt;/math&gt;, which is a leaf (however, it is capable with respect to the lower exponent-''p'' central series).
For &lt;math&gt;p=2&lt;/math&gt; the ''SmallGroup identifier'' of &lt;math&gt;C_p&lt;/math&gt; is &lt;math&gt;\langle 2,1\rangle&lt;/math&gt;,
for &lt;math&gt;p=3&lt;/math&gt; it is &lt;math&gt;\langle 3,1\rangle&lt;/math&gt;.

[[File:TreeOf2Groups.png|thumb|alt=2-groups|Figure 2: The coclass graph of finite 2-groups with coclass 1]]

===Coclass 1===
The coclass graph &lt;math&gt;\mathcal{G}(p,1)=\mathcal{T}^1(R)\dot{\cup}\mathcal{G}_0(p,1)&lt;/math&gt; of finite ''p''-groups of coclass &lt;math&gt;1&lt;/math&gt;
consists of the unique coclass tree with root &lt;math&gt;R=C_p\times C_p&lt;/math&gt;, the ''elementary abelian'' ''p''-group ''of rank'' &lt;math&gt;2&lt;/math&gt;, and a single '''isolated vertex''' (a terminal orphan without proper parent in the same coclass graph, since the directed edge to the trivial group &lt;math&gt;1&lt;/math&gt; has depth &lt;math&gt;2&lt;/math&gt;), the ''cyclic group'' &lt;math&gt;C_{p^2}&lt;/math&gt; of order &lt;math&gt;p^2&lt;/math&gt; in the sporadic part &lt;math&gt;\mathcal{G}_0(p,1)&lt;/math&gt; (however, this group is capable with respect to the lower exponent-''p'' central series).
The tree &lt;math&gt;\mathcal{T}^1(R)=\mathcal{T}^1(S_1)&lt;/math&gt; is the coclass tree of the unique infinite pro-''p'' group &lt;math&gt;S_1&lt;/math&gt; of coclass &lt;math&gt;1&lt;/math&gt;.

For &lt;math&gt;p=2&lt;/math&gt;, resp. &lt;math&gt;p=3&lt;/math&gt;, the SmallGroup identifier of the root &lt;math&gt;R&lt;/math&gt; is &lt;math&gt;\langle 4,2\rangle&lt;/math&gt;, resp. &lt;math&gt;\langle 9,2\rangle&lt;/math&gt;, and a tree diagram of the coclass graph from branch &lt;math&gt;\mathcal{B}(2)&lt;/math&gt; up to branch &lt;math&gt;\mathcal{B}(7)&lt;/math&gt; (counted with respect to the ''p''-logarithm of the order of the branch root) is drawn in Figure 2, resp.  Figure 3, where all groups of order at least &lt;math&gt;p^3&lt;/math&gt; are '''metabelian''', that is non-abelian with derived length &lt;math&gt;2&lt;/math&gt; (vertices represented by black discs in contrast to contour squares indicating abelian groups). In Figure 3, smaller black discs denote metabelian 3-groups where even the maximal subgroups are non-abelian, a feature which does not occur for the metabelian 2-groups in Figure 2, since they all possess an abelian subgroup of index &lt;math&gt;p&lt;/math&gt; (usually exactly one). The coclass tree of &lt;math&gt;\mathcal{G}(2,1)&lt;/math&gt;, resp. &lt;math&gt;\mathcal{G}(3,1)&lt;/math&gt;, has periodic root &lt;math&gt;\langle 8,3\rangle&lt;/math&gt; and period of length &lt;math&gt;1&lt;/math&gt; starting with branch &lt;math&gt;\mathcal{B}(3)&lt;/math&gt;, resp. periodic root &lt;math&gt;\langle 81,9\rangle&lt;/math&gt; and period of length &lt;math&gt;2&lt;/math&gt; starting with branch &lt;math&gt;\mathcal{B}(4)&lt;/math&gt;.
Both trees have branches of bounded depth &lt;math&gt;1&lt;/math&gt;, so their virtual periodicity is in fact a '''strict periodicity'''.

However, the coclass tree of &lt;math&gt;\mathcal{G}(p,1)&lt;/math&gt; with &lt;math&gt;p\ge 5&lt;/math&gt; has '''unbounded depth''' and contains non-metabelian groups, and the coclass tree of &lt;math&gt;\mathcal{G}(p,1)&lt;/math&gt; with &lt;math&gt;p\ge 7&lt;/math&gt; has even '''unbounded width''', that is the number of descendants of a fixed order increases indefinitely with growing order
.&lt;ref name=&quot;DEF&quot;&gt;
{{cite journal|
author=Dietrich, H., Eick, B., Feichtenschlager, D.|
year=2008|
title=Investigating ''p''-groups by coclass with GAP|
journal=Contemporary Mathematics, Computational group theory and the theory of groups|
volume=470|
pages=45–61
|doi=10.1090/conm/470/09185}}
&lt;/ref&gt;

With the aid of kernels and targets of Artin transfers, the diagrams in Figure 2,3 can be endowed with additional information and redrawn as [[Artin transfer (group theory)#Systematic library of SDTs|structured descendant trees]].

The concrete examples &lt;math&gt;\mathcal{G}(2,1)&lt;/math&gt; and &lt;math&gt;\mathcal{G}(3,1)&lt;/math&gt; provide an opportunity to give a '''parametrized''' power-commutator '''presentation'''
&lt;ref name=&quot;Bl&quot;&gt;
{{cite journal|
author=Blackburn, N.|
year=1958|
title=On a special class of ''p''-groups|
journal=Acta Math.|
volume=100|
pages=45–92
|doi=10.1007/bf02559602}}
&lt;/ref&gt;
(here a polycyclic presentation) for the complete coclass tree, mentioned in the lead section as a benefit of the descendant tree concept and as a consequence of the periodicity of the pruned coclass tree.
In both cases, the group &lt;math&gt;G&lt;/math&gt; is generated by two elements &lt;math&gt;x,y&lt;/math&gt; but the presentation contains the series of '''higher commutators''' &lt;math&gt;s_j&lt;/math&gt;, &lt;math&gt;2\le j\le n-1=\mathrm{cl}(G)&lt;/math&gt;, starting with the '''main commutator''' &lt;math&gt;s_2=\lbrack y,x\rbrack&lt;/math&gt;.
The nilpotency is formally expressed by &lt;math&gt;s_n=1&lt;/math&gt;, when the group is of order &lt;math&gt;\vert G\vert=p^n&lt;/math&gt;.

[[File:TreeOf3Groups.png|thumb|alt=3-groups|Figure 3: The coclass graph of finite 3-groups with coclass 1]]

For &lt;math&gt;p=2&lt;/math&gt;, there are two parameters &lt;math&gt;0\le w,z\le 1&lt;/math&gt; and the pc-presentation is given by

{{EquationNote|2}}
&lt;math&gt;\begin{align}G^n(z,w)= &amp; \langle x,y,s_2,\ldots,s_{n-1}\mid\\
&amp; x^2=s_{n-1}^w,\ y^2=s_2^{-1}s_{n-1}^z,\ \lbrack s_2,y\rbrack=1,\\
&amp; s_2=\lbrack y,x\rbrack,\ s_j=\lbrack s_{j-1},x\rbrack\text{ for }3\le j\le n-1\rangle\end{align}&lt;/math&gt;

The 2-groups of maximal class, that is of coclass &lt;math&gt;1&lt;/math&gt;, form three '''periodic infinite sequences''',
:*the '''dihedral''' groups, &lt;math&gt;D(2^n)=G^n(0,0)&lt;/math&gt;, &lt;math&gt;n\ge 3&lt;/math&gt;, forming the mainline (with infinitely capable vertices),
:*the generalized '''quaternion''' groups, &lt;math&gt;Q(2^n)=G^n(0,1)&lt;/math&gt;, &lt;math&gt;n\ge 3&lt;/math&gt;, which are all terminal vertices,
:*the '''semidihedral''' groups, &lt;math&gt;S(2^n)=G^n(1,0)&lt;/math&gt;, &lt;math&gt;n\ge 4&lt;/math&gt;, which are also leaves.

For &lt;math&gt;p=3&lt;/math&gt;, there are three parameters &lt;math&gt;0\le a\le 1&lt;/math&gt; and &lt;math&gt;-1\le w,z\le 1&lt;/math&gt; and the pc-presentation is given by

{{EquationNote|3}}
&lt;math&gt;\begin{align}G^n_a(z,w)= &amp; \langle x,y,s_2,\ldots,s_{n-1}\mid\\
&amp; x^3=s_{n-1}^w,\ y^3=s_2^{-3}s_3^{-1}s_{n-1}^z,\ \lbrack y,s_2\rbrack=s_{n-1}^a,\\
&amp; s_2=\lbrack y,x\rbrack,\ s_j=\lbrack s_{j-1},x\rbrack\text{ for }3\le j\le n-1\rangle\end{align}&lt;/math&gt;

3-groups with parameter &lt;math&gt;a=0&lt;/math&gt; possess an abelian maximal subgroup, those with parameter &lt;math&gt;a=1&lt;/math&gt; do not.
More precisely, an existing abelian maximal subgroup is unique, except for the two groups &lt;math&gt;G^3_0(0,0)&lt;/math&gt; and &lt;math&gt;G^3_0(0,1)&lt;/math&gt;, where all four maximal subgroups are abelian.

In contrast to any bigger coclass &lt;math&gt;r\ge 2&lt;/math&gt;, the coclass graph &lt;math&gt;\mathcal{G}(p,1)&lt;/math&gt; exclusively contains ''p''-groups &lt;math&gt;G&lt;/math&gt; with abelianization &lt;math&gt;G/G^\prime&lt;/math&gt; of type &lt;math&gt;(p,p)&lt;/math&gt;, except for its unique isolated vertex. The case &lt;math&gt;p=2&lt;/math&gt; is distinguished by the truth of the reverse statement: Any &lt;math&gt;2&lt;/math&gt;-group with abelianization of type &lt;math&gt;(2,2)&lt;/math&gt; is of coclass &lt;math&gt;1&lt;/math&gt; (O. Taussky's Theorem
&lt;ref name=&quot;Ta&quot;&gt;
{{cite journal|
author=Taussky, O.|
year=1937|
title=A remark on the class field tower|
journal=J. London Math. Soc.|
volume=12|
pages=82–85}}
&lt;/ref&gt;).

[[File:GraphOf3Groups.png|thumb|alt=interface|Figure 4: The interface between finite 3-groups of coclass 1 and 2 of type (3,3)]]

===Coclass 2===
The genesis of the coclass graph &lt;math&gt;\mathcal{G}(p,r)&lt;/math&gt; with &lt;math&gt;r\ge 2&lt;/math&gt; is not uniform. ''p''-groups with several distinct abelianizations contribute to its constitution.
For coclass &lt;math&gt;r=2&lt;/math&gt;, there are essential contributions from groups &lt;math&gt;G&lt;/math&gt; with abelianizations  &lt;math&gt;G/G^\prime&lt;/math&gt; of the types
&lt;math&gt;(p,p)&lt;/math&gt;, &lt;math&gt;(p^2,p)&lt;/math&gt;, &lt;math&gt;(p,p,p)&lt;/math&gt;, and an isolated contribution by the cyclic group of order &lt;math&gt;p^3&lt;/math&gt;.

====Abelianization of type (''p'',''p'')====
As opposed to ''p''-groups of coclass &lt;math&gt;2&lt;/math&gt; with abelianization of type &lt;math&gt;(p^2,p)&lt;/math&gt; or &lt;math&gt;(p,p,p)&lt;/math&gt;,
which arise as regular descendants of abelian ''p''-groups of the same types,
''p''-groups of coclass &lt;math&gt;2&lt;/math&gt; with abelianization of type &lt;math&gt;(p,p)&lt;/math&gt;
arise from irregular descendants of a non-abelian ''p''-group of coclass &lt;math&gt;1&lt;/math&gt; which is not coclass settled.

For the prime &lt;math&gt;p=2&lt;/math&gt;, such groups do not exist at all,
since the group &lt;math&gt;\langle 8,3\rangle&lt;/math&gt; is coclass settled,
which is the deeper reason for Taussky's Theorem.
This remarkable fact has been observed by G. Bagnera
&lt;ref name=&quot;Bg&quot;&gt;
{{cite journal|
author=Bagnera, G.|
year=1898|
title=La composizione dei gruppi finiti il cui grado &amp;egrave; la quinta potenza di un numero primo|
journal=Ann. di Mat. (Ser. 3)|
volume=1|
pages=137–228
|doi=10.1007/bf02419191}}
&lt;/ref&gt;
in 1898 already.

For odd primes &lt;math&gt;p\ge 3&lt;/math&gt;, the existence of ''p''-groups of coclass &lt;math&gt;2&lt;/math&gt; with abelianization of type &lt;math&gt;(p,p)&lt;/math&gt; is due to the fact that the group &lt;math&gt;G^3_0(0,0)&lt;/math&gt; is not coclass settled. Its nuclear rank equals &lt;math&gt;2&lt;/math&gt;, which gives rise to a '''bifurcation''' of the descendant tree &lt;math&gt;\mathcal{T}(G^3_0(0,0))&lt;/math&gt; into two coclass graphs. The regular component &lt;math&gt;\mathcal{T}^1(G^3_0(0,0))&lt;/math&gt; is a subtree of the unique tree &lt;math&gt;\mathcal{T}^1(C_p\times C_p)&lt;/math&gt; in the coclass graph &lt;math&gt;\mathcal{G}(p,1)&lt;/math&gt;. The irregular component &lt;math&gt;\mathcal{T}^2(G^3_0(0,0))&lt;/math&gt; becomes a subgraph &lt;math&gt;\mathcal{G}=\mathcal{G}_{(p,p)}(p,2)&lt;/math&gt; of the coclass graph &lt;math&gt;\mathcal{G}(p,2)&lt;/math&gt; when the connecting edges of depth &lt;math&gt;2&lt;/math&gt; of the irregular immediate descendants of &lt;math&gt;G^3_0(0,0)&lt;/math&gt; are removed.

For &lt;math&gt;p=3&lt;/math&gt;, this subgraph &lt;math&gt;\mathcal{G}&lt;/math&gt; is drawn in Figure 4.
It has seven top level vertices of three important kinds, all having order &lt;math&gt;243=3^5&lt;/math&gt;,
which have been discovered by G. Bagnera
.&lt;ref name=&quot;Bg&quot; /&gt;

:*Firstly, there are two terminal '''Schur &amp;sigma;-groups''' &lt;math&gt;\langle 243,5\rangle&lt;/math&gt; and &lt;math&gt;\langle 243,7\rangle&lt;/math&gt; in the sporadic part &lt;math&gt;\mathcal{G}_0(3,2)&lt;/math&gt; of the coclass graph &lt;math&gt;\mathcal{G}(3,2)&lt;/math&gt;.
:*Secondly, the two groups &lt;math&gt;G=\langle 243,4\rangle&lt;/math&gt; and &lt;math&gt;G=\langle 243,9\rangle&lt;/math&gt; are roots of finite trees &lt;math&gt;\mathcal{T}^2(G)&lt;/math&gt; in the sporadic part &lt;math&gt;\mathcal{G}_0(3,2)&lt;/math&gt; (however, since they are not coclass settled, the complete trees &lt;math&gt;\mathcal{T}(G)&lt;/math&gt; are infinite) .
:*And, finally, the three groups &lt;math&gt;\langle 243,3\rangle&lt;/math&gt;, &lt;math&gt;\langle 243,6\rangle&lt;/math&gt; and &lt;math&gt;\langle 243,8\rangle&lt;/math&gt; give rise to (infinite) coclass trees, e. g., &lt;math&gt;\mathcal{T}^2(\langle 729,40\rangle)&lt;/math&gt;, &lt;math&gt;\mathcal{T}^2(\langle 243,6\rangle)&lt;/math&gt;, &lt;math&gt;\mathcal{T}^2(\langle 243,8\rangle)&lt;/math&gt;, each having a metabelian mainline, in the coclass graph &lt;math&gt;\mathcal{G}(3,2)&lt;/math&gt; (again, none of these three groups is coclass settled).
Displaying additional information on kernels and targets of Artin transfers, we can draw these trees as [[Artin transfer (group theory)#Systematic library of SDTs|structured descendant trees]].


Generally, a '''Schur group''' (called a ''closed'' group by I. Schur, who coined the concept) is a pro-''p'' group &lt;math&gt;G&lt;/math&gt; whose relation rank &lt;math&gt;r(G)=\mathrm{dim}_{\mathbb{F}_p}(\mathrm{H}^2(G,\mathbb{F}_p))&lt;/math&gt; coincides with its generator rank &lt;math&gt;d(G)=\mathrm{dim}_{\mathbb{F}_p}(\mathrm{H}^1(G,\mathbb{F}_p))&lt;/math&gt;.
A '''&amp;sigma;-group''' is a pro-''p'' group &lt;math&gt;G&lt;/math&gt; which possesses an automorphism &lt;math&gt;\sigma\in\mathrm{Aut}(G)&lt;/math&gt; inducing the inversion &lt;math&gt;x\mapsto x^{-1}&lt;/math&gt; on its abelianization &lt;math&gt;G/G^\prime&lt;/math&gt;.
A '''Schur &amp;sigma;-group''' is a Schur group &lt;math&gt;G&lt;/math&gt; which is also a &amp;sigma;-group and has a finite abelianization &lt;math&gt;G/G^\prime&lt;/math&gt;.

It should be pointed out that &lt;math&gt;\langle 243,3\rangle&lt;/math&gt; is not root of a coclass tree,
since its immediate descendant &lt;math&gt;\langle 729,40\rangle&lt;/math&gt;,
which is root of a coclass tree with metabelian mainline vertices,
has two siblings &lt;math&gt;\langle 729,35\rangle&lt;/math&gt;, resp. &lt;math&gt;\langle 729,34\rangle&lt;/math&gt;,
which give rise to a single, resp. three, coclass tree(s) with non-metabelian mainline vertices having cyclic centres of order &lt;math&gt;3&lt;/math&gt;
and branches of considerable complexity but nevertheless of bounded depth &lt;math&gt;5&lt;/math&gt;.

{| class=&quot;wikitable collapsible&quot; style=&quot;float:right; text-align:center;&quot;
|+ Table 1: Quotients of the groups G=G(f,g,h) &lt;ref name=&quot;ELNO&quot;/&gt;
! Parameters &lt;br/&gt;&lt;math&gt;(f,g,h)&lt;/math&gt;
! Abelianization &lt;br/&gt;&lt;math&gt;G/G^\prime&lt;/math&gt;
! Class-2 quotient &lt;br/&gt;&lt;math&gt;G/\gamma_3(G)&lt;/math&gt;
! Class-3 quotient &lt;br/&gt;&lt;math&gt;G/\gamma_4(G)&lt;/math&gt;
! Class-4 quotient &lt;br/&gt;&lt;math&gt;G/\gamma_5(G)&lt;/math&gt;
|-
| &lt;math&gt;(0,1,0)&lt;/math&gt; || &lt;math&gt;(3,3)&lt;/math&gt; || &lt;math&gt;\langle 27,3\rangle&lt;/math&gt; || &lt;math&gt;\langle 243,3\rangle&lt;/math&gt; || &lt;math&gt;\langle 729,40\rangle&lt;/math&gt;
|-
| &lt;math&gt;(0,1,2)&lt;/math&gt; || &lt;math&gt;(3,3)&lt;/math&gt; || &lt;math&gt;\langle 27,3\rangle&lt;/math&gt; || &lt;math&gt;\langle 243,6\rangle&lt;/math&gt; || &lt;math&gt;\langle 729,49\rangle&lt;/math&gt;
|-
| &lt;math&gt;(1,1,2)&lt;/math&gt; || &lt;math&gt;(3,3)&lt;/math&gt; || &lt;math&gt;\langle 27,3\rangle&lt;/math&gt; || &lt;math&gt;\langle 243,8\rangle&lt;/math&gt; || &lt;math&gt;\langle 729,54\rangle&lt;/math&gt;
|-
| &lt;math&gt;(1,0,0)&lt;/math&gt; || &lt;math&gt;(9,3)&lt;/math&gt; || &lt;math&gt;\langle 81,3\rangle&lt;/math&gt; || &lt;math&gt;\langle 243,15\rangle&lt;/math&gt; || &lt;math&gt;\langle 729,79\rangle&lt;/math&gt;
|-
| &lt;math&gt;(0,0,1)&lt;/math&gt; || &lt;math&gt;(9,3)&lt;/math&gt; || &lt;math&gt;\langle 81,3\rangle&lt;/math&gt; || &lt;math&gt;\langle 243,17\rangle&lt;/math&gt; || &lt;math&gt;\langle 729,84\rangle&lt;/math&gt;
|-
| &lt;math&gt;(0,0,0)&lt;/math&gt; || &lt;math&gt;(3,3,3)&lt;/math&gt; || &lt;math&gt;\langle 81,12\rangle&lt;/math&gt; || &lt;math&gt;\langle 243,53\rangle&lt;/math&gt; || &lt;math&gt;\langle 729,395\rangle&lt;/math&gt;
|}

====Pro-3 groups of coclass 2 with non-trivial centre====
B. Eick, C. R. Leedham-Green, M. F. Newman and E. A. O'Brien &lt;ref name=&quot;ELNO&quot;/&gt; have constructed a family of infinite pro-3 groups with coclass &lt;math&gt;2&lt;/math&gt; having a non-trivial centre of order &lt;math&gt;3&lt;/math&gt;. The members are characterized by three parameters &lt;math&gt;(f,g,h)&lt;/math&gt;.
Their finite quotients generate all mainline vertices with bicyclic centres of type &lt;math&gt;(3,3)&lt;/math&gt; of six coclass trees in the coclass graph &lt;math&gt;\mathcal{G}(3,2)&lt;/math&gt;.
The association of parameters to the roots of these six trees is given in Table 1,
the tree diagrams are indicated in Figures 4 and 5, and
the parametrized pro-3 presentation is given by

{{EquationNote|4}}
&lt;math&gt;\begin{align}G(f,g,h)= &amp; \langle a,t,z\mid\\
&amp; a^3=z^f,\ \lbrack t,t^a\rbrack=z^g,\ t^{1+a+a^2}=z^h,\\
&amp; z^3=1,\ \lbrack z,a\rbrack=1,\ \lbrack z,t\rbrack=1\rangle\end{align}&lt;/math&gt;

[[File:TreeOf3Groups39.png|thumb|alt=interface|Figure 5: Finite 3-groups of coclass 2 of type (9,3)]]

====Abelianization of type (''p''²,''p'')====
For &lt;math&gt;p=3&lt;/math&gt;, the top levels of the subtree &lt;math&gt;\mathcal{T}^2(\langle 27,2\rangle)&lt;/math&gt; of the coclass graph &lt;math&gt;\mathcal{G}(3,2)&lt;/math&gt; are drawn in Figure 5. The most important vertices of this tree are the eight siblings sharing the common parent &lt;math&gt;\langle 81,3\rangle&lt;/math&gt;, which are of three important kinds.

:*Firstly, there are three leaves &lt;math&gt;\langle 243,20\rangle&lt;/math&gt;, &lt;math&gt;\langle 243,19\rangle&lt;/math&gt;, &lt;math&gt;\langle 243,16\rangle&lt;/math&gt; having cyclic centre of order &lt;math&gt;9&lt;/math&gt;, and a single leaf &lt;math&gt;\langle 243,18\rangle&lt;/math&gt; with bicyclic centre of type &lt;math&gt;(3,3)&lt;/math&gt;. 
:*Secondly, the group &lt;math&gt;G=\langle 243,14\rangle&lt;/math&gt; is root of a finite tree &lt;math&gt;\mathcal{T}(G)=\mathcal{T}^2(G)&lt;/math&gt;.
:*And, finally, the three groups &lt;math&gt;\langle 243,13\rangle&lt;/math&gt;, &lt;math&gt;\langle 243,15\rangle&lt;/math&gt; and &lt;math&gt;\langle 243,17\rangle&lt;/math&gt; give rise to infinite coclass trees, e. g., &lt;math&gt;\mathcal{T}^2(\langle 2187,319\rangle)&lt;/math&gt;, &lt;math&gt;\mathcal{T}^2(\langle 243,15\rangle)&lt;/math&gt;, &lt;math&gt;\mathcal{T}^2(\langle 243,17\rangle)&lt;/math&gt;, each having a metabelian mainline, the first with cyclic centres of order &lt;math&gt;3&lt;/math&gt;, the second and third with bicyclic centres of type &lt;math&gt;(3,3)&lt;/math&gt;.

Here, it should be emphasized that &lt;math&gt;\langle 243,13\rangle&lt;/math&gt; is not root of a coclass tree,
since aside from its descendant &lt;math&gt;\langle 2187,319\rangle&lt;/math&gt;,
which is root of a coclass tree with metabelian mainline vertices,
it possesses five further descendants
which give rise to coclass trees with non-metabelian mainline vertices having cyclic centres of order &lt;math&gt;3&lt;/math&gt;
and branches of considerable complexity, here partially even with '''unbounded depth'''. &lt;ref name=&quot;ELNO&quot;/&gt;

[[File:TreeOf2Groups222.png|thumb|alt=interface|Figure 6: Finite 2-groups of coclass 2,3,4 and type (2,2,2)]]

====Abelianization of type (''p'',''p'',''p'')====
For &lt;math&gt;p=2&lt;/math&gt;, resp. &lt;math&gt;p=3&lt;/math&gt;, there exists a unique coclass tree with ''p''-groups of type &lt;math&gt;(p,p,p)&lt;/math&gt; in the coclass graph &lt;math&gt;\mathcal{G}(p,2)&lt;/math&gt;.
Its root is the elementary abelian ''p''-group of type &lt;math&gt;(p,p,p)&lt;/math&gt;, that is, &lt;math&gt;\langle 8,5\rangle&lt;/math&gt;, resp. &lt;math&gt;\langle 27,5\rangle&lt;/math&gt;.
This unique tree corresponds to the pro-2 group of the family &lt;math&gt;\#59&lt;/math&gt; by M. F. Newman and E. A. O'Brien, &lt;ref name=&quot;NmOb&quot;/&gt;
resp. the pro-3 group given by the parameters &lt;math&gt;(f,g,h)=(0,0,0)&lt;/math&gt; in Table 1.
For &lt;math&gt;p=2&lt;/math&gt;, the tree is indicated in Figure 6.

===Coclass 3===
Here again, ''p''-groups with several distinct abelianizations contribute to the constitution of the coclass graph &lt;math&gt;\mathcal{G}(p,3)&lt;/math&gt; .
There are regular, resp. irregular, essential contributions from groups &lt;math&gt;G&lt;/math&gt; with abelianizations &lt;math&gt;G/G^\prime&lt;/math&gt; of the types
&lt;math&gt;(p^3,p)&lt;/math&gt;, &lt;math&gt;(p^2,p^2)&lt;/math&gt;, &lt;math&gt;(p^2,p,p)&lt;/math&gt;, &lt;math&gt;(p,p,p,p)&lt;/math&gt;, resp. &lt;math&gt;(p,p)&lt;/math&gt;,  &lt;math&gt;(p^2,p)&lt;/math&gt;, &lt;math&gt;(p,p,p)&lt;/math&gt;, and an isolated contribution by the cyclic group of order &lt;math&gt;p^4&lt;/math&gt;.

====Abelianization of type (''p'',''p'',''p'')====
Since the elementary abelian ''p''-group &lt;math&gt;C_p\times C_p\times C_p&lt;/math&gt; of rank &lt;math&gt;3&lt;/math&gt;, that is,
&lt;math&gt;\langle 8,5\rangle&lt;/math&gt;, resp. &lt;math&gt;\langle 27,5\rangle&lt;/math&gt;, for &lt;math&gt;p=2&lt;/math&gt;, resp. &lt;math&gt;p=3&lt;/math&gt;,
is not coclass settled, it gives rise to a multifurcation.
The regular component &lt;math&gt;\mathcal{T}^2(C_p\times C_p\times C_p)&lt;/math&gt; has been described in the section about coclass &lt;math&gt;2&lt;/math&gt;.
The irregular component &lt;math&gt;\mathcal{T}^3(C_p\times C_p\times C_p)&lt;/math&gt; becomes a subgraph &lt;math&gt;\mathcal{G}=\mathcal{G}_{(p,p,p)}(p,3)&lt;/math&gt; of the coclass graph &lt;math&gt;\mathcal{G}(p,3)&lt;/math&gt; when the connecting edges of depth &lt;math&gt;2&lt;/math&gt; of the irregular immediate descendants of &lt;math&gt;C_p\times C_p\times C_p&lt;/math&gt; are removed.

For &lt;math&gt;p=2&lt;/math&gt;, this subgraph &lt;math&gt;\mathcal{G}&lt;/math&gt; is contained in Figure 6.
It has nine top level vertices of order &lt;math&gt;32=2^5&lt;/math&gt; which can be divided into terminal and capable vertices:
:* the groups &lt;math&gt;\langle 32,32\rangle&lt;/math&gt; and &lt;math&gt;\langle 32,33\rangle&lt;/math&gt; are leaves,
:* the five groups &lt;math&gt;\langle 32,27..31\rangle&lt;/math&gt; and the two groups &lt;math&gt;\langle 32,34..35\rangle&lt;/math&gt; are infinitely capable.

The trees arising from the capable vertices are associated with infinite pro-2 groups by M. F. Newman and E. A. O'Brien
&lt;ref name=&quot;NmOb&quot;/&gt;
in the following manner.
&lt;math&gt;\langle 32,28\rangle&lt;/math&gt; gives rise to
&lt;math&gt;\mathcal{T}^3(\langle 64,140\rangle)&lt;/math&gt; associated with family &lt;math&gt;\#73&lt;/math&gt;,
and
&lt;math&gt;\mathcal{T}^3(\langle 64,147\rangle)&lt;/math&gt; associated with family &lt;math&gt;\#74&lt;/math&gt;.
&lt;math&gt;\mathcal{T}^3(\langle 32,29\rangle)&lt;/math&gt; is associated with family &lt;math&gt;\#75&lt;/math&gt;.
&lt;math&gt;\mathcal{T}^3(\langle 32,30\rangle)&lt;/math&gt; is associated with family &lt;math&gt;\#76&lt;/math&gt;.
&lt;math&gt;\mathcal{T}^3(\langle 32,31\rangle)&lt;/math&gt; is associated with family &lt;math&gt;\#77&lt;/math&gt;.
&lt;math&gt;\langle 32,34\rangle&lt;/math&gt; gives rise to
&lt;math&gt;\mathcal{T}^3(\langle 64,174\rangle)&lt;/math&gt; associated with family &lt;math&gt;\#78&lt;/math&gt;.
&lt;math&gt;\mathcal{T}^3(\langle 32,35\rangle)&lt;/math&gt; is associated with family &lt;math&gt;\#79&lt;/math&gt;.

{| class=&quot;wikitable collapsible&quot; style=&quot;float:right; text-align:center;&quot;
|+ Table 2: Class-2 quotients Q of certain metabelian 2-groups G of type (2,2,2) &lt;ref name=&quot;BLS&quot;/&gt;
! SmallGroups &lt;br/&gt;identifier of Q
! Hall Senior &lt;br/&gt;classification of Q
! Schur multiplier &lt;br/&gt;&lt;math&gt;\mathcal{M}(Q)&lt;/math&gt;
! 2-rank of G' &lt;br/&gt;&lt;math&gt;r_2(G^\prime)&lt;/math&gt;
! 4-rank of G' &lt;br/&gt;&lt;math&gt;r_4(G^\prime)&lt;/math&gt;
! Maximum of &lt;br/&gt;&lt;math&gt;r_2(H_i/H_i^\prime)&lt;/math&gt;
|-
| &lt;math&gt;\langle 32,32\rangle&lt;/math&gt; || 32.040 || &lt;math&gt;(2)&lt;/math&gt; || &lt;math&gt;2&lt;/math&gt; || &lt;math&gt;0&lt;/math&gt;  || &lt;math&gt;2&lt;/math&gt;
|-
| &lt;math&gt;\langle 32,33\rangle&lt;/math&gt; || 32.041 || &lt;math&gt;(2)&lt;/math&gt; || &lt;math&gt;2&lt;/math&gt; || &lt;math&gt;0&lt;/math&gt;  || &lt;math&gt;2&lt;/math&gt;
|-
| &lt;math&gt;\langle 32,29\rangle&lt;/math&gt; || 32.037 || &lt;math&gt;(2,2)&lt;/math&gt; || &lt;math&gt;2&lt;/math&gt; || &lt;math&gt;1&lt;/math&gt;  || &lt;math&gt;3&lt;/math&gt;
|-
| &lt;math&gt;\langle 32,30\rangle&lt;/math&gt; || 32.038 || &lt;math&gt;(2,2)&lt;/math&gt; || &lt;math&gt;2&lt;/math&gt; || &lt;math&gt;1&lt;/math&gt;  || &lt;math&gt;3&lt;/math&gt;
|-
| &lt;math&gt;\langle 32,35\rangle&lt;/math&gt; || 32.035 || &lt;math&gt;(2,2)&lt;/math&gt; || &lt;math&gt;2&lt;/math&gt; || &lt;math&gt;1&lt;/math&gt;  || &lt;math&gt;3&lt;/math&gt;
|-
| &lt;math&gt;\langle 32,28\rangle&lt;/math&gt; || 32.036 || &lt;math&gt;(2,2,2)&lt;/math&gt; || &lt;math&gt;2&lt;/math&gt; || &lt;math&gt;2&lt;/math&gt;  || &lt;math&gt;3&lt;/math&gt;
|-
| &lt;math&gt;\langle 32,27\rangle&lt;/math&gt; || 32.033 || &lt;math&gt;(2,2,2,2)&lt;/math&gt; || &lt;math&gt;3&lt;/math&gt; || &lt;math&gt;2&lt;/math&gt; or &lt;math&gt;3&lt;/math&gt;  || &lt;math&gt;4&lt;/math&gt;
|}

Seven of these nine top level vertices have been investigated by E. Benjamin, F. Lemmermeyer and C. Snyder
&lt;ref name=&quot;BLS&quot;&gt;
{{cite journal|
author=Benjamin, E., Lemmermeyer, F., Snyder, C.|
year=2003|
title=Imaginary quadratic fields with &lt;math&gt;\mathrm{Cl}_2(k)\simeq (2,2,2)&lt;/math&gt;|
journal=J. Number Theory|
volume=103|
pages=38–70}}
&lt;/ref&gt;
with respect to their occurrence as class-2 quotients &lt;math&gt;Q=G/\gamma_3(G)&lt;/math&gt; of bigger metabelian 2-groups &lt;math&gt;G&lt;/math&gt; of type &lt;math&gt;(2,2,2)&lt;/math&gt; and with coclass &lt;math&gt;3&lt;/math&gt;,
which are exactly the members of the descendant trees of the seven vertices.
These authors use the classification of 2-groups by M. Hall and J. K. Senior
&lt;ref name=&quot;HaSn&quot;&gt;
{{cite book|
author=Hall, M., Senior, J. K.|
year=1964|
title=The groups of order &lt;math&gt;2^n&lt;/math&gt; &lt;math&gt;(n\le 6)&lt;/math&gt;|
publisher=Macmillan, New York}}
&lt;/ref&gt;
which is put in correspondence with the SmallGroups Library &lt;ref name=&quot;BEO&quot;/&gt; in Table 2.
The complexity of the descendant trees of these seven vertices increases with the 2-ranks and 4-ranks indicated in Table 2, where the maximal subgroups of index &lt;math&gt;2&lt;/math&gt; in &lt;math&gt;G&lt;/math&gt; are denoted by &lt;math&gt;H_i&lt;/math&gt;, for &lt;math&gt;1\le i\le 7&lt;/math&gt;.

==History==
Descendant trees with central quotients as parents (1.) are implicit in P. Hall's 1940 paper
&lt;ref&gt;
{{cite journal|
author=Hall, P.|
year=1940|
title=The classification of prime-power groups|
journal=J. Reine Angew. Math.|
volume=182|
pages=130–141}}
&lt;/ref&gt;
about isoclinism of groups.
Trees with last non-trivial lower central quotients as parents (2.) were first presented by C. R. Leedham-Green
at the International Congress of Mathematicians in Vancouver, 1974
.&lt;ref name=&quot;Nm&quot; /&gt;
The first extensive tree diagrams have been drawn manually
by J. A. Ascione, G. Havas and C. R. Leedham-Green (1977)
,&lt;ref name=&quot;AHL&quot;&gt;
{{cite journal|
author=Ascione, J. A., Havas, G., Leedham-Green, C. R.|
year=1977|
title=A computer aided classification of certain groups of prime power order|
journal=Bull. Austral. Math. Soc.|
volume=17|
pages=257–274
|doi=10.1017/s0004972700010467}}
&lt;/ref&gt;
by J. A. Ascione (1979)
,&lt;ref name=&quot;As&quot;&gt;
{{cite book|
author=Ascione, J. A.|
year=1979|
title=On 3-groups of second maximal class|
publisher=Ph. D. Thesis, Australian National University, Canberra}}
&lt;/ref&gt;
and by B. Nebelung (1989)
.&lt;ref name=&quot;Ne&quot;&gt;
{{cite book|
author=Nebelung, B.|
year=1989| 
title=Klassifikation metabelscher 3-Gruppen mit Faktorkommutatorgruppe vom Typ (3,3) und Anwendung auf das Kapitulationsproblem|
publisher=Inauguraldissertation, Universit&amp;auml;t zu K&amp;ouml;ln}}
&lt;/ref&gt;
In the former two cases, the parent definition by means of the lower exponent-''p'' central series (3.) was adopted in view of computational advantages, in the latter case, where theoretical aspects were focussed, the parents were taken with respect to the usual lower central series (2.).

==See also==
* The kernels and targets of [[Artin transfer (group theory)|Artin transfers]] have recently turned out to be compatible with parent-descendant relations between finite ''p''-groups and can favourably be used to endow descendant trees with additional structure.

==References==
{{Reflist}}

[[Category:Group theory]]
[[Category:P-groups]]
[[Category:Subgroup series]]
[[Category:Topological groups]]
[[Category:Trees (data structures)]]</text>
      <sha1>apvmkeq1zgbjfg5nsola2sp19gtahon</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Technology tree</title>
    <ns>0</ns>
    <id>670346</id>
    <revision>
      <id>620471571</id>
      <parentid>619364234</parentid>
      <timestamp>2014-08-09T06:53:21Z</timestamp>
      <contributor>
        <username>BreakfastJr</username>
        <id>18072051</id>
      </contributor>
      <minor/>
      <comment>/* History */ Cosmetic changes</comment>
      <text xml:space="preserve" bytes="8862">{{Annotated image | float=right | width=250 | height=171 | image-width=1200 | image=Freeciv-2.1.8 technology tree.png
| caption=One part of ''[[Freeciv]]''’s technology tree. Note the complex dependencies between technologies. 
|annotations=
}}
In [[strategy computer game]]s, the '''technology tree''' or '''tech tree''' is a [[hierarchy|hierarchical]] visual representation of the possible sequences of upgrades a player can take, by means of [[research]]. The diagram is tree-shaped in the sense that it branches at certain intervals, allowing the player to choose one sequence or another.&lt;ref name=&quot;fundamentals&quot;&gt;{{cite book|last=Rollings|first=Andrew|authorlink=|author2=Ernest Adams |title=Fundamentals of Game Design|publisher=Prentice Hall|date=2006|location=|url=http://wps.prenhall.com/bp_gamedev_1/54/14053/3597646.cw/index.html}}
&lt;/ref&gt; Typically, at the beginning of a session of a strategy game, a player may only have a few options for technologies to research. Each technology that a player researches will open up more options, but may or may not, depending on the computer game the player is playing, close off the paths to other options. The tech tree is the representation of all possible paths of research a player can take.

A player who is engaged in research activities is said to be &quot;teching up,&quot; &quot;going up the tech tree,&quot; or &quot;moving up the tech tree.&quot; Analysis of a tech tree can lead players to memorize and use specific [[build order]]s.

==Types of tech tree==
===Prerequisites for technology advances===
In many [[real-time strategy]] (RTS) games, the player needs particular buildings in order to research specific techs or build specific advanced units (''[[StarCraft]]'', ''[[Age of Empires]]'', ''[[Empire Earth]]'', ''[[Total Annihilation]]''). In many [[turn-based strategy]] (TBS) games the prerequisite is one or more lower-level technologies, with no dependency on specific buildings (''[[Master of Orion]]'' series, ''[[Civilization (computer game)|Civilization]]'' series, ''[[Space Empires]]'' series).&lt;ref name=&quot;SeeGameManuals&quot;&gt;See the relevant games' manuals.&lt;/ref&gt;&lt;ref&gt;''[[Master of Orion]]'' (1993), the ''[[Galactic Civilizations]]'' series and ''[[Sword of the Stars]]'' have no research buildings; players simply allocate some of each colony's output to research (see game manuals). In the ''[[Civilization (series)|Civilization]]'' series (1991 onwards) and ''[[Master of Orion II]]'' (1996) one can do research without buildings, but it's much faster when supported by the right buildings (see game manuals). In the ''[[Space Empires]]'' series (1993 onwards) and in ''[[Ascendancy (computer game)|Ascendancy]]'' (1995) research can only be done via buildings, but these can research any technology (see game manuals).&lt;/ref&gt;&lt;ref name=&quot;RakrentRTSBasicsRnD&quot;&gt;{{ cite web | url=http://www.rakrent.com/rtsc/rtsc_rnd.htm | title=RTS Basics: R &amp; D }} and the ''[[StarCraft]]'' manual. Although multi-epoch games like the ''[[Age of Empires]]'' and ''[[Empire Earth]]'' series have a larger number of research options and a significant proportion of civilian research options, the research options all depend on having the right buildings. See the relevant games' manuals.&lt;/ref&gt; Most strategy games however use both systems. both requiring dedicated buildings and in advanced cases pre requisite technology, sometimes culminating in a game ending super-weapon of some kind.

===Complexity===
The structures of tech trees vary quite widely. In the simplest cases{{Citation needed|date=August 2009}} (e.g. ''[[Master of Orion]]'') there are several completely separate research areas and one could research all the way up to the highest level in one area without researching other areas (although this would often be suicidal). In the most complex cases{{Citation needed|date=August 2009}} (e.g. ''[[Civilization (computer game)|Civilization]]'') every technology above the starting level has more than one prerequisite and one has to research most of the lower-level technologies in order to research any of the top-level technologies. And there are many possibilities between these two extremes, for example in ''[[Space Empires]]'' researching to a specified level in one field may enable the player both to research to a higher level in that field ''and'' to start research in a new field which was previously not available.&lt;ref name=&quot;SeeGameManuals&quot; /&gt;

Major [[4X]] games like ''[[Civilization (computer game)|Civilization]]'' and ''Master of Orion'' have a much larger technology tree than most other strategy games; as an extreme example, ''[[Space Empires III]]'' has over 200 technologies.&lt;ref name=&quot;JoystiqSoaSEDesignerInterview&quot;&gt;{{ cite web | url=http://www.joystiq.com/2008/02/01/joystiq-interview-ironclad-talks-4x-strategy-with-sins-of-a-sol/ | title=Joystiq interview: Ironclad talks 4X strategy with Sins of a Solar Empire | date=February 1, 2008 | accessdate=2008-06-14 }}&lt;/ref&gt;&lt;ref name=&quot;NumbersOfTechNodes&quot;&gt;In ''Warcraft III'' one can reach the highest level of one branch of the technology tree in five steps; ''Master of Orion'' (original version) has 10 levels per subject, and 2 to 5 technologies per level; the ''Civilization IV'' technology tree requires nearly 60 steps to reach the end (see game manuals).&lt;/ref&gt;

===Are all technologies available?===
Some RTSs make different techs available to different races or cultures (especially ''[[StarCraft]]''; but many RTSs have special units or buildings for different cultures, e.g. ''[[Age of Empires]]'' expansion pack and later versions, ''[[Red Alert 2]]''). Most TBSs make all technologies available to all cultures (e.g. ''[[Civilization (series)|Civilization]]''). ''[[Master of Orion]]'' (original version) is a complex special case in this respect: the full tree is the same for all; but in each game each player gets a subset of the full tech tree that depends on which race was selected.

===Balance between civilian and military techs===
In many RTS games tech advances are almost exclusively military (e.g. ''[[StarCraft]]''). But in most TBS and some RTS games the research and production costs of top-end military techs are so high that you have to build up your economy and your research productivity first (RTS - ''[[Age of Empires]]'' and ''[[Empire Earth]]'', where one of the most significant costs is going up an epoch; TBS - the ''[[Civilization (series)|Civilization]]'' series and ''[[Master of Orion]]'' series).

===What happens after researching everything ===
In many games there's nothing useful to do and the player may scrap research centers to save maintenance costs and/or devote the resources to something else (''[[Space Empires]]'' series).

In later installments of the ''[[Civilization (series)|Civilization]]'' series the last technology (called &quot;future tech&quot;) represents an amalgamation of all possible future discoveries and can be researched repeatedly. In [[Civilization V]], it increases a player's score, while in [[Civilization IV]] it raises the health and the happiness in the empire. Note that to reach the last technology, all spaceship technologies required to win must also have been discovered, so the game will be nearing its conclusion.

In the ''[[Galactic Civilizations]]'' series the final technology solves the nature of existence, and ''is'' victory.

In the ''[[Master of Orion]]'' series more advanced research reduces the size and cost of spaceship components, and &quot;hyper-advanced&quot; research in areas which have military applications therefore enables players to build more high-tech weapons into a given ship size and at lower production cost.

In ''[[Rise of Nations]]'', the final four technologies result in such an advantage that the game will likely end quickly. Also, the &quot;knowledge&quot; resource needed to research is also used late in the game to produce cruise missiles and nuclear weapons.

== History ==
The tech tree was originally designed for [[Francis Tresham (game designer)|Francis Tresham]]'s ''[[Civilization (board game)|Civilization]]'', which was released in 1980 by [[Avalon Hill]].

The arcade shoot 'em up ''[[Gradius (video game)|Gradius]]'' used a power-up system functionally identical to a tech tree in 1985.

Tech trees started showing up in [[turn-based strategy]] games around 1990, where ''[[Mega Lo Mania]]'' had a system of research levels/epochs that allowed the deployment of better units and defences. The 1991 video game ''[[Civilization (video game)|Civilization]]'' was probably the first game to feature the same basic structure of tech trees seen in games today.{{Citation needed|date=May 2007}}

==See also==
* [[Timeline of historic inventions|Timeline of Historic Inventions]] 

==References==
{{reflist}}

{{Real-time strategy gameplay}}
{{DEFAULTSORT:Technology Tree}}
[[Category:Video game gameplay]]
[[Category:Trees (data structures)]]</text>
      <sha1>j8mcz9m2xzwo3vv5vlont56pr3wpnho</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Program structure tree</title>
    <ns>0</ns>
    <id>12656919</id>
    <revision>
      <id>624092147</id>
      <parentid>615612998</parentid>
      <timestamp>2014-09-04T00:51:30Z</timestamp>
      <contributor>
        <username>Libcub</username>
        <id>6307086</id>
      </contributor>
      <comment>/* References */ added cat</comment>
      <text xml:space="preserve" bytes="9648">A '''program structure tree''' (PST) is a [[hierarchical]] diagram that displays the nesting relationship of [[single-entry single-exit]] (SESE) fragments/regions, showing the organization of a [[computer program]]. Nodes in this tree represent SESE regions of the program, while edges represent [[nesting (computing)|nesting]] regions. The PST is defined for all control flow graphs.

==Bibliographical Notes==

These notes list important works which fueled research on parsing of programs and/or (work)flow graphs (adapted from Section 3.5 in {{cite thesis |type=Ph.D. |first=Artem |last=Polyvyanyy |title=Structuring Process Models |publisher=University of Potsdam |year=2012 |url=http://nbn-resolving.de/urn:nbn:de:kobv:517-opus-59024}}).

*The connectivity properties are the basic properties of graphs and are useful when testing whether a graph is planar or when determining if two graphs are isomorphic. John Hopcroft and Robert Endre Tarjan (1973) developed an optimal (to within a constant factor) algorithm for dividing a graph into triconnected components.&lt;ref name=&quot;HopcroftT73&quot;&gt;{{citation | last1 = Hopcroft | first1 = John | author1-link = John Hopcroft | last2 = Tarjan | first2 = Robert | author2-link = Robert Tarjan | title = Dividing a graph into triconnected components | journal = SIAM Journal on Computing | volume = 2 | issue = 3 | pages = 135–158 | year = 1973 | doi = 10.1137/0202012}}.&lt;/ref&gt; The algorithm is based on the depth-first search of graphs and requires &lt;math&gt;O(|V|+|E|)&lt;/math&gt; time and space to examine a graph with &lt;math&gt;|V|&lt;/math&gt; vertices and &lt;math&gt;|E|&lt;/math&gt; edges.

*Robert Endre Tarjan and Jacobo Valdes (1980) used triconnected components for structural analysis of biconnected flow graphs.&lt;ref name=&quot;TarjanV80&quot;&gt;{{citation | last1 = Tarjan | first1 = Robert | author1-link = Robert Tarjan | last2 = Valdes | first2 = Jacobo | title = Prime subprogram parsing of a program | conference = Symposium on Principles of Programming Languages (POPL) | pages = 95–105 | year = 1980 | doi = 10.1145/567446.567456}}.&lt;/ref&gt; The triconnected components of the undirected version of a flow graph are shown to be useful for discovering structural information of directed flow graphs. The triconnected components can be discovered efficiently and form a hierarchy of SESE fragments of a flow graph.

*Giuseppe Di Battista and Roberto Tamassia (1990) introduced SPQR-trees&lt;ref name=&quot;BattistaT90&quot;&gt;{{citation | last1 = Di Battista | first1 = Giuseppe | last2 = Tamassia | first2 = Roberto | author2-link = Roberto Tamassia | year = 1990 | contribution = On-line graph algorithms with SPQR-trees | title = [[International Colloquium on Automata, Languages and Programming|Proc. 17th International Colloquium on Automata, Languages and Programming]] | series = Lecture Notes in Computer Science | publisher = Springer-Verlag | volume = 443 | pages = 598–611 | doi = 10.1007/BFb0032061}}&lt;/ref&gt; - a data structure which represents decomposition of a biconnected graph with respect to its triconnected components. Essentially, SPQR-trees are the parse trees of Tarjan and Valdes.&lt;ref name=&quot;TarjanV80&quot; /&gt; The authors showed the usefulness of SPQR-trees for various on-line graph algorithms, e.g., transitive closure, planarity testing, and minimum spanning tree.&lt;ref name=&quot;BattistaT90&quot; /&gt; In particular, the authors proposed an efficient solution to the problem of on-line maintenance of the triconnected components of a graph.&lt;ref&gt;{{citation | last1 = Di Battista | first1 = Giuseppe | last2 = Tamassia | first2 = Roberto | author2-link = Roberto Tamassia | year = 1996 | journal = Algorithmica | title =  On-line maintenance of triconnected components with SPQR-trees | volume = 15 | issue = 4 | pages = 302–318 | doi = 10.1007/BF01961541}}&lt;/ref&gt;

*Richard C. Johnson et al. (1994) proposed a program structure tree (PST), a hierarchical representation of program structure based on single edge entry and single edge exit regions.&lt;ref name=&quot;JohnsonPP94&quot;&gt;{{citation | first1=Richard Craig | last1=Johnson | first2=David | last2=Pearson | first3=Keshav | last3=Pingali | title = The Program Structure Tree: Computing Control Regions in Linear Time | conference = SIGPLAN Conference on Programming Language Design and Implementation (PLDI) | pages = 171–185 | year = 1994 | doi = 10.1145/178243.178258}}&lt;/ref&gt;&lt;ref&gt;{{cite thesis |type=Ph.D. |first=Richard Craig | last=Johnson | title=Efficient Program Analysis using Dependence Flow Graphs |publisher=Cornell University |year=1995 |url=http://ecommons.library.cornell.edu/handle/1813/6073}}&lt;/ref&gt; The PST can be computed in &lt;math&gt;O(|E|)&lt;/math&gt; time for an arbitrary flow graph, where &lt;math&gt;E&lt;/math&gt; is the set of edges in the graph. The disadvantage of the PST is that it exploits the notion of a SESE fragment based on edge entries and exits only. Thus, the PST does not capture those SESE fragments which are based on vertex entries and exits.

*Carsten Gutwenger and [[Petra Mutzel]] (2001) shared their practical experience on linear time computation of the triconnected components of biconnected graphs.&lt;ref&gt;{{citation
 | last1 = Gutwenger | first1 = Carsten | last2 = Mutzel | first2 = Petra | author2-link = Petra Mutzel | contribution = A linear time implementation of SPQR-trees | title = [[International Symposium on Graph Drawing|Proc. 8th International Symposium on Graph Drawing (GD 2000)]] | year = 2001 | series = Lecture Notes in Computer Science | publisher = Springer-Verlag | volume = 1984 | pages = 77–90 | doi = 10.1007/3-540-44541-2_8}}&lt;/ref&gt; They have identified and corrected the faulty parts of the algorithm in&lt;ref name=&quot;HopcroftT73&quot; /&gt; and applied the resulting algorithm to the computation of SPQR-trees. The implementation is publically available.

*Chun Ouyang et al. (2006-2009) used parsing to translate BPMN diagrams into BPEL processes.&lt;ref&gt;{{citation | last1 = Ouyang | first1 = Chun | last2 = Dumas | first2 = Marlon | last3 = ter Hofstede | first3 = Arthur H. M. | last4 = van der Aalst | first4 = Wil M. P. | year = 2006 | conference = International/European Conference on Web Services (ICWS) | title = From BPMN process models to BPEL web services | pages = 285–292}}&lt;/ref&gt;&lt;ref&gt;{{citation | last1 = Ouyang | first1 = Chun | last2 = Dumas | first2 = Marlon | last3 = van der Aalst | first3 = Wil M. P. | last4 = ter Hofstede | first4 = Arthur H. M. | last5 = Mendling | first5 = Jan  | year = 2009 | journal = ACM Transactions on Software Engineering and Methodology (TOSEM) | title = From business process models to process-oriented software systems | volume = 19 | issue = 1 | pages = 2:1–2:37 | doi = 10.1007/BF01961541}}&lt;/ref&gt; The employed notion of a fragment is similar to the notion of a region in.&lt;ref name=&quot;JohnsonPP94&quot; /&gt; However, the developed parsing algorithm is non-deterministic, i.e., the parse tree is not unique for a given diagram.

*Jussi Vanhatalo et al. (2008-2009) introduced the Refined Process Structure Tree (RPST).&lt;ref&gt;{{citation | last1 = Vanhatalo | first1 = Jussi | last2 = Voelzer | first2 = Hagen | last3 = Koehler | first3 = Jana | year = 2008 | conference= Business Process Management (BPM) | title = The refined process structure tree | pages = 100–115 | doi = 10.1007/978-3-540-85758-7_10}}&lt;/ref&gt;&lt;ref&gt;{{citation | last1 = Vanhatalo | first1 = Jussi | last2 = Voelzer | first2 = Hagen | last3 = Koehler | first3 = Jana | year = 2009 | journal = Data and Knowledge Engineering (DKE) | title = The refined process structure tree | volume = 68 | issue = 9 | pages = 793–818 | doi = 10.1016/j.datak.2009.02.015}}&lt;/ref&gt;&lt;ref&gt;{{cite thesis |type=Ph.D. |first=Jussi |last=Vanhatalo |title=Process Structure Trees: Decomposing a Business Process Model into a Hierarchy of Single-Entry-Single-Exit Fragments|publisher=University of Stuttgart |year=2009}}&lt;/ref&gt; Given a workflow graph, the RPST is unique, modular, and is finer grained than any other known parse tree, i.e., it discovers more SESE fragments than any other technique. In fact, the RPST captures all canonical fragments of a workflow graph which, in turn, represent all SESE fragments of the graph. The RPST can be computed for an arbitrary program/workflow graph.

*Artem Polyvyanyy, Jussi Vanhatalo, and Hagen Voelzer (2010) proposed a simplified algorithm for computation of the RPST.&lt;ref name=&quot;PolyvyanyyVV10&quot;&gt;{{citation | last1  = Polyvyanyy | first1 = A. | last2  = Vanhatalo | first2 = J. | last3  = Völzer | first3 = H. | contribution = Simplified Computation and Generalization of the Refined Process Structure Tree | year  = 2010 | title = Web Services and Formal Methods | publisher = Springer Berlin Heidelberg | url = http://domino.watson.ibm.com/library/cyberdig.nsf/1e4115aea78b6e7c85256b360066f0d4/e15d645190e222888525764800305d25?OpenDocument | doi = 10.1007/978-3-642-19589-1_2 | isbn = 978-3-642-19588-4 | pages = 25–41 | series = Lecture Notes in Computer Science | volume = 6551 }}&lt;/ref&gt; This simplified algorithm can be employed in a straightforward way as a subroutine for computation of the RPST of an arbitrary program/workflow graph. Both algorithms, the original and the simplified one, allow for an eﬃcient computation of the RPST. However, they provide different structural characterizations of canonical SESE fragments.

==External links==
* [http://code.google.com/p/jbpt/ Java implementation of the Refined Process Structure Tree] in the jBPT library (see RPST class in jbpt-deco module). The implementation follows the algorithm described in&lt;ref name=&quot;PolyvyanyyVV10&quot; /&gt;

==References==
{{reflist}}

[[Category:Programming constructs]]
[[Category:Trees (data structures)]]

{{Compu-prog-stub}}</text>
      <sha1>ocovuk906nm3c08ohaa68yo93dv1v96</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Treemapping</title>
    <ns>0</ns>
    <id>1592887</id>
    <revision>
      <id>624092180</id>
      <parentid>618351114</parentid>
      <timestamp>2014-09-04T00:51:44Z</timestamp>
      <contributor>
        <username>Libcub</username>
        <id>6307086</id>
      </contributor>
      <comment>/* External links */ added cat</comment>
      <text xml:space="preserve" bytes="12484">[[File:Gradient grouped treemap.jpg|400px|thumb|Treemap of soft drink preference in a small group of people. Color and gradients are used to group items, while still identifying individual items.]]
[[File:US Presidential Elections 2012.png|400px|thumb|Treemap of votes by county, state and locally predominant recipient in the [[United States presidential election, 2012|US Presidential Elections of 2012]].]]
[[File:Heatmap incito.png|400px|thumb|Treemap showing changes in waiting times for patients of [[Primary Care Trust|English PCTs]].]]
[[File:Tree Map.png|400px|thumb|[[TreeSize]] Treemap visualizing hard disk space usage.]]
In [[information visualization]] and [[computing]], '''treemapping''' is a method for displaying [[hierarchical]] data by using [[Nesting (computing)|nested]] rectangles.

== Main idea ==
Treemaps display hierarchical (tree-structured) data as a set of nested rectangles. Each branch of the tree is given a rectangle, which is then tiled with smaller rectangles representing sub-branches. A leaf node's rectangle has an area proportional to a specified dimension on the data. Often the leaf nodes are colored to show a separate [[Dimension (metadata)|dimension of the data]].

When the color and size dimensions are correlated in some way with the tree structure, one can often easily see patterns that would be difficult to spot in other ways, such as if a certain color is particularly relevant. A second advantage of treemaps is that, by construction, they make efficient use of space. As a result, they can legibly display thousands of items on the screen simultaneously.

== The tiling algorithm ==
To create a treemap, one must define a [[tessellation|tiling]] [[algorithm]], that is, a way to divide a rectangle into sub-rectangles of specified areas. 
Ideally, a treemap algorithm would create rectangles of [[aspect ratio (image)|aspect ratio]] close to one, furthermore preserve some sense of the ordering in the input data, and change to reflect changes in the underlying data. Unfortunately, these properties have an inverse relationship. As the aspect ratio is optimized, the order of placement becomes less predictable. As the order becomes more stable, the aspect ratio is degraded.

To date, six primary rectangular treemap algorithms have been developed:

{| class=&quot;wikitable sortable&quot;
|+ Treemap algorithms&lt;ref name=&quot;shneiderman_1&quot;/&gt;
|-
! Algorithm
! Order
! Aspect ratios
! Stability
|-
| '''BinaryTree'''
| partially ordered
| high
| stable
|-
| '''Mixed Treemaps'''&lt;ref name=&quot;mixed_algorithm&quot;&gt;{{cite web |url=http://www.magnaview.nl/documents/Visualizing_Business_Data_with_Generalized_Treemaps.pdf |format=PDF |title=Visualizing Business Data with Generalized Treemaps |author=Roel Vliegen |coauthors=Erik-Jan van der Linden and [[Jack van Wijk|Jarke J. vanWijk]] |accessdate=February 24, 2010}}&lt;/ref&gt;
| ordered
| lowest
| stable
|-
| '''Ordered'''
| partially ordered
| medium
| medium stability
|-
| '''Slice And Dice'''
| ordered
| very high
| stable
|-
| '''Squarified'''&lt;ref name=&quot;squarified_algorithm&quot;&gt;{{Cite book| last1 = Bruls | first1 = Mark
 | last2 = Huizing | first2 = Kees
 | last3 = van Wijk | first3 = Jarke J.
 | editor1-last = de Leeuw | editor1-first = W.
 | editor2-last = van Liere | editor2-first = R.
 | contribution = Squarified treemaps
 | pages = 33–42
 | publisher = Springer-Verlag
 | title = Data Visualization 2000: Proc. Joint Eurographics and IEEE TCVG Symp. on Visualization
 | url = http://www.win.tue.nl/~vanwijk/stm.pdf
 | year = 2000| postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;&amp;#123;&amp;#123;inconsistent citations&amp;#125;&amp;#125;
 }}.&lt;/ref&gt;
| unordered
| lowest
| medium stability
|-
| '''Strip'''
| ordered
| medium 
| medium stability
|-
|}

In addition, several algorithms have been proposed that use non-rectangular regions:
* '''Jigsaw Treemaps'''&lt;ref name=&quot;jigsaw_algorithm&quot;&gt;{{Cite book| last1 = Wattenberg | first1 = Martin
 | editor1-last = Stasko| editor1-first = John T.
 | editor2-last = Ward | editor2-first = Matthew O.
 | contribution = A Note on Space-Filling Visualizations and Space-Filling Curves
 | pages = 24 
 | publisher = IEEE Computer Society
 | title = IEEE Symposium on Information Visualization (InfoVis 2005), 23-25 October 2005, Minneapolis, MN, USA
 | url = http://hint.fm/papers/158-wattenberg-final3.pdf
 | year = 2005| postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;&amp;#123;&amp;#123;inconsistent citations&amp;#125;&amp;#125;
 }}.&lt;/ref&gt;  - based on the geometry of space-filling curves
* '''GosperMaps'''&lt;ref name=&quot;gosper_map&quot;&gt;{{Cite journal| last1 = Auber | first1 = David
 | title = Gosper ''Map'': Using a Gosper Curve for laying out hierarchical data
 | last2 = Huet | first2 = Charles
 | last3 = Lambert | first3 = Antoine
 | last4 = Renoust | first4 = Benjamin
 | last5 = Sallaberry | first5 = Arnaud
 | last6 = Saulnier | first6 = Agnes
 | contribution = GosperMap: Using a Gosper Curve for Laying out Hierarchical Data
 | pages = 1820–1832
 | volume = 19
 | issue = 11
 | journal = IEEE Transactions on Visualization and Computer Graphics
 | url = http://www.computer.org/csdl/trans/tg/2013/11/ttg2013111820-abs.html
 | year = 2013| pmid = 24029903
 | doi = 10.1109/TVCG.2013.91
 | postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;&amp;#123;&amp;#123;inconsistent citations&amp;#125;&amp;#125;
 }}.&lt;/ref&gt; - based on the geometry of Gosper curves, [ordered, very high Aspect ratio, stable]
* '''Voronoi Treemaps'''&lt;ref name=&quot;voronoi_algorithm&quot;&gt;{{Cite book| last1 = Balzer | first1 = Michael
 | last2 = Deussen | first2 = Oliver
 | editor1-last = Stasko| editor1-first = John T.
 | editor2-last = Ward | editor2-first = Matthew O.
 | contribution = Voronoi Treemaps
 | pages = 7 
 | publisher = IEEE Computer Society
 | title = IEEE Symposium on Information Visualization (InfoVis 2005), 23-25 October 2005, Minneapolis, MN, USA
 | url = http://www.informatik.uni-konstanz.de/index.php?eID=tx_nawsecuredl&amp;u=0&amp;file=fileadmin/informatik/ag-deussen/2005/Balzer_et_al._--_Voronoi_Treemaps.pdf
 | year = 2005| postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;&amp;#123;&amp;#123;inconsistent citations&amp;#125;&amp;#125;
 }}.&lt;/ref&gt; - based on [[voronoi diagram]] calculations
* '''Convex Treemaps'''&lt;ref name=&quot;convex_algorithm&quot;&gt;{{cite web |url=http://people.csail.mit.edu/konak/papers/socg_2008-circular_partitions_with_applications_to_visualization_and_embeddings.html |title= Circular Partitions with Applications to Visualization and Embeddings |author=Krzysztof Onak |author2=Anastasios Sidiropoulos  |accessdate=June 26, 2011}}&lt;/ref&gt; - convex polygons are used instead of rectangles
* '''Circular Treemaps''' - circles are used instead of rectangles

== History ==
Area-based visualizations have existed for decades. For example, [[mosaic plot]]s (also known as Marimekko diagrams) use rectangular tilings to show joint distributions (i.e., most commonly they are essentially stacked column plots where the columns are of different widths). The main distinguishing feature of a treemap, however, is the recursive construction that allows it to be extended to hierarchical data with any number of levels. This idea was invented by [[University of Maryland, College Park]] professor [[Ben Shneiderman]] in the early 1990s.&lt;ref name=&quot;shneiderman_1&quot;&gt;{{cite web |url=http://www.cs.umd.edu/hcil/treemap-history/index.shtml |title=Treemaps for space-constrained visualization of hierarchies |author=Shneiderman, Ben |author2=Plaisant, Catherine  |date=June 25, 2009 |accessdate=February 23, 2010}}&lt;/ref&gt; Shneiderman and his collaborators then deepened the idea by introducing a variety of interactive techniques for filtering and adjusting treemaps.

These early treemaps all used the simple &quot;slice-and-dice&quot; tiling algorithm. Despite many desirable properties (it is stable, preserves ordering, and is easy to implement), the slice-and-dice method often produces tilings with many long, skinny rectangles. In 1994 [[Hascoet &amp; Beaudouin-Lafon]] invented a &quot;squarifying&quot; algorithm, later popularized by [[Jarke van Wijk]], that created tilings whose rectangles were closer to square. In 1999 [[Martin M. Wattenberg|Martin Wattenberg]] used a variation of the &quot;squarifying&quot; algorithm that he called &quot;pivot and slice&quot; to create the first Web-based treemap, the SmartMoney Map of the Market, which displayed data on hundreds of companies in the U.S. stock market. Following its launch, treemaps enjoyed a surge of interest, especially in financial contexts.{{Citation needed|date=August 2008}}

A third wave of treemap innovation came around 2004, after [[Marcos Weskamp]] created the [[Newsmap]], a treemap that displayed news headlines. This example of a non-analytical treemap inspired many imitators, and introduced treemaps to a new, broad audience.{{Citation needed|date=March 2010}} In recent years, treemaps have made their way into the mainstream media, including usage by the New York Times.&lt;ref name=&quot;NYTimes&quot;&gt;{{cite news|url=http://www.nytimes.com/imagepages/2007/02/25/business/20070225_CHRYSLER_GRAPHIC.html|title=The health of the car, van, SUV, and truck market|date=February 25, 2007|accessdate=March 12, 2010 | work=The New York Times | first1=Amanda | last1=Cox | first2=Hannah | last2=Fairfield}}&lt;/ref&gt;&lt;ref name=&quot;NYTimes2&quot;&gt;{{cite news|url=http://www.nytimes.com/packages/html/newsgraphics/2011/0119-budget/index.html?hp|title=Obama's 2012 Budget Proposal: How $3.7 Trillion is Spent |date=February 14, 2011|accessdate=February 15, 2011 | work=The New York Times | first1=Shan | last1=Carter | first2=Amanda | last2=Cox}}&lt;/ref&gt;

[[File:Benin English.png|thumb|400px|right|Treemap of Benin's exports by product category, 2009. The Product Exports Treemaps are one of the most recent applications of these kind of visualizations, developed by the Harvard-MIT [[The Observatory of Economic Complexity|Observatory of Economic Complexity]]]]

== See also ==

* [[Disk space analyzer]]
* [[Information visualization]]
* [[List of treemapping software]]
*[[List of countries by economic complexity]], which includes a list of Products Exports Treemaps.

==References==
{{Reflist}}

== External links ==
{{Commons category|Treemaps}}
* [http://www.perceptualedge.com/articles/b-eye/treemaps.pdf An article by Ben Shneiderman on the use of treemaps] (as a guest on www.perceptualedge.com [http://www.perceptualedge.com])
* [http://treevis.net Comprehensive survey and bibliography] of Tree Visualization techniques
* [http://www.magnaview.nl/documents/Visualizing_Business_Data_with_Generalized_Treemaps.pdf Generalized treemaps]
* [http://www.cs.umd.edu/hcil/treemap-history/index.shtml History of Treemaps] by Ben Shneiderman.
* [http://dx.doi.org/10.1006/ijhc.1995.1053 Hypermedia exploration with interactive dynamic maps] Paper by Hascoet and Beaudouin-Lafon introducing the squarified treemap layout algorithm (named &quot;improved treemap layout&quot; at the time).
* [http://iv.slis.indiana.edu/sw/treemap.html Indiana University] description
* [http://Panopticon.com/images/stories/white_papers/wp_treemap_data_visualizations_for_multi-dimensional_data.pdf White paper on Treemap Visualizations for Analyzing Multi-Dimensional, Hierarchical Data Sets] from [[Panopticon Software]]
* [http://www.dealmapper.co.uk Live interactive treemap based on crowd-sourced discounted deals] from ''[[Flytail Group]]''
* [http://www.panopticon.com/demo/13 Counterparty Risk Analytics using Interactive Treemap] from ''[[Panopticon Software]]''
* [http://hivegroup.com/gallery/olympics2010/ Treemap sample in English] from ''[[The Hive Group]]''
* [http://www.treemap.com/datasets/ Several treemap examples] made with [[Macrofocus]] [[Macrofocus TreeMap|TreeMap]]
* [http://www.drasticdata.nl/ Visualizations using dynamic treemaps] and [http://www.drasticdata.nl/DDHome.php?m=drastictreemapdesktop treemapping software] by drasticdata
* [http://atlas.media.mit.edu/explore/tree_map/export/deu/all/show/2009/ Product Exports Treemaps developed by the Harvard-MIT Observartory of Economic Complexity] 


&lt;!--Categories--&gt;
[[Category:User interface techniques]]
[[Category:Infographics]]
[[Category:Statistical charts and diagrams]]
[[Category:Trees (data structures)]]</text>
      <sha1>ogenivml9jgl1k31bqzcwsewfeq16yb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>K-ary tree</title>
    <ns>0</ns>
    <id>2145273</id>
    <revision>
      <id>625997076</id>
      <parentid>618926046</parentid>
      <timestamp>2014-09-17T21:02:02Z</timestamp>
      <contributor>
        <username>Scire9</username>
        <id>10322341</id>
      </contributor>
      <minor/>
      <comment>Added links to other tree structures.</comment>
      <text xml:space="preserve" bytes="3230">In [[graph theory]], a '''k-ary tree''' is a rooted [[tree (graph theory)|tree]] in which each node has no more than ''k'' children.  It is also sometimes known as a '''k-way tree''', an '''N-ary tree''', or an '''M-ary tree'''. A [[binary tree]] is the special case where ''k=2''.

==Types of k-ary trees==
*A '''full k-ary tree''' is a k-ary tree where within each level every node has either ''0'' or ''k'' children. 
*A '''perfect k-ary tree''' is a full &lt;ref name=&quot;orderedtrees&quot;&gt;{{cite web|url=http://cs.lmu.edu/~ray/notes/orderedtrees/|title=Ordered Trees|accessdate=19 November 2012}}&lt;/ref&gt; k-ary tree in which all [[leaf node]]s are at the same depth.&lt;ref&gt;{{cite web|url=http://xlinux.nist.gov/dads/HTML/perfectKaryTree.html|title=perfect k-ary tree|last=Black|first=Paul E.|date=20 April 2011|publisher=U.S. National Institute of Standards and Technology|accessdate=10 October 2011}}&lt;/ref&gt;
*A '''complete k-ary tree''' is a k-ary tree which is maximally space efficient.  It must be completely filled on every level (meaning that each node on that level has k children) except for the last level.  However, if the last level is not complete, then all nodes of the tree must be &quot;as far left as possible&quot;. &lt;ref name=&quot;orderedtrees&quot;&gt;&lt;/ref&gt;

==Properties of k-ary trees==

*For a k-ary tree with height ''h'', the upper bound for the maximum number of leaves is &lt;math&gt;k^h&lt;/math&gt;. 
* The height ''h ''of a '''k-ary tree''' does not include the root node, with a tree containing only a root node having a height of 0.  
*The total number of nodes in a '''perfect''' '''k-ary tree''' is &lt;math&gt;(k^{h+1} - 1)/(k-1)&lt;/math&gt;, while the height ''h'' is

:&lt;math&gt;\left\lceil\log_k (k - 1) + \log_k (\mathit{number\_of\_nodes}) - 1\right\rceil.&lt;/math&gt;

Note : A Tree containing only one node is taken to be of height 0 for this formula to be applicable.

Note : Formula is not applicable for a 2-ary tree with height 0, as the ceiling operator approximates and simplifies the true formula, which can be described as
:&lt;math&gt;\log_k [(k - 1) * \mathit{number\_of\_nodes} + 1] - 1, k \ge 2.&lt;/math&gt;

==Methods for storing k-ary trees==

===Arrays===

k-ary trees can also be stored in breadth-first order as an [[implicit data structure]] in [[array data structure|array]]s, and if the tree is a complete k-ary tree, this method wastes no space. In this compact arrangement, if a node has an index ''i'', its ''c''-th child is found at index &lt;math&gt;k*i+1+c&lt;/math&gt;, while its parent (if any) is found at index ''&lt;math&gt;\left \lfloor \frac{i-1}{k} \right \rfloor&lt;/math&gt;'' (assuming the root has index zero). This method benefits from more compact storage and better [[locality of reference]], particularly during a preorder traversal.

==See also==
*[[Left-child right-sibling binary tree]]
*[[Binary tree]]

==References==
{{Reflist}}
*{{cite book
  |last = Storer
  |first = James A.
  |title = An  Introduction to Data Structures and Algorithms
  |year = 2001
  |isbn = 3-7643-4253-6
  |publisher = Birkhäuser Boston}}

==External links==
*[http://www.brpreiss.com/books/opus5/html/page257.html N-ary trees, Bruno R. Preiss, P.Eng.]


{{CS-Trees}}






{{Comp-sci-stub}}

[[Category:Trees (graph theory)]]
[[Category:Trees (data structures)]]</text>
      <sha1>3gdjvhuv76cueer3n1mzh3zip724c5o</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Weight-balanced tree</title>
    <ns>0</ns>
    <id>4849574</id>
    <revision>
      <id>553230798</id>
      <parentid>546565520</parentid>
      <timestamp>2013-05-02T18:11:59Z</timestamp>
      <contributor>
        <ip>111.119.165.146</ip>
      </contributor>
      <comment>/* Timing analysis */</comment>
      <text xml:space="preserve" bytes="1813">A '''weight-balanced binary tree''' is a [[binary tree]] which is balanced based on knowledge of the probabilities of searching for each individual node. Within each [[subtree]], the node with the highest weight appears at the root. This can result in more efficient searching performance.

Construction of such a tree is similar to that of a [[Treap]], but node weights are chosen randomly in the latter.

[[Image:Weight_balanced_tree2.jpg|frame|Example of weight balanced tree]]
==The diagram==
In the diagram to the right, the letters represent node ''values'' and the numbers represent node ''weights''. Values are used to order the tree, as in a general [[binary search tree]]. The weight may be thought of as a probability or activity count associated with the node. In the diagram, the root is G because its weight is the greatest in the tree. The left subtree begins with A because, out of all nodes with values that come before G, A has the highest weight. Similarly, N is the highest-weighted node that comes after G.

==Timing analysis== 
A weight balanced tree gives close to optimal values for the expected length of successful search calculations. From the above example we get

ELOSS = depth(node A)*probability(node A) + depth(node C)*probability(node C) + ...

ELOSS =  2*0.17 + 5*0.03 + 4*0.09 + 3*0.12 + 1*0.20 + 3*0.11 + 3*0.10 + 2*0.18

ELOSS = 2.5

This is the expected number of nodes that will be examined before finding the desired node.

== See also ==
* [[Binary tree]]
* [[AVL tree]]
* [[B-tree]]
* [[Binary space partitioning]]
* [[Red-black tree]]
* [[Treap]]

== References ==
* [[Jean-Paul Tremblay and Grant A. Cheston]]. ''Data Structures and Software Development in an object-oriented domain'', Eiffel Edition. Prentice Hall, 2001. ISBN 0-13-787946-6.

[[Category:Binary trees]]</text>
      <sha1>qqjzjm3aiwcpsy34ctegrjf1efwe3oi</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Left-child right-sibling binary tree</title>
    <ns>0</ns>
    <id>11261383</id>
    <revision>
      <id>588817710</id>
      <parentid>575685933</parentid>
      <timestamp>2014-01-02T14:31:03Z</timestamp>
      <contributor>
        <username>Ysangkok</username>
        <id>235750</id>
      </contributor>
      <comment>remove unsourced non-reversibility claim</comment>
      <text xml:space="preserve" bytes="1335">In [[computer science]], a '''left-child, right-sibling binary tree''' is a [[binary tree]] representation of a [[k-ary tree]].&lt;ref&gt;
{{cite book
 | title = Introduction To Algorithms
 | edition = 2nd
 | author = Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein
 | publisher = MIT Press
 | year = 2001
 | isbn = 9780262032933
 | pages = 214–217
 | url = http://books.google.com/books?id=NLngYyWFl_YC&amp;pg=PA214
 }}&lt;/ref&gt; The process of converting from a k-ary tree to an LC-RS binary tree (sometimes called ''Knuth transform''&lt;ref&gt;Computer Data Structures. John L. Pfaltz.&lt;/ref&gt;).
[[File:N-ary to binary.svg|thumb|right|250px|6-ary tree represented as a binary tree]]
To form a binary tree from an arbitrary k-ary tree by this method, the root of the original tree is made the root of the binary tree. Then, starting with the root, each node's leftmost child in the original tree is made its left child in the binary tree, and its nearest sibling to the right in the original tree is made its right child in the binary tree.&lt;ref&gt;{{DADS|Left child-right sibling binary tree|leftChildrightSiblingBinaryTree}}&lt;/ref&gt;

If the original tree was sorted, the new tree will be a [[binary search tree]].

==See also==
*[[Grafting (algorithm)]]

==References==
&lt;references/&gt;


{{algorithm-stub}}

[[Category:Binary trees]]</text>
      <sha1>alm2ugrjf5j49qx4zjdxupa8h5dhjok</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tree rotation</title>
    <ns>0</ns>
    <id>30816</id>
    <revision>
      <id>619871917</id>
      <parentid>601939206</parentid>
      <timestamp>2014-08-04T21:08:24Z</timestamp>
      <contributor>
        <username>Tar-Elessar</username>
        <id>21984574</id>
      </contributor>
      <comment>Added animation</comment>
      <text xml:space="preserve" bytes="8244">[[File:BinaryTreeRotations.svg|thumb|300px|Generic tree rotations.]]
In [[discrete mathematics]], '''tree rotation''' is an operation on a [[binary tree]] that changes the structure without interfering with the order of the elements. A tree rotation moves one node up in the tree and one node down. It is used to change the shape of the tree, and in particular to decrease its height by moving smaller subtrees down and larger subtrees up, resulting in improved performance of many tree operations.

There exists an inconsistency in different descriptions as to the definition of the '''direction of rotations'''. Some say that the direction of a rotation depends on the side which the tree nodes are shifted upon whilst others say that it depends on which child takes the root's place (opposite of the former). This article takes the approach of the side where the nodes get shifted to.

==Illustration==
[[Image:Tree rotation.png|left]][[Image:Tree rotation animation 250x250.gif|right|Animation of tree rotations taking place.]]

The right rotation operation as shown in the image to the right is performed with ''Q'' as the root and hence is a right rotation on, or rooted at, ''Q''. This operation results in a rotation of the tree in the clockwise direction. The inverse operation is the left rotation, which results in a movement in a counter-clockwise direction (the left rotation shown above is rooted at ''P''). The key to understanding how a rotation functions is to understand its constraints. In particular the order of the leaves of the tree (when read left to right for example) cannot change (another way to think of it is that the order that the leaves would be visited in an in-order traversal must be the same after the operation as before).  Another constraint is the main property of a binary search tree, namely that the right child is greater than the parent and the left child is less than the parent. Notice that the right child of a left child of the root of a sub-tree (for example node B in the diagram for the tree rooted at Q) can become the left child of the root, that itself becomes the right child of the &quot;new&quot; root in the rotated sub-tree, without violating either of those constraints. As you can see in the diagram, the order of the leaves doesn't change. The opposite operation also preserves the order and is the second kind of rotation.  

Assuming this is a [[binary search tree]], as stated above, the elements must be interpreted as variables that can be compared to each other. The alphabetic characters to the left are used as placeholders for these variables.  In the animation to the right, capital alphabetic characters are used as variable placeholders while lowercase Greek letters are placeholders for an entire set of variables.  The circles represent individual nodes and the triangles represent subtrees.  Each subtree could be empty, consist of a single node, or consist of any number of nodes.

==Detailed illustration==
[[Image:Tree Rotations.gif|thumb|Pictorial description of how rotations are made.]]

When a subtree is rotated, the subtree side upon which it is rotated increases its height by one node while the other subtree decreases its height. This makes tree rotations useful for rebalancing a tree.

Using the terminology of '''Root''' for the parent node of the subtrees to rotate, '''Pivot''' for the node which will become the new parent node, '''RS''' for rotation side upon to rotate and '''OS''' for opposite side of rotation. In the above diagram for the root Q, the '''RS''' is C and the '''OS''' is P. The pseudo code for the rotation is:

 Pivot = Root.OS
 Root.OS = Pivot.RS
 Pivot.RS = Root
 Root = Pivot

This is a constant time operation.

The programmer must also make sure that the root's parent points to the pivot after the rotation.  Also, the programmer should note that this operation may result in a new root for the entire tree and take care to update pointers accordingly.

==Inorder Invariance==
The tree rotation renders the inorder traversal of the binary tree [[Invariant (computer science)|invariant]]. This implies the order of the elements are not affected when a rotation is performed in any part of the tree. Here are the inorder traversals of the trees shown above:

&lt;pre&gt;
Left tree: ((A, P, B), Q, C)        Right tree: (A, P, (B, Q, C))
&lt;/pre&gt;

Computing one from the other is very simple. The following is example [[Python (programming language)|Python]] code that performs that computation:

&lt;source lang=&quot;python&quot;&gt;
def right_rotation(treenode):
  left, Q, C = treenode
  A, P, B = left
  return (A, P, (B, Q, C))
&lt;/source&gt;

Another way of looking at it is:

Right Rotation of node Q:

&lt;pre&gt;
Let P be Q's left child.
Set Q's left child to be P's right child.
Set P's right child to be Q.
&lt;/pre&gt;

Left Rotation of node P:

&lt;pre&gt;
Let Q be P's right child.
Set P's right child to be Q's left child.
Set Q's left child to be P.
&lt;/pre&gt;

All other connections are left as-is.

There are also ''double rotations'', which are combinations of left and right rotations. A ''double left'' rotation at X can be defined to be a right rotation at the right child of X followed by a left rotation at X; similarly, a ''double right'' rotation at X can be defined to be a left rotation at the left child of X followed by a right rotation at X.

Tree rotations are used in a number of tree [[data structures]] such as [[AVL tree]]s, [[red-black tree]]s, [[splay tree]]s, and [[treap]]s. They require only constant time because they are ''local'' transformations: they only operate on 5 nodes, and need not examine the rest of the tree.

==Rotations for rebalancing==
[[Image:Tree Rebalancing.gif|thumb|Pictorial description of how rotations cause rebalancing in an AVL tree.]]

A tree can be rebalanced using rotations. After a rotation, the side of the rotation increases its height by 1 whilst the side opposite the rotation decreases its height similarly.  Therefore, one can strategically apply rotations to nodes whose left child and right child differ in height by more than 1.  Self-balancing binary search trees apply this operation automatically. A type of tree which uses this rebalancing technique is the [[AVL tree]].

==Rotation distance==
The '''rotation distance''' between any two binary trees with the same number of nodes is the minimum number of rotations needed to transform one into the other. With this distance, the set of ''n''-node binary trees becomes a [[metric space]]: the distance is symmetric, positive when given two different trees, and satisfies the [[triangle inequality]].

It is an [[open problem]] whether there exists a [[polynomial time]] [[algorithm]] for calculating rotation distance.

[[Daniel Sleator]], [[Robert Tarjan]] and [[William Thurston]] showed that the rotation distance between any two ''n''-node trees (for ''n'' ≥ 11) is at most 2''n''&amp;nbsp;&amp;minus;&amp;nbsp;6, and that infinitely many pairs of trees are this far apart.&lt;ref&gt;{{citation|last1=Sleator|first1=Daniel D.|authorlink1=Daniel Sleator|last2=Tarjan|first2=Robert E.|authorlink2=Robert Tarjan|last3=Thurston|first3=William P.|authorlink3=William Thurston|title=Rotation distance, triangulations, and hyperbolic geometry|journal=Journal of the American Mathematical Society|volume=1|issue=3|year=1988|pages=647–681|doi=10.2307/1990951|mr=928904 |jstor=1990951|publisher=American Mathematical Society}}.&lt;/ref&gt;

==See also==
{{portal|Computer science}}
* [[AVL tree]], [[red-black tree]], and [[splay tree]], kinds of [[binary search tree]] data structures that use rotations to maintain balance.
* [[Associativity]] of a binary operation means that performing a tree rotation on it does not change the final result.
* The [[Day–Stout–Warren algorithm]] balances an unbalanced BST.
* [[Tamari lattice]], a partially ordered set in which the elements can be defined as binary trees and the ordering between elements is defined by tree rotation.

==References==
{{reflist}}

==External links==
* [http://www.cs.queensu.ca/home/jstewart/applets/bst/bst-rotation.html Java applets demonstrating tree rotations]
* [http://fortheloot.com/public/AVLTreeTutorial.rtf The AVL Tree Rotations Tutorial] (RTF) by John Hargrove

[[Category:Binary trees]]</text>
      <sha1>9fmbx1gninjxjwem5cehcyi2k9ism8k</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cartesian tree</title>
    <ns>0</ns>
    <id>15843635</id>
    <revision>
      <id>615523348</id>
      <parentid>545157540</parentid>
      <timestamp>2014-07-04T04:20:14Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <minor/>
      <comment>/* References */ cap</comment>
      <text xml:space="preserve" bytes="20463">[[File:Cartesian tree.svg|thumb|240px|A sequence of numbers and the Cartesian tree derived from them.]]

In [[computer science]], a '''Cartesian tree''' is a [[binary tree]] derived from a sequence of numbers; it can be uniquely defined from the properties that it is [[Heap (data structure)|heap]]-ordered and that a [[Tree traversal|symmetric (in-order) traversal]] of the tree returns the original sequence. Introduced by {{harvtxt|Vuillemin|1980}} in the context of geometric [[range searching]] [[data structure]]s, Cartesian trees have also been used in the definition of the [[treap]] and [[randomized binary search tree]] data structures for [[binary search]] problems. The Cartesian tree for a sequence may be constructed in [[linear time]] using a [[Stack (data structure)|stack]]-based algorithm for finding [[all nearest smaller values]] in a sequence.

==Definition==
The Cartesian tree for a sequence of distinct numbers can be uniquely defined by the following properties:
#The Cartesian tree for a sequence has one node for each number in the sequence. Each node is associated with a single sequence value.
#A [[Tree traversal|symmetric (in-order) traversal]] of the tree results in the original sequence. That is, the left subtree consists of the values earlier than the root in the sequence order, while the right subtree consists of the values later than the root, and a similar ordering constraint holds at each lower node of the tree.
#The tree has the [[Binary heap|heap property]]: the parent of any non-root node has a smaller value than the node itself.&lt;ref&gt;In some references, the ordering is reversed, so the parent of any node always has a larger value and the root node holds the maximum value.&lt;/ref&gt;
Based on the heap property, the root of the tree must be the smallest number in the sequence. From this, the tree itself may also be defined recursively: the root is the minimum value of the sequence, and the left and right subtrees are the Cartesian trees for the subsequences to the left and right of the root value. Therefore, the three properties above uniquely define the Cartesian tree.

If a sequence of numbers contains repetitions, the Cartesian tree may be defined by determining a consistent tie-breaking rule (for instance, determining that the first of two equal elements is treated as the smaller of the two) before applying the above rules.

An example of a Cartesian tree is shown in the figure above.

==Range searching and lowest common ancestors==
[[File:Cartesian tree range searching.svg|thumb|300px|Two-dimensional range-searching using a Cartesian tree: the bottom point (red in the figure) within a three-sided region with two vertical sides and one horizontal side (if the region is nonempty) may be found as the nearest common ancestor of the leftmost and rightmost points (the blue points in the figure) within the slab defined by the vertical region boundaries. The remaining points in the three-sided region may be found by splitting it by a vertical line through the bottom point and recursing.]]
Cartesian trees may be used as part of an efficient [[data structure]] for [[Range_Minimum_Query|range minimum queries]], a [[range searching]] problem involving queries that ask for the minimum value in a contiguous subsequence of the original sequence.&lt;ref&gt;{{harvtxt|Gabow|Bentley|Tarjan|1984}}; {{harvtxt|Bender|Farach-Colton|2000}}.&lt;/ref&gt; In a Cartesian tree, this minimum value may be found at the [[lowest common ancestor]] of the leftmost and rightmost values in the subsequence. For instance, in the subsequence (12,10,20,15) of the sequence shown in the first illustration, the minimum value of the subsequence (10) forms the lowest common ancestor of the leftmost and rightmost values (12 and 15). Because lowest common ancestors may be found in constant time per query, using a data structure that takes linear space to store and that may be constructed in linear time,&lt;ref&gt;{{harvtxt|Harel|Tarjan|1984}}; {{harvtxt|Schieber|Vishkin|1988}}.&lt;/ref&gt; the same bounds hold for the range minimization problem.

{{harvtxt|Bender|Farach-Colton|2000}} reversed this relationship between the two data structure problems by showing that lowest common ancestors in an input tree could be solved efficiently applying a non-tree-based technique for range minimization. Their data structure uses an [[Euler tour]] technique to transform the input tree into a sequence and then finds range minima in the resulting sequence.  The sequence resulting from this transformation has a special form (adjacent numbers, representing heights of adjacent nodes in the tree, differ by ±1) which they take advantage of in their data structure; to solve the range minimization problem for sequences that do not have this special form, they use Cartesian trees to transform the range minimization problem into a lowest common ancestor problem, and then apply the Euler tour technique to transform the problem again into one of range minimization for sequences with this special form.

The same range minimization problem may also be given an alternative interpretation in terms of two dimensional range searching. A collection of finitely many points in the [[Cartesian plane]] may be used to form a Cartesian tree, by sorting the points by their ''x''-coordinates and using the ''y''-coordinates in this order as the sequence of values from which this tree is formed. If ''S'' is the subset of the input points within some vertical slab defined by the inequalities ''L''&amp;nbsp;≤&amp;nbsp;''x''&amp;nbsp;≤&amp;nbsp;''R'', ''p'' is the leftmost point in ''S'' (the one with minimum ''x''-coordinate), and ''q'' is the rightmost point in ''S'' (the one with maximum ''x''-coordinate) then the lowest common ancestor of ''p'' and ''q'' in the Cartesian tree is the bottommost point in the slab. A three-sided range query, in which the task is to list all points within a region bounded by the three inequalities ''L''&amp;nbsp;≤&amp;nbsp;''x''&amp;nbsp;≤&amp;nbsp;''R'' and ''y''&amp;nbsp;≤&amp;nbsp;''T'', may be answered by finding this bottommost point ''b'', comparing its ''y''-coordinate to ''T'', and (if the point lies within the three-sided region) continuing recursively in the two slabs bounded between ''p'' and ''b'' and between ''b'' and ''q''. In this way, after the leftmost and rightmost points in the slab are identified, all points within the three-sided region may be listed in constant time per point.&lt;ref name=&quot;gbt&quot;&gt;{{harvtxt|Gabow|Bentley|Tarjan|1984}}.&lt;/ref&gt;

The same construction, of lowest common ancestors in a Cartesian tree, makes it possible to construct a data structure with linear space that allows the distances between pairs of points in any [[ultrametric space]] to be queried in constant time per query. The distance within an ultrametric is the same as the [[widest path problem|minimax path]] weight in the [[minimum spanning tree]] of the metric.&lt;ref&gt;{{harvtxt|Hu|1961}}; {{harvtxt|Leclerc|1981}}&lt;/ref&gt; From the minimum spanning tree, one can construct a Cartesian tree, the root node of which represents the heaviest edge of the minimum spanning tree. Removing this edge partitions the minimum spanning tree into two subtrees, and Cartesian trees recursively constructed for these two subtrees form the children of the root node of the Cartesian tree. The leaves of the Cartesian tree represent points of the metric space, and the lowest common ancestor of two leaves in the Cartesian tree is the heaviest edge between those two points in the minimum spanning tree, which has weight equal to the distance between the two points. Once the minimum spanning tree has been found and its edge weights sorted, the Cartesian tree may be constructed in linear time.&lt;ref&gt;{{harvtxt|Demaine|Landau|Weimann|2009}}.&lt;/ref&gt;

==Treaps==
: ''Main article: [[Treap]]''
Because a Cartesian tree is a binary tree, it is natural to use it as a [[binary search tree]] for an ordered sequence of values.  However, defining a Cartesian tree based on the same values that form the search keys of a binary search tree does not work well: the Cartesian tree of a sorted sequence is just a [[path graph|path]], rooted at its leftmost endpoint, and binary searching in this tree degenerates to [[sequential search]] in the path. However, it is possible to generate more-balanced search trees by generating ''priority'' values for each search key that are different than the key itself, sorting the inputs by their key values, and using the corresponding sequence of priorities to generate a Cartesian tree. This construction may equivalently be viewed in the geometric framework described above, in which the ''x''-coordinates of a set of points are the search keys and the ''y''-coordinates are the priorities.

This idea was applied by {{harvtxt|Seidel|Aragon|1996}}, who suggested the use of random numbers as priorities. The data structure resulting from this random choice is called a [[treap]], due to its combination of binary search tree and binary heap features. An insertion into a treap may be performed by inserting the new key as a leaf of an existing tree, choosing a priority for it, and then performing [[tree rotation]] operations along a path from the node to the root of the tree to repair any violations of the heap property caused by this insertion; a deletion may similarly be performed by a constant amount of change to the tree followed by a sequence of rotations along a single path in the tree.

If the priorities of each key are chosen randomly and independently once whenever the key is inserted into the tree, the resulting Cartesian tree will have the same properties as a [[random binary search tree]], a tree computed by inserting the keys in a randomly chosen [[permutation]] starting from an empty tree, with each insertion leaving the previous tree structure unchanged and inserting the new node as a leaf of the tree. Random binary search trees had been studied for much longer, and are known to behave well as search trees (they have [[logarithm]]ic depth with high probability); the same good behavior carries over to treaps. It is also possible, as suggested by Aragon and Seidel, to reprioritize frequently-accessed nodes, causing them to move towards the root of the treap and speeding up future accesses for the same keys.

==Efficient construction==
A Cartesian tree may be constructed in [[linear time]] from its input sequence.
One method is to simply process the sequence values in left-to-right order, maintaining the Cartesian tree of the nodes processed so far, in a structure that allows both upwards and downwards traversal of the tree. To process each new value ''x'', start at the node representing the value prior to ''x'' in the sequence and follow the path from this node to the root of the tree until finding a value ''y'' smaller than ''x''. This node ''y'' is the parent of ''x'', and the previous right child of ''y'' becomes the new left child of ''x''. The total time for this procedure is linear, because the time spent searching for the parent ''y'' of each new node ''x'' can be [[Potential_method|charged]] against the number of nodes that are removed from the rightmost path in the tree.&lt;ref name=&quot;gbt&quot;/&gt;

An alternative linear-time construction algorithm is based on the [[all nearest smaller values]] problem. In the input sequence, one may define the ''left neighbor'' of a value ''x'' to be the value that occurs prior to ''x'', is smaller than ''x'', and is closer in position to ''x'' than any other smaller value. The ''right neighbor'' is defined symmetrically. The sequence of left neighbors may be found by an algorithm that maintains a [[stack (data structure)|stack]] containing a subsequence of the input. For each new sequence value ''x'', the stack is popped until it is empty or its top element is smaller than ''x'', and then ''x'' is pushed onto the stack. The left neighbor of ''x'' is the top element at the time ''x'' is pushed. The right neighbors may be found by applying the same stack algorithm to the reverse of the sequence. The parent of ''x'' in the Cartesian tree is either the left neighbor of ''x'' or the right neighbor of ''x'', whichever exists and has a larger value. The left and right neighbors may also be constructed efficiently by [[parallel algorithm]]s, so this formulation may be used to develop efficient parallel algorithms for Cartesian tree construction.&lt;ref&gt;{{harvtxt|Berkman|Schieber|Vishkin|1993}}.&lt;/ref&gt;

==Application in sorting==
[[File:Bracketing pairs.svg|thumb|300px|Pairs of consecutive sequence values (shown as the thick red edges) that bracket a sequence value ''x'' (the darker blue point). The cost of including ''x'' in the sorted order produced by the Levcopoulos–Petersson algorithm is proportional to the [[logarithm]] of this number of bracketing pairs.]]
{{harvtxt|Levcopoulos|Petersson|1989}} describe a [[sorting algorithm]] based on Cartesian trees. They describe the algorithm as based on a tree with the maximum at the root, but it may be modified straightforwardly to support a Cartesian tree with the convention that the minimum value is at the root. For consistency, it is this modified version of the algorithm that is described below.

The Levcopoulos–Petersson algorithm can be viewed as a version of [[selection sort]] or [[heap sort]] that maintains a [[priority queue]] of candidate minima, and that at each step finds and removes the minimum value in this queue, moving this value to the end of an output sequence. In their algorithm, the priority queue consists only of elements whose parent in the Cartesian tree has already been found and removed. Thus, the algorithm consists of the following steps:
# Construct a Cartesian tree for the input sequence
# Initialize a priority queue, initially containing only the tree root
# While the priority queue is non-empty:
#* Find and remove the minimum value ''x'' in the priority queue
#* Add ''x'' to the output sequence
#* Add the Cartesian tree children of ''x'' to the priority queue

As Levcopoulos and Petersson show, for input sequences that are already nearly sorted, the size of the priority queue will remain small, allowing this method to take advantage of the nearly-sorted input and run more quickly. Specifically, the worst-case running time of this algorithm is O(''n''&amp;nbsp;log&amp;nbsp;''k''), where ''k'' is the average, over all values ''x'' in the sequence, of the number of consecutive pairs of sequence values that bracket ''x''. They also prove a lower bound stating that, for any ''n'' and ''k''&amp;nbsp;=&amp;nbsp;ω(1), any comparison-based sorting algorithm must use Ω(''n''&amp;nbsp;log&amp;nbsp;''k'') comparisons for some inputs.

==History==
Cartesian trees were introduced and named by {{harvtxt|Vuillemin|1980}}. The name is derived from the [[Cartesian coordinate]] system for the plane: in Vuillemin's version of this structure, as in the two-dimensional range searching application discussed above, a Cartesian tree for a point set has the sorted order of the points by their ''x''-coordinates as its symmetric traversal order, and it has the heap property according to the ''y''-coordinates of the points.
{{harvtxt|Gabow|Bentley|Tarjan|1984}} and subsequent authors followed the definition here in which a Cartesian tree is defined from a sequence; this change generalizes the geometric setting of Vuillemin to allow sequences other than the sorted order of ''x''-coordinates, and allows the Cartesian tree to be applied to non-geometric problems as well.

==Notes==
{{reflist}}

==References==
*{{citation
 | last1 = Bender | first1 = Michael A.
 | last2 = Farach-Colton | first2 = Martin
 | contribution = The LCA problem revisited
 | pages = 88–94
 | publisher = Springer-Verlag, Lecture Notes in Computer Science 1776
 | title = Proceedings of the 4th Latin American Symposium on Theoretical Informatics
 | url = http://www.cs.sunysb.edu/~bender/pub/lca.ps
 | year = 2000}}.
*{{citation
 | last1 = Berkman | first1 = Omer
 | last2 = Schieber | first2 = Baruch
 | last3 = Vishkin | first3 = Uzi | author3-link = Uzi Vishkin
 | doi = 10.1006/jagm.1993.101
 | issue = 3
 | journal = Journal of Algorithms
 | pages = 344–370
 | title = Optimal doubly logarithmic parallel algorithms based on finding all nearest smaller values
 | volume = 14
 | year = 1993}}.
*{{citation|contribution=On Cartesian trees and range minimum queries|first1=Erik D.|last1=Demaine|author1-link=Erik Demaine|first2=Gad M.|last2=Landau|first3=Oren|last3=Weimann|series=Lecture Notes in Computer Science|volume=5555|year=2009|pages=341–353|doi=10.1007/978-3-642-02927-1_29|title=Automata, Languages and Programming, 36th International Colloquium, ICALP 2009, Rhodes, Greece, July 5-12, 2009|isbn=978-3-642-02926-4}}.
*{{citation
 | last1 = Fischer | first1 = Johannes
 | last2 = Heun | first2 = Volker
 | contribution = Theoretical and Practical Improvements on the RMQ-Problem, with Applications to LCA and LCE
 | title = Proceedings of the 17th Annual Symposium on Combinatorial Pattern Matching
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | volume = 4009
 | year = 2006
 | pages = 36–48
 | doi = 10.1007/11780441_5
 | isbn = 978-3-540-35455-0}}
*{{citation
 | last1 = Fischer | first1 = Johannes
 | last2 = Heun | first2 = Volker
 | contribution = A New Succinct Representation of RMQ-Information and Improvements in the Enhanced Suffix Array.
 | title = Proceedings of the International Symposium on Combinatorics, Algorithms, Probabilistic and Experimental Methodologies 
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | volume = 4614
 | year = 2007
 | pages = 459–470
 | doi = 10.1007/978-3-540-74450-4_41
 | isbn = 978-3-540-74449-8}}
*{{citation
 | last1 = Gabow | first1 = Harold N.
 | last2 = Bentley | first2 = Jon Louis | author2-link = Jon Bentley
 | last3 = Tarjan | first3 = Robert E. | author3-link = Robert Tarjan
 | contribution = Scaling and related techniques for geometry problems
 | doi = 10.1145/800057.808675
 | isbn = 0-89791-133-4
 | location = New York, NY, USA
 | pages = 135–143
 | publisher = ACM
 | title = [[Symposium on Theory of Computing|STOC '84: Proc. 16th ACM Symp. Theory of Computing]]
 | year = 1984}}.
*{{citation
 | last1 = Harel | first1 = Dov
 | last2 = Tarjan | first2 = Robert E. | author2-link = Robert Tarjan
 | doi = 10.1137/0213024
 | issue = 2
 | journal = [[SIAM Journal on Computing]]
 | pages = 338–355
 | title = Fast algorithms for finding nearest common ancestors
 | volume = 13
 | year = 1984}}.
*{{citation|title=The maximum capacity route problem|first=T. C.|last=Hu|journal=Operations Research|volume=9|issue=6|year=1961|pages=898–900|jstor=167055|doi=10.1287/opre.9.6.898}}.
*{{citation
 | last = Leclerc | first = Bruno
 | mr = 623034
 | issue = 73
 | journal = Centre de Mathématique Sociale. École Pratique des Hautes Études. Mathématiques et Sciences Humaines
 | language = French
 | pages = 5–37, 127
 | title = Description combinatoire des ultramétriques
 | year = 1981}}.
*{{citation
 | last1 = Levcopoulos | first1 = Christos
 | last2 = Petersson | first2 = Ola
 | contribution = Heapsort - Adapted for Presorted Files
 | doi = 10.1007/3-540-51542-9_41
 | location = London, UK
 | pages = 499–509
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | title = WADS '89: Proceedings of the Workshop on Algorithms and Data Structures
 | volume = 382
 | year = 1989}}.
*{{citation
 | last1 = Seidel | first1 = Raimund
 | last2 = Aragon | first2 = Cecilia R.
 | doi = 10.1007/s004539900061
 | issue = 4/5
 | journal = Algorithmica
 | pages = 464–497
 | title = Randomized Search Trees
 | url = http://citeseer.ist.psu.edu/seidel96randomized.html
 | volume = 16
 | year = 1996}}.
*{{citation
 | last1 = Schieber | first1 = Baruch
 | last2 = Vishkin | first2 = Uzi | author2-link = Uzi Vishkin
 | doi = 10.1137/0217079
 | journal = [[SIAM Journal on Computing]]
 | pages = 1253–1262
 | title = On finding lowest common ancestors: simplification and parallelization
 | volume = 17
 | year = 1988
 | issue = 6}}.
*{{citation
 | last = Vuillemin | first = Jean
 | doi = 10.1145/358841.358852
 | issue = 4
 | journal = Commun. ACM
 | location = New York, NY, USA
 | pages = 229–239
 | publisher = ACM
 | title = A unifying look at data structures
 | volume = 23
 | year = 1980}}.

{{CS-Trees}}
{{Sorting}}

[[Category:Binary trees]]
[[Category:Sorting algorithms]]</text>
      <sha1>5ziqyg84d1zxmmdkwbbh2eeegxcnxjj</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Random binary tree</title>
    <ns>0</ns>
    <id>22045750</id>
    <revision>
      <id>617399839</id>
      <parentid>616406952</parentid>
      <timestamp>2014-07-18T02:00:05Z</timestamp>
      <contributor>
        <ip>2601:0:200:62A:20E:C6FF:FE88:F031</ip>
      </contributor>
      <comment>Undid revision 616406952 by [[Special:Contributions/103.228.42.133|103.228.42.133]] ([[User talk:103.228.42.133|talk]])</comment>
      <text xml:space="preserve" bytes="13803">{{Probabilistic}}
In [[computer science]] and [[probability theory]], a '''random binary tree''' refers to a [[binary tree]] selected at random from some [[probability distribution]] on binary trees. Two different distributions are commonly used: binary trees formed by inserting nodes one at a time according to a [[random permutation]], and binary trees chosen from a [[Uniform distribution (discrete)|uniform discrete distribution]] in which all distinct trees are equally likely. It is also possible to form other distributions, for instance by repeated splitting. Adding and removing nodes directly in a random binary tree will in general disrupt its random structure, but the [[treap]] and related randomized binary search tree [[data structure]]s use the principle of binary trees formed from a random permutation in order to maintain a [[balanced binary search tree]] dynamically as nodes are inserted and deleted.

For random trees that are not necessarily binary, see [[random tree]].

==Binary trees from random permutations==
For any set of numbers (or, more generally, values from some [[total order]]), one may form a [[binary search tree]] in which each number is inserted in sequence as a leaf of the tree, without changing the structure of the previously inserted numbers. The position into which each number should be inserted is uniquely determined by a [[binary search]] in the tree formed by the previous numbers. For instance, if the three numbers (1,3,2) are inserted into a tree in that sequence, the number 1 will sit at the root of the tree, the number 3 will be placed as its right child, and the number 2 as the left child of the number 3. There are six different permutations of the numbers (1,2,3), but only five trees may be constructed from them. That is because the permutations (2,1,3) and (2,3,1) form the same tree.

===Expected depth of a node===
For any fixed choice of a value {{mvar|x}} in a given set of {{mvar|n}} numbers, if one randomly permutes the numbers and forms a binary tree from them as described above, the [[expected value]] of the length of the path from the root of the tree to {{mvar|x}} is at most {{math|2 log ''n'' + ''O''(1)}}, where &quot;{{math|log}}&quot; denotes the [[natural logarithm]] function and the {{mvar|O}} introduces [[big O notation]]. For, the expected number of ancestors of {{mvar|x}} is by linearity of expectation equal to the sum, over all other values {{mvar|y}} in the set, of the probability that {{mvar|y}} is an ancestor of {{mvar|x}}. And a value {{mvar|y}} is an ancestor of {{mvar|x}} exactly when {{mvar|y}} is the first element to be inserted from the elements in the interval {{math|[''x'',''y'']}}. Thus, the values that are adjacent to {{mvar|x}} in the sorted sequence of values have probability {{math|1/2}} of being an ancestor of {{mvar|x}}, the values one step away have probability {{math|1/3}}, etc. Adding these probabilities for all positions in the sorted sequence gives twice a [[Harmonic number]], leading to the bound above. A bound of this form holds also for the expected search length of a path to a fixed value {{mvar|x}} that is not part of the given set.&lt;ref&gt;{{harvtxt|Hibbard|1962}}; {{harvtxt|Knuth|1973}}; {{harvtxt|Mahmoud|1992}}, p. 75.&lt;/ref&gt;

===The longest path===
Although not as easy to analyze as the average path length, there has also been much research on determining the expectation (or high probability bounds) of the length of the longest path in a binary search tree generated from a random insertion order. It is now known that this length, for a tree with {{mvar|n}} nodes, is almost surely
:&lt;math&gt;\frac{1}{\beta}\log n \approx 4.311\log n,&lt;/math&gt;
where {{math|''β''}} is the unique number in the range {{math|0 &lt; ''β'' &lt; 1}} satisfying the equation
:&lt;math&gt;\displaystyle 2\beta e^{1-\beta}=1.&lt;/math&gt;&lt;ref&gt;{{harvtxt|Robson|1979}}; {{harvtxt|Pittel|1985}}; {{harvtxt|Devroye|1986}}; {{harvtxt|Mahmoud|1992}}, pp. 91–99; {{harvtxt|Reed|2003}}.&lt;/ref&gt;

===Expected number of leaves===
In the random permutation model, each of the numbers from the set of numbers used to form the tree, except for the smallest and largest of the numbers, has probability {{math|1/3}} of being a leaf in the tree, for it is a leaf when it inserted after its two neighbors, and any of the six permutations of these two neighbors and it are equally likely. By similar reasoning, the smallest and largest of the numbers have probability {{math|1/2}} of being a leaf. Therefore, the expected number of leaves is the sum of these probabilities, which for {{math|''n'' ≥ 2}} is exactly {{math|(''n'' + 1)/3}}.

===Treaps and randomized binary search trees===
In applications of binary search tree data structures, it is rare for the values in the tree to be inserted without deletion in a random order, limiting the direct applications of random binary trees. However, algorithm designers have devised data structures that allow insertions and deletions to be performed in a binary search tree, at each step maintaining as an invariant the property that the shape of the tree is a random variable with the same distribution as a random binary search tree.

If a given set of ordered numbers is assigned numeric priorities (distinct numbers unrelated to their values), these priorities may be used to construct a [[Cartesian tree]] for the numbers, a binary tree that has as its inorder traversal sequence the sorted sequence of the numbers and that is [[binary heap|heap-ordered]] by priorities. Although more efficient construction algorithms are known, it is helpful to think of a Cartesian tree as being constructed by inserting the given numbers into a binary search tree in priority order. Thus, by choosing the priorities either to be a set of independent random real numbers in the unit interval, or by choosing them to be a random permutation of the numbers from {{math|1}} to {{mvar|n}} (where {{mvar|n}} is the number of nodes in the tree), and by maintaining the heap ordering property using [[tree rotation]]s after any insertion or deletion of a node, it is possible to maintain a data structure that behaves like a random binary search tree. Such a data structure is known as a [[treap]] or a randomized binary search tree.&lt;ref&gt;{{harvtxt|Martinez|Roura|1992}}; {{harvtxt|Seidel|Aragon|1996}}.&lt;/ref&gt;

==Uniformly random binary trees==
The number of binary trees with ''n'' nodes is a [[Catalan number]]: for {{math|1=''n''&amp;nbsp;=&amp;nbsp;1, 2, 3, ...}} these numbers of trees are
:{{math|1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, …}} {{OEIS|id=A000108}}.
Thus, if one of these trees is selected uniformly at random, its probability is the [[multiplicative inverse|reciprocal]] of a Catalan number. Trees in this model have expected depth proportional to the square root of {{mvar|n}}, rather than to the logarithm;&lt;ref&gt;{{harvtxt|Knuth|2005}}, p. 15.&lt;/ref&gt; however, the [[Strahler number]] of a uniformly random binary tree, a more sensitive measure of the distance from a leaf in which a node has Strahler number {{mvar|i}} whenever it has either a child with that number or two children with number {{math|''i'' &amp;minus; 1}}, is with high probability logarithmic.&lt;ref&gt;{{harvtxt|Devroye|Kruszewski|1995}}. That it is at most logarithmic is trivial, because the Strahler number of every tree is bounded by the logarithm of the number of its nodes.&lt;/ref&gt;

Due to their large heights, this model of equiprobable random trees is not generally used for binary search trees, but it has been applied to problems of modeling the [[parse tree]]s of [[Expression (mathematics)|algebraic expressions]] in [[compiler]] design&lt;ref&gt;{{harvtxt|Mahmoud|1992}}, p. 63.&lt;/ref&gt; (where the above-mentioned bound on Strahler number translates into the [[register allocation|number of registers]] needed to evaluate an expression&lt;ref&gt;{{harvtxt|Flajolet|Raoult|Vuillemin|1979}}.&lt;/ref&gt;) and for modeling [[evolutionary tree]]s.&lt;ref&gt;{{harvtxt|Aldous|1996}}.&lt;/ref&gt; In some cases the analysis of random binary trees under the random permutation model can be automatically transferred to the uniform model.&lt;ref&gt;{{harvtxt|Mahmoud|1992}}, p. 70.&lt;/ref&gt;

==Random split trees==
{{harvtxt|Devroye|Kruszewski|1996}} generate random binary trees with {{mvar|n}} nodes by generating a real-valued random variable {{mvar|x}} in the unit interval {{math|(0,1)}}, assigning the first {{math|''xn''}} nodes (rounded down to an integer number of nodes) to the left subtree, the next node to the root, and the remaining nodes to the right subtree, and continuing recursively in each subtree. If {{mvar|x}} is chosen uniformly at random in the interval, the result is the same as the random binary search tree generated by a random permutation of the nodes, as any node is equally likely to be chosen as root; however, this formulation allows other distributions to be used instead. For instance, in the uniformly random binary tree model, once a root is fixed each of its two subtrees must also be uniformly random, so the uniformly random model may also be generated by a different choice of distribution for {{mvar|x}}. As Devroye and Kruszewski show, by choosing a [[beta distribution]] on {{mvar|x}} and by using an appropriate choice of shape to draw each of the branches, the mathematical trees generated by this process can be used to create realistic-looking botanical trees.

==Notes==
{{reflist|colwidth=40em}}

==References==
*{{citation
 | last = Aldous | first = David
 | contribution = Probability distributions on cladograms
 | editor1-last = Aldous | editor1-first = David
 | editor2-last = Pemantle | editor2-first = Robin
 | pages = 1–18
 | publisher = Springer-Verlag
 | series = The IMA Volumes in Mathematics and its Applications
 | title = Random Discrete Structures
 | volume = 76
 | year = 1996}}.
*{{citation
 | last = Devroye | first = Luc
 | doi = 10.1145/5925.5930
 | issue = 3
 | journal = Journal of the ACM
 | pages = 489–498
 | title = A note on the height of binary search trees
 | volume = 33
 | year = 1986}}.
*{{citation|first1=Luc|last1=Devroye|first2=Paul|last2=Kruszewski|title=A note on the Horton-Strahler number for random trees|journal=Information Processing Letters|volume=56|issue=2|year=1995|doi=10.1016/0020-0190(95)00114-R|pages=95–99}}.
*{{citation
 | last1 = Devroye | first1 = Luc
 | last2 = Kruszewski | first2 = Paul
 | contribution = The botanical beauty of random binary trees
 | doi = 10.1007/BFb0021801
 | editor-last = Brandenburg | editor-first = Franz J.
 | pages = 166–177
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | title = Graph Drawing: 3rd Int. Symp., GD'95, Passau, Germany, September 20-22, 1995
 | volume = 1027
 | year = 1996
 | isbn = 3-540-60723-4}}.
*{{citation|last=Drmota|first=Michael|title=Random Trees : An Interplay between Combinatorics and Probability|publisher=Springer-Verlag|year=2009|isbn=978-3-211-75355-2}}.
*{{citation|first1=P.|last1=Flajolet|author1-link=Philippe Flajolet|first2=J. C.|last2=Raoult|first3=J.|last3=Vuillemin|title=The number of registers required for evaluating arithmetic expressions|journal=Theoretical Computer Science|volume=9|issue=1|year=1979|pages=99–125|doi=10.1016/0304-3975(79)90009-4}}.
*{{citation
 | last = Hibbard | first = T.
 | doi = 10.1145/321105.321108
 | issue = 1
 | journal = Journal of the ACM
 | pages = 13–28
 | title = Some combinatorial properties of certain trees with applications to searching and sorting
 | volume = 9
 | year = 1962}}.
*{{citation
 | last = Knuth | first = Donald M. | author-link = Donald Knuth
 | contribution = 6.2.2 Binary Tree Searching
 | pages = 422–451
 | publisher = Addison-Wesley
 | title = [[The Art of Computer Programming]]
 | volume = III
 | year = 1973}}.
*{{citation
 | last = Knuth | first = Donald M. | author-link = Donald Knuth
 | contribution = Draft of Section 7.2.1.6: Generating All Trees
 | title = [[The Art of Computer Programming]]
 | volume = IV
 | url = http://www-cs-faculty.stanford.edu/~knuth/fasc4a.ps.gz
 | year = 2005}}.
*{{citation
 | last = Mahmoud | first = Hosam M.
 | publisher = John Wiley &amp; Sons
 | title = Evolution of Random Search Trees
 | year = 1992}}.
*{{citation
 | last1 = Martinez | first1 = Conrado
 | last2 = Roura | first2 = Salvador
 | doi = 10.1145/274787.274812
 | issue = 2
 | journal = Journal of the ACM
 | pages = 288–323
 | publisher = ACM Press
 | title = Randomized binary search trees
 | url = http://citeseer.ist.psu.edu/article/martinez97randomized.html
 | volume = 45
 | year = 1998}}.
*{{citation
 | last = Pittel | first = B.
 | doi = 10.1214/aop/1176993000
 | issue = 2
 | journal = Annals of Probability
 | pages = 414–427
 | title = Asymptotical growth of a class of random trees
 | volume = 13
 | year = 1985}}.
*{{citation
 | last = Reed | first = Bruce | authorlink = Bruce Reed (mathematician)
 | doi = 10.1145/765568.765571
 | issue = 3
 | journal = Journal of the ACM
 | pages = 306–332
 | title = The height of a random binary search tree
 | volume = 50
 | year = 2003}}.
*{{citation
 | last = Robson | first = J. M.
 | journal = Australian Computer Journal
 | pages = 151–153
 | title = The height of binary search trees
 | volume = 11
 | year = 1979}}.
*{{citation
 | last1 = Seidel | first1 = Raimund
 | last2 = Aragon | first2 = Cecilia R.
 | doi = 10.1007/s004539900061
 | issue = 4/5
 | journal = Algorithmica
 | pages = 464–497
 | title = Randomized Search Trees
 | url = http://citeseer.ist.psu.edu/seidel96randomized.html
 | volume = 16
 | year = 1996}}.

==External links==

*[http://opendatastructures.org/versions/edition-0.1e/ods-java/7_Random_Binary_Search_Tree.html Open Data Structures - Chapter 7 - Random Binary Search Trees]

[[Category:Binary trees]]
[[Category:Randomness]]
[[Category:Probabilistic data structures]]</text>
      <sha1>lne8t9dcmutv2e0iq70338fuubbhrt4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>AVL tree</title>
    <ns>0</ns>
    <id>2118</id>
    <revision>
      <id>623942552</id>
      <parentid>618018029</parentid>
      <timestamp>2014-09-03T01:35:45Z</timestamp>
      <contributor>
        <username>Dexbot</username>
        <id>16752040</id>
      </contributor>
      <minor/>
      <comment>Removing Link GA template ([[d:Wikidata:Development plan#Badges|handled by wikidata]])</comment>
      <text xml:space="preserve" bytes="13675">{{Infobox data structure
|name=AVL tree
|type=tree
|invented_by=[[Georgii Adelson-Velskii|G. M. Adelson-Velskii]] and [[Yevgeniy Landis|E. M. Landis]]
|invented_year=1962
|
|space_avg=O(n)
|space_worst=O(n)
|search_avg=O(log n)
|search_worst=O(log n)
|insert_avg=O(log n)
|insert_worst=O(log n)
|delete_avg=O(log n)
|delete_worst=O(log n)
}}
[[Image:AVLtreef.svg|thumb|right|251px|Example AVL tree]]

In [[computer science]], an '''AVL tree''' (Adelson-Velskii and Landis' tree, named after the inventors) is a [[self-balancing binary search tree]]. It was the first such [[data structure]] to be invented.&lt;ref&gt;[[Robert Sedgewick (computer scientist)|Robert Sedgewick]], ''Algorithms'', Addison-Wesley, 1983, ISBN 0-201-06672-6, page 199, chapter 15: Balanced Trees.&lt;/ref&gt; In an AVL tree, the [[tree height|heights]] of the two [[child node|child]] subtrees of any node differ by at most one; if at any time they differ by more than one, rebalancing is done to restore this property. Lookup, insertion, and deletion all take [[big O notation|O]](log ''n'') time in both the average and worst cases, where ''n'' is the number of nodes in the tree prior to the operation. Insertions and deletions may require the tree to be rebalanced by one or more [[tree rotation]]s.

The AVL tree is named after its two [[Soviet Union|Soviet]] inventors, [[Georgii Adelson-Velskii|G. M. Adelson-Velskii]] and [[Yevgeniy Landis|E. M. Landis]], who published it in their 1962 paper &quot;An algorithm for the organization of information&quot;.&lt;ref&gt;{{cite journal|last=Adelson-Velskii|first=G.|author2=E. M. Landis|year=1962|title=An algorithm for the organization of information|journal=[[Proceedings of the USSR Academy of Sciences]]|volume=146|pages=263–266}} {{ru icon}} English translation by Myron J. Ricci in ''Soviet Math. Doklady'', 3:1259&amp;ndash;1263, 1962.&lt;/ref&gt;

AVL trees are often compared with [[red-black tree]]s because both support the same set of operations and take [[big O notation|O]](log ''n'') time for the basic operations.  For lookup-intensive applications, AVL trees are faster than red-black trees because they are more rigidly balanced.&lt;ref&gt;{{cite web|last = Pfaff|first = Ben|title = Performance Analysis of BSTs in System Software| publisher = [[Stanford university|Stanford University]]|date=June 2004|url = http://www.stanford.edu/~blp/papers/libavl.pdf|format = PDF}}&lt;/ref&gt; Similar to red-black trees, AVL trees are height-balanced. Both are in general not [[Weight-balanced tree|weight-balanced]] nor μ-balanced for any &lt;math&gt;\scriptstyle \mu\leq\tfrac12&lt;/math&gt;;&lt;ref&gt;[http://cs.stackexchange.com/questions/421/avl-trees-are-not-weight-balanced AVL trees are not weight-balanced? (meaning: AVL trees are not μ-balanced?)] &lt;br&gt;Thereby: A Binary Tree is called &lt;math&gt;\mu&lt;/math&gt;-balanced, with &lt;math&gt;0 \le\mu\leq\tfrac12&lt;/math&gt;, if for every node &lt;math&gt;N&lt;/math&gt;, the inequality
: &lt;math&gt;\tfrac12-\mu\le\tfrac{|N_l|}{|N|+1}\le \tfrac12+\mu&lt;/math&gt;
holds and &lt;math&gt;\mu&lt;/math&gt; is minimal with this property. &lt;math&gt;|N|&lt;/math&gt; is the number of nodes below the tree with &lt;math&gt;N&lt;/math&gt; as root (including the root) and &lt;math&gt;N_l&lt;/math&gt; is the left child node of &lt;math&gt;N&lt;/math&gt;.&lt;/ref&gt; that is, sibling nodes can have hugely differing numbers of descendants.

==Operations==
[[File:BinaryTreeRotations.svg|thumb|300px|Tree rotations]]
Basic operations of an AVL tree involve carrying out the same actions as would be carried out on an unbalanced [[binary search tree]], but modifications are followed by zero or more operations called [[tree rotation]]s, which help to restore the height balance of the subtrees.

===Searching===
Once a node has been found in a balanced tree, the ''next'' or ''previous'' nodes can be explored in [[amortized complexity|amortized]] constant time. Some instances of exploring these &quot;nearby&quot; nodes require traversing up to 2×log(''n'') links (particularly when moving from the rightmost leaf of the root's left sub tree to the leftmost leaf of the root's right sub tree; in the example AVL tree, moving from node 14 to the ''next but one'' node 19 takes 4 steps). However, exploring all ''n'' nodes of the tree in this manner would use each link exactly twice: one traversal to enter the sub tree rooted at that node, another to leave that node's sub tree after having explored it. And since there are ''n''−1 links in any tree, the amortized cost is found to be 2×(''n''−1)/''n'', or approximately 2.

===Insertion===
[[File:AVL Tree Rebalancing.svg|thumb|350px|Pictorial description of how rotations cause rebalancing tree, and then retracing one's steps toward the root updating the balance factor of the nodes. The numbered circles represent the nodes being balanced. The lettered triangles represent subtrees which are themselves balanced BSTs. A blue number next to a node denotes possible balance factors.]]

After inserting a node, it is necessary to check each of the node's ancestors for consistency with the rules of AVL. The balance factor is calculated as follows: balanceFactor = height(left subtree) - height(right subtree). For each node checked, if the balance factor remains −1, 0, or +1 then no rotations are necessary. However, if balance factor becomes less than -1 or greater than +1, the subtree rooted at this node is unbalanced. If insertions are performed serially, after each insertion, at most one of the following cases needs to be resolved to restore the entire tree to the rules of AVL.

Let us first assume the balance factor of a node L is 2 (as opposed to the other possible unbalanced value -2). This case is depicted in the left column of the illustration with L=5. We then look at the left subtree (the larger one) with root P. If this subtree does not lean to the right - i.e. P has balance factor 0 or 1 - we can rotate the whole tree to the right to get a balanced tree. This is labelled as the &quot;Left Left Case&quot; in the illustration with P=4. If the subtree does lean to the right - i.e. P has balance factor -1 - we first rotate the subtree to the left and end up the previous case. The second case is labelled as &quot;Left Right Case&quot; with P=5 in the illustration.

If the balance factor of the node L is -2 (this case is depicted in the right column of the illustration L=3) we can mirror the above algorithm. I.e. if the root P of the right subtree has balance factor 0 or -1 we can rotate the whole tree to the left to get a balanced tree. This is labelled as the &quot;Right Right Case&quot; in the illustration with P=4. If the root P of the right subtree has balance factor 1 we can rotate the subtree to the right to end up in the &quot;Right Right Case&quot;.

The whole algorithm looks like this:

  if (balance_factor(L) == 2) { //The left column
    let P=left_child(L)
    if (balance_factor(P) == -1) { //The &quot;Left Right Case&quot;
       rotate_left(P) //reduce to &quot;Left Left Case&quot;
    }
    //Left Left Case
    rotate_right(L);
  } else { // balance_factor(L) == -2, the right column
    let P=right_child(L)
    if (balance_factor(P) == 1) { //The &quot;Right Left Case&quot;
       rotate_right(P) //reduce to &quot;Right Right Case&quot;
    }
    //Right Right Case
    rotate_left(L);
  }

The names of the cases refer to the portion of the tree that is reduced in height.

In order to restore the balance factors of all nodes, first observe that all nodes requiring correction lie along the path used during the initial insertion. If the above procedure is applied to nodes along this path, starting from the bottom (i.e. the node furthest away from the root), then every node in the tree will again have a balance factor of -1, 0, or 1.

===Deletion===
Let''' node X''' be the node with the value we need to delete, and let''' node Y''' be a node in the tree we need to find to take node X's place, and let '''node Z''' be the actual node we take out of the tree.

Steps to consider when deleting a node in an AVL tree are the following:
# If '''node X''' is a leaf or has only one child, skip to step 5. (node Z will be node X)
# Otherwise, determine '''node Y''' by finding the largest node in '''node X'''&lt;nowiki/&gt;'s left sub tree (in-order predecessor) or the smallest in its right sub tree (in-order successor).
# Replace '''node X''' with '''node Y''' (remember, tree structure doesn't change here, only the values). In this step,''' node X''' is essentially deleted when its internal values were overwritten with '''node Y'''&lt;nowiki/&gt;'s.
# Choose '''node Z''' to be the old '''node Y'''.
# Attach '''node Z'''&lt;nowiki/&gt;'s subtree to its parent (if it has a subtree). If '''node Z'''&lt;nowiki/&gt;'s parent is null, update root. (node Z is currently root)
# Delete '''node Z'''.
#  Retrace the path back up the tree (starting with node Z's parent) to the root, adjusting the balance factors as needed.
As with all binary trees, a node's in-order successor is the left-most child of its right subtree, and a node's in-order predecessor is the right-most child of its left subtree. In either case, this node will have zero or one children. Delete it according to one of the two simpler cases above.
[[File:binary search tree delete.svg|thumb|640px|center|Deleting a node with two children from a binary search tree using the inorder predecessor (rightmost node in the left subtree, labelled '''6''').]]

In addition to the balancing described above for insertions, if the balance factor for the tree is 2 and that of the left subtree is 0, a right rotation must be performed on P.  The mirror of this case is also necessary.

The retracing can stop if the balance factor becomes −1 or +1 indicating that the height of that subtree has remained unchanged.
If the balance factor becomes 0 then the height of the subtree has decreased by one and the retracing needs to continue.
If the balance factor becomes −2 or +2 then the subtree is unbalanced and needs to be rotated to fix it.
If the rotation leaves the subtree's balance factor at 0 then the retracing towards the root must continue since the height of this subtree has decreased by one.
This is in contrast to an insertion where a rotation resulting in a balance factor of 0 indicated that the subtree's height has remained unchanged.

The time required is O(log ''n'') for lookup, plus a maximum of O(log ''n'') rotations on the way back to the root, so the operation can be completed in O(log ''n'') time.

==Comparison to other structures==
Both AVL trees and red-black trees are self-balancing binary search trees and they are very similar mathematically.&lt;ref&gt;In fact, each AVL tree can be colored red-black.&lt;/ref&gt; The operations to balance the trees are different, but both occur on the average in O(1) with maximum in O(log ''n''). The real difference between the two is the limiting height.
For a tree of size &lt;math&gt; n &lt;/math&gt;:
*An AVL tree's height is strictly less than:&lt;ref&gt;{{cite book|last=Burkhard|first=Walt|title=Advanced Data Structures|url=http://ieng6.ucsd.edu/~cs100s/public/Notes/CS100s12.pdf|date=Spring 2012|publisher=[http://softreserves.ucsd.edu/ A.S. Soft Reserves], [[UC San Diego]]|location=[[La Jolla]]|page=103|chapter=AVL Dictionary Data Type Implementation}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last=Knuth|first=Donald E.|title=Sorting and searching|year=2000|publisher=Addison-Wesley|location=Boston [u.a.]|isbn=0-201-89685-0|pages=460|edition=2. ed., 6. printing, newly updated and rev.}}&lt;/ref&gt;
*:&lt;math&gt;\log_\varphi(\sqrt 5 (n+2)) - 2 = { \log_2(\sqrt 5 (n+2)) \over \log_2(\varphi) } - 2 = \log_\varphi(2) \cdot \log_2(\sqrt 5 (n+2)) - 2 \approx 1.44\log_2(n+2) - 0.328&lt;/math&gt;
*:where &lt;math&gt;\varphi&lt;/math&gt; is the [[golden ratio]].
*A [[red-black tree]]'s height is at most &lt;math&gt;2\log_2(n+1)&lt;/math&gt;&lt;ref&gt;[[Red-black tree#Proof of asymptotic bounds|Proof of asymptotic bounds]]&lt;/ref&gt;

AVL trees are more rigidly balanced than [[red-black tree]]s, leading to slower insertion and removal but faster retrieval.

==See also==
*[[tree data structure|Trees]]
*[[Tree rotation]]
*[[Splay tree]]
*[[Scapegoat tree]]
*[[B-tree]]
*[[T-tree]]
*[[List of data structures]]

==References==
&lt;references /&gt;

==Further reading==
* [[Donald Knuth]]. ''The Art of Computer Programming'', Volume 3: ''Sorting and Searching'', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Pages 458&amp;ndash;475 of section 6.2.3: Balanced Trees.

==External links==
{{Wikibooks|Algorithm Implementation|Trees/AVL tree|AVL tree}}
{{Commons category|AVL-trees}}
*[https://github.com/vilkov/libxdg/wiki xdg library] by Dmitriy Vilkov: Serializable straight C-implementation could easily be taken from this library under [[GNU Lesser General Public License|GNU-LGPL]] and [[Academic Free License|AFL v2.0]] licenses.
*[http://www.nist.gov/dads/HTML/avltree.html Description from the Dictionary of Algorithms and Data Structures]
*[http://github.com/pgrafov/python-avl-tree/ Python Implementation]
*[http://piumarta.com/software/tree/ Single C header file by Ian Piumarta]
*[http://www.strille.net/works/media_technology_projects/avl-tree_2001/ AVL Tree Demonstration]
*[http://www.qmatica.com/DataStructures/Trees/AVL/AVLTree.html AVL tree applet – all operations]
*[http://github.com/fbuihuu/libtree Fast and efficient implementation of AVL Trees]
*[https://github.com/mondrake/Rbppavl PHP Implementation]
*[https://github.com/chdemko/php-sorted-collections AVL Threaded Tree PHP Implementation]
*[http://www.codeproject.com/Articles/12347/AVL-Binary-Tree-for-C C++ implementation which can be used as an array]
*[http://code.google.com/p/self-balancing-avl-tree/ Self balancing AVL tree with Concat and Split operations]

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Avl Tree}}
[[Category:1962 in computer science]]
[[Category:Binary trees]]
[[Category:Soviet inventions]]</text>
      <sha1>mm7lve6ln4h02yb2ar5uvqpshnf8tzw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Huffman coding</title>
    <ns>0</ns>
    <id>13883</id>
    <revision>
      <id>622125908</id>
      <parentid>622061264</parentid>
      <timestamp>2014-08-21T00:04:27Z</timestamp>
      <contributor>
        <username>Sebastiangarth</username>
        <id>9330384</id>
      </contributor>
      <comment>/* Compression */ + &quot;the most&quot;</comment>
      <text xml:space="preserve" bytes="30600">{{more footnotes|date=January 2011}}
[[Image:Huffman tree 2.svg|thumb|Huffman tree generated from the exact frequencies of the text &quot;this is an example of a huffman tree&quot;. The frequencies and codes of each character are below. Encoding the sentence with this code requires 135 bits, as opposed to 288 bits if 36 characters of 8 bits were used. (This assumes that the code tree structure is known to the decoder and thus does not need to be counted as part of the transmitted information.)]]
{| class=&quot;wikitable sortable&quot; style=&quot;float:right; clear:right;&quot;
!Char!!Freq!!Code
|-
|space||7||111
|-
|a    ||4||010
|-
|e    ||4||000
|-
|f    ||3||1101
|-
|h    ||2||1010
|-
|i    ||2||1000
|-
|m    ||2||0111
|-
|n    ||2||0010
|-
|s    ||2||1011
|-
|t    ||2||0110
|-
|l    ||1||11001
|-
|o    ||1||00110
|-
|p    ||1||10011
|-
|r    ||1||11000
|-
|u    ||1||00111
|-
|x    ||1||10010
|}
In [[computer science]] and [[information theory]], a '''Huffman code''' is an optimal [[prefix code]] found using the algorithm developed by [[David A. Huffman]] while he was a [[Doctor of Philosophy|Ph.D.]] student at [[Massachusetts Institute of Technology|MIT]], and published in the 1952 paper &quot;A Method for the Construction of Minimum-Redundancy Codes&quot;.&lt;ref&gt;{{cite doi|10.1109/JRPROC.1952.273898}}&lt;/ref&gt;  The process of finding and/or using such a code is called '''Huffman coding''' and is a common technique in [[entropy encoding]], including in [[lossless data compression]].  The algorithm's output can be viewed as a [[variable-length code]] table for encoding a source symbol (such as a character in a file).  Huffman's algorithm derives this table  based on the estimated probability or frequency of occurrence (''weight'') for each possible value of the source symbol.  As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols.  Huffman's method can be efficiently implemented, finding a code in [[linear time]] to the number of input weights if these weights are sorted.&lt;ref&gt;{{cite journal | first = Jan | last = van Leeuwen | authorlink = Jan van Leeuwen | url = http://www.staff.science.uu.nl/~leeuw112/huffman.pdf | title = On the construction of Huffman trees | journal = ICALP | year =1976 | pages = 382-410 | accessdate = 20 February 2014}}&lt;/ref&gt;  However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.

== History ==

In 1951, [[David A. Huffman]] and his [[MIT]] [[information theory]] classmates were given the choice of a term paper or a final [[exam]]. The professor, [[Robert M. Fano]], assigned a [[term paper]] on the problem of finding the most efficient binary code. Huffman, unable to prove any codes were the most efficient, was about to give up and start studying for the final when he hit upon the idea of using a frequency-sorted [[binary tree]] and quickly proved this method the most efficient.&lt;ref&gt;see Ken Huffman (1991)&lt;/ref&gt;

In doing so, the student outdid his professor, who had worked with [[information theory]] inventor [[Claude Shannon]] to develop a similar code.  By building the tree from the bottom up instead of 
the top down, Huffman avoided the major flaw of the suboptimal [[Shannon-Fano coding]].

== Terminology ==

Huffman coding uses a specific method for choosing the representation for each symbol, resulting in a prefix code (sometimes called &quot;prefix-free codes&quot;, that is, the bit string representing some particular symbol is never a prefix of the bit string representing any other symbol).  Huffman coding is such a widespread method for creating prefix codes that the term &quot;Huffman code&quot; is widely used as a synonym for &quot;prefix code&quot; even when such a code is not produced by Huffman's algorithm.

== Problem definition ==

=== Informal description ===
;Given: A set of symbols and their weights (usually [[Proportionality (mathematics)|proportional]] to probabilities).
;Find: A [[Prefix code|prefix-free binary code]] (a set of codewords) with minimum [[Expected value|expected]] codeword length (equivalently, a tree with minimum [[weighted path length from the root]]).

=== Formalized description ===
'''Input'''.&lt;br&gt;
Alphabet &lt;math&gt;A = \left\{a_{1},a_{2},\cdots,a_{n}\right\}&lt;/math&gt;, which is the symbol alphabet of size &lt;math&gt;n&lt;/math&gt;. &lt;br&gt;
Set &lt;math&gt;W = \left\{w_{1},w_{2},\cdots,w_{n}\right\}&lt;/math&gt;, which is the set of the (positive) symbol weights (usually proportional to probabilities), i.e. &lt;math&gt;w_{i} = \mathrm{weight}\left(a_{i}\right), 1\leq i \leq n&lt;/math&gt;. &lt;br&gt;
&lt;br&gt;
'''Output'''.&lt;br&gt;
Code &lt;math&gt;C \left(A,W\right) = \left\{c_{1},c_{2},\cdots,c_{n}\right\}&lt;/math&gt;, which is the set of (binary) codewords, where &lt;math&gt;c_{i}&lt;/math&gt; is the codeword for &lt;math&gt;a_{i}, 1 \leq i \leq n&lt;/math&gt;.&lt;br&gt;
&lt;br&gt;
'''Goal'''.&lt;br&gt;
Let &lt;math&gt;L\left(C\right) = \sum_{i=1}^{n}{w_{i}\times\mathrm{length}\left(c_{i}\right)}&lt;/math&gt; be the weighted path length of code &lt;math&gt;C&lt;/math&gt;. Condition: &lt;math&gt;L\left(C\right) \leq L\left(T\right)&lt;/math&gt; for any code &lt;math&gt;T\left(A,W\right)&lt;/math&gt;.

=== Samples ===
{|class=&quot;wikitable&quot;
!rowspan=&quot;2&quot; style=&quot;background:#efefef&quot;| Input (''A'', ''W'')
!style=&quot;background:#efefef;font-weight:normal&quot;| Symbol (''a''&lt;sub&gt;''i''&lt;/sub&gt;)
|align=&quot;center&quot; style=&quot;background:#efefef&quot;| a
|align=&quot;center&quot; style=&quot;background:#efefef&quot;| b
|align=&quot;center&quot; style=&quot;background:#efefef&quot;| c
|align=&quot;center&quot; style=&quot;background:#efefef&quot;| d
|align=&quot;center&quot; style=&quot;background:#efefef&quot;| e
!style=&quot;background:#efefef&quot;| Sum
|-
!style=&quot;background:#efefef;font-weight:normal&quot;| Weights (''w''&lt;sub&gt;''i''&lt;/sub&gt;)
|align=&quot;center&quot;| 0.10
|align=&quot;center&quot;| 0.15
|align=&quot;center&quot;| 0.30
|align=&quot;center&quot;| 0.16
|align=&quot;center&quot;| 0.29
|align=&quot;center&quot;| = 1
|-
!rowspan=&quot;3&quot; style=&quot;background:#efefef&quot;| Output ''C''
!style=&quot;background:#efefef;font-weight:normal&quot;| Codewords (''c''&lt;sub&gt;''i''&lt;/sub&gt;)
|align=&quot;center&quot;| &lt;tt&gt;010&lt;/tt&gt;
|align=&quot;center&quot;| &lt;tt&gt;011&lt;/tt&gt;
|align=&quot;center&quot;| &lt;tt&gt;11&lt;/tt&gt;
|align=&quot;center&quot;| &lt;tt&gt;00&lt;/tt&gt;
|align=&quot;center&quot;| &lt;tt&gt;10&lt;/tt&gt;
|rowspan=&quot;2&quot;|&amp;nbsp;
|-
!style=&quot;background:#efefef;font-weight:normal&quot;| Codeword length (in bits)&lt;br /&gt;(''l''&lt;sub&gt;''i''&lt;/sub&gt;)
|align=&quot;center&quot;| 3
|align=&quot;center&quot;| 3
|align=&quot;center&quot;| 2
|align=&quot;center&quot;| 2
|align=&quot;center&quot;| 2
|-
!style=&quot;background:#efefef;font-weight:normal&quot;| Contribution to weighted path length&lt;br /&gt;(''l''&lt;sub&gt;''i''&lt;/sub&gt; ''w''&lt;sub&gt;''i''&lt;/sub&gt; )
|align=&quot;center&quot;| 0.30
|align=&quot;center&quot;| 0.45
|align=&quot;center&quot;| 0.60
|align=&quot;center&quot;| 0.32
|align=&quot;center&quot;| 0.58
|align=&quot;center&quot;| ''L''(''C'') = 2.25
|-
!rowspan=&quot;3&quot; style=&quot;background:#efefef&quot;| Optimality
!style=&quot;background:#efefef;font-weight:normal&quot;| Probability budget&lt;br /&gt;(2&lt;sup&gt;-''l''&lt;sub&gt;''i''&lt;/sub&gt;&lt;/sup&gt;)
| align=&quot;center&quot; | 1/8
| align=&quot;center&quot; | 1/8
| align=&quot;center&quot; | 1/4
| align=&quot;center&quot; | 1/4
| align=&quot;center&quot; | 1/4
| align=&quot;center&quot; | = 1.00
|-
! style=&quot;background: #efefef; font-weight: normal;&quot; | Information content (in bits)&lt;br /&gt;(−'''log'''&lt;sub&gt;2&lt;/sub&gt; ''w''&lt;sub&gt;''i''&lt;/sub&gt;) ≈
|align=&quot;center&quot;| 3.32
|align=&quot;center&quot;| 2.74
|align=&quot;center&quot;| 1.74
|align=&quot;center&quot;| 2.64
|align=&quot;center&quot;| 1.79
|align=&quot;center&quot;| &amp;nbsp;
|-
! style=&quot;background: #efefef; font-weight: normal;&quot; | Contribution to entropy&lt;br /&gt;(−''w''&lt;sub&gt;''i''&lt;/sub&gt; '''log'''&lt;sub&gt;2&lt;/sub&gt; ''w''&lt;sub&gt;''i''&lt;/sub&gt;)
|align=&quot;center&quot;| 0.332
|align=&quot;center&quot;| 0.411
|align=&quot;center&quot;| 0.521
|align=&quot;center&quot;| 0.423
|align=&quot;center&quot;| 0.518
|align=&quot;center&quot;| ''H''(''A'') = 2.205
|}

For any code that is ''biunique'', meaning that the code is [[Variable-length_code#Uniquely_decodable_codes|''uniquely decodeable'']], the sum of the probability budgets across all symbols is always less than or equal to one. In this example, the sum is strictly equal to one; as a result, the code is termed a ''complete'' code. If this is not the case, you can always derive an equivalent code by adding extra symbols (with associated null probabilities), to make the code complete while keeping it ''biunique''.

As defined by [[A Mathematical Theory of Communication|Shannon (1948)]], the information content ''h'' (in bits) of each symbol ''a''&lt;sub&gt;i&lt;/sub&gt; with non-null probability is

:&lt;math&gt;h(a_i) = \log_2{1 \over w_i}. &lt;/math&gt;

The [[information entropy|entropy]] ''H'' (in bits) is the weighted sum, across all symbols ''a''&lt;sub&gt;''i''&lt;/sub&gt; with non-zero probability ''w''&lt;sub&gt;''i''&lt;/sub&gt;, of the information content of each symbol:

:&lt;math&gt; H(A) = \sum_{w_i &gt; 0} w_i h(a_i) = \sum_{w_i &gt; 0} w_i \log_2{1 \over w_i} = - \sum_{w_i &gt; 0} w_i \log_2{w_i}. &lt;/math&gt;

(Note: A symbol with zero probability has zero contribution to the entropy, since &lt;math&gt;\lim_{w \to 0^+} w \log_2 w = 0&lt;/math&gt; So for simplicity, symbols with zero probability can be left out of the formula above.)

As a consequence of [[Shannon's source coding theorem]], the entropy is a measure of the smallest codeword length that is theoretically possible for the given alphabet with associated weights. In this example, the weighted average codeword length is 2.25 bits per symbol, only slightly larger than the calculated entropy of 2.205 bits per symbol. So not only is this code optimal in the sense that no other feasible code performs better, but it is very close to the theoretical limit established by Shannon.

Note that, in general, a Huffman code need not be unique.  Thus the set of Huffman codes for a given probability distribution is a non-empty subset of the codes minimizing &lt;math&gt;L(C)&lt;/math&gt; for that probability distribution.  (However, for each minimizing codeword length assignment, there exists at least one Huffman code with those lengths.)

== Basic technique ==

===Compression===
[[Image:Huffman coding example.svg|thumb|A source generates 4 different symbols &lt;math&gt;\{a_1 , a_2 , a_3 , a_4 \}&lt;/math&gt; with probability &lt;math&gt;\{0.4 ; 0.35 ; 0.2 ; 0.05 \}&lt;/math&gt;. A binary tree is generated from left to right taking the two least probable symbols and putting them together to form another equivalent symbol having a probability that equals the sum of the two symbols. The process is repeated until there is just one symbol. The tree can then be read backwards, from right to left, assigning different bits to different branches. The final Huffman code is:
{|class=&quot;wikitable&quot;
! Symbol !! Code
|-
|a1 || 0
|-
|a2 || 10
|-
|a3 || 110
|-
|a4 || 111
|-
|}
The standard way to represent a signal made of 4 symbols is by using 2 bits/symbol, but the [[Information entropy|entropy]] of the source is 1.74 bits/symbol. If this Huffman code is used to represent the signal, then the average length is lowered to 1.85 bits/symbol; it is still far from the theoretical limit because the probabilities of the symbols are different from negative powers of two.]]

The technique works by creating a [[binary tree]] of nodes. These can be stored in a regular [[Array data type|array]], the size of which depends on the number of symbols, &lt;math&gt;n&lt;/math&gt;. A node can be either a [[leaf node]] or an [[internal node]]. Initially, all nodes are leaf nodes, which contain the '''symbol''' itself, the '''weight''' (frequency of appearance) of the symbol and optionally, a link to a '''parent''' node which makes it easy to read the code (in reverse) starting from a leaf node. Internal nodes contain symbol '''weight''', links to '''two child nodes''' and the optional link to a '''parent''' node. As a common convention, bit '0' represents following the left child and bit '1' represents following the right child. A finished tree has up to &lt;math&gt;n&lt;/math&gt; leaf nodes and &lt;math&gt;n-1&lt;/math&gt; internal nodes. A Huffman tree that omits unused symbols produces the most optimal code lengths.

The process essentially begins with the leaf nodes containing the probabilities of the symbol they represent, then a new node whose children are the 2 nodes with smallest probability is created, such that the new node's probability is equal to the sum of the children's probability. With the previous 2 nodes merged into one node (thus not considering them anymore), and with the new node being now considered, the procedure is repeated until only one node remains, the Huffman tree.

The simplest construction algorithm uses a [[priority queue]] where the node with lowest probability is given highest priority:

# Create a leaf node for each symbol and add it to the priority queue.
# While there is more than one node in the queue:
## Remove the two nodes of highest priority (lowest probability) from the queue
## Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes' probabilities.
## Add the new node to the queue.
# The remaining node is the root node and the tree is complete.

Since efficient priority queue data structures require O(log ''n'') time per insertion, and a tree with ''n'' leaves has 2''n''−1 nodes, this algorithm operates in O(''n'' log ''n'') time, where ''n'' is the number of symbols.

If the symbols are sorted by probability, there is a [[linear-time]] (O(''n'')) method to create a Huffman tree using two [[Queue (data structure)|queues]], the first one containing the initial weights (along with pointers to the associated leaves), and combined weights (along with pointers to the trees) being put in the back of the second queue. This assures that the lowest weight is always kept at the front of one of the two queues:

#Start with as many leaves as there are symbols.
#Enqueue all leaf nodes into the first queue (by probability in increasing order so that the least likely item is in the head of the queue).
#While there is more than one node in the queues:
##Dequeue the two nodes with the lowest weight by examining the fronts of both queues.
##Create a new internal node, with the two just-removed nodes as children (either node can be either child) and the sum of their weights as the new weight.
##Enqueue the new node into the rear of the second queue.
#The remaining node is the root node; the tree has now been generated.

Although linear-time given sorted input, in the general case of arbitrary input, using this algorithm requires pre-sorting.  Thus, since sorting takes O(''n'' log ''n'') time in the general case, both methods have the same overall complexity.

In many cases, time complexity is not very important in the choice of algorithm here, since ''n'' here is the number of symbols in the alphabet, which is typically a very small number (compared to the length of the message to be encoded); whereas complexity analysis concerns the behavior when ''n'' grows to be very large.

It is generally beneficial to minimize the variance of codeword length. For example, a communication buffer receiving Huffman-encoded data may need to be larger to deal with especially long symbols if the tree is especially unbalanced. To minimize variance, simply break ties between queues by choosing the item in the first queue.   This modification will retain the mathematical optimality of the Huffman coding while both minimizing variance and minimizing the length of the longest character code.

Here's an example of optimized Huffman coding using the French subject string &quot;j'aime aller sur le bord de l'eau les jeudis ou les jours impairs&quot;. Note that original Huffman coding tree structure would be different from the given example:

[[Image:Huffman huff demo.gif|center]]

===Decompression===
Generally speaking, the process of decompression is simply a matter of translating the stream of prefix codes to individual byte values, usually by traversing the Huffman tree node by node as each bit is read from the input stream (reaching a leaf node necessarily terminates the search for that particular byte value). Before this can take place, however, the Huffman tree must be somehow reconstructed. In the simplest case, where character frequencies are fairly predictable, the tree can be preconstructed (and even statistically adjusted on each compression cycle) and thus reused every time, at the expense of at least some measure of compression efficiency. Otherwise, the information to reconstruct the tree must be sent a priori. A naive approach might be to prepend the frequency count of each character to the compression stream. Unfortunately, the overhead in such a case could amount to several kilobytes, so this method has little practical use. If the data is compressed using [[canonical Huffman code|canonical encoding]], the compression model can be precisely reconstructed with just &lt;math&gt;B2^B&lt;/math&gt; bits of information (where &lt;math&gt;B&lt;/math&gt; is the number of bits per symbol). Another method is to simply prepend the Huffman tree, bit by bit, to the output stream. For example, assuming that the value of 0 represents a parent node and 1 a leaf node, whenever the latter is encountered the tree building routine simply reads the next 8 bits to determine the character value of that particular leaf. The process continues recursively until the last leaf node is reached; at that point, the Huffman tree will thus be faithfully reconstructed. The overhead using such a method ranges from roughly 2 to 320 bytes (assuming an 8-bit alphabet). Many other techniques are possible as well. In any case, since the compressed data can include unused &quot;trailing bits&quot; the decompressor must be able to determine when to stop producing output. This can be accomplished by either transmitting the length of the decompressed data along with the compression model or by defining a special code symbol to signify the end of input (the latter method can adversely affect code length optimality, however).

== Main properties ==
The probabilities used can be generic ones for the application domain that are based on average experience, or they can be the actual frequencies found in the text being compressed.
This requires that a [[frequency table]] must be stored with the compressed text. See the Decompression section above for more information about the various techniques employed for this purpose.

=== Optimality ===

Although Huffman's original algorithm is optimal for a symbol-by-symbol coding (i.e., a stream of unrelated symbols) with a known input probability distribution, it is not optimal when the symbol-by-symbol restriction is dropped, or when the [[probability mass function]]s are unknown.  Also, if symbols are not [[independent and identically distributed]], a single code may be insufficient for optimality.  Other methods such as [[arithmetic coding]] and [[LZW]] coding often have better compression capability:  Both of these methods can combine an arbitrary number of symbols for more efficient coding, and generally adapt to the actual input statistics, useful when input probabilities are not precisely known or vary significantly within the stream.  However, these methods have higher computational complexity.  Also, both arithmetic coding and LZW were historically a subject of some concern over [[patent]] issues. However, as of mid-2010, the most commonly used techniques for these alternatives to Huffman coding have passed into the public domain as the early patents have expired.

However, the limitations of Huffman coding should not be overstated; it can be used adaptively, accommodating unknown, changing, or context-dependent probabilities.  In the case of known independent and identically distributed random variables, combining symbols (&quot;blocking&quot;) reduces inefficiency in a way that approaches optimality as the number of symbols combined increases.  Huffman coding is optimal when each input symbol is a known independent and identically distributed random variable having a probability that is an the inverse of a power of two.

Prefix codes tend to have inefficiency on small alphabets, where probabilities often fall between these optimal points.  The worst case for Huffman coding can happen when the probability of a symbol exceeds 2&lt;sup&gt;−1&lt;/sup&gt; = 0.5, making the upper limit of inefficiency unbounded. These situations often respond well to a form of blocking called [[run-length encoding]]; for the simple case of [[Bernoulli process]]es, [[Golomb coding]] is a provably optimal run-length code.

For a set of symbols with a uniform probability distribution and a number of members which is a [[power of two]], Huffman coding is equivalent to simple binary [[Block code|block encoding]], e.g., [[ASCII]] coding.  This reflects the fact that compression is not possible with such an input.

== Variations ==
Many variations of Huffman coding exist, some of which use a Huffman-like algorithm, and others of which find optimal prefix codes (while, for example, putting different restrictions on the output). Note that, in the latter case, the method need not be Huffman-like, and, indeed, need not even be [[polynomial time]]. An exhaustive list of papers on Huffman coding and its variations is given by &quot;Code and Parse Trees for Lossless Source Encoding&quot;[http://scholar.google.com/scholar?hl=en&amp;lr=&amp;cluster=6556734736002074338].

=== ''n''-ary Huffman coding ===
The '''''n''-ary Huffman''' algorithm uses the {0, 1, ... , ''n'' − 1} alphabet to encode message and build an ''n''-ary tree. This approach was considered by Huffman in his original paper. The same algorithm applies as for binary (''n'' equals 2) codes, except that the ''n'' least probable symbols are taken together, instead of just the 2 least probable. Note that for ''n'' greater than 2, not all sets of source words can properly form an ''n''-ary tree for Huffman coding. In this case, additional 0-probability place holders must be added. This is because the tree must form an ''n'' to 1 contractor; for binary coding, this is a 2 to 1 contractor, and any sized set can form such a contractor.  If the number of source words is congruent to 1 modulo ''n''-1, then the set of source words will form a proper Huffman tree.

=== Adaptive Huffman coding ===
A variation called '''[[adaptive Huffman coding]]''' involves calculating the probabilities dynamically based on recent actual frequencies in the sequence of source symbols, and changing the coding tree structure to match the updated probability estimates. It is used rarely in practice, since the cost of updating the tree makes it slower than optimized [[Arithmetic_coding#Adaptive_arithmetic_coding|adaptive arithmetic coding]], that is more flexible and has a better compression.

=== Huffman template algorithm ===
Most often, the weights used in implementations of Huffman coding represent numeric probabilities, but the algorithm given above does not require this; it requires only that the weights form a [[total order|totally ordered]] [[Monoid#Commutative monoid|commutative monoid]], meaning a way to order weights and to add them. The '''Huffman template algorithm''' enables one to use any kind of weights (costs, frequencies, pairs of weights, non-numerical weights) and one of many combining methods (not just addition). Such algorithms can solve other minimization problems, such as minimizing &lt;math&gt;\max_i\left[w_{i}+\mathrm{length}\left(c_{i}\right)\right]&lt;/math&gt;, a problem first applied to circuit design.

=== Length-limited Huffman coding/minimum variance huffman coding ===
'''Length-limited Huffman coding''' is a variant where the goal is still to achieve a minimum weighted path length, but there is an additional restriction that the length of each codeword must be less than a given constant. The [[package-merge algorithm]] solves this problem with a simple [[Greedy algorithm|greedy]] approach very similar to that used by Huffman's algorithm. Its time complexity is &lt;math&gt;O(nL)&lt;/math&gt;, where &lt;math&gt;L&lt;/math&gt; is the maximum length of a codeword. No algorithm is known to solve this problem in [[Big O notation#Orders of common functions|linear or linearithmic]] time, unlike the presorted and unsorted conventional Huffman problems, respectively.

=== Huffman coding with unequal letter costs ===
In the standard Huffman coding problem, it is assumed that each symbol in the set that the code words are constructed from has an equal cost to transmit: a code word whose length is ''N'' digits will always have a cost of ''N'', no matter how many of those digits are 0s, how many are 1s, etc. When working under this assumption, minimizing the total cost of the message and minimizing the total number of digits are the same thing.

''Huffman coding with unequal letter costs'' is the generalization without this assumption: the letters of the encoding alphabet may have non-uniform lengths, due to characteristics of the transmission medium. An example is the encoding alphabet of [[Morse code]], where a 'dash' takes longer to send than a 'dot', and therefore the cost of a dash in transmission time is higher. The goal is still to minimize the weighted average codeword length, but it is no longer sufficient just to minimize the number of symbols used by the message. No algorithm is known to solve this in the same manner or with the same efficiency as conventional Huffman coding.

=== Optimal alphabetic binary trees (Hu-Tucker coding) ===
In the standard Huffman coding problem, it is assumed that any codeword can correspond to any input symbol. In the alphabetic version, the alphabetic order of inputs and outputs must be identical. Thus, for example, &lt;math&gt;A = \left\{a,b,c\right\}&lt;/math&gt; could not be assigned code &lt;math&gt;H\left(A,C\right) = \left\{00,1,01\right\}&lt;/math&gt;, but instead should be assigned either &lt;math&gt;H\left(A,C\right) =\left\{00,01,1\right\}&lt;/math&gt; or &lt;math&gt;H\left(A,C\right) = \left\{0,10,11\right\}&lt;/math&gt;. This is also known as the '''Hu-Tucker''' problem, after the authors of the paper presenting the first [[linearithmic]] solution to this optimal binary alphabetic problem,&lt;ref&gt;{{cite doi|10.1137/0121057}}&lt;/ref&gt; which has some similarities to Huffman algorithm, but is not a variation of this algorithm. These optimal alphabetic binary trees are often used as [[binary search tree]]s.

=== The canonical Huffman code ===

If weights corresponding to the alphabetically ordered inputs are in numerical order, the Huffman code has the same lengths as the optimal alphabetic code, which can be found from calculating these lengths, rendering Hu-Tucker coding unnecessary. The code resulting from numerically (re-)ordered input is sometimes called the ''[[canonical Huffman code]]'' and is often the code used in practice, due to ease of encoding/decoding. The technique for finding this code is sometimes called '''Huffman-Shannon-Fano coding''', since it is optimal like Huffman coding, but alphabetic in weight probability, like [[Shannon-Fano coding]]. The Huffman-Shannon-Fano code corresponding to the example is &lt;math&gt;\{000,001,01,10,11\}&lt;/math&gt;, which, having the same codeword lengths as the original solution, is also optimal. But in ''[[canonical Huffman code]]'', the result is &lt;math&gt;\{110,111,00,01,10\}&lt;/math&gt;.

== Applications ==
[[Arithmetic coding]] can be viewed as a generalization of Huffman coding, in the sense that they produce the same output when every symbol has a probability of the form 1/2&lt;sup&gt;''k''&lt;/sup&gt;; in particular it tends to offer significantly better compression for small alphabet sizes. Huffman coding nevertheless remains in wide use because of its simplicity and high speed. Intuitively, arithmetic coding can offer better compression than Huffman coding because its &quot;code words&quot; can have effectively non-integer bit lengths, whereas code words in Huffman coding can only have an integer number of bits. Therefore, there is an inefficiency in Huffman coding where a code word of length ''k'' only optimally matches a symbol of probability 1/2&lt;sup&gt;''k''&lt;/sup&gt; and other probabilities are not represented as optimally; whereas the code word length in arithmetic coding can be made to exactly match the true probability of the symbol.

Huffman coding today is often used as a &quot;back-end&quot; to some other compression methods.
[[DEFLATE (algorithm)|DEFLATE]] ([[PKZIP]]'s algorithm) and multimedia [[codec]]s such as [[JPEG]] and [[MP3]] have a front-end model and [[quantization (signal processing)|quantization]] followed by Huffman coding (or variable-length prefix-free codes with a similar structure, although perhaps not necessarily designed by using Huffman's algorithm{{clarify|date=February 2012}}).

==See also==
*[[Adaptive Huffman coding]]
*[[Canonical Huffman code]]
*[[Data compression]]
*[[Huffyuv]]
*[[Lempel–Ziv–Welch]]
*[[Modified Huffman coding]] - used in [[fax machines]]
*[[Shannon-Fano coding]]
*[[Varicode]]

== Notes ==
{{Reflist}}

== References==
* D.A. Huffman, &quot;A Method for the Construction of Minimum-Redundancy Codes&quot;, Proceedings of the I.R.E., September 1952, pp 1098–1102. Huffman's original article.
* Ken Huffman. [http://www.huffmancoding.com/my-uncle/scientific-american Profile: David A. Huffman], [[Scientific American]], September 1991, pp.&amp;nbsp;54–58
* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 16.3, pp.&amp;nbsp;385–392.

== External links ==
{{external links|date=January 2014}}
{{Commons category|Huffman coding}}
* [http://scanftree.com/Data_Structure/huffman-code Huffman Coding with c Algorithm ]
* [http://demo.tinyray.com/huffman Huffman Encoding process animation]
* [http://www.cs.pitt.edu/~kirk/cs1501/animations/Huffman.html Huffman Encoding &amp; Decoding Animation]
* [http://alexvn.freeservers.com/s1/huffman_template_algorithm.html n-ary Huffman Template Algorithm]
* [http://huffman.ooz.ie/ Huffman Tree visual graph generator]
* [http://www.research.att.com/projects/OEIS?Anum=A098950 Sloane A098950] Minimizing k-ordered sequences of maximum height Huffman tree
* [http://www.siggraph.org/education/materials/HyperGraph/video/mpeg/mpegfaq/huffman_tutorial.html A quick tutorial on generating a Huffman tree]
* Pointers to [http://web-cat.cs.vt.edu/AlgovizWiki/HuffmanCodingTrees Huffman coding visualizations]
* [http://rosettacode.org/wiki/Huffman_codes Explanation of Huffman coding with examples in several languages]
* [http://www.hightechdreams.com/weaver.php?topic=huffmancoding Interactive Huffman Tree Construction]
* [http://github.com/elijahbal/huffman-coding/ A C program doing basic Huffman coding on binary and text files]
* zipFileMaker C++ code[https://gist.github.com/MinhasKamal/9286934]
* [http://www.reznik.org/software.html#ABC Efficient implementation of Huffman codes for blocks of binary sequences]
{{Compression Methods}}

{{DEFAULTSORT:Huffman Coding}}
[[Category:1952 in computer science]]
[[Category:Lossless compression algorithms]]
[[Category:Binary trees]]</text>
      <sha1>kppkw5sm47plsvgo3af5gl05sziwmip</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Threaded binary tree</title>
    <ns>0</ns>
    <id>4262609</id>
    <revision>
      <id>610403251</id>
      <parentid>604785984</parentid>
      <timestamp>2014-05-27T21:28:45Z</timestamp>
      <contributor>
        <ip>68.183.37.54</ip>
      </contributor>
      <comment>The diagram shows that I has no in-order successor and I has no added right child pointer.</comment>
      <text xml:space="preserve" bytes="6858">{{cleanup-rewrite|date=October 2011}}

[[Image:Threaded tree.svg|right|400|thumb|A '''threaded tree''', with the special threading links shown by dashed arrows]]
A '''threaded binary tree''' defined as follows:

&lt;blockquote&gt;
&quot;A binary tree is ''threaded'' by making all right child pointers that would normally be null point to the inorder successor of the node ('''if''' it exists) , and all left child pointers that would normally be null point to the inorder predecessor of the node.&quot;&lt;ref&gt;Van Wyk, Christopher J. &lt;u&gt;Data Structures and C Programs&lt;/u&gt;, Addison-Wesley, 1988, p. 175. ISBN 978-0-201-16116-8.&lt;/ref&gt;
&lt;/blockquote&gt;

A threaded [[binary tree]] makes it possible to traverse the values in the [[binary tree]] via a linear traversal that is more rapid than a recursive [[in-order traversal]]. It is also possible to discover the parent of a node from a threaded binary tree, without explicit use of parent pointers or a stack, albeit slowly. This can be useful where stack space is limited, or where a stack of parent pointers is unavailable (for finding the parent pointer via [[Depth-first search|DFS]]).

To see how this is possible, consider a node ''k'' that has a right child ''r''.  Then the left pointer of ''r'' must be either a child or a thread back to ''k''. In the case that ''r'' has a left child, that left child must in turn have either a left child of its own or a thread back to ''k'', and so on for all successive left children.  So by following the chain of left pointers from ''r'', we will eventually find a thread pointing back to ''k''.  The situation is symmetrically similar when ''q'' is the left child of ''p''&amp;mdash;we can follow ''q'''s right children to a thread pointing ahead to ''p''.

==Types of threaded binary trees==

# Single Threaded: each node is threaded towards '''either''(right)' the in-order predecessor '''or''' successor.
# Double threaded: each node is threaded towards '''both''(left &amp; right)' the in-order predecessor '''and''' successor.

In [[Python (programming language)|Python]]:
&lt;source lang=&quot;python&quot;&gt;
def parent(node):
    if node is node.tree.root:
        return None
    else:
        x = node
        y = node
        while True:
            if is_thread(y.right):
                p = y.right
                if p is None or p.left is not node:
                    p = x
                    while not is_thread(p.left):
                        p = p.left
                    p = p.left
                return p
            elif is_thread(x.left):
                p = x.left
                if p is None or p.right is not node:
                    p = y
                    while not is_thread(p.right):
                        p = p.right
                    p = p.right
                return p
            x = x.left
            y = y.right
&lt;/source&gt;

== The array of Inorder traversal ==
Threads are reference to the predecessors and successors of the node according to an inorder traversal. 

Inorder of the threaded tree is ABCDEFGHI, the predecessor of E is D, the successor of E is F.

[[File:ThreadTree Inorder Array.png]]

== Example ==
[[File:ThreadTree Inorder Array123456789.png]]

Let's make the Threaded Binary tree out of a normal binary tree...

[[File:Normal Binary Tree.png]]

The INORDER traversal for the above tree is—D B A E C. So, the respective Threaded Binary tree will be --

[[File:Threaded Binary Tree.png]]
The a=b formula was used in the late 1800s

== Null link ==

An m-way threaded binary tree, there are ''' n*m - (n-1) ''' links are void in a tree with n nodes.

== Non recursive Inorder traversal for a Threaded Binary Tree ==

As this is a non-recursive method for traversal, it has to be an iterative procedure; meaning, all the steps for the traversal of a node have to be under a loop so that the same can be applied to all the nodes in the tree.
We will consider the INORDER traversal again. Here, for every node, we'll visit the left sub-tree (if it exists) first (if and only if we haven't visited it earlier); then we visit (i.e. print its value, in our case) the node itself and then the right sub-tree (if it exists). If the right sub-tree is not there, we check for the threaded link and make the threaded node the current node in consideration. Please, follow the example given below.

[[File:Threaded Binary Tree.png]]

===Algorithm===
Step-1: For the current node check whether it has a left child which is not there in the visited list. If it has then go to step-2 or else step-3.

Step-2: Put that left child in the list of visited nodes and make it your current node in consideration. Go to step-6.

Step-3: For the current node check whether it has a right child. If it has then go to step-4 else go to step-5

Step-4: Make that right child as your current node in consideration. Go to step-6. 

Step-5: Check for the threaded node and if its there make it your current node.

Step-6: Go to step-1 if all the nodes are not over otherwise quit

{| class=&quot;wikitable sortable&quot;

! || || Li
|-
|step-1||'A' has a left child i.e. B, which has not been visited.So, we put B in our &quot;list of visited nodes&quot; and B becomes our current node in consideration.||B|||| 
|-
 
|step-2||'B' also has a left child, 'D', which is not there in our list of visited nodes. So, we put 'D' in that list and make it our current node in consideration.||B D||||
|-
 
|step-3||'D' has no left child, so we print 'D'. Then we check for its right child. 'D' has no right child and thus we check for its thread-link. It has a thread going till node 'B'. So, we make 'B' as our current node in consideration.||B D||D
|-
|step-4||'B' certainly has a left child but its already in our list of visited nodes. So, we print 'B'. Then we check for its right child but it doesn't exist. So, we make its threaded node (i.e. 'A') as our current node in consideration.||B D||D B
|-
|step-5||'A' has a left child, 'B', but its already there in the list of visited nodes. So, we print 'A'. Then we check for its right child. 'A' has a right child, 'C' and it's not there in our list of visited nodes. So, we add it to that list and we make it our current node in consideration.|| B D C||D B A
|-
|step-6||'C' has 'E' as the left child and it's not there in our list of visited nodes even. So, we add it to that list and make it our current node in consideration.|| B D C E|| D B A
|-
| step-7|| || and finally.....||D B A E C
|-
|}

== References ==

{{reflist}}
Working with c and Data Structure

== External links ==
* [http://www.eternallyconfuzzled.com/tuts/datastructures/jsw_tut_bst1.aspx#thread Tutorial on threaded binary trees]
* [http://adtinfo.org/libavl.html/Threaded-Binary-Search-Trees.html GNU libavl 2.0.2, Section on threaded binary search trees]

{{DEFAULTSORT:Threaded Binary Tree}}
[[Category:Articles with example Python code]]
[[Category:Binary trees]]</text>
      <sha1>3rws6p1qc3ofxd6n6o4hflem9ptqi9a</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>T-tree</title>
    <ns>0</ns>
    <id>1333086</id>
    <revision>
      <id>618299101</id>
      <parentid>606306810</parentid>
      <timestamp>2014-07-24T17:27:52Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* Performance */Task 5: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <text xml:space="preserve" bytes="7795">{{one source|date=June 2013}}
&lt;!-- Image with unknown copyright status removed: [[Image:Ttreenonde.png|thumb|right|251px|An example of a T-tree node structure.]] --&gt;
[[Image:T-tree-1.png|thumb|right|251px|An example T-tree.]]

In [[computer science]] a '''T-tree''' is a type of [[binary tree]]
[[data structure]] that is used by [[main memory database|main-memory databases]], such as
[[Datablitz]], [[EXtremeDB]], [[MySQL Cluster]], [[TimesTen|Oracle TimesTen]] and MobileLite.

A T-tree is a [[Height-balanced tree|balanced]] index tree data structure optimized for cases
where both the index and the actual data are fully kept in memory,
just as a [[B-tree]] is an index structure optimized for storage on block
oriented secondary storage devices like hard disks.  T-trees seek to gain the performance benefits
of in-memory tree structures such as [[AVL trees]] while avoiding the large storage space overhead which
is common to them.

T-trees do not keep copies of the indexed data fields within the index tree nodes themselves. Instead, they take advantage of the fact that the actual data is always in main memory together with the index so that they just contain pointers to the actual data fields.

The 'T' in T-tree refers to the shape of the node data structures
in the original paper that first described this type of index.&lt;ref&gt;[http://www.vldb.org/conf/1986/P294.PDF Tobin J. Lehman and Michael J. Carey, A Study of Index Structures for Main Memory Database Management Systems. VLDB 1986]&lt;/ref&gt;

== Performance ==

Although T-trees seem to be widely used for main-memory databases, recent research indicates that they actually do not perform better than B-trees on modern hardware:

{{cite conference
 | first = Jun
 | last = Rao
 |author2=Kenneth A. Ross
  | title = Cache conscious indexing for decision-support in main memory
 | booktitle = Proceedings of the 25th International Conference on Very Large Databases (VLDB 1999)
 | pages = 78–89
 | publisher = Morgan Kaufmann
 | year = 1999
 | url = http://www.vldb.org/dblp/db/conf/vldb/RaoR99.html
}}

{{cite conference
 | first = Kyungwha
 | last = Kim
 | coauthors = Junho Shim, and Ig-hoon Lee
 | title = Cache conscious trees: How do they perform on contemporary commodity microprocessors?
 | booktitle = Proceedings of the 5th International Conference on Computational Science and Its Applications (ICCSA 2007)
 | pages = 189–200
 | publisher = Springer
 | year = 2007
 | doi = 10.1007/978-3-540-74472-6_15
}}

The main reason seems to be that the traditional assumption of memory references having uniform cost is no longer valid given the current speed gap between cache access and main memory access.&lt;!-- explain please! --&gt;

==Node structures==
A T-tree node usually consists of pointers to the parent node, the left and right child node,
an ordered array of data pointers and some extra control data. Nodes with two [[subtree]]s
are called ''internal nodes'', nodes without [[subtree]]s are called ''leaf nodes''
and nodes with only one [[subtree]] are named ''half-leaf'' nodes.
A node is called the ''bounding node'' for a value if the value is between the node's current minimum and maximum value, inclusively.

[[Image:T-tree-2.png|thumb|right|251px|Bound values.]]

For each internal node, leaf or half leaf nodes exist that contain the predecessor of its smallest
data value (called the ''greatest lower bound'') and one that contains the successor of its largest
data value (called the ''least upper bound''). Leaf and half-leaf nodes can contain any number of
data elements from one to the maximum size of the data array. Internal nodes keep their occupancy
between predefined minimum and maximum numbers of elements

==Algorithms==

{{Expand section|date=June 2008}}

===Search===
* Search starts at the root node
* If the current node is the bounding node for the search value then search its data array. Search fails if the value is not found in the data array.
* If the search value is less than the minimum value of the current node then continue search in its left subtree. Search fails if there is no left subtree.
* If the search value is greater than the maximum value of the current node then continue search in its right subtree. Search fails if there is no right subtree.

===Insertion===
* Search for a bounding node for the new value. If such a node exist then
** check whether there is still space in its data array, if so then insert the new value and finish
** if no space is available then remove the minimum value from the node's data array and insert the new value. Now proceed to the node holding the greatest lower bound for the node that the new value was inserted to. If the removed minimum value still fits in there then add it as the new maximum value of the node, else create a new right subnode for this node.
* If no bounding node was found then insert the value into the last node searched if it still fits into it. In this case the new value will either become the new minimum or maximum value. If the value doesn't fit anymore then create a new left or right subtree.

If a new node was added then the tree might need to be rebalanced, as described below.

===Deletion===
* Search for bounding node of the value to be deleted. If no bounding node is found then finish.
* If the bounding node does not contain the value then finish.
* delete the value from the node's data array

Now we have to distinguish by node type:

* Internal node:
If the node's data array now has less than the minimum number of elements then move the greatest lower bound value of this node to its data value. Proceed with one of the following two steps for the half leaf or leaf node the value was removed from.
* Leaf node:
If this was the only element in the data array then delete the node. Rebalance the tree if needed.
* Half leaf node:
If the node's data array can be merged with its leaf's data array without overflow then do so and remove the leaf node. Rebalance the tree if needed.

=== Rotation and balancing ===
{{Expand section|date=June 2008}}
A T-tree is implemented on top of an underlying [[self-balancing binary search tree]].
Specifically, Lehman and Carey's article describes a T-tree balanced like an [[AVL tree]]: it becomes out of balance when a node's child trees differ in height by at least two levels.
This can happen after an insertion or deletion of a node.
After an insertion or deletion, the tree is scanned from the leaf to the root.
If an imbalance is found, one [[tree rotation]] or pair of rotations is performed, which is guaranteed to balance the whole tree.

When the rotation results in an internal node having fewer than the minimum number of items, items from the node's new child(ren) are moved into the internal node.

==Notes==
{{Empty section|date=June 2008}}

==See also==
* [[Tree (graph theory)]]
* [[Tree (set theory)]]
* [[Tree structure]]
* [[Exponential tree]]

===Other trees===

* [[B-tree]] ([[2-3 tree]],[[2-3-4 tree]], [[B+ tree]], [[B*-tree]], [[UB-tree]])
* [[DSW algorithm]]
* [[Dancing tree]]
* [[Fusion tree]]
* [[kd-tree]]
* [[Octree]]
* [[Quadtree]]
* [[R-tree]]
* [[Radix tree]]
* T-tree
* [[T-pyramid]]
* [[Top Trees]]

==References==
&lt;references /&gt;
{{Expand section|date=June 2008}}

==External links==
*[http://www.oracle.com/technology/products/timesten/htdocs/faq/technical_faq.html##6 Oracle TimesTen FAQ entry on index mentioning T-Trees]
*[http://www.oracle.com/technology/products/timesten/pdf/wp/timesten_tech_wp_dec_2005.pdf Oracle Whitepaper: Oracle TimesTen Products and Technologies]
* [http://www.dependability.org/wg10.4/timedepend/08-Rasto.pdf DataBlitz presentation mentioning T-Trees]
*[http://code.google.com/p/ttree/ An Open-source T*-tree Library]

{{CS-Trees}}

{{DEFAULTSORT:T-Tree}}
[[Category:Binary trees]]</text>
      <sha1>jeszasui2ziwpfv8nabaqsw8h91490a</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Binary expression tree</title>
    <ns>0</ns>
    <id>30121233</id>
    <revision>
      <id>623620916</id>
      <parentid>619148151</parentid>
      <timestamp>2014-08-31T20:39:59Z</timestamp>
      <contributor>
        <ip>77.56.53.183</ip>
      </contributor>
      <comment>/* See also */</comment>
      <text xml:space="preserve" bytes="8140">A '''binary expression tree''' is a specific application of a [[binary tree]] to evaluate certain expressions. Two common types of expressions that a binary expression tree can represent are [[algebra]]ic&lt;ref name=&quot;brpreiss&quot;&gt;{{cite web |url=http://www.brpreiss.com/books/opus5/html/page264.html#SECTION0010500000000000000000|title=Expression Trees|author=Bruno R. Preiss|year=1998|work= |publisher= |accessdate=December 20, 2010}}&lt;/ref&gt;
and [[boolean algebra|boolean]].  These trees can represent expressions that contain both [[unary operation|unary]] and [[binary function|binary]] operators.&lt;ref name=&quot;brpreiss&quot;/&gt;

In general, expression trees are special kind of binary trees. A binary tree is a tree in which all nodes contain zero, one or two children. This restricted structure simplifies the programmatic processing of Expression trees.

== Overview ==
[[File:Exp-tree-ex-11.svg|thumb|250px|right|Expression tree]]
The leaves of a binary expression tree are operands, such as constants or variable names, and the other nodes contain operators. These particular trees happen to be binary, because all of the operations are binary, and although this is the simplest case, it is possible for nodes to have more than two children. It is also possible for a node to have only one child, as is the case with the unary minus operator. An expression tree, ''T'', can be evaluated by applying the operator at the root to the values obtained by recursively evaluating the left and right subtrees.&lt;ref name=&quot;Gopal2010&quot; /&gt;

=== Traversal ===
An algebraic expression can be produced from a binary expression tree by recursively producing a parenthesized left expression, then printing out the operator at the root, and finally recursively producing a parenthesized right expression. This general strategy (left, node, right) is known as an [[tree traversal|in-order travesal]].
An alternate traversal strategy is to recursively print out the left subtree, the right subtree, and then the operator. This traversal strategy is generally known as [[tree traversal|post-order traversal]].
A third strategy is to print out the operator first and then recursively print out the left and right subtree.&lt;ref name=&quot;Gopal2010&quot; /&gt;

These three standard depth-first traversals are representations of the three different expression formats: infix, postfix, and prefix. An infix expression is produced by the inorder traversal, a postfix expression is produced by the post-order traversal, and a prefix expression is produced by the pre-order traversal.&lt;ref name=&quot;Gilberg&quot; /&gt;
{{-}}

====Infix Traversal====
When an infix expression is printed, an opening and closing parenthesis must be added at the beginning and ending of each expression. As every subtree represents a subexpression, an opening parenthesis is printed at its start and the closing parenthesis is printed after processing all of its children.

Pseudocode:

&lt;source lang=&quot;c&quot;&gt;
Algorithm infix (tree)
/*Print the infix expression for an expression tree.
 Pre : tree is a pointer to an expression tree
 Post: the infix expression has been printed*/
 if (tree not empty)
    if (tree token is operator)
       print (open parenthesis)
    end if
    infix (tree left subtree)
    print (tree token)
    infix (tree right subtree)
    if (tree token is operator)
       print (close parenthesis)
    end if
 end if
end infix
&lt;/source&gt;

====Postfix Traversal====
The postfix expression is formed by the basic postorder traversal of any binary tree. It does not require parentheses.

Pseudocode:

&lt;source lang=&quot;c&quot;&gt;
Algorithm postfix (tree)
/*Print the postfix expression for an expression tree.
 Pre : tree is a pointer to an expression tree
 Post: the postfix expression has been printed*/
 if (tree not empty)
    postfix (tree left subtree)
    postfix (tree right subtree)
    print (tree token)
 end if
end postfix
&lt;/source&gt;

====Prefix Traversal====
The prefix expression formed by prefix traversal uses the standard pre-order tree traversal. No parentheses are necessary.

Pseudocode:

&lt;source lang=&quot;c&quot;&gt;
Algorithm prefix (tree)
/*Print the prefix expression for an expression tree.
 Pre : tree is a pointer to an expression tree
 Post: the prefix expression has been printed*/
 if (tree not empty)
    print (tree token)
    prefix (tree left subtree)
    prefix (tree right subtree) and check if stack is not empty
 end if
end prefix
&lt;/source&gt;

== Construction of an Expression Tree ==
The evaluation of the tree takes place by reading the postfix expression one symbol at a time. If the symbol is an operand, one-node tree is created and a pointer is pushed onto a [[Stack (abstract data type)|stack]]. If the symbol is an operator, the pointers are popped to two trees ''T1'' and ''T2'' from the stack and a new tree whose root is the operator and whose left and right children point to ''T2'' and ''T1'' respectively is formed . A pointer to this new tree is then pushed to the Stack.&lt;ref&gt;Mark Allen Weiss,''Data Structures and Algorithm Analysis in C,2nd edition'',  Addison Wesley publications&lt;/ref&gt;

=== Example ===
The input is: a b + c d e + * *
Since the first two symbols are operands, one-node trees are created and pointers are pushed to them onto a stack. For convenience the stack will grow from left to right.

[[File:Exp-tree-ex-2.svg|thumb|180px|center|Stack growing from left to right]]

The next symbol is a '+'. It pops the two pointers to the trees, a new tree is formed, and a pointer to it is pushed onto to the stack.

[[File:Exp-tree-ex-3.svg|thumb|229px|center|Formation of a new tree]]

Next, c, d, and e are read. A one-node tree is created for each and a pointer to the corresponding tree is pushed onto the stack.

[[File:Exp-tree-ex-6.svg|thumb|359px|center|Creating a one-node tree]]

Continuing, a '+' is read, and it merges the last two trees.

[[File:Exp-tree-ex-7.svg|thumb|429px|center|Merging two trees]]

Now, a '*' is read. The last two tree pointers are popped and a new tree is formed with a '*' as the root.

[[File:Exp-tree-ex-8.svg|thumb|503px|center|Forming a new tree with a root]]

Finally, the last symbol is read. The two trees are merged and a pointer to the final tree remains on the stack.&lt;ref name=&quot;Gopal2010.353&quot; /&gt;

[[File:Exp-tree-ex-9.svg|thumb|503px|center|Steps to construct an expression tree  a b + c d e + * *]]

== Algebraic expressions ==
[[File:Exp-tree-ex-12.svg|thumb|270px|right|Binary algebraic expression tree equivalent to ((5 + z) / -8) * (4 ^ 2)]] 
Algebraic expression trees represent expressions that contain [[number]]s, [[Variable (mathematics)|variables]], and unary and binary operators. Some of the common operators are × ([[multiplication]]), ÷ ([[Division (mathematics)|division]]), + ([[addition]]), − ([[subtraction]]), ^ ([[exponentiation]]), and - ([[negation]]). The operators are contained in the [[internal node]]s of the tree, with the numbers and variables in the [[leaf nodes]].&lt;ref name=&quot;brpreiss&quot;/&gt; The nodes of binary operators have two [[child nodes]], and the unary operators have one child node.
{{-}}

== Boolean expressions ==
[[File:Exp-tree-ex-13.svg|thumb|270px|right|Binary boolean expression tree equivalent to ((true &lt;math&gt;\lor&lt;/math&gt; false) &lt;math&gt;\land&lt;/math&gt; &lt;math&gt;\neg&lt;/math&gt;false) &lt;math&gt;\lor&lt;/math&gt; (true &lt;math&gt;\lor&lt;/math&gt; false))]] 
Boolean expressions are represented very similarly to algebraic expressions, the only difference being the specific values and operators used. Boolean expressions use ''true'' and ''false'' as constant values, and the operators include &lt;math&gt;\land&lt;/math&gt; ([[Logical and|AND]]), &lt;math&gt;\lor&lt;/math&gt; ([[Logical or|OR]]), &lt;math&gt;\neg&lt;/math&gt; ([[Logical not|NOT]]).
{{-}}

== See also ==
*[[Expression (mathematics)]]

== References ==
{{Reflist|refs=
&lt;ref name=&quot;Gopal2010&quot;&gt;Gopal, Arpita. ''Magnifying Data Structures''. PHI Learning, 2010, p. 352.&lt;/ref&gt;
&lt;ref name=&quot;Gopal2010.353&quot;&gt;Gopal, Arpita. ''Magnifying Data Structures''. PHI Learning, 2010, p. 353.&lt;/ref&gt;
&lt;ref name=&quot;Gilberg&quot;&gt;Richard F. Gilberg &amp; Behrouz A. Forouzan. ''Data Structures: A Pseudocode Approach with C''. Thomson Course Technology, 2005, p. 280.&lt;/ref&gt;
}}

[[Category:Binary trees]]</text>
      <sha1>dnc8dli39dgrkf1upgggmx89ugzjp2d</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Top tree</title>
    <ns>0</ns>
    <id>31075365</id>
    <revision>
      <id>600865884</id>
      <parentid>600865781</parentid>
      <timestamp>2014-03-23T12:03:43Z</timestamp>
      <contributor>
        <username>The.lorrr</username>
        <id>11791835</id>
      </contributor>
      <comment>/* References */</comment>
      <text xml:space="preserve" bytes="19854">A '''Top tree''' is a [[data structure]] based on a binary tree for unrooted dynamic [[Tree (data structure)|trees]] that is used mainly for various path-related operations. It allows simple [[divide-and-conquer algorithm]]s. It has since been augmented to maintain dynamically various properties of a [[Tree (data structure)|tree]] such as diameter, center and median.

A Top tree &lt;math&gt;\Re&lt;/math&gt; is defined for an ''underlying tree'' &lt;math&gt;\mathcal{T}&lt;/math&gt; and a set &lt;math&gt;\partial{T}&lt;/math&gt; of at most two vertices called as [[#External Boundary Vertices|External Boundary Vertices]]

[[Image:Top tree.jpg|thumb|250px|An image depicting a Top tree built on an underlying tree (black nodes)A tree divided into edge clusters and the complete top-tree for it. Filled nodes in the top-tree are path-clusters, while small circle nodes are leaf-clusters. The big circle node is the root. Capital letters denote clusters, non-capital letters are nodes.]]

==Glossary==
===Boundary Node===
See [[#Boundary Vertex|Boundary Vertex]]

===Boundary Vertex===
A vertex in a connected subtree is a ''Boundary Vertex'' if it is connected to a vertex outside the subtree by an edge.

====External Boundary Vertices====
Up to a pair of vertices in the Top Tree &lt;math&gt;\Re&lt;/math&gt; can be called as External Boundary Vertices, they can be thought of as Boundary Vertices of the cluster which represents the entire Top Tree.

===Cluster===
A ''cluster'' is a connected subtree with at most two [[#Boundary Vertex|Boundary Vertices]].
The set of [[#Boundary Vertex|Boundary Vertices]] of a given cluster &lt;math&gt;\mathcal{C}&lt;/math&gt; is denoted as &lt;math&gt;\partial{C}.&lt;/math&gt;
With each cluster &lt;math&gt;\mathcal{C}&lt;/math&gt; the user may associate some meta information &lt;math&gt;I(\mathcal{C}),&lt;/math&gt;  and give methods to maintain it under the various [[#Internal Operations|internal operations]].

====Path Cluster====
If &lt;math&gt;\pi(\mathcal{C})&lt;/math&gt; contains at least one edge then &lt;math&gt;\mathcal{C}&lt;/math&gt; is called a ''Path Cluster''.

====Point Cluster====
See [[#Leaf Cluster|Leaf Cluster]]

====Leaf Cluster====
If &lt;math&gt;\pi(\mathcal{C})&lt;/math&gt; does not contain any edge i.e. &lt;math&gt;\mathcal{C}&lt;/math&gt; has only one [[#Boundary Vertex|Boundary Vertex]] then &lt;math&gt;\mathcal{C}&lt;/math&gt; is called a ''Leaf Cluster''.

====Edge Cluster====
A Cluster containing a single edge is called an ''Edge Cluster''.

=====Leaf Edge Cluster=====
A Leaf in the original Cluster is represented by a Cluster with just a single Boundary Vertex and is called a ''Leaf Edge Cluster''.

=====Path Edge Cluster=====
Edge Clusters with two Boundary Nodes are called ''Path Edge Cluster''.

===Internal Node===
A node in &lt;math&gt;\mathcal{C}&lt;/math&gt; '''\''' &lt;math&gt;\partial{C}&lt;/math&gt; is called an ''Internal Node'' of &lt;math&gt;\mathcal{C}.&lt;/math&gt;

===Cluster Path===
The path between the [[#Boundary Vertex|Boundary Vertices]] of &lt;math&gt;\mathcal{C}&lt;/math&gt; is called the ''cluster path'' of &lt;math&gt;\mathcal{C}&lt;/math&gt; and it is denoted by &lt;math&gt;\pi(\mathcal{C}).&lt;/math&gt;

===Mergeable Clusters===
Two Clusters &lt;math&gt;\mathcal{A}&lt;/math&gt; and &lt;math&gt;\mathcal{B}&lt;/math&gt; are ''Mergeable'' if &lt;math&gt;\mathcal{A}\cap\mathcal{B}&lt;/math&gt; is a singleton set (they have exactly one node in common) and &lt;math&gt;\mathcal{A}\cup\mathcal{B}&lt;/math&gt; is a Cluster.

==Introduction==
''Top Trees'' are used for maintaining a Dynamic forest (set of trees) under [[#Dynamic Operations|link and cut operations.]]

The basic idea is to maintain a balanced [[Binary tree]] &lt;math&gt;\Re&lt;/math&gt; of logarithmic height in the number of nodes in the original tree &lt;math&gt;\mathcal{T}&lt;/math&gt;( i.e. in &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; time) ; the '''Top Tree''' essentially represents the recursive subdivision of the original tree &lt;math&gt;\mathcal{T}&lt;/math&gt; into [[#Cluster|clusters]]''.

In general the tree &lt;math&gt;\mathcal{T}&lt;/math&gt; may have weight on its edges.

There is a one to one correspondence with the edges of the original tree &lt;math&gt;\mathcal{T}&lt;/math&gt; and the leaf nodes of the Top Tree &lt;math&gt;\Re&lt;/math&gt; and each internal node of &lt;math&gt;\Re&lt;/math&gt; represents a cluster that is formed due to the union of the clusters that are its children.

The Top Tree data structure can be initialized in &lt;math&gt;\mathcal{O}(n)&lt;/math&gt; time.
&lt;math&gt;\mathcal{T}&lt;/math&gt;

Therefore the Top Tree &lt;math&gt;\Re&lt;/math&gt; over (&lt;math&gt;\mathcal{T},&lt;/math&gt; &lt;math&gt;\partial{T}&lt;/math&gt;) is a binary tree such that

* The nodes of &lt;math&gt;\Re&lt;/math&gt; are clusters of (&lt;math&gt;\mathcal{T},&lt;/math&gt; &lt;math&gt;\partial{T}&lt;/math&gt; );
* The leaves of &lt;math&gt;\Re&lt;/math&gt; are the edges of &lt;math&gt;\mathcal{T};&lt;/math&gt;
* Sibling clusters are neighbours in the sense that they intersect in a single vertex, and then their parent cluster is their union.
* Root of &lt;math&gt;\Re&lt;/math&gt; is the tree &lt;math&gt;\mathcal{T}&lt;/math&gt; itself, with a set of at most two External Boundary Vertices.

A tree with a single vertex has an empty top tree, and one with just an edge is just a single node.

These trees are freely [[Augmenting a Data Structure|augmentable]] allowing the user a wide variety of flexibility and productivity without going into the details of the internal workings of the data structure, something which is also referred to as the ''Black Box''.

==Dynamic Operations==
The following three are the user allowable Forest Updates.
* '''Link(v, w):''' Where &lt;math&gt;v&lt;/math&gt; and &lt;math&gt;w&lt;/math&gt; are vertices in different trees &lt;math&gt;\mathcal{T}&lt;/math&gt;&lt;sub&gt;1&lt;/sub&gt; and &lt;math&gt;\mathcal{T}&lt;/math&gt;&lt;sub&gt;2&lt;/sub&gt;. It returns a single top tree representing &lt;math&gt;\Re&lt;/math&gt;&lt;sub&gt;v&lt;/sub&gt;&lt;math&gt;\cup&lt;/math&gt;&lt;math&gt;\Re&lt;/math&gt;&lt;sub&gt;w&lt;/sub&gt;&lt;math&gt;\cup{(v,w)}&lt;/math&gt;

*'''Cut(v, w)''': Removes the Edge &lt;math&gt;{(v,w)}&lt;/math&gt; from a tree &lt;math&gt;\mathcal{T}&lt;/math&gt; with Top Tree &lt;math&gt;\Re,&lt;/math&gt; thereby turning it into two trees &lt;math&gt;\mathcal{T}&lt;/math&gt;&lt;sub&gt;v&lt;/sub&gt; and &lt;math&gt;\mathcal{T}&lt;/math&gt;&lt;sub&gt;w&lt;/sub&gt; and returning two Top Trees &lt;math&gt;\Re&lt;/math&gt;&lt;sub&gt;v&lt;/sub&gt; and &lt;math&gt;\Re&lt;/math&gt;&lt;sub&gt;w&lt;/sub&gt;.

*'''Expose(S)''': Is called as a subroutine for implementing most of the queries on a Top Tree. &lt;math&gt;S&lt;/math&gt; contains at most 2 vertices. It makes original external vertices to be normal vertices and makes verices from &lt;math&gt;S&lt;/math&gt; the new External Boundary Vertices of the Top Tree. If &lt;math&gt;S&lt;/math&gt; is nonempty it returns the new Root cluster &lt;math&gt;\mathcal{C}&lt;/math&gt; with &lt;math&gt;\partial{C} = S.&lt;/math&gt; '''Expose({v,w})''' fails if the vertices are from different trees.

==Internal Operations==
The [[#Dynamic Operations|Forest updates]] are all carried out by a sequence of at most &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; Internal Operations, the sequence of which is computed in further &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; time.  It may happen that during a tree update, a leaf  cluster may change to a path cluster and the converse. Updates to top tree are done exclusively by these internal operations.

The &lt;math&gt;I(\mathcal{C})&lt;/math&gt; is updated by calling a user defined function associated with each internal operation.

*'''Merge'''&lt;math&gt;(\mathcal{A},\mathcal{B}){:}&lt;/math&gt; Here &lt;math&gt;\mathcal{A}&lt;/math&gt; and &lt;math&gt;\mathcal{B}&lt;/math&gt; are ''Mergeable Clusters'', it returns &lt;math&gt;\mathcal{C}&lt;/math&gt; as the parent cluster of &lt;math&gt;\mathcal{A}&lt;/math&gt; and &lt;math&gt;\mathcal{B}&lt;/math&gt; and with boundary vertices as the boundary vertices of &lt;math&gt;\mathcal{A}\cup\mathcal{B}.&lt;/math&gt; Computes &lt;math&gt;I(\mathcal{C})&lt;/math&gt; using &lt;math&gt;I(\mathcal{A})&lt;/math&gt; and &lt;math&gt;I(\mathcal{B}).&lt;/math&gt;

*'''Split'''&lt;math&gt;(\mathcal{C}){:}&lt;/math&gt; Here &lt;math&gt;\mathcal{C}&lt;/math&gt; is the root cluster &lt;math&gt;\mathcal{A}\cup\mathcal{B}.&lt;/math&gt; It updates &lt;math&gt;I(\mathcal{A})&lt;/math&gt; and &lt;math&gt;I(\mathcal{B})&lt;/math&gt; using &lt;math&gt;I(\mathcal{C})&lt;/math&gt; and than it deletes the cluster &lt;math&gt;\mathcal{C}&lt;/math&gt; from &lt;math&gt;\Re&lt;/math&gt;.

Split is usually implemented using '''Clean'''&lt;math&gt;(\mathcal{C})&lt;/math&gt; method which calls user method for updates of &lt;math&gt;I(\mathcal{A})&lt;/math&gt; and &lt;math&gt;I(\mathcal{B})&lt;/math&gt; using &lt;math&gt;I(\mathcal{C})&lt;/math&gt; and updates &lt;math&gt;I(\mathcal{C})&lt;/math&gt; such that it's known there is no pending update needed in its children. Than the &lt;math&gt;\mathcal{C}&lt;/math&gt; is discarded without calling user defined functions. '''Clean''' is often required for queries without need to '''Split'''.
If Split does not use Clean subroutine, and Clean is required, its effect could be achieved with overhead by combining '''Merge''' and '''Split'''.

The next two functions are analogous to the above two and are used for base clusters.

*'''Create&lt;math&gt;(v,w){:}&lt;/math&gt;''' Creates a cluster &lt;math&gt;\mathcal{C}&lt;/math&gt; for the edge &lt;math&gt;(v,w).&lt;/math&gt; Sets &lt;math&gt;\partial{C} =  \partial&lt;/math&gt;&lt;math&gt;(v,w).&lt;/math&gt; &lt;math&gt;I(\mathcal{C})&lt;/math&gt; is computed from scratch.

*'''Eradicate&lt;math&gt;(\mathcal{C}){:}&lt;/math&gt;''' &lt;math&gt;\mathcal{C}&lt;/math&gt; is the edge cluster &lt;math&gt;(v,w).&lt;/math&gt; User defined function is called to process &lt;math&gt;I(\mathcal{C})&lt;/math&gt; and than the cluster &lt;math&gt;\mathcal{C}&lt;/math&gt; is deleted from the top tree.

==Non local search==
User can define '''Choose&lt;math&gt;(\mathcal{C}){:}&lt;/math&gt;''' operation which for a root (nonleaf) cluster selects one of its child clusters. The Top Tree blackbox provides '''Search&lt;math&gt;(\mathcal{C}){:}&lt;/math&gt;''' routine, which organizes '''Choose''' queries and reorganization of the Top tree (using the Internal operations) such that it locates the only edge in intersection of all selected clusters. Sometimes the search should be limited to a path. There is a variant of nonlocal search for such purposes.
If there are two external boundary vertices in the root cluster &lt;math&gt;\mathcal{C}&lt;/math&gt;, the edge is searched only on the path &lt;math&gt;\pi(\mathcal{C})&lt;/math&gt;. It is sufficient to do following modification: If only one of root cluster children is path cluster, it is selected by default without calling the '''Choose''' operation.

===Examples of non local search===
Finding i-th edge on longer path from &lt;math&gt;v&lt;/math&gt; to &lt;math&gt;w&lt;/math&gt; could be done by '''&lt;math&gt;\mathcal{C}&lt;/math&gt;=Expose({v,w})''' followed by '''Search(&lt;math&gt;\mathcal{C}&lt;/math&gt;)''' with appropriate '''Choose'''. To implement the '''Choose''' we use global variable representing &lt;math&gt;v&lt;/math&gt; and global variable representing &lt;math&gt;i.&lt;/math&gt; Choose selects the cluster &lt;math&gt;\mathcal{A}&lt;/math&gt; with &lt;math&gt;v\in\partial{A}&lt;/math&gt; iff length of &lt;math&gt;\pi(\mathcal{A})&lt;/math&gt; is at least &lt;math&gt;i&lt;/math&gt;. To support the operation the length must be maintained in the &lt;math&gt;I&lt;/math&gt;.

Similar task could be formulated for graph with edges with nonunit lengths. In that case the distance could address an edge or a vertex between two edges. We could define Choose such that the edge leading to the vertex is returned in the latter case. There could be defined update increasing all edge lengths along a path by a constant. In such scenario these updates are done in constant time just in root cluster. '''Clean''' is required to distribute the delayed update to the children. The '''Clean''' should be called before the '''Search''' is invoked. To maintain length in &lt;math&gt;I&lt;/math&gt; would in that case require to maintain unitlength in &lt;math&gt;I&lt;/math&gt; as well.

Finding center of tree containing vertex &lt;math&gt;v&lt;/math&gt; could be done by finding either bicenter edge or edge with center as one endpoint. The edge could be found by '''&lt;math&gt;\mathcal{C}&lt;/math&gt;=Expose({v})''' followed by '''Search(&lt;math&gt;\mathcal{C}&lt;/math&gt;)''' with appropriate '''Choose'''. The choose selects between children &lt;math&gt;\mathcal{A},&lt;/math&gt; &lt;math&gt;\mathcal{B}&lt;/math&gt; with &lt;math&gt;a\in \partial{A}\cap \partial{B}&lt;/math&gt; the child with higher maxdistance&lt;math&gt;(a)&lt;/math&gt;. To support the operation the maximal distance in the cluster subtree from a boundary vertex should be maintained in the &lt;math&gt;I&lt;/math&gt;. That requires maintenance of the cluster path length as well.

==Interesting Results and Applications==

A number of interesting applications originally implemented by other methods have been easily implemented using the Top Trees interface. Some of them include

*([SLEATOR AND TARJAN 1983]). We can maintain a dynamic collection of weighted trees in &lt;math&gt; \mathcal{O}(\log n)&lt;/math&gt; time per link and cut, supporting queries about the maximum edge weight between any two vertices in &lt;math&gt;O (\log n)&lt;/math&gt; time.
**Proof outline: It involves maintaining at each node the maximum weight (max_wt) on its cluster path, if it is a point cluster then max_wt(&lt;math&gt;\mathcal{C}&lt;/math&gt;) is initialsed as &lt;math&gt;-\infty.&lt;/math&gt; When a cluster is a union of two clusters then it is the maximum value of the two merged clusters. If we have to find the max wt between &lt;math&gt;v&lt;/math&gt; and &lt;math&gt;w&lt;/math&gt; then we do &lt;math&gt;\mathcal{C}=&lt;/math&gt; Expose&lt;math&gt;(v,w),&lt;/math&gt; and report max_wt&lt;math&gt;(\mathcal{C}).&lt;/math&gt;

*([SLEATOR AND TARJAN 1983]). In the scenario of the above application we can also add a common weight &lt;math&gt;x&lt;/math&gt; to all edges on a given path &lt;math&gt;v&lt;/math&gt; · · ·&lt;math&gt;w&lt;/math&gt; in &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; time.
**Proof outline: We introduce a weight called extra(&lt;math&gt;\mathcal{C}&lt;/math&gt;) to be added to all the edges in &lt;math&gt;\pi(\mathcal{C}).&lt;/math&gt; Which is maintained appropriately ; split(&lt;math&gt;\mathcal{C}&lt;/math&gt;) requires that, for each path child &lt;math&gt;\mathcal{A}&lt;/math&gt; of &lt;math&gt;\mathcal{C},&lt;/math&gt; we set max_wt(A) := max_wt(&lt;math&gt;\mathcal{A}&lt;/math&gt;) + extra(&lt;math&gt;\mathcal{C}&lt;/math&gt;) and extra(&lt;math&gt;\mathcal{A}&lt;/math&gt;) := extra(&lt;math&gt;\mathcal{A}&lt;/math&gt;) + extra(&lt;math&gt;\mathcal{C}&lt;/math&gt;). For &lt;math&gt;\mathcal{C}&lt;/math&gt; := join(&lt;math&gt;\mathcal{A},&lt;/math&gt; &lt;math&gt;\mathcal{B}&lt;/math&gt;), we set max_wt(&lt;math&gt;\mathcal{C}&lt;/math&gt;) := max {max_wt(&lt;math&gt;\mathcal{A}&lt;/math&gt;), max_wt(&lt;math&gt;\mathcal{B}&lt;/math&gt;)} and extra(&lt;math&gt;\mathcal{C}&lt;/math&gt;) := 0. Finally, to find the maximum weight on the path &lt;math&gt;v&lt;/math&gt; · · ·&lt;math&gt;w,&lt;/math&gt; we set &lt;math&gt;\mathcal{C}&lt;/math&gt; := Expose&lt;math&gt;(v,w)&lt;/math&gt; and return max_wt(&lt;math&gt;\mathcal{C}&lt;/math&gt;).

*([GOLDBERG ET AL. 1991]). We can ask for the maximum weight in the underlying tree containing a given vertex &lt;math&gt;v&lt;/math&gt; in &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; time.
**Proof outline: This requires maintaining additional information about the maximum weight non cluster path edge in a cluster under the Merge and Split operations.

*The distance between two vertices &lt;math&gt;v&lt;/math&gt; and &lt;math&gt;w&lt;/math&gt; can be found in &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; time as length(Expose&lt;math&gt;(v,w)&lt;/math&gt;).
**Proof outline:We will maintain the length length(&lt;math&gt;\mathcal{C}&lt;/math&gt;) of the cluster path. The length is maintained as the maximum weight except that, if &lt;math&gt;\mathcal{C}&lt;/math&gt; is created by a join(Merge), length(&lt;math&gt;\mathcal{C}&lt;/math&gt;) is the sum of lengths stored with its path children.

*Queries regarding diameter of a tree and its subsequent maintenance takes &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; time.

*The Center and Median can me maintained under Link(Merge) and Cut(Split) operations and queried by non local search in &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; time.

*The graph could be maintained allowing to update the edge set and ask queries on edge 2-connectivity. Amortized complexity of updates is &lt;math&gt;O(\log^4 n)&lt;/math&gt;. Queries could be implemented even faster. The algorithm is not trivial, &lt;math&gt;I(\mathcal{C})&lt;/math&gt; uses &lt;math&gt;\Theta(\log^2 n)&lt;/math&gt; space ([HOLM, LICHTENBERG, THORUP 2000]).

*The graph could be maintained allowing to update the edge set and ask queries on vertex 2-connectivity. Amortized complexity of updates is &lt;math&gt;O(\log^5 n)&lt;/math&gt;. Queries could be implemented even faster. The algorithm is not trivial, &lt;math&gt;I(\mathcal{C})&lt;/math&gt; uses &lt;math&gt;\Theta(\log^2 n)&lt;/math&gt; space ([HOLM, LICHTENBERG, THORUP 2001]).

*Top Trees can be used to compress trees in a way that is never much worse than [[Directed acyclic graph|DAG]] compression, but may be exponentially better. &lt;ref&gt;Tree Compression with Top Trees. BILLE, GOERTZ, LANDAU, WEIMANN 2013 arXiv:1304.5702 [cs.DS]&lt;/ref&gt;

==Implementation==

Top Trees have been implemented in a variety of ways, some of them include implementation using a ''Multilevel Partition'' (Top-trees and dynamic graph algorithms Jacob Holm and Kristian de Lichtenberg. Technical Report), and even by using [[Sleator-Tarjan s-t trees]] (typically with amortized time bounds), [[Fredericksons Topology Trees]] (with worst case time bounds) (Alstrup et al. Maintaining Information in Fully Dynamic Trees with Top Trees).

Amortized implementations are more simple, and with small multiplicative factors in time complexity. 
On the contrary the worst case implementations allow speeding up queries by switching off unneeded info updates during the query (implemented by [[persistent data structure|persistence]] techniques). After the query is answered the original state of the top tree is used and the query version is discarded.

===Using Multilevel Partitioning===

Any partitioning of clusters of a tree &lt;math&gt;\mathcal{T}&lt;/math&gt; can be represented by a Cluster Partition Tree CPT&lt;math&gt;(\mathcal{T}),&lt;/math&gt; by replacing each cluster in the tree &lt;math&gt;\mathcal{T}&lt;/math&gt; by an edge. If we use a strategy P for partitioning &lt;math&gt;\mathcal{T}&lt;/math&gt; then the CPT would be CPT&lt;sub&gt;P&lt;/sub&gt;&lt;math&gt;\mathcal{T}.&lt;/math&gt; This is done recursively till only one edge remains.

We would notice that all the nodes of the corresponding Top Tree &lt;math&gt;\Re&lt;/math&gt; are uniquely mapped into the edges of this multilevel partition. There may be some edges in the multilevel partition that do not correspond to any node in the Top tree, these are the edges which represent only a single child in the level below it, i.e. a simple cluster. Only the edges that correspond to composite clusters correspond to nodes in the Top Tree &lt;math&gt;\Re.&lt;/math&gt;

A Partitioning Strategy is important while we partition the Tree &lt;math&gt;\mathcal{T}&lt;/math&gt; into clusters. Only a careful strategy ensures that we end up in an &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; height Multilevel Partition ( and therefore the Top Tree).
* The number of edges in subsequent levels should decrease by a constant factor.
* If a lower level is changed by an update then we should be able to update the one immediately above it using at most a constant number of insertions and deletions.

The above partitioning strategy ensures the maintenance of the Top Tree in  &lt;math&gt;\mathcal{O}(\log n)&lt;/math&gt; time.

==References==
* Stephen Alstrup, Jacob Holm, Kristian De Lichtenberg, and [[Mikkel Thorup]], ''Maintaining information in fully dynamic trees with top trees'', ACM Transactions on Algorithms (TALG), Vol. 1 (2005), 243&amp;ndash;264, {{doi|10.1145/1103963.1103966}}
* Stephen Alstrup, Jacob Holm, Kristian De Lichtenberg, and [[Mikkel Thorup]], ''Poly-logarithmic deterministic {{Sic|hide=y|fully|-}}dynamic algorithms for connectivity, minimum spanning tree, 2-edge, and biconnectivity'', Journal of the ACM (JACM), Vol. 48 Issue 4(July 2001), 723&amp;ndash;760, {{doi|10.1145/502090.502095}}
* [[Donald Knuth]]. ''The Art of Computer Programming: Fundamental Algorithms'', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4 . Section 2.3: Trees, pp.&amp;nbsp;308&amp;ndash;423.
* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7 . Section 10.4: Representing rooted trees, pp.&amp;nbsp;214&amp;ndash;217. Chapters 12&amp;ndash;14 (Binary Search Trees, Red-Black Trees, Augmenting Data Structures), pp.&amp;nbsp;253&amp;ndash;320.
{{Reflist}}

== External links ==
* [http://arxiv.org/abs/cs.DS/0310065 Maintaining Information in Fully Dynamic Trees with Top Trees. Alstrup et al]
* [http://www.cs.princeton.edu/~rwerneck/docs/TW05.htm Self Adjusting Top Trees. Tarjan and Werneck]
* [http://portal.acm.org/citation.cfm?id=1070547&amp;dl=&amp;coll=&amp;CFID=15151515&amp;CFTOKEN=6184618 Self-Adjusting Top Trees. Tarjan and Werneck, Proc. 16th SoDA, 2005]

{{CS-Trees}}

[[Category:Binary trees]]</text>
      <sha1>jx3xkwtqpgdiclf3n4ssybdc771bvf6</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Scapegoat tree</title>
    <ns>0</ns>
    <id>1377178</id>
    <revision>
      <id>626597160</id>
      <parentid>614067435</parentid>
      <timestamp>2014-09-22T09:51:10Z</timestamp>
      <contributor>
        <username>C.hahn</username>
        <id>8192357</id>
      </contributor>
      <text xml:space="preserve" bytes="10831">{{multiple issues|
{{more footnotes|date=March 2014}}
{{refimprove|date=March 2014}}
}}


{{Infobox data structure
|name=Scapegoat tree
|type=tree
|invented_by=[[Arne Andersson]],[[Igal Galperin]], [[Ronald L. Rivest]]
|invented_year=1962
|
|space_avg=O(n)
|space_worst=O(n)
|search_avg=O(log n)
|search_worst=O(log n)
|insert_avg=O(log n)
|insert_worst=O(log n)
|delete_avg=O(log n)
|delete_worst=O(log n)
}}

In [[computer science]], a '''scapegoat tree''' is a [[self-balancing binary search tree]], invented by [[Arne Andersson (computer scientist)|Arne Andersson]]&lt;ref name=anderson1&gt;{{Cite conference | title=Improving partial rebuilding by using simple balance criteria | journal= Journal of Algorithms | first=Arne | last=Andersson | booktitle=Proc. Workshop on Algorithms and Data Structures | pages=393–402 | year=1989 | publisher=Springer-Verlag | doi=10.1007/3-540-51542-9_33}}&lt;/ref&gt; and again by [[Igal Galperin]] and [[Ronald L. Rivest]].&lt;ref name=galperin_rivest&gt;{{Cite journal | first1=Igal | last1=Galperin | first2=Ronald L. | last2=Rivest | title=Scapegoat trees | journal=Proceedings of the fourth annual ACM-SIAM Symposium on Discrete algorithms | pages=165–174  | year=1993 | url=http://portal.acm.org/citation.cfm?id=313676 | postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt;  It provides worst-case [[big O notation|''O'']](log ''n'') lookup time, and ''O''(log ''n'') [[amortized analysis|amortized]] insertion and deletion time.

Unlike most other self-balancing binary search trees that provide worst case ''O''(log ''n'') lookup time, scapegoat trees have no additional per-node memory overhead compared to a regular [[binary search tree]]: a node stores only a key and two pointers to the child nodes. This makes scapegoat trees easier to implement and, due to [[data structure alignment]], can reduce node overhead by up to one-third.

==Theory==
A binary search tree is said to be weight balanced if half the nodes are on the left of the root, and half on the right.
An α-weight-balanced node is therefore defined as meeting the following conditions:
 size(left) &lt;= α*size(node)
 size(right) &lt;= α*size(node)
Where size can be defined recursively as:
 function size(node)
  if node = nil
   return 0
  else
   return size(node-&gt;left) + size(node-&gt;right) + 1
 end

An α of 1 therefore would describe a linked list as balanced, whereas an α of 0.5 would only match [[Binary tree#Types of binary trees|almost complete binary trees]].

A binary search tree that is α-weight-balanced must also be '''α-height-balanced''', that is 
 height(tree) &lt;= log&lt;sub&gt;1/α&lt;/sub&gt;(NodeCount) + 1

Scapegoat trees are not guaranteed to keep α-weight-balance at all times, but are always loosely α-height-balanced in that
 height(scapegoat tree) &lt;= log&lt;sub&gt;1/α&lt;/sub&gt;(NodeCount) + 1

This makes scapegoat trees similar to [[red-black trees]] in that they both have restrictions on their height. They differ greatly though in their implementations of determining where the rotations (or in the case of scapegoat trees, rebalances) take place. Whereas red-black trees store additional 'color' information in each node to determine the location, scapegoat trees find a '''scapegoat''' which isn't α-weight-balanced to perform the rebalance operation on. This is loosely similar to [[AVL trees]], in that the actual rotations depend on 'balances' of nodes, but the means of determining the balance differs greatly. Since AVL trees check the balance value on every insertion/deletion, it is typically stored in each node; scapegoat trees are able to calculate it only as needed, which is only when a scapegoat needs to be found.

Unlike most other self-balancing search trees, scapegoat trees are entirely flexible as to their balancing. They support any α such that 0.5 &lt; α &lt; 1. A high α value results in fewer balances, making insertion quicker but lookups and deletions slower, and vice versa for a low α. Therefore in practical applications, an α can be chosen depending on how frequently these actions should be performed.

==Operations==

===Insertion===

Insertion is implemented with the same basic ideas as an [[Binary search tree#Insertion|unbalanced binary search tree]], however with a few significant changes.

When finding the insertion point, the depth of the new node must also be recorded. This is implemented via a simple counter that gets incremented during each iteration of the lookup, effectively counting the number of edges between the root and the inserted node. If this node violates the α-height-balance property (defined above), a rebalance is required.

To rebalance, an entire subtree rooted at a '''scapegoat''' undergoes a balancing operation. The scapegoat is defined as being an ancestor of the inserted node which isn't α-weight-balanced. There will always be at least one such ancestor. Rebalancing any of them will restore the α-height-balanced property.

One way of finding a scapegoat, is to climb from the new node back up to the root and select the first node that isn't α-weight-balanced.

Climbing back up to the root requires O(log ''n'') storage space, usually allocated on the stack, or parent pointers. This can actually be avoided by pointing each child at its parent as you go down, and repairing on the walk back up.

To determine whether a potential node is a viable scapegoat, we need to check its α-weight-balanced property. To do this we can go back to the definition:
 size(left) &lt;= α*size(node)
 size(right) &lt;= α*size(node)
However a large optimisation can be made by realising that we already know two of the three sizes, leaving only the third having to be calculated.

Consider the following example to demonstrate this. Assuming that we're climbing back up to the root:
 size(parent) = size(node) + size(sibling) + 1
But as:
 size(inserted node) = 1.
The case is trivialized down to:
 size[x+1] = size[x] + size(sibling) + 1
Where x = this node, x + 1 = parent and size(sibling) is the only function call actually required.

Once the scapegoat is found, the subtree rooted at the scapegoat is completely rebuilt to be perfectly balanced.&lt;ref name=galperin_rivest/&gt;  This can be done in O(''n'') time by traversing the nodes of the subtree to find their values in sorted order and recursively choosing the median as the root of the subtree.

As rebalance operations take O(''n'') time (dependent on the number of nodes of the subtree), insertion has a worst-case performance of O(''n'') time.  However, because these worst-case scenarios are spread out, insertion takes O(log ''n'') amortized time.

====Sketch of proof for cost of insertion====

Define the Imbalance of a node ''v'' to be the absolute value of the difference in size between its left node and right node minus 1, or 0, whichever is greater.  In other words:

&lt;math&gt;I(v) = max(|left(v) - right(v)| - 1, 0) \ &lt;/math&gt;

Immediately after rebuilding a subtree rooted at ''v'', I(''v'') = 0.

'''Lemma:''' Immediately before rebuilding the subtree rooted at ''v'', &lt;br /&gt;
&lt;math&gt;I(v) = \Omega (|v|) \ &lt;/math&gt;&lt;br /&gt;
(&lt;math&gt;\Omega \ &lt;/math&gt; is [[Big O Notation]].)

Proof of lemma:

Let &lt;math&gt;v_0&lt;/math&gt; be the root of a subtree immediately after rebuilding.  &lt;math&gt;h(v_0) = log(|v_0| + 1) \ &lt;/math&gt;.  If there are &lt;math&gt;\Omega (|v_0|)&lt;/math&gt; degenerate insertions (that is, where each inserted node increases the height by 1), then &lt;br /&gt;
&lt;math&gt;I(v) =  \Omega (|v_0|) \ &lt;/math&gt;,&lt;br /&gt;
&lt;math&gt;h(v) = h(v_0) + \Omega (|v_0|) \ &lt;/math&gt; and&lt;br /&gt;
&lt;math&gt;log(|v|) \le log(|v_0| + 1) + 1 \ &lt;/math&gt;.

Since &lt;math&gt;I(v) = \Omega (|v|)&lt;/math&gt; before rebuilding, there were &lt;math&gt;\Omega (|v|)&lt;/math&gt; insertions into the subtree rooted at &lt;math&gt;v&lt;/math&gt; that did not result in rebuilding.  Each of these insertions can be performed in &lt;math&gt;O(log n)&lt;/math&gt; time.  The final insertion that causes rebuilding costs &lt;math&gt;O(|v|)&lt;/math&gt;.  Using [[aggregate analysis]] it becomes clear that the amortized cost of an insertion is &lt;math&gt;O(log n)&lt;/math&gt;:

&lt;math&gt;{\Omega (|v|) O(\log{n}) + O(|v|) \over \Omega (|v|)} = O(\log{n}) \ &lt;/math&gt;

===Deletion===

Scapegoat trees are unusual in that deletion is easier than insertion. To enable deletion, scapegoat trees need to store an additional value with the tree data structure. This property, which we will call MaxNodeCount simply represents the highest achieved NodeCount. It is set to NodeCount whenever the entire tree is rebalanced, and after insertion is set to max(MaxNodeCount, NodeCount).

To perform a deletion, we simply remove the node as you would in a simple binary search tree, but if
 NodeCount &lt;= α*MaxNodeCount
then we rebalance the entire tree about the root, remembering to set MaxNodeCount to NodeCount.

This gives deletion its worst-case performance of O(n) time; however, it is amortized to O(log ''n'') average time.

====Sketch of proof for cost of deletion====

Suppose the scapegoat tree has &lt;math&gt;n&lt;/math&gt; elements and has just been rebuilt (in other words, it is a complete binary tree).  At most &lt;math&gt;n/2 - 1&lt;/math&gt; deletions can be performed before the tree must be rebuilt.  Each of these deletions take &lt;math&gt;O(\log{n})&lt;/math&gt; time (the amount of time to search for the element and flag it as deleted).  The &lt;math&gt;n/2&lt;/math&gt; deletion causes the tree to be rebuilt and takes &lt;math&gt;O(\log{n}) + O(n)&lt;/math&gt; (or just &lt;math&gt;O(n)&lt;/math&gt;) time.  Using aggregate analysis it becomes clear that the amortized cost of a deletion is &lt;math&gt;O(\log{n})&lt;/math&gt;:

&lt;math&gt;
{\sum_{1}^{{n \over 2}} O(\log{n}) + O(n) \over {n \over 2}} = 
{{n \over 2}O(\log{n}) + O(n) \over {n \over 2}} = 
O(\log{n}) \ 
&lt;/math&gt;

===Lookup===

Lookup is not modified from a standard binary search tree, and has a worst-case time of O(log ''n''). This is in contrast to [[splay tree]]s which have a worst-case time of O(''n''). The reduced node memory overhead compared to other self-balancing binary search trees can further improve [[locality of reference]] and caching.

==See also==
* [[Splay tree]]
* [[tree data structure|Trees]]
* [[Tree rotation]]
* [[AVL tree]]
* [[B-tree]]
* [[T-tree]]
* [[List of data structures]]

==References==
{{Reflist}}

==External links==
*[http://people.ksp.sk/~kuko/bak/index.html Scapegoat Tree Applet] by Kubo Kovac
*[http://cg.scs.carleton.ca/~morin/teaching/5408/refs/gr93.pdf Scapegoat Trees: Galperin and Rivest's paper describing scapegoat trees]
*[http://publications.csail.mit.edu/lcs/pubs/pdf/MIT-LCS-TR-700.pdf On Consulting a Set of Experts and Searching (full version paper)]
*[http://opendatastructures.org/versions/edition-0.1g/ods-python/8_Scapegoat_Trees.html Open Data Structures - Chapter 8 - Scapegoat Trees]

{{CS-Trees}}

{{DEFAULTSORT:Scapegoat Tree}}
[[Category:Binary trees]]</text>
      <sha1>68c4p8l95wfu7vx3lxj4cjjbdumli28</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tango tree</title>
    <ns>0</ns>
    <id>9871765</id>
    <revision>
      <id>540232384</id>
      <parentid>531490502</parentid>
      <timestamp>2013-02-25T12:22:05Z</timestamp>
      <contributor>
        <username>FrescoBot</username>
        <id>9021902</id>
      </contributor>
      <minor/>
      <comment>Bot: [[Wikipedia:Manual of Style/Layout#Standard appendices and footers|standard sections headers]] and minor changes</comment>
      <text xml:space="preserve" bytes="8201">A '''Tango tree''' is a type of [[binary search tree]] proposed by [[Erik D. Demaine]], [[Dion Harmon]], [[John Iacono]], and [[Mihai Patrascu]] in 2004. It is an [[online algorithm|online]] binary search tree that achieves an &lt;math&gt;O(\log \log n)&lt;/math&gt; [[competitive ratio]] relative to the optimal [[offline algorithm|offline]] binary search tree, while only using &lt;math&gt;O(\log \log n)&lt;/math&gt; additional bits of memory per node. This improved upon the previous best known competitive ratio, which was &lt;math&gt;O(\log n)&lt;/math&gt;.

==Structure==
Tango trees work by partitioning a binary search tree into a set of ''preferred paths'', which are themselves stored in auxiliary trees (so the tango tree is represented as a tree of trees).

===Reference Tree===
To construct a tango tree, we simulate a [[complete binary tree|complete]] binary search tree called the ''reference tree'', which is simply a traditional binary search tree containing all the elements. This tree never shows up in the actual implementation, but is the conceptual basis behind the following pieces of a tango tree.

===Preferred Paths===
First, we define for each node its ''preferred child'', which informally is the most-recently-touched child by a traditional binary search tree lookup. More formally, consider a [[subtree]] ''T'', rooted at ''p'', with children ''l'' (left) and ''r'' (right). We set ''r'' as the preferred child of ''p'' if the most recently accessed node in ''T'' is in the subtree rooted at ''r'', and ''l'' as the preferred child otherwise. Note that if the most recently accessed node of ''T'' is ''p'' itself, then ''l'' is the preferred child by definition.

A preferred path is defined by starting at the root and following the preferred children until reaching a leaf node. Removing the nodes on this path partitions the remainder of the tree into a number of subtrees, and we [[recursion|recurse]] on each subtree (forming a preferred path from its root, which partitions the subtree into more subtrees).

===Auxiliary Trees===
To represent a preferred path, we store its nodes in a [[balanced binary search tree]], specifically a [[red-black tree]]. For each non-leaf node ''n'' in a preferred path ''P'', it has a non-preferred child ''c'', which is the root of a new auxiliary tree. We attach this other auxiliary tree's root (''c'') to ''n'' in ''P'', thus linking the auxiliary trees together. We also augment the auxiliary tree by storing at each node the minimum and maximum depth (depth in the reference tree, that is) of nodes in the subtree under that node.

==Algorithm==
===Searching===
To search for an element in the tango tree, we simply simulate searching the reference tree. We start by searching the preferred path connected to the root, which is simulated by searching the auxiliary tree corresponding to that preferred path. If the auxiliary tree doesn't contain the desired element, the search terminates on the parent of the root of the subtree containing the desired element (the beginning of another preferred path), so we simply proceed by searching the auxiliary tree for that preferred path, and so forth.

===Updating===
In order to maintain the structure of the tango tree (auxiliary trees correspond to preferred paths), we must do some updating work whenever preferred children change as a result of searches. When a preferred child changes, the top part of a preferred path becomes detached from the bottom part (which becomes its own preferred path) and reattached to another preferred path (which becomes the new bottom part). In order to do this efficiently, we'll define ''cut'' and ''join'' operations on our auxiliary trees.

====Join====
Our ''join'' operation will combine two auxiliary trees as long as they have the property that the top node of one (in the reference tree) is a child of the bottom node of the other (essentially, that the corresponding preferred paths can be concatenated). This will work based on the ''concatenate'' operation of red-black trees, which combines two trees as long as they have the property that all elements of one are less than all elements of the other, and ''split'', which does the reverse. In the reference tree, note that there exist two nodes in the top path such that a node is in the bottom path if and only if its key-value is between them. Now, to join the bottom path to the top path, we simply ''split'' the top path between those two nodes, then ''concatenate'' the two resulting auxiliary trees on either side of the bottom path's auxiliary tree, and we have our final, joined auxiliary tree.

====Cut====
Our ''cut'' operation will break a preferred path into two parts at a given node, a top part and a bottom part. More formally, it'll partition an auxiliary tree into two auxiliary trees, such that one contains all nodes at or above a certain depth in the reference tree, and the other contains all nodes below that depth. As in ''join'', note that the top part has two nodes that bracket the bottom part. Thus, we can simply ''split'' on each of these two nodes to divide the path into three parts, then ''concatenate'' the two outer ones so we end up with two parts, the top and bottom, as desired.

==Analysis==
In order to bound the competitive ratio for tango trees, we must find a lower bound on the performance of the optimal offline tree that we use as a benchmark. Once we find an upper bound on the performance of the tango tree, we can divide them to bound the competitive ratio.

===Interleave Bound===
To find a lower bound on the work done by the optimal offline binary search tree, we again use the notion of preferred children. When considering an access sequence (a sequence of searches), we keep track of how many times a reference tree node's preferred child switches. The total number of switches (summed over all nodes) gives an [[asymptotic analysis|asymptotic]] lower bound on the work done by any binary search tree algorithm on the given access sequence. This is called the ''interleave lower bound''.&lt;ref name=&quot;DHIP&quot;&gt;Demaine, E., Harmon, D., Iacono, J., and Patrascu, M. SIAM Journal on Computing 2007 37:1, 240-251. http://dx.doi.org/10.1137/S0097539705447347&lt;/ref&gt;

===Tango Tree===
In order to connect this to tango trees, we will find an upper bound on the work done by the tango tree for a given access sequence. Our upper bound will be &lt;math&gt;(k+1) O(\log \log n)&lt;/math&gt;, where ''k'' is the number of interleaves.

The total cost is divided into two parts, searching for the element, and updating the structure of the tango tree to maintain the proper invariants (switching preferred children and re-arranging preferred paths).

====Searching====
To see that the searching (not updating) fits in this bound, simply note that every time an auxiliary tree search is unsuccessful and we have to move to the next auxiliary tree, that results in a preferred child switch (since the parent preferred path now switches directions to join the child preferred path). Since all auxiliary tree searches are unsuccessful except the last one (we stop once a search is successful, naturally), we search &lt;math&gt;k+1&lt;/math&gt; auxiliary trees. Each search takes &lt;math&gt;O(\log \log n)&lt;/math&gt;, because an auxiliary tree's size is bounded by &lt;math&gt;\log n&lt;/math&gt;, the height of the reference tree.

====Updating====
The update cost fits within this bound as well, because we only have to perform one ''cut'' and one ''join'' for every visited auxiliary tree. A single ''cut'' or ''join'' operation takes only a constant number of searches, ''splits'', and ''concatenates'', each of which takes logarithmic time in the size of the auxiliary tree, so our update cost is &lt;math&gt;(k+1) O(\log \log n)&lt;/math&gt;.

===Competitive Ratio===
Tango trees are &lt;math&gt;O(\log \log n)&lt;/math&gt;-competitive, because the work done by the optimal offline binary search tree is at least linear in ''k'' (the total number of preferred child switches), and the work done by the tango tree is at most &lt;math&gt;(k+1) O(\log \log n)&lt;/math&gt;.

==See also==
* [[Splay tree]]
* [[Binary search tree]]
* [[Red-black tree]]
* [[Tree (data structure)]]

==References==

&lt;references/&gt;

{{DEFAULTSORT:Tango Tree}}
[[Category:Binary trees]]</text>
      <sha1>q3tynwcfcpstp5spbg42hf8wj9bwyay</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Binary search tree</title>
    <ns>0</ns>
    <id>4320</id>
    <revision>
      <id>622308742</id>
      <parentid>620932581</parentid>
      <timestamp>2014-08-22T08:01:48Z</timestamp>
      <contributor>
        <ip>77.56.33.157</ip>
      </contributor>
      <comment>Questioning behavior on inserting new node same as existing one in tree</comment>
      <text xml:space="preserve" bytes="27890">{{Infobox data structure
|name=Binary search tree
|type=tree
|invented_by=P.F. Windley, [[Andrew Donald Booth|A.D. Booth]], A.J.T. Colin, and T.N. Hibbard
|invented_year=1960
|
&lt;!-- NOTE:
    Base of logarithms doesn't matter in big O notation. O(log n) is the same as O(lg n) or O(ln n) or O(log_2 n). A change of base is just a constant factor. So don't change these O(log n) complexities to O(lg n) or something else just to indicate a base-2 log. The base doesn't matter.
--&gt;
|space_avg=O(n)
|space_worst=O(n)
|search_avg=O(log n)
|search_worst=O(n)
|insert_avg=O(log n)
|insert_worst=O(n)
|delete_avg=O(log n)
|delete_worst=O(n)
}}

[[File:Binary search tree.svg|right|200px|thumb|A binary search tree of size 9 and depth 3, with root 8 and leaves 1, 4, 7 and 13]]

In [[computer science]], a '''binary search tree''' ('''BST'''), sometimes also called an '''ordered''' or '''sorted binary tree''', is a [[node (computer science)|node]]-based [[binary tree]] data structure where each node has a comparable key (and an associated value) and satisfies the restriction that the key in any node is larger than the keys in all nodes in that node's left subtree and smaller than the keys in all nodes in that node's right sub-tree. Each node has no more than two child nodes. Each child must either be a leaf node or the root of another binary search tree. The left sub-tree contains only nodes with keys less than the parent node; the right sub-tree contains only nodes with keys greater than the parent node. BSTs are also dynamic [[data structure]]s, and the size of a BST is only limited by the amount of free memory in the operating system. The main advantage of binary search trees is that it remains ordered, which provides quicker search times than many other data structures. The common properties of binary search trees are as follows:&lt;ref&gt;{{citation
|last1=Gilberg
|first1=R.
|last2=Forouzan
|first2=B.
|title=Data Structures: A Pseudocode Approach With C++
|publisher=Brooks/Cole
|location=Pacific Grove, CA
|year=2001
|isbn=0-534-95216-X
|page=339
|chapter=8
}}&lt;/ref&gt;
* The left [[tree (data structure)#Subtree|subtree]] of a node contains only nodes with keys less than the node's key.
* The right subtree of a node contains only nodes with keys greater than the node's key.
* The left and right subtree each must also be a binary search tree.
* Each node can have up to two successor nodes.
* There must be no duplicate nodes.
* A unique path exists from the root to every other node.
Generally, the information represented by each node is a record rather than a single data element.  However, for sequencing purposes, nodes are compared according to their keys rather than any part of their associated records.

The major advantage of binary search trees over other data structures is that the related [[sorting algorithm]]s and [[search algorithm]]s such as [[in-order traversal]] can be very efficient. The other advantages are:

* Binary Search Tree is fast in insertion and deletion etc. when balanced.
* Very efficient and its code is easier than other data structures.
* Stores keys in the nodes in a way that searching, insertion and deletion can be done efficiently.
* Implementation is very simple in Binary Search Trees.
* Nodes in tree are dynamic in nature.

Binary search trees are a fundamental data structure used to construct more abstract data structures such as [[set (computer science)|sets]], [[set (computer science)#Multiset|multisets]], and [[associative array]]s. Some of their disadvantages are as follows:

* The shape of the binary search tree totally depends on the order of insertions, and it can be degenerated.
* When inserting or searching for an element in binary search tree, the key of each visited node has to be compared with the key of the element to be inserted or found, i.e., it takes a long time to search an element in a binary search tree.
* The keys in the binary search tree may be long and the run time may increase.
* After a long intermixed sequence of random insertion and deletion, the expected height of the tree approaches square root of the number of keys which grows much faster than &lt;math&gt;\log n&lt;/math&gt;.

== Determining whether a tree is a BST or not ==

Sometimes we already have a binary tree, and we need to determine whether or not it is a BST. This is an interesting problem which has a simple recursive solution.

The BST property—every node on the right subtree has to be larger than the current node and every node on the left subtree has to be smaller than (or equal to - should not be the case as only unique vales should be in the tree - this also poses the question as to if such nodes should be left or right of this parent) the current node—is the key to figuring out whether a tree is a BST or not. On first thought it might look like we can simply traverse the tree, at every node check whether the node contains a value larger than the value at the left child and smaller than the value on the right child, and if this condition holds for all the nodes in the tree then we have a BST. This is the so-called &quot;Greedy approach,&quot; making a decision based on local properties. But this approach clearly won't work for the following tree:

      20
     /  \
   10    30
        /  \
       5    40

In the tree above, each node meets the condition that the node contains a value larger than its left child and smaller than its right child hold, and yet it is not a BST: the value 5 is on the right subtree of the node containing 20, a violation of the BST property!

How do we solve this? It turns out that instead of making a decision based solely on the values of a node and its children, we also need information flowing down from the parent as well. In the case of the tree above, if we could remember about the node containing the value 20, we would see that the node with value 5 is violating the BST property contract.

So the condition we need to check at each node is: a) if the node is the left child of its parent, then it must be smaller than (or equal to) the parent and it must pass down the value from its parent to its right subtree to make sure none of the nodes in that subtree is greater than the parent, and similarly b) if the node is the right child of its parent, then it must be larger than the parent and it must pass down the value from its parent to its left subtree to make sure none of the nodes in that subtree is lesser than the parent.

A simple but elegant recursive solution in Java can explain this further:
&lt;source lang=&quot;java&quot;&gt;
public static boolean isBST(TreeNode node, int maxData, int minData) {
    if (node == null)
        return true;
    
    if (node.getData() &gt;= maxData || node.getData() &lt;= minData)
        return false;
    
    return (isBST(node.left, node.getData(), minData) &amp;&amp; isBST(node.right, maxData, node.getData()));
}
&lt;/source&gt;

The initial call to this function can be something like this:

&lt;source lang=&quot;java&quot;&gt;
if (isBST(root, Integer.MAX_VALUE, Integer.MIN_VALUE))
    System.out.println(&quot;This is a BST.&quot;);
else
    System.out.println(&quot;This is NOT a BST!&quot;);
&lt;/source&gt;

Essentially we keep creating a valid range (starting from [ MIN_VALUE, MAX_VALUE]) and keep shrinking it down for each node as we go down recursively.

== Difference between binary tree and binary search tree ==

Binary tree: In short, a [[binary tree]] is a tree where each node has up to two leaves. In a binary tree, a left child node and a right child node contain values which can be either greater, less, or equal to parent node.

     3
    / \
   4   5

Binary Search Tree: In binary search tree, the left child contains nodes with values less than the parent node and where the right child only contains nodes with values greater than the parent node. There must be no duplicate nodes.

     4
    / \
   3   5

==Operations==
Operations, such as '''''find''''', on a binary search tree require comparisons between nodes. These comparisons are made with calls to a ''comparator'', which is a [[subroutine]] that computes the total order (linear order) on any two keys. This ''comparator'' can be explicitly or implicitly defined, depending on the language in which the binary search tree was implemented. A common ''comparator'' is the less-than function, for example, ''a'' &amp;lt; ''b'', where ''a'' and ''b'' are keys of two nodes ''a'' and ''b'' in a binary search tree.

===Searching===
Searching a binary search tree for a specific key can be a [[recursion (computer science)|recursive]] or an [[iteration#Computing|iterative]] process.

We begin by examining the [[tree (data structure)#root nodes|root node]]. If the tree is ''null'', the key we are searching for does not exist in the tree. Otherwise, if the key equals that of the root, the search is successful and we return the node. If the key is less than that of the root, we search the left subtree. Similarly, if the key is greater than that of the root, we search the right subtree. This process is repeated until the key is found or the remaining subtree is ''null''. If the searched key is not found before a ''null'' subtree is reached, then the item must not be present in the tree. This is easily expressed as a recursive algorithm:

 '''function''' &lt;u&gt;Find-recursive&lt;/u&gt;(key, node):  ''// call initially with node = root''
     '''if''' node = Null '''or''' node.key = key '''then'''
         '''return''' node
     '''else if''' key &lt; node.key '''then'''
         '''return''' &lt;u&gt;Find-recursive&lt;/u&gt;(key, node.left)
     '''else'''
         '''return''' &lt;u&gt;Find-recursive&lt;/u&gt;(key, node.right)

The same algorithm can be implemented iteratively:
 '''function''' &lt;u&gt;Find&lt;/u&gt;(key, root):
     current-node := root
     '''while''' current-node '''is not Null do'''
         '''if''' current-node.key = key '''then'''
             '''return''' current-node
         '''else if''' key &lt; current-node.key '''then'''
             current-node := current-node.left
         '''else'''
             current-node := current-node.right
     '''return Null'''

Because in the worst case this algorithm must search from the root of the tree to the leaf farthest from the root, the search operation takes time proportional to the tree's ''height'' (see [[Tree (data structure)#Terminology|tree terminology]]). On average, binary search trees with ''n'' nodes have [[big O notation|O]](log ''n'') height. However, in the worst case, binary search trees can have O(''n'') height, when the unbalanced tree resembles a [[linked list]] ([[binary Tree#Types of binary trees|degenerate tree]]).

===Insertion===&lt;!-- This section is linked from [[Red-black tree]] --&gt;
Insertion begins as a search would begin; if the key is not equal to that of the root, we search the left or right subtrees as before. Eventually, we will reach an external node and add the new key-value pair (here encoded as a record 'newNode') as its right or left child, depending on the node's key. In other words, we examine the root and recursively insert the new node to the left subtree if its key is less than that of the root, or the right subtree if its key is greater than or equal to the root.

Here's how a typical binary search tree insertion might be performed in a binary tree in [[C++]]:

&lt;source lang=&quot;cpp&quot;&gt;
void insert(Node*&amp; root, int data) {
  if (!root) 
    root = new Node(data);
  else if (data &lt; root-&gt;data)
    insert(root-&gt;left, data);
  else if (data &gt; root-&gt;data)
    insert(root-&gt;right, data);
}
&lt;/source&gt;

The above ''destructive'' procedural variant modifies the tree in place. It uses only constant heap space (and the iterative version uses constant stack space as well), but the prior version of the tree is lost. Alternatively, as in the following [[Python (programming language)|Python]] example, we can reconstruct all ancestors of the inserted node; any reference to the original tree root remains valid, making the tree a [[persistent data structure]]:

&lt;source lang=&quot;python&quot;&gt;
 def binary_tree_insert(node, key, value):
     if node is None:
         return TreeNode(None, key, value, None)
     if key == node.key:
         return TreeNode(node.left, key, value, node.right)
     if key &lt; node.key:
         return TreeNode(binary_tree_insert(node.left, key, value), node.key, node.value, node.right)
     else:
         return TreeNode(node.left, node.key, node.value, binary_tree_insert(node.right, key, value))
&lt;/source&gt;

The part that is rebuilt uses O(log ''n'') space in the average case and O(''n'') in the worst case (see [[big-O notation]]).

In either version, this operation requires time proportional to the height of the tree in the worst case, which is O(log ''n'') time in the average case over all trees, but O(''n'') time in the worst case.

Another way to explain insertion is that in order to insert a new node in the tree, its key is first compared with that of the root. If its key is less than the root's, it is then compared with the key of the root's left child. If its key is greater, it is compared with the root's right child. This process continues, until the new node is compared with a leaf node, and then it is added as this node's right or left child, depending on its key.

There are other ways of inserting nodes into a binary tree, but this is the only way of inserting nodes at the leaves and at the same time preserving the BST structure.

===Deletion===&lt;!--This section is linked from [[Red-black tree]]--&gt;
There are three possible cases to consider:
* '''Deleting a leaf (node with no children):''' Deleting a leaf is easy, as we can simply remove it from the tree.
* '''Deleting a node with one child:''' Remove the node and replace it with its child.
* '''Deleting a node with two children:''' Call the node to be deleted ''N''.  Do not delete ''N''.  Instead, choose either its [[tree traversal|in-order]] successor node or its in-order predecessor node, ''R''.  Copy the value of ''R'' to ''N'', then recursively call delete on ''R'' until reaching one of the first two cases.

Broadly speaking, nodes with children are harder to delete. As with all binary trees, a node's in-order successor is its right subtree's left-most child, and a node's in-order predecessor is the left subtree's right-most child. In either case, this node will have zero or one children. Delete it according to one of the two simpler cases above.
[[File:binary search tree delete.svg|thumb|640px|center|Deleting a node with two children from a binary search tree. First the rightmost node in the left subtree, the inorder predecessor '''6''', is identified. Its value is copied into the node being deleted. The inorder predecessor can then be easily deleted because it has at most one child. The same method works symmetrically using the inorder successor labelled '''9'''.]]

Consistently using the in-order successor or the in-order predecessor for every instance of the two-child case can lead to an [[self-balancing binary search tree|unbalanced]] tree, so some implementations select one or the other at different times.

Runtime analysis: Although this operation does not always traverse the tree down to a leaf, this is always a possibility; thus in the worst case it requires time proportional to the height of the tree. It does not require more even when the node has two children, since it still follows a single path and does not visit any node twice.

&lt;source lang=&quot;python&quot;&gt;
def find_min(self):   # Gets minimum node (leftmost leaf) in a subtree
    current_node = self
    while current_node.left_child:
        current_node = current_node.left_child
    return current_node

def replace_node_in_parent(self, new_value=None):
    if self.parent:
        if self == self.parent.left_child:
            self.parent.left_child = new_value
        else:
            self.parent.right_child = new_value
    if new_value:
        new_value.parent = self.parent

def binary_tree_delete(self, key):
    if key &lt; self.key:
        self.left_child.binary_tree_delete(key)
    elif key &gt; self.key:
        self.right_child.binary_tree_delete(key)
    else: # delete the key here
        if self.left_child and self.right_child: # if both children are present
            successor = self.right_child.find_min()
            self.key = successor.key
            successor.binary_tree_delete(successor.key)
        elif self.left_child:   # if the node has only a *left* child
            self.replace_node_in_parent(self.left_child)
        elif self.right_child:  # if the node has only a *right* child
            self.replace_node_in_parent(self.right_child)
        else: # this node has no children
            self.replace_node_in_parent(None)
&lt;/source&gt;

===Traversal===
{{main|Tree traversal}}
Once the binary search tree has been created, its elements can be retrieved [[in-order traversal|in-order]] by [[recursion|recursively]] traversing the left subtree of the root node, accessing the node itself, then recursively traversing the right subtree of the node, continuing this pattern with each node in the tree as it's recursively accessed. As with all binary trees, one may conduct a [[pre-order traversal]] or a [[post-order traversal]], but neither are likely to be useful for binary search trees. An in-order traversal of a binary search tree will always result in a sorted list of node items (numbers, strings or other comparable items).

The code for in-order traversal in Python is given below. It will call '''callback''' for every node in the tree.

&lt;source lang=&quot;python&quot;&gt;
def traverse_binary_tree(node, callback):
    if node is None:
        return
    traverse_binary_tree(node.leftChild, callback)
    callback(node.value)
    traverse_binary_tree(node.rightChild, callback)
&lt;/source&gt;

With respect to the example defined in the lead section of this article, 
* The pre-order traversal is: 8, 3, 1, 6, 4, 7, 10, 14, 13.
* The in-order traversal is: 1, 3, 4, 6, 7, 8, 10, 13, 14.
* The post-order traversal is: 1, 4, 7, 6, 3, 13, 14, 10, 8.

Traversal requires [[big O notation#Related asymptotic notations|O(''n'')]] time, since it must visit every node. This algorithm is also O(''n''), so it is [[asymptotically optimal]].

===Sort===
[[File:Binary tree sort(2).png|thumbnail]]
A binary search tree can be used to implement a simple but efficient [[sorting algorithm]]. Similar to [[heapsort]], we insert all the values we wish to sort into a new ordered data structure—in this case a binary search tree—and then traverse it in order, building our result:

&lt;source lang=&quot;python&quot;&gt;
def build_binary_tree(values):
    tree = None
    for v in values:
        tree = binary_tree_insert(tree, v)
    return tree

def get_inorder_traversal(root):
    '''
    Returns a list containing all the values in the tree, starting at *root*.
    Traverses the tree in-order(leftChild, root, rightChild).
    '''
    result = []
    traverse_binary_tree(root, lambda element: result.append(element))
    return result
&lt;/source&gt;

The worst-case time of &lt;code&gt;build_binary_tree&lt;/code&gt; is &lt;math&gt;O(n^2)&lt;/math&gt;—if you feed it a sorted list of values, it chains them into a [[linked list]] with no left subtrees. For example, &lt;code&gt;build_binary_tree([1, 2, 3, 4, 5])&lt;/code&gt; yields the tree &lt;code&gt;(1 (2 (3 (4 (5)))))&lt;/code&gt;.

There are several schemes for overcoming this flaw with simple binary trees; the most common is the [[self-balancing binary search tree]]. If this same procedure is done using such a tree, the overall worst-case time is O(''n''log ''n''), which is [[asymptotically optimal]] for a [[comparison sort]]. In practice, the poor [[CPU cache|cache]] performance and added overhead in time and space for a tree-based sort (particularly for node [[dynamic memory allocation|allocation]]) make it inferior to other asymptotically optimal sorts such as  [[heapsort]] for static list sorting. On the other hand, it is one of the most efficient methods of ''incremental sorting'', adding items to a list over time while keeping the list sorted at all times.

==Types==
There are many types of binary search trees. [[AVL tree]]s and [[red-black tree]]s are both forms of [[self-balancing binary search tree]]s. A [[splay tree]] is a binary search tree that automatically moves frequently accessed elements nearer to the root. In a [[treap]] (''tree [[heap (data structure)|heap]]''), each node also holds a (randomly chosen) priority and the parent node has higher priority than its children. [[Tango tree]]s are trees optimized for fast searches.

Two other titles describing binary search trees are that of a ''complete'' and ''degenerate'' tree.

A complete binary tree is a binary tree, which is completely filled, with the possible exception of the bottom level, which is filled from left to right. In complete binary tree, all nodes are far left as possible. It is a tree with n levels, where for each level d &lt;= n - 1, the number of existing nodes at level d is equal to 2&lt;sup&gt;d&lt;/sup&gt;. This means all possible nodes exist at these levels. An additional requirement for a complete binary tree is that for the nth level, while every node does not have to exist, the nodes that do exist must fill from left to right.

A degenerate tree is a tree where for each parent node, there is only one associated child  node. It is unbalanced and, in the worst case, performance degrades to that of a linked list. If your added node function does not handle re-balancing, then you can easily construct a degenerate tree by feeding it with data that is already sorted.What this means is that in a performance measurement, the tree will  essentially behave like a linked list data structure.

===Performance comparisons===
D. A. Heger (2004)&lt;ref&gt;{{Citation | title=A Disquisition on The Performance Behavior of Binary Search Tree Data Structures | first1=Dominique A. | last1=Heger | year=2004 | journal=European Journal for the Informatics Professional | volume=5 | url=http://www.cepis.org/upgrade/files/full-2004-V.pdf | issue=5 | pages=67–75}}&lt;/ref&gt; presented a performance comparison of binary search trees. [[Treap]] was found to have the best average performance, while [[red-black tree]] was found to have the smallest amount of performance variations.

===Optimal binary search trees===
{{Main|Optimal binary search tree}}
[[File:BinaryTreeRotations.svg|thumb|300px|Tree rotations are very common internal operations in binary trees to keep perfect, or near-to-perfect, internal balance in the tree.]]
If we do not plan on modifying a search tree, and we know exactly how often each item will be accessed, we can construct&lt;ref&gt;{{cite web|last=Gonnet|first=Gaston|title=Optimal Binary Search Trees|url=http://linneus20.ethz.ch:8080/4_7_1.html|work=Scientific Computation|publisher=ETH Zürich|accessdate=1 December 2013}}&lt;/ref&gt; an ''optimal binary search tree'', which is a search tree where the average cost of looking up an item (the ''expected search cost'') is minimized.

Even if we only have estimates of the search costs, such a system can considerably speed up lookups on average. For example, if you have a BST of English words used in a [[spell checker]], you might balance the tree based on word frequency in [[text corpus|text corpora]], placing words like ''the'' near the root and words like ''agerasia'' near the leaves. Such a tree might be compared with [[Huffman tree]]s, which similarly seek to place frequently used items near the root in order to produce a dense information encoding; however, Huffman trees store data elements only in leaves, and these elements need not be ordered.

If we do not know the sequence in which the elements in the tree will be accessed in advance, we can use [[splay tree]]s which are asymptotically as good as any static search tree we can construct for any particular sequence of lookup operations.

''Alphabetic trees'' are Huffman trees with the additional constraint on order, or, equivalently, search trees with the modification that all elements are stored in the leaves. Faster algorithms exist for ''optimal alphabetic binary trees'' (OABTs).

{{clear}}

==See also==
{{colbegin|2}}
*[[Search tree]]
*[[Binary search algorithm]]
*[[Randomized binary search tree]]
*[[Tango tree]]s
*[[Self-balancing binary search tree]]
*[[Geometry of binary search trees]]
*[[Red-black tree]]
*[[AVL trees]]
{{colend}}

==References==
{{Reflist}}

==Further reading==
*{{DADS|Binary Search Tree|binarySearchTree}}
*{{cite book|last1=Cormen|first1=Thomas H. |authorlink1=Thomas H. Cormen|last2=Leiserson|first2=Charles E. |authorlink2=Charles E. Leiserson|last3=Rivest|first3=Ronald L. |authorlink3=Ronald L. Rivest|authorlink4=Clifford Stein|first4=Clifford |last4=Stein|title=[[Introduction to Algorithms]]|edition=2nd|year=2001|publisher=MIT Press &amp; McGraw-Hill|isbn=0-262-03293-7|pages=253–272, 356–363|chapter=12: Binary search trees, 15.5: Optimal binary search trees}}
*{{cite web|url=http://nova.umuc.edu/~jarc/idsv/lesson1.html|title=Binary Tree Traversals|last=Jarc|first=Duane J.|date=3 December 2005|work=Interactive Data Structure Visualizations|publisher=[[University of Maryland]]}}
*{{cite book|last=Knuth|first=Donald|authorlink=Donald Knuth|title=[[The Art of Computer Programming]]|edition=3rd|volume=3: &quot;Sorting and Searching&quot;|year=1997|publisher=Addison-Wesley|isbn=0-201-89685-0|pages=426–458|chapter=6.2.2: Binary Tree Searching}}
*{{cite web|url=http://employees.oneonta.edu/zhangs/PowerPointPlatform/resources/samples/binarysearchtree.ppt|title=Binary Search Tree|last=Long|first=Sean|work=Data Structures and Algorithms Visualization-A PowerPoint Slides Based Approach|publisher=[[SUNY Oneonta]]|format=[[Microsoft PowerPoint|PPT]]}}
*{{cite web|url=http://cslibrary.stanford.edu/110/BinaryTrees.html|title=Binary Trees|last=Parlante|first=Nick|year=2001|work=CS Education Library|publisher=[[Stanford University]]}}

==External links==
*[http://en.literateprograms.org/Category:Binary_search_tree Literate implementations of binary search trees in various languages] on LiteratePrograms
*{{cite web|url=http://goletas.com/csharp-collections/|title=Goletas.Collections|last=Goleta|first=Maksim|date=27 November 2007|work=goletas.com}} Includes an iterative [[C Sharp (programming language)|C#]] implementation of AVL trees.
*{{cite web|url=http://cg.scs.carleton.ca/~dana/pbst|title=Persistent Binary Search Trees|last=Jansens|first=Dana|publisher=Computational Geometry Lab, School of Computer Science, [[Carleton University]]}} C implementation using [[GLib]].
*[http://btv.melezinek.cz Binary Tree Visualizer] (JavaScript animation of various BT-based data structures)
*{{cite web|url=http://people.ksp.sk/~kuko/bak/|title=Binary Search Trees|last=Kovac|first=Kubo|publisher=Korešpondenčný seminár z programovania|format=[[Java applet]]}}
*{{cite web|url=http://jdserver.homelinux.org/wiki/Binary_Search_Tree|title=Binary Search Tree|last=Madru|first=Justin|date=18 August 2009|work=JDServer}} C++ implementation.
*{{cite web|url=http://1wt.eu/articles/ebtree/|title=Elastic Binary Trees (ebtree)|last=Tarreau|first=Willy|year=2011|work=1wt.eu}}
*[http://code.activestate.com/recipes/286239/ Binary Search Tree Example in Python]
*{{cite web|url=http://msdn.microsoft.com/en-us/library/1sf8shae%28v=vs.80%29.aspx|title=References to Pointers (C++)|year=2005|work=[[MSDN]]|publisher=[[Microsoft]]}} Gives an example binary tree implementation.
*{{cite web|last=Igushev|first=Eduard|title=Binary Search Tree C++ implementation|url=http://igushev.com/implementations/binary-search-tree-cpp/}}
*{{cite web|last=Stromberg|first=Daniel|title=Python Search Tree Empirical Performance Comparison|url=http://stromberg.dnsalias.org/~strombrg/python-tree-and-heap-comparison/}}

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Binary search tree}}
[[Category:Articles with example C++ code]]
[[Category:Articles with example Python code]]
[[Category:Binary trees]]
[[Category:Data types]]</text>
      <sha1>msi9v5ay31unkt07vaim1vzj43x8hxh</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Order statistic tree</title>
    <ns>0</ns>
    <id>36548922</id>
    <revision>
      <id>614904747</id>
      <parentid>614900951</parentid>
      <timestamp>2014-06-29T17:06:33Z</timestamp>
      <contributor>
        <ip>37.201.182.82</ip>
      </contributor>
      <text xml:space="preserve" bytes="2508">In [[computer science]], an '''order statistic tree''' is a variant of the [[binary search tree]] (or more generally, a [[B-tree]]&lt;ref&gt;{{cite web |url=http://www.chiark.greenend.org.uk/~sgtatham/algorithms/cbtree.html |title=Counted B-Trees |date=11 December 2004 |accessdate=18 January 2014}}&lt;/ref&gt;) that supports two additional operation beyond insertion, lookup and deletion:

* [[Selection algorithm|Select(''i'')]] — find the ''i'''th smallest element stored in the tree
* Rank(''x'') – find the rank of element ''x'' in the tree, i.e. its index in the sorted list of elements of the tree

Both operations can be performed in {{math|''O''(log ''n'')}} time in the [[average case]]; when a [[self-balancing binary search tree|self-balancing tree]] is used as the base data structure, this bound also applies in the worst case.

To turn a regular search tree into an order statistic tree, the nodes of the tree need to store one additional value, which is the size of the subtree rooted at that node (i.e., the number of nodes below it). All operations that modify the tree must adjust this information to preserve the [[Loop invariant|invariant]] that

 size[x] = size[left[x]] + size[right[x]] + 1

where &lt;code&gt;size[nil] = 0&lt;/code&gt; by definition. Select can then be implemented as&lt;ref&gt;{{Introduction to Algorithms|2}}&lt;/ref&gt;{{rp|342}}

 '''function''' Select(t, i)
     // Returns the i'th element (zero-indexed) of the elements in t
     r ← size[left[t]]
     '''if''' i = r
         return key[t]
     '''else if''' i &lt; r
         return Select(left[t], i)
     '''else'''
         return Select(right[t], i - (r + 1))


Rank can be implemented as&lt;ref&gt;{{Introduction to Algorithms|3}}&lt;/ref&gt;{{rp|342}}

 '''function''' Rank(T, x)
     // Returns the position of x (one-indexed) in the linear sorted list of elements of the tree T
     r ← size[left[x]] + 1
     y ← x
     '''while''' y ≠ T.root
          '''if''' y = right[y.p]
               r ← r + size[left[y.p]] + 1
          y ← y.p
     '''return''' r

==References==
{{reflist}}

==External links==
* [http://pine.cs.yale.edu/pinewiki/OrderStatisticsTree Order statistic tree] on PineWiki, Yale University.
* The [[Python (programming language)|Python]] package [http://stutzbachenterprises.com/blist/ blist] uses order statistic B-trees to implement [[List (abstract data type)|lists]] with fast insertion at arbitrary positions.

{{CS-Trees}}

[[Category:Binary trees]]
[[Category:Selection algorithms]]


{{Algorithm-stub}}</text>
      <sha1>cqwasmfhn6fck31ofop8ctx44c8kqsa</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>MVP tree</title>
    <ns>0</ns>
    <id>26240020</id>
    <revision>
      <id>591259171</id>
      <parentid>530474030</parentid>
      <timestamp>2014-01-18T11:48:18Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <minor/>
      <comment>Qwertyus moved page [[MVP Tree]] to [[MVP tree]]</comment>
      <text xml:space="preserve" bytes="470">An '''MVP tree''' is an abstract [[data structure]] for indexing objects from large [[metric space]]s for [[Nearest neighbor search|similarity search]] queries.&lt;ref&gt;[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.7492 CiteSeerX — Indexing Large Metric Spaces For Similarity Search Queries&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

==References==
{{Reflist}}

{{DEFAULTSORT:Mvp Tree}}
[[Category:Data]]
[[Category:Binary trees]]


{{computer-stub}}


{{CS-Trees}}</text>
      <sha1>jp47bg3p51zysqsk0mxdrxn9lh9kfw1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Splay tree</title>
    <ns>0</ns>
    <id>28382</id>
    <revision>
      <id>624823203</id>
      <parentid>622582737</parentid>
      <timestamp>2014-09-09T16:12:29Z</timestamp>
      <contributor>
        <ip>173.165.248.193</ip>
      </contributor>
      <comment>/* Implementation */</comment>
      <text xml:space="preserve" bytes="20306">{{Use dmy dates|date=July 2012}}
{{Infobox data structure
|name=Splay tree
|type=tree
|invented_by=[[Daniel Dominic Sleator]] and [[Robert Endre Tarjan]]
|invented_year=1985
|space_avg=O(n)
|space_worst=O(n)
|search_avg=O(log n)
|search_worst=amortized O(log n)
|insert_avg=O(log n)
|insert_worst=amortized O(log n)
|delete_avg=O(log n)
|delete_worst=amortized O(log n)
}}

A '''splay tree''' is a self-adjusting [[binary search tree]] with the additional property that recently accessed elements are quick to access again.  It performs basic operations such as insertion, look-up and removal in [[big O notation|O]](log n) [[amortized analysis|amortized]] time. For many sequences of non-random operations, splay trees perform better than other search trees, even when the specific pattern of the sequence is unknown.  The splay tree was invented by [[Daniel Dominic Sleator]] and [[Robert Endre Tarjan]] in 1985.&lt;ref name=&quot;SleatorTarjan&quot; /&gt;

All normal operations on a binary search tree are combined with one basic operation, called ''splaying''. Splaying the tree for a certain element rearranges the tree so that the element is placed at the root of the tree.  One way to do this is to first perform a standard binary tree search for the element in question, and then use [[tree rotation]]s in a specific fashion to bring the element to the top. Alternatively, a top-down algorithm can combine the search and the tree reorganization into a single phase.

== Advantages ==
Good performance for a splay tree depends on the fact that it is self-optimizing, in that frequently accessed nodes will move nearer to the root where they can be accessed more quickly. The worst-case height—though unlikely—is O(n), with the average being O(log ''n'').
Having frequently used nodes near the root is an advantage for nearly all practical applications (also see [[Locality of reference]]),{{Citation needed|date=January 2010}} and is particularly useful for implementing [[cache (computing)|cache]]s and [[Garbage collection (computer science)|garbage collection]] algorithms.

Advantages include:
* Simple implementation—simpler than self-balancing binary search trees, such as [[red-black tree]]s or [[AVL tree]]s.{{Citation needed|date=March 2014}}
* Comparable performance—[[average-case performance]] is as efficient as other trees.{{Citation needed|date=January 2010}}
* Small memory footprint—splay trees do not need to store any bookkeeping data.
* Possibility of creating a [[persistent data structure]] version of splay trees—which allows access to both the previous and new versions after an update. This can be useful in [[functional programming]], and requires amortized O(log ''n'') space per update.
* Working well with nodes containing identical keys—contrary to other types of self-balancing trees. Even with identical keys, performance remains amortized O(log ''n''). All tree operations preserve the order of the identical nodes within the tree, which is a property similar to [[Sorting algorithm#Classification|stable sorting algorithms]]. A carefully designed find operation can return the leftmost or rightmost node of a given key.

== Disadvantages ==
The most significant disadvantage of splay trees is that the height of a splay tree can be linear.  For example, this will be the case after accessing all ''n'' elements in non-decreasing order.  Since the height of a tree corresponds to the worst-case access time, this means that the actual cost of an operation can be high. However the [[amortized]] access cost of this worst case is logarithmic, O(log ''n'').  Also, the expected access cost can be reduced to O(log ''n'') ''by using a randomized variant.&lt;ref&gt;{{cite web|title=Randomized Splay Trees: Theoretical and Experimental Results|url=http://www2.informatik.hu-berlin.de/~albers/papers/ipl02.pdf|accessdate=31 May 2011}}&lt;/ref&gt;

A splay tree can be worse than a static tree by at most a constant factor.{{Citation needed|date=May 2014}}

The representation of splay trees can change even when they are accessed in a 'read-only' manner (i.e. by ''find'' operations).  This complicates the use of such splay trees in a multi-threaded environment.  Specifically, extra management is needed if multiple threads are allowed to perform ''find'' operations concurrently.

==Operations==

===Splaying===
When a node ''x'' is accessed, a splay operation is performed on ''x'' to move it to the root. To perform a splay operation we carry out a sequence of ''splay steps'', each of which moves ''x'' closer to the root. By performing a splay operation on the node of interest after every access, the recently accessed nodes are kept near the root and the tree remains roughly balanced, so that we achieve the desired amortized time bounds.

Each particular step depends on three factors:
* Whether ''x'' is the left or right child of its parent node, ''p'',
* whether ''p'' is the root or not, and if not
* whether ''p'' is the left or right child of ''its'' parent, ''g'' (the ''grandparent'' of x).

It is important to remember to set ''gg'' (the ''great-grandparent'' of x) to now point to x after any splay operation. If ''gg'' is null, then x obviously is now the root and must be updated as such.

There are three types of splay steps, each of which has a left- and right-handed case. For the sake of brevity, only one of these two is shown for each type. These three types are:

'''Zig Step:''' This step is done when ''p'' is the root. The tree is rotated on the edge between ''x'' and ''p''.  Zig steps exist to deal with the parity issue and will be done only as the last step in a splay operation and only when ''x'' has odd depth at the beginning of the operation.

[[File:splay tree zig.svg|center]]

'''Zig-zig Step:''' This step is done when ''p'' is not the root and ''x'' and ''p'' are either both right children or are both left children. The picture below shows the case where ''x'' and ''p'' are both left children. The tree is rotated on the edge joining ''p'' with ''its'' parent ''g'', then rotated on the edge joining ''x'' with ''p''. Note that zig-zig steps are the only thing that differentiate splay trees from the ''rotate to root'' method introduced by Allen and Munro&lt;ref name=&quot;AllenMunro&quot;&gt;{{Citation |author=Allen, Brian; and Munro, Ian |title=Self-organizing search trees |journal=Journal of the ACM |volume=25 |pages=526–535 |year=1978 |issue=4 |doi=10.1145/322092.322094 }}&lt;/ref&gt; prior to the introduction of splay trees.

[[Image:Zigzig.gif|center]]

'''Zig-zag Step:''' This step is done when ''p'' is not the root and ''x'' is a right child and ''p'' is a left child or vice versa. The tree is rotated on the edge between ''p'' and x, and then rotated on the resulting edge between ''x'' and g.

[[Image:Zigzag.gif|center]]

===Insertion===
To insert a node ''x'' into a splay tree:
#First insert the node as with a normal [[binary search tree]].
#Then splay the newly inserted node ''x'' to the top of the tree.

===Deletion===
To delete a node ''x'', we use the same method as with a binary search tree:  if ''x'' has two children, we swap its value with that of either the rightmost node of its left sub tree (its in-order predecessor) or the leftmost node of its right subtree (its in-order successor). Then we remove that node instead. In this way, deletion is reduced to the problem of removing a node with 0 or 1 children.

Unlike a binary search tree, in a splay tree after deletion, we splay the parent of the removed node to the top of the tree.
'''OR'''
The node to be deleted is first splayed, i.e. brought to the root of the tree and then deleted. This leaves the tree with two sub trees. The maximum element of the left sub tree (''': METHOD 1'''), or minimum of the right sub tree (''': METHOD 2''') is then splayed to the root. The right sub tree is made the right child of the resultant left sub tree (for '''METHOD 1'''). The root of left sub tree is the root of melded tree.

== Implementation ==
Below there is an implementation of splay trees in C++, which uses pointers to represent each node on the tree. This implementation is based on the second method of deletion on a splay tree.  Also, unlike the above definition, this C++ version does ''not'' splay the tree on finds - it only splays on insertions and deletions.

&lt;source lang=&quot;cpp&quot;&gt;
#include &lt;functional&gt;

#ifndef SPLAY_TREE
#define SPLAY_TREE

template&lt; typename T, typename Comp = std::less&lt; T &gt; &gt;
class splay_tree {
private:
  Comp comp;
  unsigned long p_size;
  
  struct node {
    node *left, *right;
    node *parent;
    T key;
    node( const T&amp; init = T( ) ) : left( 0 ), right( 0 ), parent( 0 ), key( init ) { }
  } *root;
  
  void left_rotate( node *x ) {
    node *y = x-&gt;right;
    if(y) {
      x-&gt;right = y-&gt;left;
      if( y-&gt;left ) y-&gt;left-&gt;parent = x;
      y-&gt;parent = x-&gt;parent;
    }
    
    if( !x-&gt;parent ) root = y;
    else if( x == x-&gt;parent-&gt;left ) x-&gt;parent-&gt;left = y;
    else x-&gt;parent-&gt;right = y;
    if(y) y-&gt;left = x;
    x-&gt;parent = y;
  }
  
  void right_rotate( node *x ) {
    node *y = x-&gt;left;
    if(y) {
      x-&gt;left = y-&gt;right;
      if( y-&gt;right ) y-&gt;right-&gt;parent = x;
      y-&gt;parent = x-&gt;parent;
    }
    if( !x-&gt;parent ) root = y;
    else if( x == x-&gt;parent-&gt;left ) x-&gt;parent-&gt;left = y;
    else x-&gt;parent-&gt;right = y;
    if(y) y-&gt;right = x;
    x-&gt;parent = y;
  }
  
  void splay( node *x ) {
    while( x-&gt;parent ) {
      if( !x-&gt;parent-&gt;parent ) {
        if( x-&gt;parent-&gt;left == x ) right_rotate( x-&gt;parent );
        else left_rotate( x-&gt;parent );
      } else if( x-&gt;parent-&gt;left == x &amp;&amp; x-&gt;parent-&gt;parent-&gt;left == x-&gt;parent ) {
        right_rotate( x-&gt;parent-&gt;parent );
        right_rotate( x-&gt;parent );
      } else if( x-&gt;parent-&gt;right == x &amp;&amp; x-&gt;parent-&gt;parent-&gt;right == x-&gt;parent ) {
        left_rotate( x-&gt;parent-&gt;parent );
        left_rotate( x-&gt;parent );
      } else if( x-&gt;parent-&gt;left == x &amp;&amp; x-&gt;parent-&gt;parent-&gt;right == x-&gt;parent ) {
        right_rotate( x-&gt;parent );
        left_rotate( x-&gt;parent );
      } else {
        left_rotate( x-&gt;parent );
        right_rotate( x-&gt;parent );
      }
    }
  }
  
  void replace( node *u, node *v ) {
    if( !u-&gt;parent ) root = v;
    else if( u == u-&gt;parent-&gt;left ) u-&gt;parent-&gt;left = v;
    else u-&gt;parent-&gt;right = v;
    if( v ) v-&gt;parent = u-&gt;parent;
  }
  
  node* subtree_minimum( node *u ) {
    while( u-&gt;left ) u = u-&gt;left;
    return u;
  }
  
  node* subtree_maximum( node *u ) {
    while( u-&gt;right ) u = u-&gt;right;
    return u;
  }
public:
  splay_tree( ) : root( 0 ), p_size( 0 ) { }
  
  void insert( const T &amp;key ) {
    node *z = root;
    node *p = 0;
    
    while( z ) {
      p = z;
      if( comp( z-&gt;key, key ) ) z = z-&gt;right;
      else z = z-&gt;left;
    }
    
    z = new node( key );
    z-&gt;parent = p;
    
    if( !p ) root = z;
    else if( comp( p-&gt;key, z-&gt;key ) ) p-&gt;right = z;
    else p-&gt;left = z;
    
    splay( z );
    p_size++;
  }
  
  node* find( const T &amp;key ) {
    node *z = root;
    while( z ) {
      if( comp( z-&gt;key, key ) ) z = z-&gt;right;
      else if( comp( key, z-&gt;key ) ) z = z-&gt;left;
      else return z;
    }
    return 0;
  }
        
  void erase( const T &amp;key ) {
    node *z = find( key );
    if( !z ) return;
    
    splay( z );
    
    if( !z-&gt;left ) replace( z, z-&gt;right );
    else if( !z-&gt;right ) replace( z, z-&gt;left );
    else {
      node *y = subtree_minimum( z-&gt;right );
      if( y-&gt;parent != z ) {
        replace( y, y-&gt;right );
        y-&gt;right = z-&gt;right;
        y-&gt;right-&gt;parent = y;
      }
      replace( z, y );
      y-&gt;left = z-&gt;left;
      y-&gt;left-&gt;parent = y;
    }
    
    delete z;
    p_size--;
  }
  
  const T&amp; minimum( ) { return subtree_minimum( root )-&gt;key; }
  const T&amp; maximum( ) { return subtree_maximum( root )-&gt;key; }
  
  bool empty( ) const { return root == 0; }
  unsigned long size( ) const { return p_size; }
};

#endif // SPLAY_TREE
&lt;/source&gt;

== Analysis ==
A simple [[amortized analysis]] of static splay trees can be carried out using the [[potential method]]. Suppose that size(''r'') is the number of nodes in the subtree rooted at ''r'' (including ''r'') and rank(''r'') = log&lt;sub&gt;2&lt;/sub&gt;(size(''r'')). Then the potential function P(''t'') for a splay tree ''t'' is the sum of the ranks of all the nodes in the tree. This will tend to be high for poorly balanced trees, and low for well-balanced trees. We can bound the amortized cost of any zig-zig or zig-zag operation by:

:amortized cost = cost + P(t&lt;sub&gt;f&lt;/sub&gt;) - P(t&lt;sub&gt;i&lt;/sub&gt;) &amp;le; 3(rank&lt;sub&gt;f&lt;/sub&gt;(''x'') - rank&lt;sub&gt;i&lt;/sub&gt;(''x'')),

where ''x'' is the node being moved towards the root, and the subscripts &quot;f&quot; and &quot;i&quot; indicate after and before the operation, respectively. When summed over the entire splay operation, this [[telescoping series|telescopes]] to 3(rank(root)) which is O(log ''n''). Since there's at most one zig operation, this only adds a constant.

== Performance theorems ==
There are several theorems and conjectures regarding the worst-case runtime for performing a sequence ''S'' of ''m'' accesses in a splay tree containing ''n'' elements.

;Balance Theorem:&lt;ref name=&quot;SleatorTarjan&quot;&gt;{{Citation |first1=Daniel D. |last1=Sleator |first2=Robert E. |last2=Tarjan |title=Self-Adjusting Binary Search Trees |url=http://www.cs.cmu.edu/~sleator/papers/self-adjusting.pdf |journal=Journal of the ACM ([[Association for Computing Machinery]]) |volume=32 |issue=3 |pages=652–686 |year= 1985 |doi=10.1145/3828.3835 }}&lt;/ref&gt;  The cost of performing the sequence ''S'' is &lt;math&gt;O\left[m(1 + \log n) + n\log n\right]&lt;/math&gt;.  In other words, splay trees perform as well as static balanced binary search trees on sequences of at least ''n'' accesses.
;Static Optimality Theorem:&lt;ref name=&quot;SleatorTarjan&quot; /&gt;  Let &lt;math&gt;q_i&lt;/math&gt; be the number of times element ''i'' is accessed in ''S''.  The cost of performing ''S'' is &lt;math&gt;O\left[m + \sum_{i=1}^n q_i\log\frac{m}{q_i}\right]&lt;/math&gt;.  In other words, splay trees perform as well as optimum static binary search trees on sequences of at least ''n'' accesses.
;Static Finger Theorem:&lt;ref name=&quot;SleatorTarjan&quot; /&gt;  Let &lt;math&gt;i_j&lt;/math&gt; be the element accessed in the &lt;math&gt;j^{th}&lt;/math&gt; access of ''S'' and let ''f'' be any fixed element (the finger).  The cost of performing ''S'' is &lt;math&gt;O\left[m + n\log n + \sum_{j=1}^m \log(|i_j-f| + 1)\right]&lt;/math&gt;.
;Working Set Theorem:&lt;ref name=&quot;SleatorTarjan&quot; /&gt;  Let &lt;math&gt;t(j)&lt;/math&gt; be the number of distinct elements accessed between access ''j'' and the previous time element &lt;math&gt;i_j&lt;/math&gt; was accessed.  The cost of performing ''S'' is &lt;math&gt;O\left[m + n\log n + \sum_{j=1}^m \log(t(j) + 1)\right]&lt;/math&gt;. This is equivalent to splay trees having [[key-independent optimality]].
;Dynamic Finger Theorem:&lt;ref name=&quot;ColeEtAl&quot;&gt;{{Citation |author=Cole, Richard |coauthors=Mishra, Bud; Schmidt, Jeanette; and Siegel, Alan |title=On the Dynamic Finger Conjecture for Splay Trees. Part I: Splay Sorting log n-Block Sequences |journal=[[SIAM Journal on Computing]] |volume=30 |pages=1–43 |year=2000 |doi=10.1137/s0097539797326988}}&lt;/ref&gt;&lt;ref name=&quot;Cole&quot;&gt;{{Citation |author=Cole, Richard |title=On the Dynamic Finger Conjecture for Splay Trees. Part II: The Proof |journal=SIAM Journal on Computing |volume=30 |pages=44–85 |year=2000 |doi=10.1137/S009753979732699X }}&lt;/ref&gt; The cost of performing ''S'' is &lt;math&gt;O\left[m + n + \sum_{j=1}^m \log(|i_{j + 1} - i_j| + 1)\right]&lt;/math&gt;.
;Scanning Theorem:&lt;ref name=&quot;Tarjan&quot;&gt;{{Citation |author=Tarjan, Robert E. |title=Sequential access in splay trees takes linear time |journal=Combinatorica |volume=5 |pages=367–378 |year=1985 |doi=10.1007/BF02579253 |issue=4 }}&lt;/ref&gt;  Also known as the '''Sequential Access Theorem'''.  Accessing the ''n'' elements of a splay tree in symmetric order takes O(''n'') time, regardless of the initial structure of the splay tree. The tightest upper bound proven so far is &lt;math&gt;4.5n&lt;/math&gt;.&lt;ref name=&quot;Elmasry&quot;&gt;{{Citation |author=Elmasry, Amr |title=On the sequential access theorem and Deque conjecture for splay trees |journal=Theoretical Computer Science |volume=314 |pages=459–466 |year=2004 |issue=3 |doi=10.1016/j.tcs.2004.01.019 }}&lt;/ref&gt;

== Dynamic optimality conjecture ==
{{Main|Optimal binary search tree}}
{{unsolved|computer science|Do splay trees perform as well as any other binary search tree algorithm?}}
In addition to the proven performance guarantees for splay trees there is an unproven conjecture of great interest from the original Sleator and Tarjan paper.  This conjecture is known as the ''dynamic optimality conjecture'' and it basically claims that splay trees perform as well as any other binary search tree algorithm up to a constant factor.

:'''Dynamic Optimality Conjecture:&lt;ref name=&quot;SleatorTarjan&quot; /&gt;''' Let &lt;math&gt;A&lt;/math&gt; be any binary search tree algorithm that accesses an element &lt;math&gt;x&lt;/math&gt; by traversing the path from the root to &lt;math&gt;x&lt;/math&gt; at a cost of &lt;math&gt;d(x)+1&lt;/math&gt;, and that between accesses can make any rotations in the tree at a cost of 1 per rotation.  Let &lt;math&gt;A(S)&lt;/math&gt; be the cost for &lt;math&gt;A&lt;/math&gt; to perform the sequence &lt;math&gt;S&lt;/math&gt; of accesses.  Then the cost for a splay tree to perform the same accesses is &lt;math&gt;O[n + A(S)]&lt;/math&gt;.

There are several corollaries of the dynamic optimality conjecture that remain unproven:

:'''Traversal Conjecture:&lt;ref name=&quot;SleatorTarjan&quot; /&gt;''' Let &lt;math&gt;T_1&lt;/math&gt; and &lt;math&gt;T_2&lt;/math&gt; be two splay trees containing the same elements.  Let &lt;math&gt;S&lt;/math&gt; be the sequence obtained by visiting the elements in &lt;math&gt;T_2&lt;/math&gt; in preorder (i.e., depth first search order).  The total cost of performing the sequence &lt;math&gt;S&lt;/math&gt; of accesses on &lt;math&gt;T_1&lt;/math&gt; is &lt;math&gt;O(n)&lt;/math&gt;.

:'''Deque Conjecture:&lt;ref name=&quot;Tarjan&quot; /&gt;&lt;ref name=&quot;Pettie&quot;&gt;{{Citation |author=Pettie, Seth |title=Splay Trees, Davenport-Schinzel Sequences, and the Deque Conjecture |journal=Proceedings of the 19th ACM-SIAM Symposium on Discrete Algorithms |pages=1115–1124 |year=2008 |bibcode=2007arXiv0707.2160P |volume=0707 |arxiv=0707.2160 }}&lt;/ref&gt;&lt;ref name=&quot;Sundar&quot;&gt;{{Citation |author=Sundar, Rajamani |title=On the Deque conjecture for the splay algorithm |journal=Combinatorica |volume=12 |pages=95–124 |year=1992 |issue=1 |doi=10.1007/BF01191208 }}&lt;/ref&gt;''' Let &lt;math&gt;S&lt;/math&gt; be a sequence of &lt;math&gt;m&lt;/math&gt; [[double-ended queue]] operations (push, pop, inject, eject).  Then the cost of performing &lt;math&gt;S&lt;/math&gt; on a splay tree is &lt;math&gt;O(m + n)&lt;/math&gt;.

:'''Split Conjecture:&lt;ref name=&quot;Lucas&quot;&gt;{{Citation |author=Lucas, Joan M. |title=On the Competitiveness of Splay Trees: Relations to the Union-Find Problem |journal=Online Algorithms, Center for Discrete Mathematics and Theoretical Computer Science ([[DIMACS]]) Series in Discrete Mathematics and Theoretical Computer Science Vol. 7 |pages=95–124 |year=1991 }}&lt;/ref&gt;''' Let &lt;math&gt;S&lt;/math&gt; be any permutation of the elements of the splay tree.  Then the cost of deleting the elements in the order &lt;math&gt;S&lt;/math&gt; is &lt;math&gt;O(n)&lt;/math&gt;.

==See also==
* [[Finger tree]]
* [[Link/cut tree]]
* [[Scapegoat tree]]
* [[Zipper (data structure)]]
* [[tree data structure|Trees]]
* [[Tree rotation]]
* [[AVL tree]]
* [[B-tree]]
* [[T-tree]]
* [[List of data structures]]
* [[Iacono's working set structure]]
* [[Geometry of binary search trees]]
* [[Splaysort]], a sorting algorithm using splay trees

==Notes==
{{reflist}}

==References==
* [[Donald Knuth|Knuth, Donald]]. ''[[The Art of Computer Programming]]'', Volume 3: ''Sorting and Searching'', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Page 478 of section 6.2.3.

== External links ==
* [http://www.nist.gov/dads/HTML/splaytree.html NIST's Dictionary of Algorithms and Data Structures: Splay Tree]
* [http://www.link.cs.cmu.edu/link/ftp-site/splaying/ Implementations in C and Java (by Daniel Sleator)]
* [http://wiki.algoviz.org/search/node/splay Pointers to splay tree visualizations] 
* [http://github.com/fbuihuu/libtree Fast and efficient implentation of Splay trees]
* [http://github.com/cpdomina/SplayTree Top-Down Splay Tree Java implementation]
* [http://arxiv.org/abs/1003.0139 Zipper Trees]
* [http://www.youtube.com/watch?v=G5QIXywcJlY splay tree video]

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Splay Tree}}
[[Category:Binary trees]]</text>
      <sha1>c4i013gc8af3f6n9xd3cg7itovnofmu</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Red–black tree</title>
    <ns>0</ns>
    <id>26397</id>
    <revision>
      <id>625656960</id>
      <parentid>625654954</parentid>
      <timestamp>2014-09-15T13:15:33Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{Cn}}</comment>
      <text xml:space="preserve" bytes="44184">{{refimprove|date=October 2012}}
{{Infobox data structure
|name=Red–black tree
|type=tree
|invented_by=[[Rudolf Bayer]]
|invented_year=1972
|space_avg=O(n)
|space_worst=O(n)
|search_avg=O(log n)
|search_worst=O(log n)
|insert_avg=O(log n)
|insert_worst=O(log n)
|delete_avg=O(log n)
|delete_worst=O(log n)
}}

A '''red–black tree''' is a [[data structure]] which is a type of [[self-balancing binary search tree]].

Balance is preserved by painting each node of the tree with one of two colors (typically called 'red' and 'black') in a way that satisfies certain properties, which collectively constrain how unbalanced the tree can become in the worst case. When the tree is modified, the new tree is subsequently rearranged and repainted to restore the coloring properties. The properties are designed in such a way that this rearranging and recoloring can be performed efficiently.

The balancing of the tree is not perfect but it is good enough to allow it to guarantee searching in [[Big-O notation|O(log ''n'')]] time, where ''n'' is the total number of elements in the tree. The insertion and deletion operations, along with the tree rearrangement and recoloring, are also performed in [[Big-O notation|O(log ''n'')]] time.&lt;ref&gt;{{cite web |url=http://www.cs.auckland.ac.nz/~jmor159/PLDS210/red_black.html |title=Red–Black Trees |author=John Morris}}&lt;/ref&gt;

Tracking the color of each node requires only 1 bit of information per node because there are only two colors. The tree does not contain any other data specific to its being a red–black tree so its memory footprint is almost identical to a classic (uncolored) binary search tree. In many cases the additional bit of information can be stored at no additional memory cost.

== History ==
The original data structure was invented in 1972 by [[Rudolf Bayer]]&lt;ref name=&quot;Bayer72&quot;&gt;{{cite journal
| author=Rudolf Bayer
| title=Symmetric binary B-Trees: Data structure and maintenance algorithms
| journal=Acta Informatica
| volume=1
| issue=4
| year=1972
| pages=290–306
| url=http://www.springerlink.com/content/qh51m2014673513j/
| doi=10.1007/BF00289509
}}&lt;/ref&gt; and named &quot;symmetric binary [[B-tree]],&quot; but acquired its modern name in a paper in 1978 by [[Leonidas J. Guibas]] and [[Robert Sedgewick (computer scientist)|Robert Sedgewick]] entitled &quot;A Dichromatic Framework for Balanced Trees&quot;.&lt;ref name=&quot;GS78&quot;&gt;{{cite conference
 | author=Leonidas J. Guibas and Robert Sedgewick
 | title=A Dichromatic Framework for Balanced Trees
 | booktitle=Proceedings of the 19th Annual Symposium on Foundations of Computer Science
 | year=1978
 | pages=8–21
 | url=http://doi.ieeecomputersociety.org/10.1109/SFCS.1978.3
 | doi=10.1109/SFCS.1978.3}}&lt;/ref&gt; The color &quot;red&quot; was chosen because it was the best-looking color produced by the color laser printer available to the authors while working at [[Xerox PARC]].&lt;ref name=&quot;Sedgewick12&quot;&gt;
{{cite AV media
 | people = Robert Sedgewick
 | year = 2012
 | title = Red-Black BSTs
 | url=https://class.coursera.org/algs4partI-002/lecture/50
 | publisher = Coursera
 | quote = A lot of people ask why did we use the name red–black. Well, we invented this data structure, this way of looking at balanced trees, at Xerox PARC which was the home of the personal computer and many other innovations that we live with today entering[sic] graphic user interfaces, ethernet and object-oriented programmings[sic] and many other things. But one of the things that was invented there was laser printing and we were very excited to have nearby color laser printer that could print things out in color and out of the colors the red looked the best. So, that’s why we picked the color red to distinguish red links, the types of links, in three nodes. So, that’s an answer to the question for people that have been asking.
}}
&lt;/ref&gt;

== Terminology ==
A red–black tree is a special type of [[binary tree]], used in [[computer science]] to organize pieces of comparable [[data]], such as text fragments or numbers.

The [[leaf node]]s of red–black trees do not contain data.  These leaves need not be explicit in computer memory—a null child pointer can encode the fact that this child is a leaf—but it simplifies some algorithms for operating on red–black trees if the leaves really are explicit nodes.  To save memory, sometimes a single [[sentinel node]] performs the role of all leaf nodes; all references from [[Tree (data_structure)#Internal_nodes|internal nodes]] to leaf nodes then point to the sentinel node.

Red–black trees, like all [[binary search tree]]s, allow efficient [[in-order traversal]] (that is: in the order Left–Root–Right) of their elements.  The search-time results from the traversal from root to leaf, and therefore a balanced tree of ''n'' nodes, having the least possible tree height, results in O(log ''n'') search time.

== Properties ==
[[File:Red-black tree example.svg|thumb|right|500px|alt=Diagram of binary tree. The black root node has two red children and four black grandchildren. The child nodes of the grandchildren are black nil pointers or red nodes with black nil pointers.|An example of a red–black tree]]

In addition to the requirements imposed on a [[binary search tree]] the following must be satisfied by a red–black tree:&lt;ref&gt;{{cite book |last1= Cormen |first1=Thomas |authorlink1=Thomas H. Cormen |last2=Leiserson |first2=Charles |authorlink2=Charles E. Leiserson |first3=Ronald |last3=Rivest |first4=Clifford |last4=Stein |authorlink4=Clifford Stein |authorlink3=Ron Rivest |title=[[Introduction to Algorithms]]|edition=3rd|year=2009|publisher=[[MIT Press]]|isbn=978-0-262-03384-8|pages=308–309 |chapter=13}}&lt;/ref&gt;

# A node is either red or black.
# The root is black. (This rule is sometimes omitted. Since the root can always be changed from red to black, but not necessarily vice-versa, this rule has little effect on analysis.)
# All leaves (NIL) are black. (All leaves are same color as the root.)
# Every red node must have two black child nodes.
# Every [[path (graph theory)|path]] from a given node to any of its descendant leaves contains the same number of black nodes.

These constraints enforce a critical property of red–black trees: that the path from the root to the furthest leaf is no more than twice as long as the path from the root to the nearest leaf. The result is that the tree is roughly height-balanced. Since operations such as inserting, deleting, and finding values require worst-case time proportional to the height of the tree, this theoretical upper bound on the height allows red–black trees to be efficient in the worst case, unlike ordinary [[binary search tree]]s. 

To see why this is guaranteed, it suffices to consider the effect of properties 4 and 5 together. For a red–black tree T, let B be the number of black nodes in property 5. Let the shortest possible path from the root of T to any leaf consist of B black nodes. Longer possible paths may be constructed by inserting red nodes. However, property 4 makes it impossible to insert more than one consecutive red node. Therefore the longest possible path consists of 2B nodes, alternating black and red.

The shortest possible path has all black nodes, and the longest possible path alternates between red and black nodes. Since all maximal paths have the same number of black nodes, by property 5, this shows that no path is more than twice as long as any other path.

== Analogy to B-trees of order 4 ==
[[Image:Red-black tree example (B-tree analogy).svg|500px|thumb|right|The same red–black tree as in the example above, seen as a B-tree.]]

A red–black tree is similar in structure to a [[B-tree]] of order&lt;ref group=&quot;note&quot;&gt;Using Knuth's definition of order: the maximum number of children&lt;/ref&gt; 4, where each node can contain between 1 and 3 values and (accordingly) between 2 and 4 child pointers. In such a B-tree, each node will contain only one value matching the value in a black node of the red–black tree, with an optional value before and/or after it in the same node, both matching an equivalent red node of the red–black tree.

One way to see this equivalence is to &quot;move up&quot; the red nodes in a graphical representation of the red–black tree, so that they align horizontally with their parent black node, by creating together a horizontal cluster. In the B-tree, or in the modified graphical representation of the red–black tree, all leaf nodes are at the same depth.

The red–black tree is then structurally equivalent to a B-tree of order 4, with a minimum fill factor of 33% of values per cluster with a maximum capacity of 3 values.

This B-tree type is still more general than a red–black tree though, as it allows ambiguity in a red–black tree conversion—multiple red–black trees can be produced from an equivalent B-tree of order 4. If a B-tree cluster contains only 1 value, it is the minimum, black, and has two child pointers. If a cluster contains 3 values, then the central value will be black and each value stored on its sides will be red. If the cluster contains two values, however, either one can become the black node in the red–black tree (and the other one will be red).

So the order-4 B-tree does not maintain which of the values contained in each cluster is the root black tree for the whole cluster and the parent of the other values in the same cluster. Despite this, the operations on red–black trees are more economical in time because you don't have to maintain the vector of values.{{cn|date=September 2014}} It may be costly if values are stored directly in each node rather than being stored by reference. B-tree nodes, however, are more economical in space because you don't need to store the color attribute for each node. Instead, you have to know which slot in the cluster vector is used. If values are stored by reference, e.g. objects, null references can be used and so the cluster can be represented by a vector containing 3 slots for value pointers plus 4 slots for child references in the tree. In that case, the B-tree can be more compact in memory, improving data locality.

The same analogy can be made with B-trees with larger orders that can be structurally equivalent to a colored binary tree: you just need more colors. Suppose that you add blue, then the blue–red–black tree defined like red–black trees but with the additional constraint that no two successive nodes in the hierarchy will be blue and all blue nodes will be children of a red node, then it becomes equivalent to a B-tree whose clusters will have at most 7 values in the following colors: blue, red, blue, black, blue, red, blue (For each cluster, there will be at most 1 black node, 2 red nodes, and 4 blue nodes).

For moderate volumes of values, insertions and deletions in a colored binary tree are faster compared to B-trees because colored trees don't attempt to maximize the fill factor of each horizontal cluster of nodes (only the minimum fill factor is guaranteed in colored binary trees, limiting the number of splits or junctions of clusters). B-trees will be faster for performing rotations (because rotations will frequently occur within the same cluster rather than with multiple separate nodes in a colored binary tree). However for storing large volumes, B-trees will be much faster as they will be more compact by grouping several children in the same cluster where they can be accessed locally.

All optimizations possible in B-trees to increase the average fill factors of clusters are possible in the equivalent multicolored binary tree. Notably, maximizing the average fill factor in a structurally equivalent B-tree is the same as reducing the total height of the multicolored tree, by increasing the number of non-black nodes. The worst case occurs when all nodes in a colored binary tree are black, the best case occurs when only a third of them are black (and the other two thirds are red nodes).
{{fake heading|sub=3|Notes}}
{{reflist|group=&quot;note&quot;|close}}

== Applications and related data structures ==
Red–black trees offer worst-case guarantees for insertion time, deletion time, and search time. Not only does this make them valuable in time-sensitive applications such as [[real-time computing|real-time application]]s, but it makes them valuable building blocks in other data structures which provide worst-case guarantees; for example, many data structures used in [[computational geometry]] can be based on red–black trees, and the [[Completely Fair Scheduler]] used in current [[Linux]] kernels uses red–black trees.

The [[AVL tree]] is another structure supporting O(log ''n'') search, insertion, and removal. It is more rigidly balanced than red–black trees, leading to slower insertion and removal but faster retrieval. This makes it attractive for data structures that may be built once and loaded without reconstruction, such as language dictionaries (or program dictionaries, such as the opcodes of an assembler or interpreter).

Red–black trees are also particularly valuable in [[functional programming]], where they are one of the most common [[persistent data structure]]s, used to construct [[associative array]]s and [[set (computer science)|set]]s which can retain previous versions after mutations. The persistent version of red–black trees requires O(log ''n'') space for each insertion or deletion, in addition to time.

For every [[2-4 tree]], there are corresponding red–black trees with data elements in the same order. The insertion and deletion operations on 2-4 trees are also equivalent to color-flipping and rotations in red–black trees. This makes 2-4 trees an important tool for understanding the logic behind red–black trees, and this is why many introductory algorithm texts introduce 2-4 trees just before red–black trees, even though 2-4 trees are not often used in practice.

In 2008, [[Robert Sedgewick (computer scientist)|Sedgewick]] introduced a simpler version of the red–black tree called the [[left-leaning red–black tree]]&lt;ref name=&quot;cs.princeton.edu&quot;&gt;http://www.cs.princeton.edu/~rs/talks/LLRB/RedBlack.pdf&lt;/ref&gt; by eliminating a previously unspecified degree of freedom in the implementation. The LLRB maintains an additional invariant that all red links must lean left except during inserts and deletes. Red–black trees can be made isometric to either [[2-3 tree]]s,&lt;ref&gt;http://www.cs.princeton.edu/courses/archive/fall08/cos226/lectures/10BalancedTrees-2x2.pdf&lt;/ref&gt; or 2-4 trees,&lt;ref name=&quot;cs.princeton.edu&quot;/&gt; for any sequence of operations. The 2-4 tree isometry was described in 1978 by Sedgewick.{{citequote|date=January 2013}} With 2-4 trees, the isometry is resolved by a &quot;color flip,&quot; corresponding to a split, in which the red color of two children nodes leaves the children and moves to the parent node. The [[tango tree]], a type of tree optimized for fast searches, usually{{when|date=January 2013}} uses red–black trees as part of its data structure.

== Operations ==
Read-only operations on a red–black tree require no modification from those used for [[binary search tree]]s, because every red–black tree is a special case of a simple binary search tree. However, the immediate result of an insertion or removal may violate the properties of a red–black tree. Restoring the red–black properties requires a small number ([[Big-O notation|O]](log ''n'') or [[Amortized analysis|amortized O(1)]]) of color changes (which are very quick in practice) and no more than three [[tree rotation]]s (two for insertion). Although insert and delete operations are complicated, their times remain O(log ''n'').

=== Insertion ===
Insertion begins by adding the node as any [[binary search tree#Insertion|binary search tree insertion]] does and by coloring it red. Whereas in the binary search tree, we always add a leaf, in the red–black tree leaves contain no information, so instead we add a red interior node, with two black leaves, in place of an existing black leaf.

What happens next depends on the color of other nearby nodes. The term ''uncle node'' will be used to refer to the sibling of a node's parent, as in human family trees. Note that:

* property 3 (all leaves are black) always holds.
* property 4 (both children of every red node are black) is threatened only by adding a red node, repainting a black node red, or a rotation.
* property 5 (all paths from any given node to its leaf nodes contain the same number of black nodes) is threatened only by adding a black node, repainting a red node black (or vice versa), or a rotation.

: ''Note'': The label '''N''' will be used to denote the current node (colored red). At the beginning, this is the new node being inserted, but the entire procedure may also be applied recursively to other nodes (see case 3). '''P''' will denote '''N''''s parent node, '''G''' will denote '''N''''s grandparent, and '''U''' will denote '''N''''s uncle. Note that in between some cases, the roles and labels of the nodes are exchanged, but in each case, every label continues to represent the same node it represented at the beginning of the case. Any color shown in the diagram is either assumed in its case or implied by those assumptions. A numbered triangle represents a subtree of unspecified depth. A black circle atop the triangle designates a black root node, otherwise the root node's color is unspecified.
Each case will be demonstrated with example [[C (programming language)|C]] code. The uncle and grandparent nodes can be found by these functions:

&lt;source lang=&quot;c&quot;&gt;
struct node *grandparent(struct node *n)
{
 if ((n != NULL) &amp;&amp; (n-&gt;parent != NULL))
  return n-&gt;parent-&gt;parent;
 else
  return NULL;
}

struct node *uncle(struct node *n)
{
 struct node *g = grandparent(n);
 if (g == NULL)
  return NULL; // No grandparent means no uncle
 if (n-&gt;parent == g-&gt;left)
  return g-&gt;right;
 else
  return g-&gt;left;
}
&lt;/source&gt;

'''Case 1:''' The current node '''N''' is at the root of the tree. In this case, it is repainted black to satisfy property 2 (the root is black). Since this adds one black node to every path at once, property 5 (all paths from any given node to its leaf nodes contain the same number of black nodes) is not violated.

&lt;source lang=&quot;c&quot;&gt;
void insert_case1(struct node *n)
{
 if (n-&gt;parent == NULL)
  n-&gt;color = BLACK;
 else
  insert_case2(n);
}
&lt;/source&gt;

'''Case 2:''' The current node's parent '''P''' is black, so property 4 (both children of every red node are black) is not invalidated. In this case, the tree is still valid. Property 5 (all paths from any given node to its leaf nodes contain the same number of black nodes) is not threatened, because the current node '''N''' has two black leaf children, but because '''N''' is red, the paths through each of its children have the same number of black nodes as the path through the leaf it replaced, which was black, and so this property remains satisfied.

&lt;source lang=&quot;c&quot;&gt;
void insert_case2(struct node *n)
{
 if (n-&gt;parent-&gt;color == BLACK)
  return; /* Tree is still valid */
 else
  insert_case3(n);
}
&lt;/source&gt;

: ''Note:'' In the following cases it can be assumed that '''N''' has a grandparent node '''G''', because its parent '''P''' is red, and if it were the root, it would be black. Thus, '''N''' also has an uncle node '''U''', although it may be a leaf in cases 4 and 5.

{|
|[[File:Red-black tree insert case 3.svg|right|400px|Diagram of case 3]]
'''Case 3:''' If both the parent '''P''' and the uncle '''U''' are red, then both of them can be repainted black and the grandparent '''G''' becomes red (to maintain property 5 (all paths from any given node to its leaf nodes contain the same number of black nodes)). Now, the current red node '''N''' has a black parent. Since any path through the parent or uncle must pass through the grandparent, the number of black nodes on these paths has not changed. However, the grandparent '''G''' may now violate properties 2 (The root is black) or 4 (Both children of every red node are black) (property 4 possibly being violated since '''G''' may have a red parent). To fix this, the entire procedure is recursively performed on '''G''' from case 1. Note that this is a tail-recursive call, so it could be rewritten as a loop; since this is the only loop, and any rotations occur after this loop, this proves that a constant number of rotations occur.
|}
&lt;source lang=&quot;c&quot;&gt;
void insert_case3(struct node *n)
{
 struct node *u = uncle(n), *g;

 if ((u != NULL) &amp;&amp; (u-&gt;color == RED)) {
  n-&gt;parent-&gt;color = BLACK;
  u-&gt;color = BLACK;
  g = grandparent(n);
  g-&gt;color = RED;
  insert_case1(g);
 } else {
  insert_case4(n);
 }
}
&lt;/source&gt;
: ''Note:'' In the remaining cases, it is assumed that the parent node '''P''' is the left child of its parent. If it is the right child, ''left'' and ''right'' should be reversed throughout cases 4 and 5. The code samples take care of this.

{|
|[[File:Red-black tree insert case 4.svg|right|400px|Diagram of case 4]]
'''Case 4:''' The parent '''P''' is red but the uncle '''U''' is black; also, the current node '''N''' is the right child of '''P''', and '''P''' in turn is the left child of its parent '''G'''. In this case, a [[tree rotation|left rotation]] on '''P''' that switches the roles of the current node '''N''' and its parent '''P''' can be performed; then, the former parent node '''P''' is dealt with using case 5 (relabeling '''N''' and '''P''') because property 4 (both children of every red node are black) is still violated. The rotation causes some paths (those in the sub-tree labelled &quot;1&quot;) to pass through the node '''N''' where they did not before. It also causes some paths (those in the sub-tree labelled &quot;3&quot;) not to pass through the node '''P''' where they did before. However, both of these nodes are red, so property 5 (all paths from any given node to its leaf nodes contain the same number of black nodes) is not violated by the rotation.  After this case has been completed, property 4 (both children of every red node are black) is still violated, but now we can resolve this by continuing to case 5.
|}
&lt;source lang=&quot;c&quot;&gt;
void insert_case4(struct node *n)
{
 struct node *g = grandparent(n);

 if ((n == n-&gt;parent-&gt;right) &amp;&amp; (n-&gt;parent == g-&gt;left)) {
  rotate_left(n-&gt;parent);

 /*
 * rotate_left can be the below because of already having *g =  grandparent(n) 
 *
 * struct node *saved_p=g-&gt;left, *saved_left_n=n-&gt;left;
 * g-&gt;left=n; 
 * n-&gt;left=saved_p;
 * saved_p-&gt;right=saved_left_n;
 * 
 * and modify the parent's nodes properly
 */

  n = n-&gt;left; 

 } else if ((n == n-&gt;parent-&gt;left) &amp;&amp; (n-&gt;parent == g-&gt;right)) {
  rotate_right(n-&gt;parent);

 /*
 * rotate_right can be the below to take advantage of already having *g =  grandparent(n) 
 *
 * struct node *saved_p=g-&gt;right, *saved_right_n=n-&gt;right;
 * g-&gt;right=n; 
 * n-&gt;right=saved_p;
 * saved_p-&gt;left=saved_right_n;
 * 
 */

  n = n-&gt;right; 
 }
 insert_case5(n);
}
&lt;/source&gt;
{|
|[[File:Red-black tree insert case 5.svg|right|400px|Diagram of case 5]]
'''Case 5:''' The parent '''P''' is red but the uncle '''U''' is black, the current node '''N''' is the left child of '''P''', and '''P''' is the left child of its parent '''G'''. In this case, a [[tree rotation|right rotation]] on '''G''' is performed; the result is a tree where the former parent '''P''' is now the parent of both the current node '''N''' and the former grandparent '''G'''. '''G''' is known to be black, since its former child '''P''' could not have been red otherwise (without violating property 4). Then, the colors of '''P''' and '''G''' are switched, and the resulting tree satisfies property 4 (both children of every red node are black). Property 5 (all paths from any given node to its leaf nodes contain the same number of black nodes) also remains satisfied, since all paths that went through any of these three nodes went through '''G''' before, and now they all go through '''P'''. In each case, this is the only black node of the three.
|}
&lt;source lang=&quot;c&quot;&gt;

void insert_case5(struct node *n)
{
 struct node *g = grandparent(n);

 n-&gt;parent-&gt;color = BLACK;
 g-&gt;color = RED;
 if (n == n-&gt;parent-&gt;left)
  rotate_right(g);
 else
  rotate_left(g);
}

&lt;/source&gt;
Note that inserting is actually [[in-place algorithm|in-place]], since all the calls above use [[tail recursion]].

=== Removal ===
In a regular binary search tree when deleting a node with two non-leaf children, we find either the maximum element in its left subtree (which is the in-order predecessor) or the minimum element in its right subtree (which is the in-order successor) and move its value into the node being deleted (as shown [[Binary search tree#Deletion|here]]). We then delete the node we copied the value from, which must have fewer than two non-leaf children. (Non-leaf children, rather than all children, are specified here because unlike normal binary search trees, red–black trees can have leaf nodes anywhere, so that all nodes are either internal nodes with two children or leaf nodes with, by definition, zero children.  In effect, internal nodes having two leaf children in a red–black tree are like the leaf nodes in a regular binary search tree.)  Because merely copying a value does not violate any red–black properties, this reduces to the problem of deleting a node with at most one non-leaf child.  Once we have solved that problem, the solution applies equally to the case where the node we originally want to delete has at most one non-leaf child as to the case just considered where it has two non-leaf children.

Therefore, for the remainder of this discussion we address the deletion of a node with at most one non-leaf child.  We use the label '''M''' to denote the node to be deleted; '''C''' will denote a selected child of '''M''', which we will also call &quot;its child&quot;.  If '''M''' does have a non-leaf child, call that its child, '''C'''; otherwise, choose either leaf as its child, '''C'''.

If '''M''' is a red node, we simply replace it with its child '''C''', which must be black by property 4. (This can only occur when  '''M''' has two leaf children, because if the red node '''M''' had a black non-leaf child on one side but just a leaf child on the other side, then the count of black nodes on both sides would be different, thus the tree would violate property 5.) All paths through the deleted node will simply pass through one fewer red node, and both the deleted node's parent and child must be black, so property 3 (all leaves are black) and property 4 (both children of every red node are black) still hold.

Another simple case is when '''M''' is black and '''C''' is red. Simply removing a black node could break Properties 4 (“Both children of every red node are black”) and 5 (“All paths from any given node to its leaf nodes contain the same number of black nodes”), but if we repaint '''C''' black, both of these properties are preserved.

The complex case is when both '''M''' and '''C''' are black.  (This can only occur when deleting a black node which has two leaf children, because if the black node '''M''' had a black non-leaf child on one side but just a leaf child on the other side, then the count of black nodes on both sides would be different, thus the tree would have been an invalid red–black tree by violation of property 5.)  We begin by replacing '''M''' with its child '''C'''. We will call (or ''label''—that is, ''relabel'') this child (in its new position) '''N''', and its sibling (its new parent's other child) '''S'''.  ('''S''' was previously the sibling of '''M'''.)
In the diagrams below, we will also use '''P''' for '''N''''s new parent ('''M''''s old parent), '''S&lt;sub&gt;L&lt;/sub&gt;''' for '''S''''s left child, and '''S&lt;sub&gt;R&lt;/sub&gt;''' for '''S''''s right child ('''S''' cannot be a leaf because if '''M''' and '''C''' were black, then '''P''''s one subtree which included '''M''' counted two black-height and thus '''P''''s other subtree which includes '''S''' must also count two black-height, which cannot be the case if '''S''' is a leaf node).

: ''Note'': In between some cases, we exchange the roles and labels of the nodes, but in each case, every label continues to represent the same node it represented at the beginning of the case. Any color shown in the diagram is either assumed in its case or implied by those assumptions.  White represents an unknown color (either red or black).

We will find the sibling using this function:

&lt;source lang=&quot;c&quot;&gt;
struct node *sibling(struct node *n)
{
 if (n == n-&gt;parent-&gt;left)
  return n-&gt;parent-&gt;right;
 else
  return n-&gt;parent-&gt;left;
}
&lt;/source&gt;

: ''Note'': In order that the tree remains well-defined, we need that every null leaf remains a leaf after all transformations (that it will not have any children). If the node we are deleting has a non-leaf (non-null) child '''N''', it is easy to see that the property is satisfied. If, on the other hand, '''N''' would be a null leaf, it can be verified from the diagrams (or code) for all the cases that the property is satisfied as well.

We can perform the steps outlined above with the following code, where the function &lt;code&gt;replace_node&lt;/code&gt; substitutes &lt;code&gt;child&lt;/code&gt; into &lt;code&gt;n&lt;/code&gt;'s place in the tree. For convenience, code in this section will assume that null leaves are represented by actual node objects rather than NULL (the code in the ''Insertion'' section works with either representation).

&lt;source lang=&quot;c&quot;&gt;
void delete_one_child(struct node *n)
{
 /*
  * Precondition: n has at most one non-null child.
  */
 struct node *child = is_leaf(n-&gt;right) ? n-&gt;left : n-&gt;right;

 replace_node(n, child);
 if (n-&gt;color == BLACK) {
  if (child-&gt;color == RED)
   child-&gt;color = BLACK;
  else
   delete_case1(child);
 }
 free(n);
}
&lt;/source&gt;

: ''Note'': If '''N''' is a null leaf and we do not want to represent null leaves as actual node objects, we can modify the algorithm by first calling delete_case1() on its parent (the node that we delete, &lt;code&gt;n&lt;/code&gt; in the code above) and deleting it afterwards. We can do this because the parent is black, so it behaves in the same way as a null leaf (and is sometimes called a 'phantom' leaf). And we can safely delete it at the end as &lt;code&gt;n&lt;/code&gt; will remain a leaf after all operations, as shown above.

If both '''N''' and its original parent are black, then deleting this original parent causes paths which proceed through '''N''' to have one fewer black node than paths that do not.  As this violates property 5 (all paths from any given node to its leaf nodes contain the same number of black nodes), the tree must be rebalanced.  There are several cases to consider:

'''Case 1:''' '''N''' is the new root. In this case, we are done. We removed one black node from every path, and the new root is black, so the properties are preserved.

&lt;source lang=&quot;c&quot;&gt;
void delete_case1(struct node *n)
{
 if (n-&gt;parent != NULL)
  delete_case2(n);
}
&lt;/source&gt;

: ''Note'': In cases 2, 5, and 6, we assume '''N''' is the left child of its parent '''P'''. If it is the right child, ''left'' and ''right'' should be reversed throughout these three cases. Again, the code examples take both cases into account.

{|
|[[Image:Red-black tree delete case 2 as svg.svg|right|337px|Diagram of case 2]]
'''Case 2:''' '''S''' is red. In this case we reverse the colors of '''P''' and '''S''', and then [[tree rotation|rotate]] left at '''P''', turning '''S''' into '''N''''s grandparent. Note that '''P''' has to be black as it had a red child. Although all paths still have the same number of black nodes, now '''N''' has a black sibling and a red parent, so we can proceed to step 4, 5, or 6. (Its new sibling is black because it was once the child of the red '''S'''.)
In later cases, we will relabel '''N''''s new sibling as '''S'''.
|}

&lt;source lang=&quot;c&quot;&gt;
void delete_case2(struct node *n)
{
 struct node *s = sibling(n);

 if (s-&gt;color == RED) {
  n-&gt;parent-&gt;color = RED;
  s-&gt;color = BLACK;
  if (n == n-&gt;parent-&gt;left)
   rotate_left(n-&gt;parent);
  else
   rotate_right(n-&gt;parent);
 }
 delete_case3(n);
}
&lt;/source&gt;

{|
|[[Image:Red-black tree delete case 3 as svg.svg|left|337px|Diagram of case 3]]
'''Case 3:''' '''P''', '''S''', and '''S''''s children are black. In this case, we simply repaint '''S''' red. The result is that all paths passing through '''S''', which are precisely those paths ''not'' passing through '''N''', have one less black node. Because deleting '''N''''s original parent made all paths passing through '''N''' have one less black node, this evens things up.  However, all paths through '''P''' now have one fewer black node than paths that do not pass through '''P''', so property 5 (all paths from any given node to its leaf nodes contain the same number of black nodes) is still violated.  To correct this, we perform the rebalancing procedure on '''P''', starting at case 1.
|}

&lt;source lang=&quot;c&quot;&gt;
void delete_case3(struct node *n)
{
 struct node *s = sibling(n);

 if ((n-&gt;parent-&gt;color == BLACK) &amp;&amp;
     (s-&gt;color == BLACK) &amp;&amp;
     (s-&gt;left-&gt;color == BLACK) &amp;&amp;
     (s-&gt;right-&gt;color == BLACK)) {
  s-&gt;color = RED;
  delete_case1(n-&gt;parent);
 } else
  delete_case4(n);
}
&lt;/source&gt;

{|
|[[Image:Red-black tree delete case 4 as svg.svg|right|337px|Diagram of case 4]]
'''Case 4:''' '''S''' and '''S''''s children are black, but '''P''' is red. In this case, we simply exchange the colors of '''S''' and '''P'''. This does not affect the number of black nodes on paths going through '''S''', but it does add one to the number of black nodes on paths going through '''N''', making up for the deleted black node on those paths.
|}

&lt;source lang=&quot;c&quot;&gt;
void delete_case4(struct node *n)
{
 struct node *s = sibling(n);

 if ((n-&gt;parent-&gt;color == RED) &amp;&amp;
     (s-&gt;color == BLACK) &amp;&amp;
     (s-&gt;left-&gt;color == BLACK) &amp;&amp;
     (s-&gt;right-&gt;color == BLACK)) {
  s-&gt;color = RED;
  n-&gt;parent-&gt;color = BLACK;
 } else
  delete_case5(n);
}
&lt;/source&gt;
{|
|[[Image:Red-black tree delete case 5 as svg.svg|left|243px|Diagram of case 5]]
'''Case 5:''' '''S''' is black, '''S''''s left child is red, '''S''''s right child is black, and '''N''' is the left child of its parent. In this case we rotate right at '''S''', so that '''S''''s left child becomes '''S''''s parent and '''N''''s new sibling. We then exchange the colors of '''S''' and its new parent.
All paths still have the same number of black nodes, but now '''N''' has a black sibling whose right child is red, so we fall into case 6. Neither '''N''' nor its parent are affected by this transformation.
(Again, for case 6, we relabel '''N''''s new sibling as '''S'''.)
|}
&lt;source lang=&quot;c&quot;&gt;
void delete_case5(struct node *n)
{
 struct node *s = sibling(n);

 if  (s-&gt;color == BLACK) { /* this if statement is trivial,
due to case 2 (even though case 2 changed the sibling to a sibling's child,
the sibling's child can't be red, since no red parent can have a red child). */
/* the following statements just force the red to be on the left of the left of the parent,
   or right of the right, so case six will rotate correctly. */
  if ((n == n-&gt;parent-&gt;left) &amp;&amp;
      (s-&gt;right-&gt;color == BLACK) &amp;&amp;
      (s-&gt;left-&gt;color == RED)) { /* this last test is trivial too due to cases 2-4. */
   s-&gt;color = RED;
   s-&gt;left-&gt;color = BLACK;
   rotate_right(s);
  } else if ((n == n-&gt;parent-&gt;right) &amp;&amp;
             (s-&gt;left-&gt;color == BLACK) &amp;&amp;
             (s-&gt;right-&gt;color == RED)) {/* this last test is trivial too due to cases 2-4. */
   s-&gt;color = RED;
   s-&gt;right-&gt;color = BLACK;
   rotate_left(s);
  }
 }
 delete_case6(n);
}
&lt;/source&gt;
{|
|[[Image:Red-black tree delete case 6 as svg.svg|right|337px|Diagram of case 6]]
'''Case 6:''' '''S''' is black, '''S''''s right child is red, and '''N''' is the left child of its parent '''P'''. In this case we rotate left at '''P''', so that '''S''' becomes the parent of '''P''' and '''S''''s right child. We then exchange the colors of '''P''' and '''S''', and make '''S''''s right child black. The subtree still has the same color at its root, so Properties 4 (Both children of every red node are black) and 5 (All paths from any given node to its leaf nodes contain the same number of black nodes) are not violated. However, '''N''' now has one additional black ancestor: either '''P''' has become black, or it was black and '''S''' was added as a black grandparent. Thus, the paths passing through '''N''' pass through one additional black node.

Meanwhile, if a path does not go through '''N''', then there are two possibilities:
* It goes through '''N''''s new sibling. Then, it must go through '''S''' and '''P''', both formerly and currently, as they have only exchanged colors and places. Thus the path contains the same number of black nodes.
* It goes through '''N''''s new uncle, '''S''''s right child. Then, it formerly went through '''S''', '''S''''s parent, and '''S''''s right child (which was red), but now only goes through '''S''', which has assumed the color of its former parent, and '''S''''s right child, which has changed from red to black (assuming '''S''''s color: black). The net effect is that this path goes through the same number of black nodes.

Either way, the number of black nodes on these paths does not change. Thus, we have restored Properties 4 (Both children of every red node are black) and 5 (All paths from any given node to its leaf nodes contain the same number of black nodes). The white node in the diagram can be either red or black, but must refer to the same color both before and after the transformation.
|}
&lt;source lang=&quot;c&quot;&gt;
void delete_case6(struct node *n)
{
 struct node *s = sibling(n);

 s-&gt;color = n-&gt;parent-&gt;color;
 n-&gt;parent-&gt;color = BLACK;

 if (n == n-&gt;parent-&gt;left) {
  s-&gt;right-&gt;color = BLACK;
  rotate_left(n-&gt;parent);
 } else {
  s-&gt;left-&gt;color = BLACK;
  rotate_right(n-&gt;parent);
 }
}
&lt;/source&gt;

Again, the function calls all use [[tail recursion]], so the algorithm is [[in-place algorithm|in-place]].
In the algorithm above, all cases are chained in order, except in delete case 3 where it can recurse to case 1 back to the parent node: this is the only case where an in-place implementation will effectively loop (after only one rotation in case 3).

Additionally, no tail recursion ever occurs on a child node, so the tail recursion loop can only move from a child back to its successive ancestors. No more than O(log ''n'') loops back to case 1 will occur (where ''n'' is the total number of nodes in the tree before deletion). If a rotation occurs in case 2 (which is the only possibility of rotation within the loop of cases 1–3), then the parent of the node '''N''' becomes red after the rotation and we will exit the loop. Therefore at most one rotation will occur within this loop. Since no more than two additional rotations will occur after exiting the loop, at most three rotations occur in total.

== Proof of asymptotic bounds ==
A red black tree which contains ''n'' internal nodes has a height of O(log(n)).

Definitions:

*h(''v'') = height of subtree rooted at node ''v''
*bh(''v'') = the number of black nodes (not counting ''v'' if it is red) from ''v'' to any leaf in the subtree (called the black-height).

'''Lemma:''' A subtree rooted at node ''v'' has at least &lt;math&gt;2^{bh(v)}-1&lt;/math&gt; internal nodes.

Proof of Lemma (by induction height):

Basis: h(''v'') = 0

If ''v'' has a height of zero then it must be ''null'', therefore bh(''v'') = 0.  So:

:&lt;math&gt;
2^{bh(v)}-1 = 2^{0}-1 = 1-1 = 0
&lt;/math&gt;

Inductive Step: ''v'' such that h(''v'') = k, has at least &lt;math&gt;2^{bh(v)}-1&lt;/math&gt; internal nodes implies that &lt;math&gt;v'&lt;/math&gt; such that h(&lt;math&gt;v'&lt;/math&gt;) = k+1 has at least &lt;math&gt;2^{bh(v')}-1&lt;/math&gt; internal nodes.

Since &lt;math&gt;v'&lt;/math&gt; has h(&lt;math&gt;v'&lt;/math&gt;) &gt; 0 it is an internal node.  As such it has two children each of which have a black-height of either bh(&lt;math&gt;v'&lt;/math&gt;) or bh(&lt;math&gt;v'&lt;/math&gt;)-1 (depending on whether the child is red or black, respectively).  By the inductive hypothesis each child has at least  &lt;math&gt;2^{bh(v')-1}-1&lt;/math&gt; internal nodes, so &lt;math&gt;v'&lt;/math&gt; has at least:

:&lt;math&gt;
2^{bh(v')-1}-1 + 2^{bh(v')-1}-1 + 1 = 2^{bh(v')}-1
&lt;/math&gt;

internal nodes.

Using this lemma we can now show that the height of the tree is logarithmic.  Since at least half of the nodes on any path from the root to a leaf are black (property 4 of a red–black tree), the black-height of the root is at least h(root)/2.  By the lemma we get:

:&lt;math&gt;
n \geq 2^{{h(\text{root}) \over 2}} - 1 \leftrightarrow \; \log_2{(n+1)} \geq {h(\text{root}) \over 2} \leftrightarrow \; h(\text{root}) \leq 2\log_2{(n+1)}.
&lt;/math&gt;

Therefore the height of the root is O(log(n)).

=== Insertion complexity ===
In the tree code there is only one loop where the node of the root of the red–black property that we wish to restore, x, can be moved up the tree by one level at each iteration.

Since the original height of the tree is O(log n), there are O(log n) iterations. So overall the insert routine has O(log n) complexity.

== Parallel algorithms ==
Parallel algorithms for constructing red–black trees from sorted lists of items can run in constant time or {{math|O(log log ''n'')}} time, depending on the computer model, if the number of processors available is asymptotically proportional to the number of items.  Fast search, insertion, and deletion parallel algorithms are also known.&lt;ref&gt;
{{cite journal
 | journal = Theoretical computer science
 | title = Parallel algorithms for red–black trees
 | first1 = Heejin |last1=Park |first2=Kunsoo |last2=Park
 | volume = 262
 | issue = 1–2
 | publisher = Elsevier
 | pages = 415–435
 | year = 2001
 | url = http://www.sciencedirect.com/science/article/pii/S0304397500002875
 | doi=10.1016/S0304-3975(00)00287-5
 |quote=Our parallel algorithm for constructing a red–black tree from a sorted list of ''n'' items runs in O(1) time with ''n'' processors on the CRCW PRAM and runs in O(log log ''n'') time with ''n'' / log log ''n'' processors on the EREW PRAM.
 }}&lt;/ref&gt;

== See also ==
*[[Tree data structure]]
*[[Tree rotation]]
*[[Scapegoat tree]]
*[[Splay tree]]
*[[AVL tree]]
*[[B-tree]] ([[2-3 tree]], [[2-3-4 tree]], [[B+ tree]], [[B*-tree]], [[UB-tree]])
*[[T-tree]]
*[[List of data structures]]

== Notes ==
{{reflist}}

== References ==
{{nofootnotes|date=July 2013}}
*[http://mathworld.wolfram.com/Red-BlackTree.html Mathworld: Red–Black Tree]
*[http://www.eli.sdsu.edu/courses/fall95/cs660/notes/RedBlackTree/RedBlack.html#RTFToC2 San Diego State University: CS 660: Red–Black tree notes], by Roger Whitney
* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7 . Chapter 13: Red–Black Trees, pp.&amp;nbsp;273–301.
*{{cite web|last = Pfaff|first = Ben|title = Performance Analysis of BSTs in System Software| publisher = [[Stanford university|Stanford University]]|date=June 2004|url = http://www.stanford.edu/~blp/papers/libavl.pdf|format = PDF}}
*{{cite web|last=Okasaki|first=Chris|title=Red–Black Trees in a Functional Setting|url=http://www.eecs.usma.edu/webs/people/okasaki/jfp99.ps|format=PS}}

== External links ==
*[http://en.literateprograms.org/Red-black_tree_(C) A complete and working implementation in C]
*[http://www.ece.uc.edu/~franco/C321/html/RedBlack/redblack.html Red–Black Tree Demonstration]
*[http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-10-red-black-trees-rotations-insertions-deletions/ OCW MIT Lecture by Prof. Erik Demaine on Red Black Trees] -
*{{YouTube|id=_VbTnLV8plU|title=Binary Search Tree Insertion Visualization}} – Visualization of random and pre-sorted data insertions, in elementary binary search trees, and left-leaning red–black trees
*[https://gist.github.com/pallas/10697727 An intrusive red-black tree written in C++]

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Red-Black Tree}}
[[Category:1972 in computer science]]
[[Category:Articles containing proofs]]
[[Category:Articles with example C code]]
[[Category:Binary trees]]</text>
      <sha1>423q7yr7ipj9dg2d3dykvfwqnvsmwj1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Geometry of binary search trees</title>
    <ns>0</ns>
    <id>39071852</id>
    <revision>
      <id>610271255</id>
      <parentid>607272985</parentid>
      <timestamp>2014-05-26T23:01:09Z</timestamp>
      <contributor>
        <username>Edemaine</username>
        <id>1773</id>
      </contributor>
      <comment>add NP-completeness reference</comment>
      <text xml:space="preserve" bytes="9984">In [[computer science]], one approach to the [[dynamic optimality conjecture]] on [[online algorithm]]s for [[binary search tree]]s involves reformulating the problem geometrically, in terms of augmenting a set of points in the plane with as few additional points as possible in order to avoid rectangles with only two points on their boundary.&lt;ref name=&quot;DHIKP09&quot;&gt;{{Citation 
|title=The geometry of binary search trees 
|first1=Erik D. | last1=Demaine | author1-link = Erik Demaine
|first2=Dion | last2=Harmon  
|first3=John | last3=Iacono
|first4=Daniel | last4=Kane
|first5=Mihai | last5=Pătraşcu | author5-link = Mihai Pătraşcu
|year=2009 
|journal=in Proceedings of the 20th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2009), 
|url=https://www.siam.org/proceedings/soda/2009/SODA09_055_demainee.pdf
|location=New York | pages=496–505}}&lt;/ref&gt;

==Access sequences and competitive ratio==
As typically formulated, the online binary search tree problem involves search trees defined over a fixed key set (1, 2, ..., ''n''). An ''access sequence'' is a sequence ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ... where each number ''x''&lt;sub&gt;''i''&lt;/sub&gt; is one of the given keys.

Any particular algorithm for maintaining binary search trees (such as the [[splay tree]] algorithm or [[Iacono's working set structure]]) has a ''cost'' for each access sequence that models the amount of time it would take to use the structure to search for each of the keys in the access sequence in turn. The cost of a search is modeled by assuming that the search tree algorithm has a single pointer into a binary search tree, which at the start of each search points to the root of the tree. The algorithm may then perform any sequence of the following operations:
* Move the pointer to its left child.
* Move the pointer to its right child.
* Move the pointer to its parent.
* Perform a single [[tree rotation]] on the pointer and its parent.
The search is required, at some point within this sequence of operations to move the pointer to a node containing the key, and the cost of the search is the number of operations that are performed in the sequence. The total cost cost&lt;sub&gt;''A''&lt;/sub&gt;(''X'') for algorithm ''A'' on access sequence ''X'' is the sum of the costs of the searches for each successive key in the sequence.

As is standard in [[Competitive analysis (online algorithm)|competitive analysis]], the [[competitive ratio]] of an algorithm ''A'' is defined to be the maximum, over all access sequences, of the ratio of the cost for ''A'' to the best cost that any algorithm could achieve:
:&lt;math&gt;\rho_A = \sup_X \frac{\mathrm{cost}_A(X)}{\mathrm{cost}_{\mathrm{opt}}(X)}.&lt;/math&gt;

The [[dynamic optimality conjecture]] states that [[splay tree]]s have constant competitive ratio, but this remains unproven. The geometric view of binary search trees provides a different way of understanding the problem that has led to the development of alternative algorithms that could also (conjecturally) have a constant competitive ratio.

==Translation to a geometric point set==
In the geometric view of the online binary search tree problem,
an ''access sequence'' &lt;big&gt;x&lt;sub&gt;1&lt;/sub&gt;, . . ., x&lt;sub&gt;m&lt;/sub&gt;&lt;/big&gt; (sequence of searches performed on a binary search tree (BST) with a key set &lt;big&gt;{1,2,...,n}&lt;/big&gt;) is mapped to the set of points &lt;big&gt;{(x&lt;sub&gt;i&lt;/sub&gt;, i)}&lt;/big&gt;, where X-axis represents key space and Y-axis represents time; to which a set of '''touched''' nodes is added.  By '''touched''' nodes we mean the following. Consider a BST access algorithm with a single pointer to a node in the tree. At the beginning of an access to a given key &lt;big&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/big&gt;, this pointer is initialized to the root of the tree. Whenever the pointer moves to or is initialized to a node, we say that the node is touched.&lt;ref&gt;{{Citation 
|title=Dynamic optimality—almost
|first1=Erik D. | last1=Demaine   | author1-link = Erik Demaine
|first2=Dion | last2=Harmon  
|first3=John | last3=Iacono
|first4=Mihai | last4=Pătraşcu | author4-link = Mihai Pătraşcu
|year=2007 | journal=[[SIAM Journal on Computing]]
|volume=37|number=1
|doi = 10.1137/S0097539705447347 | mr = 2306291 | url = http://erikdemaine.org/papers/Tango_SICOMP/
|pages=240–251}}&lt;/ref&gt;
We represent a BST algorithm for a given input sequence by drawing a point for each item that gets touched.

For example, assume the following BST on 4 nodes is given: [[File:StaticBinarySearchTree GeometricalView example.jpg|130px]]
The key set is {1, 2, 3, 4}.

{{double image|right|Geometrical view of binary search trees - access sequence only.jpg|320|Geometrical view of binary search trees - access sequence.jpg|320|Mapping of the access sequence 3, 1, 4, 2 only.|A geometric view of binary search tree algorithm.}}

Let 3, 1, 4, 2 be the access sequence. 
* In the first access, only the node 3 is touched. 
* In the second access, the nodes 3 and 1 are touched. 
* In the third access - 3 and 4 are touched. 
* In the fourth access, touch 3, then 1, and after that 2. 

The touches are represented geometrically: If an item ''x'' is touched in the operations for the ''i''th access, then a point (''x'',''i'') is plotted.

{{clear}}

== Arborally satisfied point sets ==

[[File:Rectangle spanned by two points example.jpg|thumb|right|Rectangle spanned by two points. This point set is NOT arborally satisfied.]]

[[File:Example of arborally satisfied set of points.jpg|thumb|right|This is an example of arborally satisfied set of points.]]

A point set is said to be '''arborally satisfied''' if the following property holds: for any
pair of points that do not both lie on the same horizontal or vertical line, there exists a third point
which lies in the rectangle spanned by the first two points (either inside or on the boundary).

=== Theorem ===
A point set containing the points &lt;big&gt;(x&lt;sub&gt;i&lt;/sub&gt;, i)&lt;/big&gt; is arborally satisfied if and only if it corresponds to a valid BST for the input sequence &lt;big&gt;x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, . . ., x&lt;sub&gt;m&lt;/sub&gt;&lt;/big&gt;.

==== Proof ====
First, prove that the point set for any valid BST algorithm is arborally satisfied.
Consider points &lt;big&gt;(x, i)&lt;/big&gt; and &lt;big&gt;(y, j)&lt;/big&gt;, where &lt;big&gt;x&lt;/big&gt; is touched at time &lt;big&gt;i&lt;/big&gt; and &lt;big&gt;y&lt;/big&gt; is touched at time &lt;big&gt;j&lt;/big&gt;. Assume by symmetry that &lt;big&gt;x &lt; y&lt;/big&gt; and &lt;big&gt;i &lt; j&lt;/big&gt;. It needs to be shown that there exists a third point in the rectangle
with corners as &lt;big&gt;(x, i)&lt;/big&gt; and &lt;big&gt;(y, j)&lt;/big&gt;. Also let &lt;big&gt;LCA&lt;sub&gt;t&lt;/sub&gt;(a, b)&lt;/big&gt; denote the [[lowest common ancestor]] of nodes &lt;big&gt;a&lt;/big&gt;
and &lt;big&gt;b&lt;/big&gt; right before time &lt;big&gt;t&lt;/big&gt;. There are a few cases:
* If &lt;big&gt;LCA&lt;sub&gt;i&lt;/sub&gt;(x, y) ≠ x&lt;/big&gt;, then use the point &lt;big&gt;(LCA&lt;sub&gt;i&lt;/sub&gt;(x, y), i)&lt;/big&gt;, since &lt;big&gt;LCA&lt;sub&gt;i&lt;/sub&gt;(x, y)&lt;/big&gt; must have been touched if &lt;big&gt;x&lt;/big&gt; was.
* If &lt;big&gt;LCA&lt;sub&gt;j&lt;/sub&gt;(x, y) ≠ y&lt;/big&gt;, then the point &lt;big&gt;(LCA&lt;sub&gt;j&lt;/sub&gt;(x, y), j)&lt;/big&gt; can be used.
* If neither of the above two cases hold, then &lt;big&gt;x&lt;/big&gt; must be an ancestor of &lt;big&gt;y&lt;/big&gt; right before time &lt;big&gt;i&lt;/big&gt; and &lt;big&gt;y&lt;/big&gt; be an ancestor of &lt;big&gt;x&lt;/big&gt; right before time &lt;big&gt;j&lt;/big&gt;. Then at some time &lt;big&gt;k&lt;/big&gt; &lt;big&gt;(i ≤ k &lt; j)&lt;/big&gt;, &lt;big&gt;y&lt;/big&gt; must have been rotated above &lt;big&gt;x&lt;/big&gt;, so the point &lt;big&gt;(y, k)&lt;/big&gt; can be used.

Next, show the other direction: given an arborally satisfied point set, a valid BST corresponding to that point set can be constructed. Organize our BST into a treap which is organized in heap-order by next-touch-time. Note that next-touch-time has ties and is thus not uniquely defined, but this isn’t a problem as long as there is a way to break ties. When time &lt;big&gt;i&lt;/big&gt; reached, the nodes touched form a connected subtree at the top, by the heap ordering property. Now, assign new next-touch-times for this subtree, and rearrange it into a new local treap.
If a pair of nodes, &lt;big&gt;x&lt;/big&gt; and &lt;big&gt;y&lt;/big&gt;, straddle the boundary between the touched and untouched part
of the treap, then if &lt;big&gt;y&lt;/big&gt; is to be touched sooner than &lt;big&gt;x&lt;/big&gt; then &lt;big&gt;(x, now) → (y, next − touch(y))&lt;/big&gt; is an unsatisfied rectangle because the leftmost such point would be the right child of &lt;big&gt;x&lt;/big&gt;, not &lt;big&gt;y&lt;/big&gt;. 

{{clear}}

=== Corollary ===
Finding the best BST execution for the input sequence &lt;big&gt;x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, . . ., x&lt;sub&gt;m&lt;/sub&gt;&lt;/big&gt; is equivalent to finding the minimum cardinality superset of points (that contains the input in geometric representation) that is arborally satisfied. The more general problem of finding the minimum cardinality arborally satisfied superset of a general set of input points (not limited to one input point per &lt;big&gt;y&lt;/big&gt; coordinate), is known to be [[NP-complete]].&lt;ref name=&quot;DHIKP09&quot;/&gt;

== Greedy algorithm ==
The following greedy algorithm constructs arborally satisfiable sets:
* Sweep the point set with a horizontal line by increasing &lt;big&gt;y&lt;/big&gt; coordinate. 
* At time &lt;big&gt;i&lt;/big&gt;, place the minimal number of points at &lt;big&gt;y = i&lt;/big&gt; to make the point set up to &lt;big&gt;y ≤ i&lt;/big&gt; arborally satisfied. This minimal set of points is uniquely defined: for any unsatisfied rectangle formed with&lt;big&gt;(x&lt;sub&gt;i&lt;/sub&gt;, i)&lt;/big&gt; in one corner, add the other corner at &lt;big&gt;y = i&lt;/big&gt;.

The algorithm has been conjectured to be optimal.&lt;ref&gt;{{Citation 
|title=Upper bounds for maximally greedy binary search trees
|first1=Kyle| last1=Fox
|year=2011 
|booktitle=Algorithms and Data Structures: 12th International Symposium, WADS 2011, New York, NY, USA, August 15–17, 2011, Proceedings
|publisher=Springer|series=Lecture Notes in Computer Science|volume=6844|doi=10.1007/978-3-642-22300-6_35
|url=http://web.engr.illinois.edu/~kylefox2/publications/greedyfuture.pdf| pages=411–422}}&lt;/ref&gt;

==See also==
*[[Binary search algorithm]]
*[[Tango tree]]s
*[[Splay tree]]s
*[[Self-balancing binary search tree]]

==References==
{{Reflist}}

[[Category:Binary trees]]
[[Category:Geometry]]</text>
      <sha1>serqvszt2m7nftz6l7o2u22snzf9i2c</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Optimal binary search tree</title>
    <ns>0</ns>
    <id>42382810</id>
    <revision>
      <id>617037922</id>
      <parentid>611636656</parentid>
      <timestamp>2014-07-15T12:07:00Z</timestamp>
      <contributor>
        <ip>214.16.248.2</ip>
      </contributor>
      <comment>Undid revision 611636656 by [[Special:Contributions/119.82.76.227|119.82.76.227]] ([[User talk:119.82.76.227|talk]])</comment>
      <text xml:space="preserve" bytes="11192">{{Multiple issues|{{refimprove|date=April 2014}}{{technical|date=April 2014}}}}

In [[computer science]],  an '''optimal binary search tree (BST)''' is a [[binary search tree]] which provides the smallest possible search time (or [[Expected value|expected search time]]) for a given sequence of accesses (or access probabilities). Optimal BSTs are generally divided into two types: static and dynamic.

In the '''static optimality''' problem, the tree cannot be modified after it has been constructed. In this case, there exists some particular layout of the nodes of the tree which provides the smallest expected search time for the given access probabilities. Various algorithms exist to construct or approximate the statically optimal tree given the information on the access probabilities of the elements.

In the '''dynamic optimality''' problem, the tree can be modified at any time, typically by permitting [[tree rotation]]s. The tree is considered to have a cursor starting at the root which it can move or use to perform modifications. In this case, there exists some minimal-cost sequence of these operations which causes the cursor to visit every node in the target access sequence in order. The [[splay tree]] is conjectured to have a constant [[competitive ratio]] compared to the dynamically optimal tree in all cases, though this has not yet been proven.

== Static Optimality ==

=== Definition ===

In the static optimality problem as defined by [[Donald E. Knuth|Knuth]],&lt;ref name=&quot;Knuth1971&quot;&gt;{{Citation |first1=Donald E. |last1=Knuth |title=Optimum binary search trees |url=http://dx.doi.org/10.1007/BF00264289 |journal=Acta Informatica |volume=1 |issue=1 |pages=14–25 |year=1971 |doi=10.1007/BF00264289 }}&lt;/ref&gt; we are given a set of n ordered elements and a set of 2n+1 probabilities. We will denote the elements a&lt;sub&gt;1&lt;/sub&gt; through a&lt;sub&gt;n&lt;/sub&gt; and the probabilities A&lt;sub&gt;1&lt;/sub&gt; through A&lt;sub&gt;n&lt;/sub&gt; and B&lt;sub&gt;0&lt;/sub&gt; through B&lt;sub&gt;n&lt;/sub&gt;. A&lt;sub&gt;i&lt;/sub&gt; is the probability of a search being done for element a&lt;sub&gt;i&lt;/sub&gt;, while B&lt;sub&gt;i&lt;/sub&gt; is the probability of a search being done for an element between a&lt;sub&gt;i&lt;/sub&gt; and a&lt;sub&gt;i+1&lt;/sub&gt;. It follows that B&lt;sub&gt;0&lt;/sub&gt; is the probability of a search being done for an element strictly less than a&lt;sub&gt;0&lt;/sub&gt;, and B&lt;sub&gt;n&lt;/sub&gt; is the probability of a search being done for an element strictly greater than a&lt;sub&gt;n&lt;/sub&gt;. These 2n+1 probabilites cover all possible searches, and therefore add up to one.

There are &lt;math&gt;{2n \choose n}\frac{1}{n+1}&lt;/math&gt; possible different binary search trees on a set of n elements.&lt;ref name=&quot;Knuth1971&quot; /&gt; The static optimality problem is therefore an [[optimization problem]] to determine which of these trees provides the minimal average search time given the 2n+1 probabilities as inputs. As the number of possible trees is exponential in the number of elements, [[brute-force search]] is not usually a feasible solution.

=== Knuth's Dynamic Programming Algorithm ===

In 1971, Knuth published a relatively straightforward [[dynamic programming]] algorithm capable of constructing the statically optimal tree in only O(n&lt;sup&gt;2&lt;/sup&gt;) time.&lt;ref name=&quot;Knuth1971&quot; /&gt; Knuth's primary insight was that the static optimality problem exhibits [[optimal substructure]]; that is, if a certain tree is statically optimal for a given probability distribution, then its left and right subtrees must also be statically optimal for their appropriate subsets of the distribution.

To see this, consider what Knuth calls the &quot;weighted path length&quot; of a tree. The weighted path length of a tree on n elements is the sum of the lengths of all 2n+1 possible search paths, weighted by their respective probabilities. The tree with the minimal weighted path length is, by definition, statically optimal.

But weighted path lengths have an interesting property. Let P be the weighted path length of a binary tree, P&lt;sub&gt;L&lt;/sub&gt; be the weighted path length of its left subtree, and P&lt;sub&gt;R&lt;/sub&gt; be the weighted path length of its right subtree. Also let W be the sum of all the probabilities in the tree. Observe that when either subtree is attached to the root, the depth of each of its elements (and thus each of its search paths) is increased by one. Also observe that the root itself has a depth of one. This means that the difference in weighted path length between a tree and its two subtrees is exactly the sum of every single probability in the tree, leading to the following recurrence:

&lt;math&gt;P = P_L + P_R + W&lt;/math&gt;

This recurrence leads to a natural dynamic programming solution. Let &lt;math&gt;P_{ij}&lt;/math&gt; be the weighted path length of the statically optimal search tree for all values between a&lt;sub&gt;i&lt;/sub&gt; and a&lt;sub&gt;j+1&lt;/sub&gt;, let &lt;math&gt;W_{ij}&lt;/math&gt; be the total weight of that tree, and let &lt;math&gt;R_{ij}&lt;/math&gt; be the index of its root. The algorithm can be built using the following formulas:

:&lt;math&gt;\begin{align}
P_{ii} = W_{ii} &amp;= B_i \operatorname{for} 0 \leq i \leq n \\
W_{ij} &amp;= W_{i,j-1} + A_j + B_j \\
P_{i, R_{i,j-1}} + P_{R_{ij},j} &amp;= \min_{i&lt;k\leq j}(P_{i,k-1} + P_{kj}) = P_{ij} - W_{ij} \operatorname{for} 0 \leq i &lt; j \leq n
\end{align}&lt;/math&gt;

The naive implementation of this algorithm actually takes O(n&lt;sup&gt;3&lt;/sup&gt;) time, but Knuth's paper includes some additional observations which can be used to produce a modified algorithm taking only O(n&lt;sup&gt;2&lt;/sup&gt;) time.

=== Mehlhorn's Approximation Algorithm ===

While the O(n&lt;sup&gt;2&lt;/sup&gt;) time taken by Knuth's algorithm is substantially better than the exponential time required for a brute-force search, it is still too slow to be practical when the number of elements in the tree is very large.

In 1975, [[Kurt Mehlhorn]] published a paper proving that a much simpler algorithm could be used to closely approximate the statically optimal tree in only O(n) time.&lt;ref name=&quot;Mehlhorm1975&quot;&gt;{{Citation |first1=Kurt |last1=Mehlhorn |title=Nearly optimal binary search trees |url=http://dx.doi.org/10.1007/BF00264563 |journal=Acta Informatica |volume=5 |issue=4 |pages=287–295 |year=1975 |doi=10.1007/BF00264563 }}&lt;/ref&gt; In this algorithm, the root of the tree is chosen so as to most closely balance the total weight (by probability) of the left and right subtrees. This strategy is then applied recursively on each subtree.

That this strategy produces a good approximation can be seen intuitively by noting that the weights of the subtrees along any path form something very close to a geometrically decreasing sequence. In fact, this strategy generates a tree whose weighted path length is at most

&lt;math&gt;2+(1 - \log(\sqrt{5} - 1))^{-1}H&lt;/math&gt;

where H is the [[entropy (information theory)|entropy]] of the probability distribution. Since no optimal binary search tree can ever do better than a weighted path length of

&lt;math&gt;(1/\log3)H&lt;/math&gt;

this approximation is very close.&lt;ref name=&quot;Mehlhorm1975&quot; /&gt;

== Dynamic Optimality ==

=== Definition ===

There are several different definitions of dynamic optimality, all of which are effectively equivalent to within a constant factor in terms of running-time.&lt;ref name=&quot;Demaine2004&quot;&gt;{{Citation |first1=Erik D. |last1=Demaine |first2=Dion |last2=Harmon |first3=John |last3=Iacono |first4=Mihai |last4=Patrascu |title=Dynamic optimality - almost |url=http://dx.doi.org/10.1109/FOCS.2004.23 |booktitle=Proceedings of the 45th Annual IEEE Symposium on Foundations of Computer Science |isbn=0-7695-2228-9 |pages=484–490 |year=2004 |doi=10.1109/FOCS.2004.23 }}&lt;/ref&gt; The problem was first introduced implicitly by [[Daniel Sleator|Sleator]] and [[Robert Tarjan|Tarjan]] in their paper on [[splay tree]]s,&lt;ref name=&quot;SplayTrees&quot;&gt;{{Citation |first1=Daniel |last1=Sleator |first2=Robert |last2=Tarjan |title=Self-adjusting binary search trees |url=http://doi.acm.org/10.1145/3828.3835 |journal=Journal of the ACM |volume=32 |issue=3 |pages=652–686 |year=1985 |doi=10.1145/3828.3835 }}&lt;/ref&gt; but [[Erik Demaine|Demaine]] et al. give a very good formal statement of it.&lt;ref name=&quot;Demaine2004&quot; /&gt;

In the dynamic optimality problem, we are given a sequence of accesses x&lt;sub&gt;1&lt;/sub&gt;, ..., x&lt;sub&gt;m&lt;/sub&gt; on the keys 1, ..., n. For each access, we are given a [[Pointer (computer programming)|pointer]] to the root of our BST and can use the pointer to perform any of the following operations:

# Move the pointer to the left child of the current node.
# Move the pointer to the right child of the current node.
# Move the pointer to the parent of the current node.
# Perform a single [[tree rotation|rotation]] on the current node and its parent.

Our BST algorithm can perform any sequence of the above operations as long as the pointer eventually ends up on the node containing the target value x&lt;sub&gt;i&lt;/sub&gt;. The time it takes a given dynamic BST algorithm to perform a sequence of accesses is equivalent to the total number of such operations performed during that sequence. Given any sequence of accesses on any set of elements, there is some BST algorithm which performs all accesses using the fewest total operations.

This model defines the fastest possible tree for a given sequence of accesses, but calculating the optimal tree in this sense therefore requires foreknowledge of exactly what the access sequence will be. If we let OPT(X) be the number of operations performed by the strictly optimal tree for an access sequence X, we can say that a tree is dynamically optimal as long as, for any X, it performs X in time O(OPT(X)) (that is, it has a constant [[competitive ratio]]).&lt;ref name=&quot;Demaine2004&quot;/&gt;

There are several data structures conjectured to have this property, but none proven. It is an [[open problem]] whether there exists a dynamically optimal data structure in this model.

=== Splay Trees ===

{{Main|Splay tree}}

The [[splay tree]] is a data structure invented in 1985 by Daniel Sleator and Robert Tarjan which is conjectured to be dynamically optimal in the required sense. That is, a splay tree is believed to perform any sufficiently long access sequence X in time O(OPT(X)).&lt;ref name=&quot;SplayTrees&quot;/&gt;

=== Tango Trees ===

{{Main|Tango tree}}

The [[tango tree]] is a data structure proposed in 2004 by Demaine et al. which has been proven to perform any sufficiently-long access sequence X in time &lt;math&gt;O(\log\log n \operatorname{OPT}(X))&lt;/math&gt;. While this is not dynamically optimal, the competitive ratio of &lt;math&gt;\log\log n&lt;/math&gt; is still very small for reasonable values of n.&lt;ref name=&quot;Demaine2004&quot;/&gt;

=== Other Results ===

In 2013, [[John Iacono]] published a paper which uses the [[geometry of binary search trees]] to provide an algorithm which is dynamically optimal if any binary search tree algorithm is dynamically optimal.&lt;ref name=&quot;Iacono2013&quot;&gt;{{Citation |first1=John |last1=Iacono |title=In pursuit of the dynamic optimality conjecture |url=http://arxiv.org/abs/1306.0207 |journal=CoRR |volume=abs/1306.0207 |year=2013 }}&lt;/ref&gt;

==See also==
* [[tree data structure|Trees]]
* [[Splay tree]]
* [[Tango tree]]
* [[Geometry of binary search trees]]
* [[List of data structures]]

==Notes==
{{reflist}}

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Optimal Binary Search Trees}}
[[Category:Binary trees]]</text>
      <sha1>ilqd1tjmbual3n1btf4t62g9dhgjnhf</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Random tree</title>
    <ns>0</ns>
    <id>22192834</id>
    <revision>
      <id>616736358</id>
      <parentid>616736127</parentid>
      <timestamp>2014-07-13T03:23:28Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>/* See also */ *[[Brownian tree]]</comment>
      <text xml:space="preserve" bytes="1655">{{Probabilistic}}
In [[mathematics]] and [[computer science]], a '''random tree''' is a [[tree (graph theory)|tree]] or [[Arborescence (graph theory)|arborescence]] that is formed by a [[stochastic process]]. Types of random trees include:
*[[Uniform spanning tree]], a spanning tree of a given graph in which each different tree is equally likely to be selected
*[[Random minimal spanning tree]], spanning trees of a graph formed by choosing random edge weights and using the minimum spanning tree for those weights
*[[Random binary tree]], binary trees with a given number of nodes, formed by inserting the nodes in a random order or by selecting all possible trees uniformly at random

*[[Recursive_tree| Random recursive tree]], increasingly labelled trees, which can be generated using a simple stochastic growth rule. 

*[[Treap]] or randomized binary search tree, a data structure that uses random choices to simulate a random binary tree for non-random update sequences
*[[Rapidly exploring random tree]], a fractal space-filling pattern used as a data structure for searching high-dimensional spaces
*[[Brownian tree]], a fractal tree structure created by diffusion-limited aggregation processes
*[[Random forest]], a machine-learning classifier based on choosing random subsets of variables for each tree and using the most frequent tree output as the overall classification
*[[Branching process]], a model of a population in which each individual has a random number of children

==See also==
*[[Brownian tree]]
*[[Lightning tree]]

{{sia}}
[[Category:Trees (graph theory)]]
[[Category:Probabilistic data structures]]
[[Category:Random graphs]]</text>
      <sha1>pq6byo1qfylpfgfczufqf8m3lj82b10</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Rapidly exploring random tree</title>
    <ns>0</ns>
    <id>14105159</id>
    <revision>
      <id>623663514</id>
      <parentid>613161274</parentid>
      <timestamp>2014-09-01T04:09:52Z</timestamp>
      <contributor>
        <ip>75.146.79.150</ip>
      </contributor>
      <text xml:space="preserve" bytes="6041">{{Probabilistic}}
{{multiple image
| direction = vertical
| width     = 300
| image1    = RRT graph1.png
| caption1  = A visualization of a RRT graph after 45 and 390 iterations
| image2    = Rapidly-exploring Random Tree (RRT) 500x373.gif
| caption2  = An animation of a RRT starting from iteration 0 to 10000
}}

A '''rapidly exploring random tree''' (RRT) is an [[algorithm]] designed to efficiently search [[convex space|nonconvex]], high-dimensional spaces by randomly building a [[space-filling tree]]. The tree is constructed incrementally from samples drawn randomly from the search space and is inherently biased to grow towards large unsearched areas of the problem. RRTs were developed by [[Steven M. LaValle]] and [[James J. Kuffner Jr.]]
&lt;ref name=lavalle_tr98&gt;
{{cite journal|last=LaValle|first=Steven M.|author-link=Steven M. LaValle|title=Rapidly-exploring random trees: A new tool for path planning|journal=Technical Report|date=October 1998|issue=TR 98-11|location=Computer Science Deptartment, Iowa State University|url=http://msl.cs.uiuc.edu/~lavalle/papers/Lav98c.pdf}}
&lt;/ref&gt;
.&lt;ref name=lavalle_ijrr01&gt;
{{cite journal|last1=LaValle|first1=Steven M.|author1-link=Steven M. LaValle|last2=Kuffner Jr.|first2 = James J.|title=Randomized Kinodynamic Planning|journal=The International Journal of Robotics Research (IJRR)|year=2001|volume=20|issue=5|doi=10.1177/02783640122067453|url=http://msl.cs.uiuc.edu/~lavalle/papers/LavKuf01b.pdf}}
&lt;/ref&gt;
They easily handle problems with obstacles and differential constraints ([[Degrees of freedom (engineering)|nonholonomic]] and kinodynamic) and have been widely used in [[autonomous]] [[robotics|robotic]] [[path planning]].

RRTs can be viewed as a technique to generate open-loop trajectories for nonlinear systems with state constraints. An RRT can also be considered as a [[Monte Carlo method|Monte-Carlo]] method to bias search into the largest [[Voronoi diagram|Voronoi regions]] of a graph in a configuration space. Some variations can even be considered [[stochastic]] [[fractal]]s.&lt;ref&gt;http://msl.cs.uiuc.edu/rrt/about.html About RRTs, by Steve LaValle&lt;/ref&gt;

==Description==
An RRT grows a tree rooted at the starting configuration by using random samples from the search space. 
As each sample is drawn, a connection is attempted between it and the nearest state in the tree.
If the connection is feasible (passes entirely through free space and obeys any constraints), this results in the addition of the new state to the tree.
With uniform sampling of the search space, the probability of expanding an existing state is proportional to the size of its [[Voronoi diagram|Voronoi region]].
As the largest [[Voronoi diagram|Voronoi regions]] belong to the states on the frontier of the search, this means that the tree preferentially expands towards large unsearched areas.

The length of the connection between the tree and a new state is frequently limited by a growth factor.
If the random sample is further from its nearest state in the tree than this limit allows, a new state at the maximum distance from the tree along the line to the random sample is used instead of the random sample itself.
The random samples can then be viewed as controlling the direction of the tree growth while the growth factor determines its rate.
This maintains the space-filling bias of the RRT while limiting the size of the incremental growth.

RRT growth can be biased by increasing the probability of sampling states from a specific area.
Most practical implementations of RRTs make use of this to guide the search towards the planning problem goals.
This is accomplished by introducing a small probability of sampling the goal to the state sampling procedure.
The higher this probability, the more greedily the tree grows towards the goal.

==Algorithm==

For a general [[configuration space]] ''C'', the algorithm in [[pseudocode]] is as follows:

{{algorithm-begin|name=BuildRRT|test}}
   Input: Initial configuration ''q''&lt;sub&gt;''init''&lt;/sub&gt;, number of vertices in RRT ''K'', incremental distance ''Δq'')
   Output: RRT graph ''G''
 
   ''G''.init(''q''&lt;sub&gt;''init''&lt;/sub&gt;)
   '''for''' ''k'' = 1 '''to''' ''K''
     ''q''&lt;sub&gt;''rand''&lt;/sub&gt; ← RAND_CONF()
     ''q''&lt;sub&gt;''near''&lt;/sub&gt; ← NEAREST_VERTEX(''q''&lt;sub&gt;''rand''&lt;/sub&gt;, ''G'')
     ''q''&lt;sub&gt;''new''&lt;/sub&gt; ← NEW_CONF(''q''&lt;sub&gt;''near''&lt;/sub&gt;, ''q''&lt;sub&gt;''rand''&lt;/sub&gt;, ''Δq'')
     ''G''.add_vertex(''q''&lt;sub&gt;''new''&lt;/sub&gt;)
     ''G''.add_edge(''q''&lt;sub&gt;''near''&lt;/sub&gt;, ''q''&lt;sub&gt;''new''&lt;/sub&gt;)
   '''return''' ''G''
{{algorithm-end}}

In the algorithm above, &quot;'''RAND_CONF'''&quot; grabs a random configuration ''q''&lt;sub&gt;''rand''&lt;/sub&gt; in ''C''. This may be replaced with a function &quot;'''RAND_FREE_CONF'''&quot; that uses samples in ''C''&lt;sub&gt;''free''&lt;/sub&gt;, while rejecting those in ''C''&lt;sub&gt;''obs''&lt;/sub&gt; using some collision detection algorithm.

&quot;'''NEAREST_VERTEX'''&quot; is a straightforward function that runs through all vertexes ''v'' in graph ''G'', calculates the distance between ''q''&lt;sub&gt;''rand''&lt;/sub&gt; and ''v'' using some measurement function thereby returning the nearest vertex.

&quot;'''NEW_CONF'''&quot; selects a new configuration ''q''&lt;sub&gt;''new''&lt;/sub&gt; by moving an incremental distance ''Δq'' from ''q''&lt;sub&gt;''near''&lt;/sub&gt; in the direction of ''q''&lt;sub&gt;''rand''&lt;/sub&gt;. (According to &lt;ref&gt;Rapidly-Exploring Random Trees: Progress and Prospects (2000), by Steven M. Lavalle ,  James J. Kuffner ,  Jr. Algorithmic and Computational Robotics: New Directions, http://eprints.kfupm.edu.sa/60786/1/60786.pdf&lt;/ref&gt; in holonomic problems, this should be omitted and ''q''&lt;sub&gt;''rand''&lt;/sub&gt; used instead of ''q''&lt;sub&gt;''new''&lt;/sub&gt;.)

==See also {{anchor|see also}}==

:*[[Probabilistic roadmap]]
:*[[Space-filling tree]]
:*[[Motion planning]]
:*[[Randomized algorithm]]

==References==
{{reflist}}

==External links==
* [http://correll.cs.colorado.edu/?p=1623 Java visualizer of RRT and RRT* including map editor]

[[Category:Search algorithms]]
[[Category:Robot control]]
[[Category:Probabilistic data structures]]</text>
      <sha1>r1qrxe15u1gdtdcvkanxq7ro7q8hi8v</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
</mediawiki>
